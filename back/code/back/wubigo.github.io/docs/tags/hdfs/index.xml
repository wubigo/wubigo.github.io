<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HDFS on wubigo</title>
    <link>https://wubigo.com/tags/hdfs/</link>
    <description>Recent content in HDFS on wubigo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Oct 2021 09:55:08 +0800</lastBuildDate>
    
	<atom:link href="https://wubigo.com/tags/hdfs/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Maximum Number Files Hdfs</title>
      <link>https://wubigo.com/post/maximum-number-files-hdfs/</link>
      <pubDate>Sat, 09 Oct 2021 09:55:08 +0800</pubDate>
      
      <guid>https://wubigo.com/post/maximum-number-files-hdfs/</guid>
      <description>HDFS The maximum number of files in HDFS depends on two things:
 total storage space in the cluster
 the heap size of the NameNode.
  1) You can find out what percentage of storage has been used from HDFS NameNode UI.
2) The basic rule of thumb is that 1 GB heap is needed for every million of files.
Each file object and each block object takes about 150 bytes of the memory.</description>
    </item>
    
  </channel>
</rss>