<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on wubigo</title>
    <link>https://wubigo.com/tags/llm/</link>
    <description>Recent content in LLM on wubigo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Apr 2023 11:00:40 +0800</lastBuildDate>
    
	<atom:link href="https://wubigo.com/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>LLM Notes</title>
      <link>https://wubigo.com/post/llm-notes/</link>
      <pubDate>Tue, 18 Apr 2023 11:00:40 +0800</pubDate>
      
      <guid>https://wubigo.com/post/llm-notes/</guid>
      <description>分词器(Tokenizer) tokenization算法大致经历了从word/char到subword的进化.
目前有三种主流的Subword分词算法，分别是Byte Pair Encoding (BPE), WordPiece和Unigram Language Model
Back in the ancient times, before 2013, we usually encoded basic unigram tokens using simple 1’s and 0’s in a process called One-Hot encoding. word2vec improved things by expanding these 1’s and 0’s into full vectors (aka word embeddings). BERT improved things further by using transformers and self-attention heads to create full contextual sentence embeddings.
传统的词编码：one-hot 分布式词编码：word embedding  word2vec  CBOW模型是在已知当前词上下文context的前提下预测当前词w(t)，类似阅读理解中的完形填空； 而Skip-Gram模型恰恰相反，是在已知当前词w(t)的前提下，预测上下文context。</description>
    </item>
    
  </channel>
</rss>