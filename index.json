[{"authors":["admin"],"categories":null,"content":" 超过10年的公有云/混合云/PaaS/SaaS/系统开发运营经验，实施过建筑，金融，医疗，教育, 媒体等行业AT2C案例 管理超过60PB的数据中心，并提供基于hadoop生态的数仓一体化平台 精通软件系统架构(ToGAF, CS/BS, DDA, SOA, EDA, 微服务，函数计算) 构建轻量级，高性能的分布式应用架构和PaaS平台，支持云端边部署和运营  ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://wubigo.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":" 超过10年的公有云/混合云/PaaS/SaaS/系统开发运营经验，实施过建筑，金融，医疗，教育, 媒体等行业AT2C案例 管理超过60PB的数据中心，并提供基于hadoop生态的数仓一体化平台 精通软件系统架构(ToGAF, CS/BS, DDA, SOA, EDA, 微服务，函数计算) 构建轻量级，高性能的分布式应用架构和PaaS平台，支持云端边部署和运营  ","tags":null,"title":"Wu Bigo","type":"author"},{"authors":null,"categories":[],"content":"","date":1658498752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658498752,"objectID":"d0b024f32136d409bda00d00ce140459","permalink":"https://wubigo.com/post/kvm-over-ip/","publishdate":"2022-07-22T22:05:52+08:00","relpermalink":"/post/kvm-over-ip/","section":"post","summary":"","tags":[],"title":"KVM(Keyboard, Video, and Mouse) Over IP","type":"post"},{"authors":null,"categories":[],"content":" START PG version: '2' services: postgresql: container_name: pg image: postgres:12 ports: - '5432:5432' volumes: - 'postgresql_data:/var/lib/postgresql/data' environment: - 'POSTGRES_PASSWORD=!Qsx4rgb' - 'PGDATA=/var/lib/postgresql/data/pgdata' # adminer: # image: adminer # restart: always # ports: # - 8080:8080 volumes: postgresql_data: driver: local  安装pgcli sudo apt install libpq-dev pip install -U pgcli export PATH=$PATH:/home/ubuntu/.local/bin pgcli -U postgres -h localhost  更改配置 postgres@localhost\u0026gt; alter system set autovacuum = off; postgres@localhost\u0026gt; SELECT pg_reload_conf(); postgres@localhost\u0026gt; show autovacuum;  ","date":1656722634,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656722634,"objectID":"852c8979baced0d5602dd1df80a9c235","permalink":"https://wubigo.com/post/postgres-and-pgcli/","publishdate":"2022-07-02T08:43:54+08:00","relpermalink":"/post/postgres-and-pgcli/","section":"post","summary":" START PG version: '2' services: postgresql: container_name: pg image: postgres:12 ports: - '5432:5432' volumes: - 'postgresql_data:/var/lib/postgresql/data' environment: - 'POSTGRES_PASSWORD=!Qsx4rgb' - 'PGDATA=/var/lib/postgresql/data/pgdata' # adminer: # image: adminer # restart: always # ports: # - 8080:8080 volumes: postgresql_data: driver: local  安装pgcli sudo apt install libpq-dev pip install -U pgcli export PATH=$PATH:/home/ubuntu/.local/bin pgcli -U postgres -h localhost  更改配置 postgres@localhost\u0026gt; alter system set autovacuum = off; postgres@localhost\u0026gt; SELECT pg_reload_conf(); postgres@localhost\u0026gt; show autovacuum;  ","tags":["PSQL"],"title":"Postgres and Pgcli","type":"post"},{"authors":null,"categories":[],"content":"因为禁用了下列服务：\nApplication Information Appinfo\n导致无法打开系统高级设置，SERVICES.MSC\nhttps://www.elevenforum.com/t/restore-default-services-in-windows-11.3109/\n","date":1656385752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656385752,"objectID":"5ed7485f5de4296d67bc1b2da59d75f2","permalink":"https://wubigo.com/post/default-services-in-windows-11/","publishdate":"2022-06-28T11:09:12+08:00","relpermalink":"/post/default-services-in-windows-11/","section":"post","summary":"因为禁用了下列服务：\nApplication Information Appinfo\n导致无法打开系统高级设置，SERVICES.MSC\nhttps://www.elevenforum.com/t/restore-default-services-in-windows-11.3109/","tags":["WIN"],"title":"Windows 11核心系统服务列表","type":"post"},{"authors":null,"categories":[],"content":" CAN消息总线格式 DBC格式 BO_ 500 IO_DEBUG: 4 IO SG_ IO_DEBUG_test_unsigned : 0|8@1+ (1,0) [0|0] \u0026quot;\u0026quot; DBG  说明：\n The message name is IO_DEBUG and MID is 500 (decimal), and the length is 4 bytes (though we only need 1 for 8-bit signal) The sender is IO 0|8: The unsigned signal starts at bit position 0, and the size of this signal is 8 (1,0): The scale and offset (discussed later) [0|0]: Min and Max is not defined (discussed later) \u0026rdquo;\u0026rdquo;: There are no units (it could be, for instance \u0026ldquo;inches\u0026rdquo;) @1+: Defines that the signal is little-endian, and unsigned: Never change this!  [1] CAN DBC\n","date":1653208253,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653208253,"objectID":"3cc53f9f5ec3115c5d99e7b4677ceb66","permalink":"https://wubigo.com/post/iov-can-msg-dbc_format/","publishdate":"2022-05-22T16:30:53+08:00","relpermalink":"/post/iov-can-msg-dbc_format/","section":"post","summary":"CAN消息总线格式 DBC格式 BO_ 500 IO_DEBUG: 4 IO SG_ IO_DEBUG_test_unsigned : 0|8@1+ (1,0) [0|0] \u0026quot;\u0026quot; DBG  说明：\n The message name is IO_DEBUG and MID is 500 (decimal), and the length is 4 bytes (though we only need 1 for 8-bit signal) The sender is IO 0|8: The unsigned signal starts at bit position 0, and the size of this signal is 8 (1,0): The scale and offset (discussed later) [0|0]: Min and Max is not defined (discussed later) \u0026rdquo;\u0026rdquo;: There are no units (it could be, for instance \u0026ldquo;inches\u0026rdquo;) @1+: Defines that the signal is little-endian, and unsigned: Never change this!","tags":["IOT","CAN"],"title":"CAN总线消息格式DBC_Format","type":"post"},{"authors":null,"categories":[],"content":" three main use cases for metadata repositories:  Finding data assets For example, a data architect may want to know which tables in which databases contain a Customer_ID.\n Tracking lineage (provenance) Many regulations require enterprises to document the lineage of data assets—in other words, where the data for those assets came from and how it was generated or transformed.\n Impact analysis If developers are making changes in a complex ecosystem, there is always a dan‐ ger of breaking something. Impact analysis allows developers to see all the data assets that rely on a particular field or integration job before making a change\n  Data governance tools record, document, and sometimes manage governance poli‐cies. The tools usually define who the data steward is for each data asset. Data stew‐ ards are responsible for making sure the data assets are correct, documenting their purpose and lineage, and defining access and lifecycle management policies for them\n","date":1652270378,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652270378,"objectID":"e79dd5167c4a0824dee0f67905fcb8da","permalink":"https://wubigo.com/post/data-lake-notes/","publishdate":"2022-05-11T19:59:38+08:00","relpermalink":"/post/data-lake-notes/","section":"post","summary":"three main use cases for metadata repositories:  Finding data assets For example, a data architect may want to know which tables in which databases contain a Customer_ID.\n Tracking lineage (provenance) Many regulations require enterprises to document the lineage of data assets—in other words, where the data for those assets came from and how it was generated or transformed.\n Impact analysis If developers are making changes in a complex ecosystem, there is always a dan‐ ger of breaking something.","tags":[],"title":"Data Lake Notes","type":"post"},{"authors":null,"categories":[],"content":" 启动EDGEX核心服务 使用容器主机网络 wget https://raw.githubusercontent.com/edgexfoundry/edgex-compose/jakarta/docker-compose-no-secty.yml  在开发环境使用host网络，便于核心服务和各种设备能在同一个子网，替换\nnetworks: edgex-network: {}  为\nnetwork_mode: host  CONSUL 配置CONSUL绑定到指定以太网卡\nenvironment: CONSUL_BIND_INTERFACE: ens3  配置DNS /etc/hosts\n10.166.44.182 edgex-core-consul 10.166.44.182 edgex-core-command 10.166.44.182 edgex-redis 10.166.44.182 edgex-core-metadata 10.166.44.182 edgex-core-data 10.166.44.182 edgex-support-notifications 10.166.44.182 edgex-support-scheduler 10.166.44.182 edgex-ui-go 10.166.44.182 edgex-kuiper 10.166.44.182 edgex-app-rules-engine  启动核心服务 docker-compose -f docker-compose-no-secty.yml up -d  检查核心服务启动正常 docker logs -f edgex-core-command edgex-core-data edgex-core-metadata  在本地启动设备服务 git clone git@github.com:edgexfoundry/device-camera-go.git  修改cmd\\res\\configuration.toml\n注册服务，消息队列，核心服务的所在的主机配置\n[Registry] Type = \u0026quot;consul\u0026quot; Host = \u0026quot;10.166.44.182\u0026quot; Port = 8500 [MessageQueue] Protocol = \u0026quot;redis\u0026quot; Host = \u0026quot;10.166.44.182\u0026quot; [Clients.core-data] Protocol = \u0026quot;http\u0026quot; Host = \u0026quot;10.166.44.182\u0026quot; Port = 59880 [Clients.core-metadata] Protocol = \u0026quot;http\u0026quot; Host = \u0026quot;10.166.44.182\u0026quot; Port = 59881  连接ONVIF摄像头 启动ONVIF摄像头服务 启动设备服务 cd device-camera-go/cmd set EDGEX_SECURITY_SECRET_STORE=false go run main.go  查看设备资源 curl -X GET http://10.166.44.182:59882/api/v2/device/name/Camera001\n建议：请使用postman测试，\n { \u0026quot;name\u0026quot;: \u0026quot;OnvifSnapshot\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifSnapshot\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifSnapshot\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;Binary\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifHostname\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifHostname\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifHostname\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] } ```` postmana返回的路径可以直接点击，并直接调用相应的资源  http://10.166.44.182:59882/api/v2/device/name/Camera001/OnvifSnapshot ```\n","date":1647572326,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647572326,"objectID":"741517d1c45ccd48ba0434536b2f58df","permalink":"https://wubigo.com/post/edgex-onvif-device-dev-setup/","publishdate":"2022-03-18T10:58:46+08:00","relpermalink":"/post/edgex-onvif-device-dev-setup/","section":"post","summary":"启动EDGEX核心服务 使用容器主机网络 wget https://raw.githubusercontent.com/edgexfoundry/edgex-compose/jakarta/docker-compose-no-secty.yml  在开发环境使用host网络，便于核心服务和各种设备能在同一个子网，替换\nnetworks: edgex-network: {}  为\nnetwork_mode: host  CONSUL 配置CONSUL绑定到指定以太网卡\nenvironment: CONSUL_BIND_INTERFACE: ens3  配置DNS /etc/hosts\n10.166.44.182 edgex-core-consul 10.166.44.182 edgex-core-command 10.166.44.182 edgex-redis 10.166.44.182 edgex-core-metadata 10.166.44.182 edgex-core-data 10.166.44.182 edgex-support-notifications 10.166.44.182 edgex-support-scheduler 10.166.44.182 edgex-ui-go 10.166.44.182 edgex-kuiper 10.166.44.182 edgex-app-rules-engine  启动核心服务 docker-compose -f docker-compose-no-secty.yml up -d  检查核心服务启动正常 docker logs -f edgex-core-command edgex-core-data edgex-core-metadata  在本地启动设备服务 git clone git@github.com:edgexfoundry/device-camera-go.git  修改cmd\\res\\configuration.toml\n注册服务，消息队列，核心服务的所在的主机配置\n[Registry] Type = \u0026quot;consul\u0026quot; Host = \u0026quot;10.","tags":["IOT"],"title":"Edgex Onvif Device Dev Setup","type":"post"},{"authors":null,"categories":[],"content":" 加载设备配置文件 device-sdk-go: service.init.go: driver.Initialize initializeOnvifClient onvif4go.NewOnvifDevice.Initialize() provision.LoadProfiles(ds.config.Device.ProfilesDir, dic) provision.LoadDevices  驱动服务客户端初始化与核心数据和命令服务 clients.BootstrapHandler -\u0026gt; InitDependencyClients // InitDependencyClients triggers Service Client Initializer to establish connection to Metadata and Core Data Services // through Metadata Client and Core Data Client. // Service Client Initializer also needs to check the service status of Metadata and Core Data Services, // because they are important dependencies of Device Service. // The initialization process should be pending until Metadata Service and Core Data Service are both available.  驱动 func (d *Driver) Initialize -\u0026gt; loadCameraConfig  diff --git a/internal/driver/driver.go b/internal/driver/driver.go @@ -388,7 +389,7 @@ func (d *Driver) Initialize(lc logger.LoggingClient, asyncCh - + debug.PrintStack()  runtime/debug.Stack() C:/local/go/src/runtime/debug/stack.go:24 +0x7a runtime/debug.PrintStack() C:/local/go/src/runtime/debug/stack.go:16 +0x19 github.com/edgexfoundry/device-camera-go/internal/driver.(*Driver).Initialize(0xc00034fa00, {0x14495f0, 0xc0003714a0}, 0xc000480060, 0x0) D:/code/go/src/github.com/edgexfoundry/device-camera-go/internal/driver/driver.go:392 +0xc7 github.com/edgexfoundry/device-sdk-go/v2/pkg/service.(*Bootstrap).BootstrapHandler(0xc0000d26e0, {0x14408d8, 0xc00036b0c0}, 0xc000388680, {{0xc08 38af7a5288f3c, 0xf37d35, 0x17238c0}, 0xdf8475800, 0x3b9aca00}, 0xc00034fa80) D:/code/go/pkg/mod/github.com/edgexfoundry/device-sdk-go/v2@v2.2.0-dev.8/pkg/service/init.go:55 +0x62d github.com/edgexfoundry/go-mod-bootstrap/v2/bootstrap.RunAndReturnWaitGroup({0x14408d8, 0xc00036b0c0}, 0xc0003827f0, {0x1446b90, 0xc000351500}, { 0x13d7f8f, 0xd}, {0x13d868d, 0xe}, {0x1445710, ...}, ...) D:/code/go/pkg/mod/github.com/edgexfoundry/go-mod-bootstrap/v2@v2.1.0/bootstrap/bootstrap.go:158 +0xa33 github.com/edgexfoundry/go-mod-bootstrap/v2/bootstrap.Run({0x14408d8, 0xc00036b0c0}, 0xc0003827f0, {0x1446b90, 0xc000351500}, {0x13d7f8f, 0xd}, { 0x13d868d, 0xe}, {0x1445710, ...}, ...) D:/code/go/pkg/mod/github.com/edgexfoundry/go-mod-bootstrap/v2@v2.1.0/bootstrap/bootstrap.go:184 +0x19c github.com/edgexfoundry/device-sdk-go/v2/pkg/service.Main({0x13d7f8f, 0xd}, {0x13e0370, 0x1a}, {0x1391bc0, 0xc00034fa00}, {0x14408d8, 0xc00036b0c 0}, 0xc0003827f0, 0xc0002c1bc0) D:/code/go/pkg/mod/github.com/edgexfoundry/device-sdk-go/v2@v2.2.0-dev.8/pkg/service/main.go:66 +0x9b3 github.com/edgexfoundry/device-sdk-go/v2/pkg/startup.Bootstrap({0x13d7f8f, 0xd}, {0x13e0370, 0x1a}, {0x1391bc0, 0xc00034fa00}) D:/code/go/pkg/mod/github.com/edgexfoundry/device-sdk-go/v2@v2.2.0-dev.8/pkg/startup/bootstrap.go:19 +0x11a main.main() D:/code/go/src/github.com/edgexfoundry/device-camera-go/cmd/main.go:22 +0x4c  OnvifClient onvif4go\nONVIF网络视频设备 ONVIF提供一系列被清楚定义的网络服务给符合ONVIF标准的设备及客户。此外，一些条件功能只能在特定的产品中才能实现。例如要实现摄像机的PTZ功能，必须要在接口处提供特定的支持服务才可以实现，可选的服务也都须被定义。产品必须详细说明所支持的服务及功能。在开发上非常简便，软件客户端可以查询符合ONVIF标准的设备，获取产品的服务与功能列表。例如图像服务是可选受理的服务，客户端可以通过设备管理服务的“能力获取”（GetCapabilities）查询该服务的可用性。这意味着集成商可以在软件中自动侦测产品所支持的服务及功能\ndevice-usb-camera wget https://raw.githubusercontent.com/pimlie/ubuntu-mainline-kernel.sh/master/ubuntu-mainline-kernel.sh ubuntu-mainline-kernel.sh -i v5.10.0 wget http://launchpadlibrarian.net/520233550/linux-libc-dev_5.10.0-14.15_amd64.deb dpkg -i linux-libc-dev_5.10.0-14.15_amd64.deb sudo dpkg-query -L linux-libc-dev | grep videodev2.h /usr/include/linux/videodev2.h export EDGEX_SECURITY_SECRET_STORE=false  device profile for OnvifSnapshot deviceResources: - name: \u0026quot;OnvifSnapshot\u0026quot; description: \u0026quot;snapshot from first ONVIF MediaProfile\u0026quot; properties: valueType: \u0026quot;Binary\u0026quot; readWrite: \u0026quot;R\u0026quot; mediaType: \u0026quot;image/jpeg\u0026quot; deviceCommands: - name: \u0026quot;OnvifSnapshot\u0026quot; isHidden: false readWrite: \u0026quot;R\u0026quot; resourceOperations: - { deviceResource: \u0026quot;OnvifSnapshot\u0026quot; }  http://:59985/api/v2/device/name/Camera001/OnvifSnapshot\n下载的文件为CBOR编码。\n","date":1647509395,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647509395,"objectID":"33068a4817a6dafcb91f15dea08c639b","permalink":"https://wubigo.com/post/edgex-device-service-onvif/","publishdate":"2022-03-17T17:29:55+08:00","relpermalink":"/post/edgex-device-service-onvif/","section":"post","summary":"加载设备配置文件 device-sdk-go: service.init.go: driver.Initialize initializeOnvifClient onvif4go.NewOnvifDevice.Initialize() provision.LoadProfiles(ds.config.Device.ProfilesDir, dic) provision.LoadDevices  驱动服务客户端初始化与核心数据和命令服务 clients.BootstrapHandler -\u0026gt; InitDependencyClients // InitDependencyClients triggers Service Client Initializer to establish connection to Metadata and Core Data Services // through Metadata Client and Core Data Client. // Service Client Initializer also needs to check the service status of Metadata and Core Data Services, // because they are important dependencies of Device Service. // The initialization process should be pending until Metadata Service and Core Data Service are both available.","tags":["IOT"],"title":"如何实现Edgex设备服务","type":"post"},{"authors":null,"categories":[],"content":" 搭建EDGEX设备服务开发环境\nEDGEX简介 EdgeX Foundry 项目成立于2017年，由Linux 基金会主持，目前已经拥有75个会员， 包括重量级的SAMSUNG，Dell，AMD，ANALOG DEVICES \u0026hellip; 但其中最重要的角色其实是戴尔。它为EdgeX Foundry提供该公司采用Apache 2.0许可证的FUSE源代码。FUSE包括10多种微服务和12500多万行代码，它们在连接标准、边缘分析、安全、系统管理和服务之间提供了互操作性。FUSE 是Dell 为了拓展边缘计算物联网服务而发展出来的，基于Java SPRING CLOUD 的微服务框架软件。这套软件最大的特征是中立于任何硬件平台和操作系统，高度模块化，可自由扩展。Dell 从私有化完成的那天开始，我们见到它一系列的并购，其中并购VMware，EMC 等动作，都直接剑指云计算和物联网技术。FUSE 的发布，更昭示了Dell谋求转型的决心。物联网的繁荣非常地依赖于物联网生态企业，全球大型IT 公司谋求物联网布局，都会提供框架软件，再培养生态伙伴来形成落地应用。而在生态伙伴的培养过程中，开源的基础框架软件是非常容易被理解和吸收的。 这也是Dell 选择跟Linux 基金会合作，并捐赠和完善代码的重要原因：Dell 需要庞大的生态伙伴，而Linux 具有开源软件界强大的号召力和影响力。\nEdgeX Foundry原来是用Java写的，导致平台体积庞大，占用资源，后来Linux基金会用Go语言对其进行了重构.\nEdgeX Foundry是一系列松耦合、开源的微服务集合。该微服务集合构成了四个微服务层及两个增强的基础系统服务，这四个微服务层包含了从物理域数据采集到信息域数据处理等一系列的服务，另外两个基础系统服务为该四个服务层提供支撑服务。\n四个微服务层分别是：\n 设备服务负责采集数据及控制设备功能。 核心服务负责本地存储分析和转发数据，以及控制命令下发。 支持服务负责日志记录、任务调度、数据清理、规则引擎和告警通知。 应用服务/导出服务负责上传数据到云端或第三方信息系统，以及接收控制命令转发给核心服务。  两个增强基础系统服务：\n安全服务、管理服务这两个软件模块虽然不直接处理边缘计算的功能性业务，但是对于边缘计算的安全性和易用性来说很重要\n启动EDGEX核心服务 wget https://raw.githubusercontent.com/edgexfoundry/edgex-compose/jakarta/docker-compose-no-secty.yml docker-compose -f docker-compose-no-secty.yml up -d  打开数据和控制服务调试开关 在consul里面配置应用：\nhttp://localhost:8500/ui/dc1/kv/edgex/appservices/2.0/app-rules-engine/Writable/LogLevel/edit\n修改INFO为DEBUG\n在IDE里面启动设备服务 以IP摄像头设备服务为例\n打开WEBCAM webcam-to-ip-camera\n配置开发环境 git clone git@github.com:edgexfoundry/device-camera-go.git git checkout jakarta cd device-camera-go go mod tidy set EDGEX_SECURITY_SECRET_STORE=false  调整IDE的当前工作目录为：device-camera-go\\cmd 否则，设备服务启动会找不到设备配置文件。\n或者移动资源文件到IDE当前工作目录\nmv -rf cmd/res .  打开设备服务调试日志并启动设备服务 res/configuration.toml @@ -1,5 +1,5 @@ [Writable] -LogLevel = \u0026quot;INFO\u0026quot; +LogLevel = \u0026quot;DEBUG\u0026quot; level=INFO ts=2022-03-11T08:13:54.8106393Z app=device-camera source=devices.go:87 msg=\u0026quot;Device Camera001 not found in Metadata, adding it ...\u0026quot; level=INFO ts=2022-03-11T08:13:54.8133486Z app=device-camera source=autodiscovery.go:33 msg=\u0026quot;AutoDiscovery stopped: disabled by configuration\u0026quot; level=INFO ts=2022-03-11T08:13:54.813521Z app=device-camera source=autodiscovery.go:42 msg=\u0026quot;AutoDiscovery stopped: ProtocolDiscovery not implemented\u0026quot; level=INFO ts=2022-03-11T08:13:54.8137182Z app=device-camera source=message.go:50 msg=\u0026quot;Service dependencies resolved...\u0026quot; level=INFO ts=2022-03-11T08:13:54.8137182Z app=device-camera source=message.go:51 msg=\u0026quot;Starting device-camera to be replaced by makefile \u0026quot; level=INFO ts=2022-03-11T08:13:54.8137182Z app=device-camera source=message.go:55 msg=\u0026quot;Camera device service started\u0026quot; level=INFO ts=2022-03-11T08:13:54.8137182Z app=device-camera source=message.go:58 msg=\u0026quot;Service started in: 649.0037ms\u0026quot;  在edgex-device-rest日志查看启动的设备服务 docker logs -f edgex-device-rest  配置设备文件 [[DeviceList]] Name = \u0026quot;Camera001\u0026quot; ProfileName = \u0026quot;camera-bosch\u0026quot; Location = \u0026quot;edgex lab\u0026quot; [DeviceList.Protocols] [DeviceList.Protocols.HTTP] Address = \u0026quot;localhost:56000\u0026quot;  设备数据采集和控制 查看设备所有的资源 通过EDGEX元数据服务查看设备资源\ncurl -X GET http://localhost:59882/api/v2/device/name/Camera001 { \u0026quot;apiVersion\u0026quot;: \u0026quot;v2\u0026quot;, \u0026quot;statusCode\u0026quot;: 200, \u0026quot;deviceCoreCommand\u0026quot;: { \u0026quot;deviceName\u0026quot;: \u0026quot;Camera001\u0026quot;, \u0026quot;profileName\u0026quot;: \u0026quot;camera-bosch\u0026quot;, \u0026quot;coreCommands\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;OnvifProfileInformation\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifProfileInformation\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifProfileInformation\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;TamperDetected\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/TamperDetected\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;TamperDetected\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;Bool\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;occupancy\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/occupancy\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;occupancy\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;Uint32\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifReboot\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifReboot\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifReboot\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;Bool\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;counter\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/counter\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;counter\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;Uint32\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifDeviceInformation\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifDeviceInformation\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifDeviceInformation\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifDateTime\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifDateTime\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifDateTime\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifDns\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifDns\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifDns\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifNetworkProtocols\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifNetworkProtocols\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifNetworkProtocols\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifNetworkDefaultGateway\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifNetworkDefaultGateway\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifNetworkDefaultGateway\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifNetworkInterfaces\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifNetworkInterfaces\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifNetworkInterfaces\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifNtp\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifNtp\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifNtp\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifUsers\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifUsers\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifUsers\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifSnapshot\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifSnapshot\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifSnapshot\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;Binary\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifUser\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifUser\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifUser\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifHostname\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifHostname\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifHostname\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;MotionDetected\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/MotionDetected\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;MotionDetected\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;Bool\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifStreamURI\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifStreamURI\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifStreamURI\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;String\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;OnvifHostnameFromDHCP\u0026quot;, \u0026quot;get\u0026quot;: true, \u0026quot;set\u0026quot;: true, \u0026quot;path\u0026quot;: \u0026quot;/api/v2/device/name/Camera001/OnvifHostnameFromDHCP\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://edgex-core-command:59882\u0026quot;, \u0026quot;parameters\u0026quot;: [ { \u0026quot;resourceName\u0026quot;: \u0026quot;OnvifHostnameFromDHCP\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;Bool\u0026quot; } ] } ] } }  访问设备资源 设备的资源地址为上面接口返回的信息：url+path (设备控制服务)\n例如: OnvifHostnameFromDHCP\n设备控制服务会调用实际的设备服务的API:\ndocker logs -f edgex-core-command edgex-core-command | level=ERROR ts=2022-03-11T08:23:01.759949942Z app=core-command source=http.go:47 X-Correlation-ID=8a4c186d-6f40-464f-be03-b5e67c866d3b msg=\u0026quot;failed to send a http request -\u0026gt; Get \\\u0026quot;http://localhost:59985/api/v2/device/name/Camera001/OnvifHostname\\\u0026quot;: dial tcp 127.0.0.1:59985: connect: connection refused\u0026quot;  因为设备服务启动的时候，配置的ip为LOCALHOST, 容器内现在无法访问到设备服务实例API, 所以连接拒绝\n在设备服务所在网络浏览器直接上面的地址即可\n清理测试环境 docker-compose -f docker-compose-no-secty.yml down docker volume rm $(docker volume ls -q)  ","date":1646984382,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646984382,"objectID":"c183691a21b20abf0bd8c05741070743","permalink":"https://wubigo.com/post/edgex-device-service-dev-env-setup/","publishdate":"2022-03-11T15:39:42+08:00","relpermalink":"/post/edgex-device-service-dev-env-setup/","section":"post","summary":"搭建EDGEX设备服务开发环境\nEDGEX简介 EdgeX Foundry 项目成立于2017年，由Linux 基金会主持，目前已经拥有75个会员， 包括重量级的SAMSUNG，Dell，AMD，ANALOG DEVICES \u0026hellip; 但其中最重要的角色其实是戴尔。它为EdgeX Foundry提供该公司采用Apache 2.0许可证的FUSE源代码。FUSE包括10多种微服务和12500多万行代码，它们在连接标准、边缘分析、安全、系统管理和服务之间提供了互操作性。FUSE 是Dell 为了拓展边缘计算物联网服务而发展出来的，基于Java SPRING CLOUD 的微服务框架软件。这套软件最大的特征是中立于任何硬件平台和操作系统，高度模块化，可自由扩展。Dell 从私有化完成的那天开始，我们见到它一系列的并购，其中并购VMware，EMC 等动作，都直接剑指云计算和物联网技术。FUSE 的发布，更昭示了Dell谋求转型的决心。物联网的繁荣非常地依赖于物联网生态企业，全球大型IT 公司谋求物联网布局，都会提供框架软件，再培养生态伙伴来形成落地应用。而在生态伙伴的培养过程中，开源的基础框架软件是非常容易被理解和吸收的。 这也是Dell 选择跟Linux 基金会合作，并捐赠和完善代码的重要原因：Dell 需要庞大的生态伙伴，而Linux 具有开源软件界强大的号召力和影响力。\nEdgeX Foundry原来是用Java写的，导致平台体积庞大，占用资源，后来Linux基金会用Go语言对其进行了重构.\nEdgeX Foundry是一系列松耦合、开源的微服务集合。该微服务集合构成了四个微服务层及两个增强的基础系统服务，这四个微服务层包含了从物理域数据采集到信息域数据处理等一系列的服务，另外两个基础系统服务为该四个服务层提供支撑服务。\n四个微服务层分别是：\n 设备服务负责采集数据及控制设备功能。 核心服务负责本地存储分析和转发数据，以及控制命令下发。 支持服务负责日志记录、任务调度、数据清理、规则引擎和告警通知。 应用服务/导出服务负责上传数据到云端或第三方信息系统，以及接收控制命令转发给核心服务。  两个增强基础系统服务：\n安全服务、管理服务这两个软件模块虽然不直接处理边缘计算的功能性业务，但是对于边缘计算的安全性和易用性来说很重要\n启动EDGEX核心服务 wget https://raw.githubusercontent.com/edgexfoundry/edgex-compose/jakarta/docker-compose-no-secty.yml docker-compose -f docker-compose-no-secty.yml up -d  打开数据和控制服务调试开关 在consul里面配置应用：\nhttp://localhost:8500/ui/dc1/kv/edgex/appservices/2.0/app-rules-engine/Writable/LogLevel/edit\n修改INFO为DEBUG\n在IDE里面启动设备服务 以IP摄像头设备服务为例\n打开WEBCAM webcam-to-ip-camera\n配置开发环境 git clone git@github.com:edgexfoundry/device-camera-go.git git checkout jakarta cd device-camera-go go mod tidy set EDGEX_SECURITY_SECRET_STORE=false  调整IDE的当前工作目录为：device-camera-go\\cmd 否则，设备服务启动会找不到设备配置文件。","tags":["IOT"],"title":"搭建Edgex设备服务开发环境","type":"post"},{"authors":null,"categories":[],"content":"[1] https://www.asyncapi.com/docs/getting-started/coming-from-openapi\n","date":1645883715,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645883715,"objectID":"c83763e287663f00922b4b3f97d57c12","permalink":"https://wubigo.com/post/openapi-vs-asyncapi/","publishdate":"2022-02-26T21:55:15+08:00","relpermalink":"/post/openapi-vs-asyncapi/","section":"post","summary":"[1] https://www.asyncapi.com/docs/getting-started/coming-from-openapi","tags":["EDA"],"title":"OpenAPI vs AsyncAPI","type":"post"},{"authors":null,"categories":[],"content":"虎年新春快乐\n Maybe this year, we ought to walk through the rooms of our lives not looking for flaws, but looking for potential—Ellen Goodman\n ","date":1643679466,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643679466,"objectID":"79c05505b889746e65f13c92bfc4a112","permalink":"https://wubigo.com/post/happy-lunar-new-year-of-tiger/","publishdate":"2022-02-01T09:37:46+08:00","relpermalink":"/post/happy-lunar-new-year-of-tiger/","section":"post","summary":"虎年新春快乐\n Maybe this year, we ought to walk through the rooms of our lives not looking for flaws, but looking for potential—Ellen Goodman\n ","tags":[],"title":"虎年新春快乐","type":"post"},{"authors":null,"categories":[],"content":" 社会价值 个人的价值在于为别人和社会解决问题和提供帮助，在帮助别人的过程 中促进个人成长。读书不是目的，读书只是为大脑提供了输入，产生对 社会有用的输出才是读书的真正目的\n思考 思考比行动重要。害怕思考，草率行动是懒惰的表现。思考就是抑制动物的 本性过程，思考是个人成长的最快途径\n沟通 理解对方是沟通的前提条件。沟通就是让对方认识到自己提议的价值， 让对方受益。无论沟通的对象是朋友，家人，同事，不要首先想着 如何改变别人，重要的调整自己。\n业务和技术 软件解决方案的出发点是解决客户问题。解决问题与技术的先进性与否无关， 很多情况下解决方案取决于公司的组织形式和资源条件。运维成本是大型 软件解决方案最主要的成本。监控的目的在于预防失败。 越早发现问题，解决成本越低。\n知识积累 价值与风险是统一的，看待机会既要看到可能的价值，同时认识到风险。 巨大的价值必然伴随着巨大的风险。偶然发现的所谓的价值机会可能只是 一个无知陷阱。知识和财富不仅在于积累更在于传承。财富最重要的是 在于保值。\n","date":1641027540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641027540,"objectID":"fb0398169a867072a82f89c67c360ed3","permalink":"https://wubigo.com/post/notes-2021/","publishdate":"2022-01-01T16:59:00+08:00","relpermalink":"/post/notes-2021/","section":"post","summary":"社会价值 个人的价值在于为别人和社会解决问题和提供帮助，在帮助别人的过程 中促进个人成长。读书不是目的，读书只是为大脑提供了输入，产生对 社会有用的输出才是读书的真正目的\n思考 思考比行动重要。害怕思考，草率行动是懒惰的表现。思考就是抑制动物的 本性过程，思考是个人成长的最快途径\n沟通 理解对方是沟通的前提条件。沟通就是让对方认识到自己提议的价值， 让对方受益。无论沟通的对象是朋友，家人，同事，不要首先想着 如何改变别人，重要的调整自己。\n业务和技术 软件解决方案的出发点是解决客户问题。解决问题与技术的先进性与否无关， 很多情况下解决方案取决于公司的组织形式和资源条件。运维成本是大型 软件解决方案最主要的成本。监控的目的在于预防失败。 越早发现问题，解决成本越低。\n知识积累 价值与风险是统一的，看待机会既要看到可能的价值，同时认识到风险。 巨大的价值必然伴随着巨大的风险。偶然发现的所谓的价值机会可能只是 一个无知陷阱。知识和财富不仅在于积累更在于传承。财富最重要的是 在于保值。","tags":["NOTES"],"title":"2021 Notes","type":"post"},{"authors":null,"categories":[],"content":" BUILD mkdir -p $GOPATH/src/github.com/mailflux git clone https://github.com/wubigo/mainflux.git cd github.com/mailflux/mailflux/ make make dockers_dev make run  Provision the System mainflux-cli provision test  GET TOKEN curl -k https://172.21.53.253/tokens { \u0026quot;email\u0026quot;: \u0026quot;bold_gould@email.com\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;12345678\u0026quot; }  ","date":1639991077,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639991077,"objectID":"91581c851ad94368f6c90cb077b302d1","permalink":"https://wubigo.com/post/mainflux-dev-setup/","publishdate":"2021-12-20T17:04:37+08:00","relpermalink":"/post/mainflux-dev-setup/","section":"post","summary":" BUILD mkdir -p $GOPATH/src/github.com/mailflux git clone https://github.com/wubigo/mainflux.git cd github.com/mailflux/mailflux/ make make dockers_dev make run  Provision the System mainflux-cli provision test  GET TOKEN curl -k https://172.21.53.253/tokens { \u0026quot;email\u0026quot;: \u0026quot;bold_gould@email.com\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;12345678\u0026quot; }  ","tags":["IIOT","IOT"],"title":"Mainflux Dev Setup","type":"post"},{"authors":null,"categories":[],"content":" Configure an MQTT Client Agent within the IoT Gateway Plug-In for KEPServerEX\nto send data to Ali IoT. The connection can be made using MQTT over Transmission\nControl Protocol (TCP) and MQTT over Transport Layer Security (TLS).\nset up Kepware KEPServerEX IoT Gateway on Windows to connect with the MQTT bridge\nof IoT Core to push streaming data to Cloud and send control messages from IoT\nCore back to KEPServerEX\nIoT Gateway is a module that provides integration with external IT systems\nand cloud platforms through a series of protocols such as MQTT and HTTP.\n A random value simulator sends values to IoT Gateway. Values go through the IoT Gateway to IoT Core. IoT Core bridges the values to Pub/Sub. Users send command messages through IoT Core. Command messages go through IoT Core to the IoT Gateway. IoT Gateway delivers the messages to the simulated device.  setup  Install Kepware KEPServerEX. Install PowerShell. Create or select a Cloud project. Enable Kepware KEPServerEX Configuration API Service. Install the Cloud SDK.  detail  Generate a self-signed certificate authority (CA) certificate. Provision an IoT Core device on Google Cloud. Configure Kepware KEPServerEX IoT Gateway as an IoT Core device. Set up a Windows scheduled task to refresh the JSON Web Token (JWT). Set up the Kepware KEPServerEX IoT Gateway to send simulated metrics to IoT Core, and verify on Pub/Sub. Set up the Kepware KEPServerEX IoT Gateway to receive command messages and send commands from IoT Core.  data collect Read data from onsite equipment using industrial protocols such as OPC-UA, Modbus and EtherNet/IP\n","date":1639633238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639633238,"objectID":"7cb5099882cb51c90bef7ef129168c66","permalink":"https://wubigo.com/post/connect-kepserver-to-ali-iot/","publishdate":"2021-12-16T13:40:38+08:00","relpermalink":"/post/connect-kepserver-to-ali-iot/","section":"post","summary":"Configure an MQTT Client Agent within the IoT Gateway Plug-In for KEPServerEX\nto send data to Ali IoT. The connection can be made using MQTT over Transmission\nControl Protocol (TCP) and MQTT over Transport Layer Security (TLS).\nset up Kepware KEPServerEX IoT Gateway on Windows to connect with the MQTT bridge\nof IoT Core to push streaming data to Cloud and send control messages from IoT\nCore back to KEPServerEX","tags":["IOT","BIGDATA"],"title":"Connect KEPServer to Ali IoT","type":"post"},{"authors":null,"categories":[],"content":"Data catalogs solve the problem by tagging fields and data sets with consistent business terms and providing a shopping-type interface that allows the users to find data sets by describing what they are looking for using the business terms that they are used to, and to understand the data in those data sets through tags and descriptions that use business terms.\nData lakes are the do-it-yourself version of a data warehouse, allowing data engineering teams to pick and choose the various metadata, storage, and compute technologies they want to use depending on the needs of their systems. Data lakes are ideal for data teams looking to build a more customized platform, often supported by a handful (or more) of data engineers.\nhttps://towardsdatascience.com/how-to-build-your-data-platform-choosing-a-cloud-data-warehouse\nhttps://www.alibabacloud.com/blog/alibaba-cloud-maxcompute-vs--aws-redshift-azure-sql-data-warehouse\n","date":1638863901,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638863901,"objectID":"db58bb75d59fa82b7c47651b6bd80b34","permalink":"https://wubigo.com/post/data-lake-vs-warehouse/","publishdate":"2021-12-07T15:58:21+08:00","relpermalink":"/post/data-lake-vs-warehouse/","section":"post","summary":"Data catalogs solve the problem by tagging fields and data sets with consistent business terms and providing a shopping-type interface that allows the users to find data sets by describing what they are looking for using the business terms that they are used to, and to understand the data in those data sets through tags and descriptions that use business terms.\nData lakes are the do-it-yourself version of a data warehouse, allowing data engineering teams to pick and choose the various metadata, storage, and compute technologies they want to use depending on the needs of their systems.","tags":["BIGDATA"],"title":"Data Lake vs Warehouse","type":"post"},{"authors":null,"categories":[],"content":" 配置 \u0026rsquo;.azure/config`\n[defaults] location = westus [cloud] name = AzureCloud [core] output = table  az config set defaults.location=westus2 defaults.group=MyResourceGroup  az v2不支持config，直接修改配置文件\n","date":1636286424,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636286424,"objectID":"415cf83df41f3800e1ea1be55127d88c","permalink":"https://wubigo.com/post/azure-config/","publishdate":"2021-11-07T20:00:24+08:00","relpermalink":"/post/azure-config/","section":"post","summary":"配置 \u0026rsquo;.azure/config`\n[defaults] location = westus [cloud] name = AzureCloud [core] output = table  az config set defaults.location=westus2 defaults.group=MyResourceGroup  az v2不支持config，直接修改配置文件","tags":["IAAS"],"title":"Azure Config","type":"post"},{"authors":null,"categories":[],"content":" AZURE REGION az account list-locations --query \u0026quot;sort_by([].{DisplayName:displayName, Name:name}, \u0026amp;DisplayName)\u0026quot; --output table  创建函数APP #!/bin/bash # Function app and storage account names must be unique. storageName=mystorageaccount$RANDOM functionAppName=myserverlessfunc$RANDOM region=westeurope # Create a resource group. az group create --name myResourceGroup --location $region # Create an Azure storage account in the resource group. az storage account create \\ --name $storageName \\ --location $region \\ --resource-group myResourceGroup \\ --sku Standard_LRS # Create a serverless function app in the resource group. az functionapp create \\ --name $functionAppName \\ --storage-account $storageName \\ --consumption-plan-location $region \\ --resource-group myResourceGroup \\ --functions-version 2  ","date":1636253449,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636253449,"objectID":"f2c0b9faa42f0fb1ad9612e6562360c2","permalink":"https://wubigo.com/post/azure-function-from-cmd/","publishdate":"2021-11-07T10:50:49+08:00","relpermalink":"/post/azure-function-from-cmd/","section":"post","summary":"AZURE REGION az account list-locations --query \u0026quot;sort_by([].{DisplayName:displayName, Name:name}, \u0026amp;DisplayName)\u0026quot; --output table  创建函数APP #!/bin/bash # Function app and storage account names must be unique. storageName=mystorageaccount$RANDOM functionAppName=myserverlessfunc$RANDOM region=westeurope # Create a resource group. az group create --name myResourceGroup --location $region # Create an Azure storage account in the resource group. az storage account create \\ --name $storageName \\ --location $region \\ --resource-group myResourceGroup \\ --sku Standard_LRS # Create a serverless function app in the resource group.","tags":["LAMBDA"],"title":"Azure Function From Cmd","type":"post"},{"authors":null,"categories":[],"content":" 安装FUNC npm i -D azure-functions-core-tools@3 export PATH=./ export CLI_DEBUG=1 func host start --verbose  安装playwright-chromium export PLAYWRIGHT_BROWSERS_PATH=0 npm install playwright-chromium@1.3.0  确认chrome的存放路径\nnode_modules/playwright-chromium/.local-browsers/chromium-792639\n创建函数 /home/ubuntu/sls/azure-sls/node_modules/.bin/func init func new func start  本地测试 export CLI_DEBUG=1 func host start --verbose  host.json\n{ \u0026quot;version\u0026quot;: \u0026quot;2.0\u0026quot;, \u0026quot;logging\u0026quot;: { \u0026quot;applicationInsights\u0026quot;: { \u0026quot;samplingSettings\u0026quot;: { \u0026quot;isEnabled\u0026quot;: true, \u0026quot;excludedTypes\u0026quot;: \u0026quot;Request\u0026quot; } } }, \u0026quot;extensionBundle\u0026quot;: { \u0026quot;id\u0026quot;: \u0026quot;Microsoft.Azure.Functions.ExtensionBundle\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;[2.*, 3.0.0)\u0026quot; } }  如果遇到如下问题\n Value cannot be null. (Parameter \u0026lsquo;provider\u0026rsquo;)\n 注释/禁用 extensionBundle\n发布 func azure functionapp publish my_function_app_name --build remote  总结 部署节点和运行环境在同一可用区加快部署速度\n","date":1636211230,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636211230,"objectID":"db06d33b1046db5dfba0d000fef1d2dc","permalink":"https://wubigo.com/post/lambda-in-azure-with-azure-functions-core-tools/","publishdate":"2021-11-06T23:07:10+08:00","relpermalink":"/post/lambda-in-azure-with-azure-functions-core-tools/","section":"post","summary":"安装FUNC npm i -D azure-functions-core-tools@3 export PATH=./ export CLI_DEBUG=1 func host start --verbose  安装playwright-chromium export PLAYWRIGHT_BROWSERS_PATH=0 npm install playwright-chromium@1.3.0  确认chrome的存放路径\nnode_modules/playwright-chromium/.local-browsers/chromium-792639\n创建函数 /home/ubuntu/sls/azure-sls/node_modules/.bin/func init func new func start  本地测试 export CLI_DEBUG=1 func host start --verbose  host.json\n{ \u0026quot;version\u0026quot;: \u0026quot;2.0\u0026quot;, \u0026quot;logging\u0026quot;: { \u0026quot;applicationInsights\u0026quot;: { \u0026quot;samplingSettings\u0026quot;: { \u0026quot;isEnabled\u0026quot;: true, \u0026quot;excludedTypes\u0026quot;: \u0026quot;Request\u0026quot; } } }, \u0026quot;extensionBundle\u0026quot;: { \u0026quot;id\u0026quot;: \u0026quot;Microsoft.Azure.Functions.ExtensionBundle\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;[2.*, 3.0.0)\u0026quot; } }  如果遇到如下问题\n Value cannot be null.","tags":["LAMBDA"],"title":"Lambda in Azure With Azure Functions Core Tools","type":"post"},{"authors":null,"categories":[],"content":" 创建函数 npm install -g serverless@2.65.0 sls -v Framework Core: 2.65.0 Plugin: 5.5.1 SDK: 4.3.0 Components: 3.17.2 sls create -t azure-nodejs -p azure-fn cd azure-fn npm install  npm list |grep serverless-azure-functions └─┬ serverless-azure-functions@2.1.3  部署函数 set AZURE_SUBSCRIPTION_ID=02a23ad5- set AZURE_TENANT_ID=e9950462 set AZURE_CLIENT_ID=39258bc8 set AZURE_CLIENT_SECRET=hYdvD0 sls deploy --dryrun  测试 sls invoke -f hello -d '{\u0026quot;name\u0026quot;: \u0026quot;Azure\u0026quot;}'  清理 empty.json\n{ \u0026quot;$schema\u0026quot;: \u0026quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026quot;, \u0026quot;contentVersion\u0026quot;: \u0026quot;1.0.0.0\u0026quot;, \u0026quot;parameters\u0026quot;: { }, \u0026quot;variables\u0026quot;: { }, \u0026quot;resources\u0026quot;: [ ], \u0026quot;outputs\u0026quot;: { } }   az group deployment create --mode complete --template-file ./empty.json --resource-group testgroup az group delete -n testgroup -y  ","date":1636171197,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636171197,"objectID":"ce2f8cafa928d9b7af71da4a8cbfb92d","permalink":"https://wubigo.com/post/lambda-in-azure-with-sls/","publishdate":"2021-11-06T11:59:57+08:00","relpermalink":"/post/lambda-in-azure-with-sls/","section":"post","summary":"创建函数 npm install -g serverless@2.65.0 sls -v Framework Core: 2.65.0 Plugin: 5.5.1 SDK: 4.3.0 Components: 3.17.2 sls create -t azure-nodejs -p azure-fn cd azure-fn npm install  npm list |grep serverless-azure-functions └─┬ serverless-azure-functions@2.1.3  部署函数 set AZURE_SUBSCRIPTION_ID=02a23ad5- set AZURE_TENANT_ID=e9950462 set AZURE_CLIENT_ID=39258bc8 set AZURE_CLIENT_SECRET=hYdvD0 sls deploy --dryrun  测试 sls invoke -f hello -d '{\u0026quot;name\u0026quot;: \u0026quot;Azure\u0026quot;}'  清理 empty.json\n{ \u0026quot;$schema\u0026quot;: \u0026quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026quot;, \u0026quot;contentVersion\u0026quot;: \u0026quot;1.0.0.0\u0026quot;, \u0026quot;parameters\u0026quot;: { }, \u0026quot;variables\u0026quot;: { }, \u0026quot;resources\u0026quot;: [ ], \u0026quot;outputs\u0026quot;: { } }   az group deployment create --mode complete --template-file .","tags":["LAMBDA"],"title":"Lambda in Azure with sls","type":"post"},{"authors":null,"categories":[],"content":" CQuasar App Flow In order to better understand how a boot file works and what it does, you need to understand how your website/app boots:\nQuasar is initialized (components, directives, plugins, Quasar i18n, Quasar icon sets) Quasar Extras get imported (Roboto font – if used, icons, animations, …) Quasar CSS \u0026amp; your app’s global CSS are imported App.vue is loaded (not yet being used) Store is imported (if using Vuex Store in src/store) Router is imported (in src/router) Boot files are imported Router default export function executed Boot files get their default export function executed (if on Electron mode) Electron is imported and injected into Vue prototype (if on Cordova mode) Listening for “deviceready” event and only then continuing with following steps Instantiating Vue with root component and attaching to DOM\nVue组件脚手架 quasar new --list quasar new [type] \u0026lt;name of your component with optional subfolder\u0026gt;  创建组件\nquasar new component com1 quasar new page page1  组件类型 layout, page, component , boot, store\n","date":1635407634,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635407634,"objectID":"e43aa22962a6075d33083057d2cb5f89","permalink":"https://wubigo.com/post/quasar-app-flow/","publishdate":"2021-10-28T15:53:54+08:00","relpermalink":"/post/quasar-app-flow/","section":"post","summary":"CQuasar App Flow In order to better understand how a boot file works and what it does, you need to understand how your website/app boots:\nQuasar is initialized (components, directives, plugins, Quasar i18n, Quasar icon sets) Quasar Extras get imported (Roboto font – if used, icons, animations, …) Quasar CSS \u0026amp; your app’s global CSS are imported App.vue is loaded (not yet being used) Store is imported (if using Vuex Store in src/store) Router is imported (in src/router) Boot files are imported Router default export function executed Boot files get their default export function executed (if on Electron mode) Electron is imported and injected into Vue prototype (if on Cordova mode) Listening for “deviceready” event and only then continuing with following steps Instantiating Vue with root component and attaching to DOM","tags":["VUE"],"title":"Quasar App Flow","type":"post"},{"authors":null,"categories":[],"content":" 计算时间 vs 请求数量 平均计算时长 427586 / 27386 = 15.6 (秒)\n设置默认配置 aws configure list-profiles default us-east-1 us-east-2 us-west-2 us-west-1 eu eu-west-1 af-south-1 ap-east-1 ap-south-1 ap-northeast-3 ap-northeast-2 ap-southeast-1 ap-southeast-2 ca-central-1 eu-west-2 eu-south-1 eu-west-3 eu-north-1 me-south-1 sa-east-1 export AWS_PROFILE=us  下载部署包 aws lambda get-function --function-name webdriver \u0026quot;Code\u0026quot;: { \u0026quot;RepositoryType\u0026quot;: \u0026quot;S3\u0026quot;, \u0026quot;Location\u0026quot;: \u0026quot;https://awslambda-ap-ne-1-tasks.s3.ap-northeast-1.amazonaws.com/snapshots/webdriver-aeb2eb63-9baf-4d06-9d3a-79459b172200?versionId=a71tk2dwwmvW1lPNB5VHKq8SbGS3laqE\u0026amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaDmFwLW5vcnRoZWFzdC0xIkcwRQIhAMRkIxPh1Fkd2nlCzgiDbsrmnCZEVunHibw2Cm6cyRIUAiB5t60IO6iESPDeUsTuQEjGyLfI73QyMK1mJY9Al70yECqNBAj8%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAIaDDkxOTk4MDkyNTEzOSIMjVD0S8e1HGmJujr6KuEDO8SCL9OcolFOwL4IKMbE3euJLEtiGjSxH6c8jRPbnjp07Zf%2BxrOfJmWT2MORQs0RAQSLJV5nOFfRWTIPI4dSNhI3Q628XqklZ8%2BF1UktvA5vRdEU3LhDvOSsDCmL19k\u0026amp;X-Amz-Signature=7f876918ec5283db390a3037512e7ad62e434330ec3e406db18b25f25f3da0fe\u0026quot;  从Location下载部署包\nPROF = \u0026quot;eu-central-1\u0026quot; aws lambda create-function --function-name webdriver --runtime nodejs12.x --zip-file fileb:///home/ubuntu/webdriver.zip --handler index.handler --role arn:aws:iam::762491489154:role/service-role/webdriver-role-3hxi35t5 --timeout 63 --memory-size 1024 --layers arn:aws:lambda:us-east-1:764866452798:layer:chrome-aws-lambda:25 --profile us  配置 role-policy.json\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: [\u0026quot;lambda.amazonaws.com\u0026quot;] }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] }  aws iam create-role --role-name lambda-s3 --assume-role-policy-document file://role-policy.json aws iam attach-role-policy --role-name lambda-s3 --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole  测试 export AWS_PROFILE=eu-south-1 aws lambda invoke --function-name webdriver --cli-binary-format raw-in-base64-out --payload '{\u0026quot;pageNo\u0026quot;: 5}' out --log-type Tail --query 'LogResult' --output text | base64 -d  调度 aws events put-rule --name webdriver-scheduled-rule --schedule-expression 'rate(30 minutes)' aws lambda add-permission --function-name webdriver --statement-id webdriver-scheduled-event --action 'lambda:InvokeFunction' --principal events.amazonaws.com --source-arn arn:aws:events:eu-central-1:762491489154:rule/webdriver-scheduled-rule aws events put-targets --rule webdriver-scheduled-rule --targets file://targets.json  调整周期 aws events put-rule --name webdriver-scheduled-rule --schedule-expression \u0026quot;rate(10 minutes)\u0026quot;  https://github.com/wubigo/API/blob/master/bash/put-rule.sh\n自动 if [ -z \u0026quot;$1\u0026quot; ] then echo \u0026quot;No region supplied\u0026quot; exit 1 fi export AWS_PROFILE=$1 FN=$(aws lambda create-function --function-name webdriver --runtime nodejs12.x --zip-file fileb:///home/ubuntu/webdriver.zip --handler index.handler --role arn:aws:iam::762491:role/lambda-s3 --timeout 63 --memory-size 1024 --layers arn:aws:lambda:${AWS_PROFILE}:762491:layer:chrome-aws-lambda:25) echo $FN aws events put-rule --name webdriver-scheduled-rule --schedule-expression 'rate(30 minutes)' EVENT=$(aws lambda add-permission --function-name webdriver --statement-id webdriver-scheduled-event --action 'lambda:InvokeFunction' --principal events.amazonaws.com --source-arn arn:aws:events:${AWS_PROFILE}:762491:rule/webdriver-scheduled-rule) echo $EVENT T=$(python3 targets.py ${AWS_PROFILE}) cat targets.json aws events put-targets --rule webdriver-scheduled-rule --targets file://targets.json sleep 5 awslogs streams /aws/lambda/webdriver awslogs get /aws/lambda/webdriver --filter-pattern=\u0026quot;[r=REPORT,...]\u0026quot;  禁止调用  aws lambda put-function-concurrency --function-name webdriver --reserved-concurrent-executions 0  function cannot be invoked while the reserved concurrency is zero.\nCalling the invoke API action failed with this message: Rate Exceeded.\n并发配额重置 aws lambda delete-function-concurrency --function-name webdriver  清理 aws lambda remove-permission --function-name webdriver --statement-id webdriver-scheduled-event aws events list-targets-by-rule --rule webdriver-scheduled-rule aws events remove-targets --rule webdriver-scheduled-rule --ids 1 aws events delete-rule --name \u0026quot;webdriver-scheduled-rule\u0026quot;  ","date":1634875176,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634875176,"objectID":"9358b6bb25855102b4c2d5f04c83a6fd","permalink":"https://wubigo.com/post/lambda-redeploy-across-region/","publishdate":"2021-10-22T11:59:36+08:00","relpermalink":"/post/lambda-redeploy-across-region/","section":"post","summary":"计算时间 vs 请求数量 平均计算时长 427586 / 27386 = 15.6 (秒)\n设置默认配置 aws configure list-profiles default us-east-1 us-east-2 us-west-2 us-west-1 eu eu-west-1 af-south-1 ap-east-1 ap-south-1 ap-northeast-3 ap-northeast-2 ap-southeast-1 ap-southeast-2 ca-central-1 eu-west-2 eu-south-1 eu-west-3 eu-north-1 me-south-1 sa-east-1 export AWS_PROFILE=us  下载部署包 aws lambda get-function --function-name webdriver \u0026quot;Code\u0026quot;: { \u0026quot;RepositoryType\u0026quot;: \u0026quot;S3\u0026quot;, \u0026quot;Location\u0026quot;: \u0026quot;https://awslambda-ap-ne-1-tasks.s3.ap-northeast-1.amazonaws.com/snapshots/webdriver-aeb2eb63-9baf-4d06-9d3a-79459b172200?versionId=a71tk2dwwmvW1lPNB5VHKq8SbGS3laqE\u0026amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaDmFwLW5vcnRoZWFzdC0xIkcwRQIhAMRkIxPh1Fkd2nlCzgiDbsrmnCZEVunHibw2Cm6cyRIUAiB5t60IO6iESPDeUsTuQEjGyLfI73QyMK1mJY9Al70yECqNBAj8%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAIaDDkxOTk4MDkyNTEzOSIMjVD0S8e1HGmJujr6KuEDO8SCL9OcolFOwL4IKMbE3euJLEtiGjSxH6c8jRPbnjp07Zf%2BxrOfJmWT2MORQs0RAQSLJV5nOFfRWTIPI4dSNhI3Q628XqklZ8%2BF1UktvA5vRdEU3LhDvOSsDCmL19k\u0026amp;X-Amz-Signature=7f876918ec5283db390a3037512e7ad62e434330ec3e406db18b25f25f3da0fe\u0026quot;  从Location下载部署包\nPROF = \u0026quot;eu-central-1\u0026quot; aws lambda create-function --function-name webdriver --runtime nodejs12.x --zip-file fileb:///home/ubuntu/webdriver.zip --handler index.handler --role arn:aws:iam::762491489154:role/service-role/webdriver-role-3hxi35t5 --timeout 63 --memory-size 1024 --layers arn:aws:lambda:us-east-1:764866452798:layer:chrome-aws-lambda:25 --profile us  配置 role-policy.","tags":["LAMBDA","SLS"],"title":"Lambda Redeploy Across Region","type":"post"},{"authors":null,"categories":[],"content":" aws客户端环境准备 git clone pip install awscli https://github.com/wubigo/API.git python API/python/aws/aws.py cp API/python/aws/cred.json ~/.aws/credentials cp API/python/aws/config ~/.aws/config  创建函数部署包 mkdir lambda_web wget https://github.com/wubigo/API/blob/master/nodejs/lambda/aws/index.js -P lambda_web zip -r webdriver.zip lambda_web/*  配置 policy.json\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: [\u0026quot;lambda.amazonaws.com\u0026quot;, \u0026quot;s3.amazonaws.com\u0026quot;] }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] }  export ACCOUNT_ID=820934811997 aws iam create-role --role-name lambda-s3 --assume-role-policy-document file://policy.json aws iam attach-role-policy --role-name lambda-s3 --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole aws iam attach-role-policy --role-name lambda-s3 --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess  复制 aws lambda create-function --function-name webdriver --runtime nodejs12.x --zip-file fileb:///home/ubuntu/webdriver.zip --handler index.handler --role arn:aws:iam::762491489154:role/service-role/webdriver-role-3hxi35t5 --timeout 63 --memory-size 1024 --layers arn:aws:lambda:us-east-1:764866452798:layer:chrome-aws-lambda:25 --profile us  调整默认配置（设置内存和超时时间） 函数计算的默认配置： 内存是128 MB， 超时是3秒。\n默认配置 pyppeteer无法正常工作。\naws lambda get-function --function-name webdriver { \u0026quot;Configuration\u0026quot;: { \u0026quot;FunctionName\u0026quot;: \u0026quot;webdriver\u0026quot;, \u0026quot;FunctionArn\u0026quot;: \u0026quot;arn:aws:lambda:ap-northeast-1:762491489154:function:webdriver\u0026quot;, \u0026quot;Runtime\u0026quot;: \u0026quot;nodejs12.x\u0026quot;, \u0026quot;Role\u0026quot;: \u0026quot;arn:aws:iam::762491489154:role/service-role/webdriver-role-3hxi35t5\u0026quot;, \u0026quot;Handler\u0026quot;: \u0026quot;index.handler\u0026quot;, \u0026quot;CodeSize\u0026quot;: 598, \u0026quot;Description\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Timeout\u0026quot;: 63, \u0026quot;MemorySize\u0026quot;: 1024, \u0026quot;LastModified\u0026quot;: \u0026quot;2021-10-08T09:44:02.000+0000\u0026quot;, \u0026quot;CodeSha256\u0026quot;: \u0026quot;Ma8ntxB5UxdLSOdotZBnSGDuBUDI+XEGlggfpPlV/AI=\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;$LATEST\u0026quot;, \u0026quot;TracingConfig\u0026quot;: { \u0026quot;Mode\u0026quot;: \u0026quot;PassThrough\u0026quot; }, \u0026quot;RevisionId\u0026quot;: \u0026quot;3c08812d-e958-4427-a47e-be628966be36\u0026quot;, \u0026quot;Layers\u0026quot;: [ { \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:lambda:ap-northeast-1:764866452798:layer:chrome-aws-lambda:25\u0026quot;, \u0026quot;CodeSize\u0026quot;: 51779390 } ],  使用lambda layer https://github.com/shelfio/chrome-aws-lambda-layer\n注意要使用相应分区\n自定义函数服务层 git clone --depth=1 https://github.com/alixaxel/chrome-aws-lambda.git \u0026amp;\u0026amp; \\ cd chrome-aws-lambda \u0026amp;\u0026amp; \\ make chrome_aws_lambda.zip aws lambda publish-layer-version --layer-name chrome_aws_lambda --zip-file fileb:///home/ubuntu/chrome-aws-lambda/chrome_aws_lambda.zip --compatible-runtimes nodejs12.x  函数定义 const chromium = require('chrome-aws-lambda'); function getRandomInt(max) { let r = Math.floor(Math.random() * max) if ( r \u0026lt;= 1 ) r = 2 return r; } exports.handler = async (event, context, callback) =\u0026gt; { console.log(\u0026quot;fn EVENT: \\n\u0026quot; + JSON.stringify(event, null, 2)) let e = JSON.parse(JSON.stringify(event, null, 2)); let pageNo = e[\u0026quot;pageNo\u0026quot;] if ( pageNo == null ) pageNo = getRandomInt(17) console.log(\u0026quot;pageNo=\u0026quot;+pageNo); let result = null; let browser = null; try { let path = await chromium.executablePath; console.log(\u0026quot;browser begin: \u0026quot;+ path+\u0026quot;\\n\u0026quot; ); browser = await chromium.puppeteer.launch({ args: chromium.args, defaultViewport: chromium.defaultViewport, executablePath: await chromium.executablePath, headless: chromium.headless, ignoreHTTPSErrors: true, }); let page = await browser.newPage(); await page.setDefaultTimeout(0); await page.goto('https://wubigo.com/post/page/'+pageNo); console.log(\u0026quot;go to wubigo\u0026quot;); result = await page.title(); console.log(\u0026quot;title: \u0026quot; +result +\u0026quot;\\n\u0026quot; ); const page2 = await browser.newPage(); await page2.setDefaultNavigationTimeout(0); const articles = await page.$$('h3.article-title'); for (let i = 0; i \u0026lt; articles.length; i++) { const link = await articles[i].$eval('a', a =\u0026gt; a.getAttribute('href')) .catch((e) =\u0026gt; console.log('catch err: ' + e)); console.log('link='+link); e[\u0026quot;link\u0026quot;+i]=link; // await articles[i].click().catch((e) =\u0026gt; console.log('err: ' + e)); await page2.goto('https://wubigo.com/'+link); // await page.goBack(); } } catch (error) { return callback(error); } finally { if (browser !== null) { await browser.close(); } } result = JSON.stringify(e) return callback(null, result); };  执行 aws lambda invoke --function-name webdriver --cli-binary-format raw-in-base64-out --payload '{\u0026quot;pageNo\u0026quot;: 5}' response.json --profile us aws lambda invoke --function-name webdriver --cli-binary-format raw-in-base64-out --payload '{\u0026quot;pageNo\u0026quot;: 5}' out --log-type Tail --query 'LogResult' --output text --profile us | base64 -d  检查结果 cat response.json  EventBridge调度 aws events put-rule --name webdriver-scheduled-rule --schedule-expression 'rate(30 minutes)' aws lambda add-permission --function-name webdriver --statement-id webdriver-scheduled-event --action 'lambda:InvokeFunction' --principal events.amazonaws.com --source-arn arn:aws:events:ap-northeast-1:762491489154:rule/webdriver-scheduled-rule aws events put-targets --rule webdriver-scheduled-rule --targets file://targets.json  targets.json\n[ { \u0026quot;Id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:lambda:ap-northeast-1:762491489154:function:webdriver\u0026quot; } ]  检查调度结果 2021-10-21T09:55:56.099Z\t397385df-555a-451a-9ba6-b9548438c797\tINFO\tfn EVENT: { \u0026quot;version\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;8945024c-4594-df8a-74f3-9c98b0e75ed5\u0026quot;, \u0026quot;detail-type\u0026quot;: \u0026quot;Scheduled Event\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;aws.events\u0026quot;, \u0026quot;account\u0026quot;: \u0026quot;762491489154\u0026quot;, \u0026quot;time\u0026quot;: \u0026quot;2021-10-21T09:55:11Z\u0026quot;, \u0026quot;region\u0026quot;: \u0026quot;ap-northeast-1\u0026quot;, \u0026quot;resources\u0026quot;: [ \u0026quot;arn:aws:events:ap-northeast-1:762491489154:rule/webdriver-scheduled-rule\u0026quot; ], \u0026quot;detail\u0026quot;: {} } # 检查日志 `log-events.json`  { \u0026ldquo;logStreamName\u0026rdquo;: \u0026ldquo;2021/10/21/[$LATEST]eae72f7f77124ab69d0a1fc398172cec\u0026rdquo;, \u0026ldquo;logGroupName\u0026rdquo;: \u0026ldquo;/aws/lambda/webdriver\u0026rdquo;, \u0026ldquo;startFromHead\u0026rdquo;: true }\n  aws logs describe-log-streams \u0026ndash;log-group-name /aws/lambda/webdriver \u0026ndash;log-stream-name-prefix 2021/10/21\naws logs get-log-events \u0026ndash;cli-input-json file://log-events.json ```\n","date":1633745758,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633745758,"objectID":"61ef5566768f331a5b5ffe00896c1103","permalink":"https://wubigo.com/post/web-test-automation-in-lambda-way/","publishdate":"2021-10-09T10:15:58+08:00","relpermalink":"/post/web-test-automation-in-lambda-way/","section":"post","summary":"aws客户端环境准备 git clone pip install awscli https://github.com/wubigo/API.git python API/python/aws/aws.py cp API/python/aws/cred.json ~/.aws/credentials cp API/python/aws/config ~/.aws/config  创建函数部署包 mkdir lambda_web wget https://github.com/wubigo/API/blob/master/nodejs/lambda/aws/index.js -P lambda_web zip -r webdriver.zip lambda_web/*  配置 policy.json\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: [\u0026quot;lambda.amazonaws.com\u0026quot;, \u0026quot;s3.amazonaws.com\u0026quot;] }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] }  export ACCOUNT_ID=820934811997 aws iam create-role --role-name lambda-s3 --assume-role-policy-document file://policy.json aws iam attach-role-policy --role-name lambda-s3 --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole aws iam attach-role-policy --role-name lambda-s3 --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess  复制 aws lambda create-function --function-name webdriver --runtime nodejs12.","tags":["WEBDRIVER","LAMBDA"],"title":"Web Test Automation in Lambda Way","type":"post"},{"authors":null,"categories":[],"content":" HDFS The maximum number of files in HDFS depends on two things:\n total storage space in the cluster\n the heap size of the NameNode.\n  1) You can find out what percentage of storage has been used from HDFS NameNode UI.\n2) The basic rule of thumb is that 1 GB heap is needed for every million of files.\nEach file object and each block object takes about 150 bytes of the memory. For example, if you have 10 million files and each file has 1 one block each, then you would need about 3GB of memory for the NameNode.\nIf I had 10 million files, each using a block, then we would be using: 10 million + 10 million = 20 million * 150 = 3,000,000,000 bytes = 3 GB MEMORY. Keep in mind the NameNode will need memory for other processes. So to support 10 million files then your NameNode will need much more than 3GB of memory.\nSo you can increase your max number of files as you go. With increasing heap for every 1 million files.\nJindoFS介绍和使用 JindoFS提供兼容对象存储的纯客户端模式（SDK）和缓存模式（Cache）， 以支持与优化Hadoop和Spark生态大数据计算对OSS的访问； 提供块存储模式（Block），以充分利用OSS的海量存储能力和优化文件系统元数据的操作。\nQ：Block模式跟HDFS相比，是更好的HDFS？\nA： HDFS的常规限制为4亿，而Block模式元数据规模上支撑10亿以上的文件数，大于HDFS的限制，而且在集群高峰期时性能更为稳定。 HDFS有Java onheap限制，而Block模式没有Java onheap和内存限制，可以支持更大的数据规模。 Block模式轻运维，不用担心坏盘或坏节点，数据1备份放置在OSS上，支持上下线节点。 支持对冷数据做透明压缩和归档，使用多种手段进行成本优化，对接对象存储，支持EB级数据规模。 Block模式支持HDFS的一些重要特性。例如，HDFS AuditLog、Ranger集成和数据加密。\nQ：Block模式具有哪些特别的优势？\nA： Block模式可以管理文件元数据和组织文件数据，因此可以不局限于OSS对象存储，完全可以满足各种大数据引擎对存储接口的需求。这些接口包括但不限于Rename的原子性和事务性能力、高性能本地写入、透明压缩、truncate、append、flush、sync和snapshot等。这些高阶存储接口对实现完整的POSIX和对接更多的大数据引擎到OSS是不可或缺的，例如，Flink、HBase、Kafka和Kudu。其他两种方式使用OSS也可以对接部分接口，但是能力和优势会有所不足。 Block模式在费用上优于其他两种方式使用OSS。Block模式中，因为全部数据中占比60%的温数据和热数据都在本地有缓存备份，大部分读请求都不会通过OSS，所以可以节省一部分费用。\n","date":1633744508,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633744508,"objectID":"b3d727576affc3cc8690f880d41465e3","permalink":"https://wubigo.com/post/maximum-number-files-hdfs/","publishdate":"2021-10-09T09:55:08+08:00","relpermalink":"/post/maximum-number-files-hdfs/","section":"post","summary":"HDFS The maximum number of files in HDFS depends on two things:\n total storage space in the cluster\n the heap size of the NameNode.\n  1) You can find out what percentage of storage has been used from HDFS NameNode UI.\n2) The basic rule of thumb is that 1 GB heap is needed for every million of files.\nEach file object and each block object takes about 150 bytes of the memory.","tags":["HDFS"],"title":"Maximum Number Files Hdfs","type":"post"},{"authors":null,"categories":[],"content":" 用户代理 mobile devices browsing the web often see a pared-down ver‐ sion of sites, lacking banner ads, Flash, and other distractions. If you try changing your User-Agent to something like the following, you might find that sites get a little easier to scrape!\nUser-Agent:Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53  scrapy architecture The data flow in Scrapy is controlled by the execution engine, and goes like this:\n The Engine gets the initial Requests to crawl from the Spider.\n The Engine schedules the Requests in the Scheduler and asks for the next Requests to crawl.\n The Scheduler returns the next Requests to the Engine.\n The Engine sends the Requests to the Downloader, passing through the Downloader Middlewares (see process_request()).\n Once the page finishes downloading the Downloader generates a Response (with that page) and sends it to the Engine, passing through the Downloader Middlewares (see process_response()).\n The Engine receives the Response from the Downloader and sends it to the Spider for processing, passing through the Spider Middleware (see process_spider_input()).\n The Spider processes the Response and returns scraped items and new Requests (to follow) to the Engine, passing through the Spider Middleware (see process_spider_output()).\n The Engine sends processed items to Item Pipelines, then send processed Requests to the Scheduler and asks for possible next Requests to crawl.\n The process repeats (from step 1) until there are no more requests from the Scheduler.\n  check cookie http://www.editthiscookie.com/\nCSS vs XPATH    Goal CSS XPath     All Elements * //*   All P Elements p //p   All Child Elements p \u0026gt; * //p/*   Element By ID #foo //*[@id=’foo’]   Element By Class .foo //*[contains(@class,’foo’)] 1   Element With Attribute *[title] //*[@title]   First Child of All P p \u0026gt; *:first-child //p/*[0]   All P with an A child Not possible //p[a]   Next Element p + * //p/following-sibling::*[0]    CSS selectors are better to use when dealing with classes, IDs and tag names. They are shorter and easier to read\nUse CSS Selectors for doing simple queries based on the attributes of the element. CSS Selectors tend to perform better, faster and more reliable than XPath in most browsers\nref https://cloud.tencent.com/developer/article/1050304\n","date":1632790623,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632790623,"objectID":"e04f34d01e41ef6f78c9f28fa89dc98d","permalink":"https://wubigo.com/post/web-scrapy/","publishdate":"2021-09-28T08:57:03+08:00","relpermalink":"/post/web-scrapy/","section":"post","summary":"用户代理 mobile devices browsing the web often see a pared-down ver‐ sion of sites, lacking banner ads, Flash, and other distractions. If you try changing your User-Agent to something like the following, you might find that sites get a little easier to scrape!\nUser-Agent:Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53  scrapy architecture The data flow in Scrapy is controlled by the execution engine, and goes like this:","tags":["BIGDATA","DATALAKE"],"title":"Web Scrapy","type":"post"},{"authors":null,"categories":[],"content":" Replication Method Replication Method - Replication method to use for extracting data from the database. STANDARD replication requires no setup on the DB side but will not be able to represent deletions incrementally. CDC uses the Binlog to detect inserts, updates, and deletes. This needs to be configured on the source database itself.\nS3 Support in Apache Hadoop Apache Hadoop ships with a connector to S3 called \u0026ldquo;S3A\u0026rdquo;, with the url prefix \u0026ldquo;s3a:\u0026ldquo;; its previous connectors \u0026ldquo;s3\u0026rdquo;, and \u0026ldquo;s3n\u0026rdquo; are deprecated and/or deleted from recent Hadoop versions.\nAmazon\u0026rsquo;s EMR Service is based upon Apache Hadoop, but contains modifications and their own closed-source S3 client\nImportant: you need a consistency layer to use Amazon S3 as a destination of MapReduce, Spark and Hive work You cannot use any of the S3 filesystem clients as a drop-in replacement for HDFS. Amazon S3 is an \u0026ldquo;object store\u0026rdquo; with\n Eventual consistency: changes made by one application (creation, updates and deletions) will not be visible until some undefined time. Non-atomic rename and delete operations. Renaming or deleting large directories takes time proportional to the number of entries -and visible to other processes during this time, and indeed, until the eventual consistency has been resolved. This breaks the commit protocol used by all these applications to safely commit the output of multiple tasks within a job. Hadoop 3.x ships with S3Guard for consistency, and the S3A Committers for committing work.  统一元数据 统一的元数据管理，可以实现：\n持久化的元数据存储 支持统一元数据之前，元数据都是在集群内部的MySQL数据库，元数据会随着集群的释放而丢失，特别是EMR提供了灵活按量模式，集群可以按需创建用完就释放。如果您需要保留现有的元数据信息，必须登录集群手动将元数据信息导出。支持统一元数据之后，释放集群不会清理元数据信息。所以，在任何时候删除OSS上或者集群HDFS上数据（包括释放集群操作）的时候，需要先确认该数据对应的元数据已经删除（即要删掉数据对应的表和数据库），否则元数据库中可能出现一些脏数据。\n计算存储分离 EMR上可以支持将数据存放在阿里云OSS中，在大数据量的情况下将数据存储在OSS上会大大降低使用的成本，EMR集群主要用来作为计算资源，在计算完成之后可以随时释放，数据在OSS上，同时也不用再考虑元数据迁移的问题。\n数据共享 使用统一的元数据库，如果您的所有数据都存放在OSS之上，则不需要做任何元数据的迁移和重建，所有集群都是可以直接访问数据，这样每个EMR集群可以做不同的业务，但是可以很方便地实现数据的共享。\n","date":1632447142,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632447142,"objectID":"dcbcf596e1f26bdee8d3315ec34a97ab","permalink":"https://wubigo.com/post/lakehouse-notes/","publishdate":"2021-09-24T09:32:22+08:00","relpermalink":"/post/lakehouse-notes/","section":"post","summary":"Replication Method Replication Method - Replication method to use for extracting data from the database. STANDARD replication requires no setup on the DB side but will not be able to represent deletions incrementally. CDC uses the Binlog to detect inserts, updates, and deletes. This needs to be configured on the source database itself.\nS3 Support in Apache Hadoop Apache Hadoop ships with a connector to S3 called \u0026ldquo;S3A\u0026rdquo;, with the url prefix \u0026ldquo;s3a:\u0026ldquo;; its previous connectors \u0026ldquo;s3\u0026rdquo;, and \u0026ldquo;s3n\u0026rdquo; are deprecated and/or deleted from recent Hadoop versions.","tags":["DW","DATALAKE"],"title":"Lakehouse Notes","type":"post"},{"authors":null,"categories":[],"content":" 打开阅读模式 Open a new tab and enter chrome://flags/#enable-reader-mode to jump directly to the Reader Mode Flag.\n使用 Reader Mode in Chrome is mad-easy to use. When you’re on a page that you’d like to push into the reader view, click on the three-dot menu button in the upper right, and then choose “Distill page.”\n","date":1629930271,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629930271,"objectID":"bd301f092243fbaf3cac78ec5b7a3680","permalink":"https://wubigo.com/post/chrome-read-mode/","publishdate":"2021-08-26T06:24:31+08:00","relpermalink":"/post/chrome-read-mode/","section":"post","summary":"打开阅读模式 Open a new tab and enter chrome://flags/#enable-reader-mode to jump directly to the Reader Mode Flag.\n使用 Reader Mode in Chrome is mad-easy to use. When you’re on a page that you’d like to push into the reader view, click on the three-dot menu button in the upper right, and then choose “Distill page.”","tags":[],"title":"Chrome Read Mode","type":"post"},{"authors":null,"categories":[],"content":" 升级YARN npm install -g yarn@1.22.11 \u0026gt; yarn@1.22.11 preinstall C:\\local\\node-v14.17.5-win-x64\\node_modules\\yarn \u0026gt; :; (node ./preinstall.js \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true) C:\\local\\node-v14.17.5-win-x64\\yarn -\u0026gt; C:\\local\\node-v14.17.5-win-x64\\node_modules\\yarn\\bin\\yarn.js C:\\local\\node-v14.17.5-win-x64\\yarnpkg -\u0026gt; C:\\local\\node-v14.17.5-win-x64\\node_modules\\yarn\\bin\\yarn.js + yarn@1.22.11 updated 1 package in 0.7s  yarn link /home/ubuntu/.config/yarn/link/matrix-js-sdk\nubuntu@ip-172-31-44-135:~/.config/yarn/link$ ll total 8 drwxrwxr-x 2 ubuntu ubuntu 4096 Aug 19 09:31 ./ drwxrwxr-x 3 ubuntu ubuntu 4096 Aug 19 07:25 ../ lrwxrwxrwx 1 ubuntu ubuntu 22 Aug 19 09:31 matrix-js-sdk -\u0026gt; ../../../matrix-js-sdk/ lrwxrwxrwx 1 ubuntu ubuntu 25 Aug 19 07:25 matrix-react-sdk -\u0026gt; ../../../matrix-react-sdk/  创建应用 yarn create react-app matrix yarn start  C:\\Users\\bigo\\AppData\\Local\\Yarn\\Data\\global\\node_modules\\.bin\\create-react-app\n/home/ubuntu/.config/yarn/global/node_modules/.bin/create-react-app\n","date":1629465584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629465584,"objectID":"fc3a17da2b4c57d26c5bbde08abcf83e","permalink":"https://wubigo.com/post/yarn2-note/","publishdate":"2021-08-20T21:19:44+08:00","relpermalink":"/post/yarn2-note/","section":"post","summary":"升级YARN npm install -g yarn@1.22.11 \u0026gt; yarn@1.22.11 preinstall C:\\local\\node-v14.17.5-win-x64\\node_modules\\yarn \u0026gt; :; (node ./preinstall.js \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true) C:\\local\\node-v14.17.5-win-x64\\yarn -\u0026gt; C:\\local\\node-v14.17.5-win-x64\\node_modules\\yarn\\bin\\yarn.js C:\\local\\node-v14.17.5-win-x64\\yarnpkg -\u0026gt; C:\\local\\node-v14.17.5-win-x64\\node_modules\\yarn\\bin\\yarn.js + yarn@1.22.11 updated 1 package in 0.7s  yarn link /home/ubuntu/.config/yarn/link/matrix-js-sdk\nubuntu@ip-172-31-44-135:~/.config/yarn/link$ ll total 8 drwxrwxr-x 2 ubuntu ubuntu 4096 Aug 19 09:31 ./ drwxrwxr-x 3 ubuntu ubuntu 4096 Aug 19 07:25 ../ lrwxrwxrwx 1 ubuntu ubuntu 22 Aug 19 09:31 matrix-js-sdk -\u0026gt; ../../../matrix-js-sdk/ lrwxrwxrwx 1 ubuntu ubuntu 25 Aug 19 07:25 matrix-react-sdk -\u0026gt; .","tags":[],"title":"YARN2 Note","type":"post"},{"authors":null,"categories":[],"content":" 准备环境 node -v v14.17.5 npm install -g yarn@1.22.11  链接SDK git clone https://github.com/matrix-org/matrix-js-sdk.git pushd matrix-js-sdk yarn link yarn install popd git clone https://github.com/matrix-org/matrix-react-sdk.git pushd matrix-react-sdk yarn link yarn link matrix-js-sdk yarn install popd git clone https://github.com/vector-im/element-web.git cd element-web yarn link matrix-js-sdk yarn link matrix-react-sdk yarn install yarn reskindex yarn start  启动 cp config.sample.json config.json curl http://127.0.0.1:8080/  登录 homeserver：http://192.168.43.16:8008/\n","date":1629355997,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629355997,"objectID":"f5a96b8ea2f8ac3c77ea49c610f464bb","permalink":"https://wubigo.com/post/matrix-web-client-dev-setup/","publishdate":"2021-08-19T14:53:17+08:00","relpermalink":"/post/matrix-web-client-dev-setup/","section":"post","summary":"准备环境 node -v v14.17.5 npm install -g yarn@1.22.11  链接SDK git clone https://github.com/matrix-org/matrix-js-sdk.git pushd matrix-js-sdk yarn link yarn install popd git clone https://github.com/matrix-org/matrix-react-sdk.git pushd matrix-react-sdk yarn link yarn link matrix-js-sdk yarn install popd git clone https://github.com/vector-im/element-web.git cd element-web yarn link matrix-js-sdk yarn link matrix-react-sdk yarn install yarn reskindex yarn start  启动 cp config.sample.json config.json curl http://127.0.0.1:8080/  登录 homeserver：http://192.168.43.16:8008/","tags":["IM"],"title":"Matrix Web Client Dev Setup","type":"post"},{"authors":null,"categories":[],"content":"The matrix specification does not enforce how users register with a server. It just specifies the URL path and absolute minimum keys. The reference homeserver uses a username/password to authenticate user, but other homeservers may use different methods. This is why you need to specify the type of method\n","date":1629279977,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629279977,"objectID":"d145cf920d56d8cbb5050d249d057e8d","permalink":"https://wubigo.com/post/matrix-encrypted-communication/","publishdate":"2021-08-18T17:46:17+08:00","relpermalink":"/post/matrix-encrypted-communication/","section":"post","summary":"The matrix specification does not enforce how users register with a server. It just specifies the URL path and absolute minimum keys. The reference homeserver uses a username/password to authenticate user, but other homeservers may use different methods. This is why you need to specify the type of method","tags":[],"title":"Matrix Encrypted Communication","type":"post"},{"authors":null,"categories":[],"content":" Matrix Traditionally Matrix decentralises communication by replicating conversation history over a mesh of servers, so that no single server has ownership of a given conversation. Meanwhile, users connect to their given homeserver from clients via plain HTTPS + DNS. This has the significant disadvantage that for a user to have full control and ownership over their communication, they need to run their own server - which comes with a cost, and requires you to be a proficient sysadmin. In order to fully democratise communication and eliminate a compulsory dependency on a homeserver, we\u0026rsquo;ve started seriously working on making Matrix run as a P2P protocol by compiling homeservers to run clientside and using P2P transports such as libp2p - while seamlessly supporting all existing Matrix clients (e.g. Riot.im), bots and bridges with negligible changes. This work includes:\nCompiling Matrix homeservers (e.g. Dendrite) to efficiently run clientside Layering HTTPS over P2P transports such as libp2p (e.g. https://github.com/matrix-org/libp2p-proxy) Switching Matrix identifiers from @user:domain tuples to be Curve25519 public keys (MSC1228) Decentralising accounts so they can be hosted concurrently on multiple nodes (e.g. a mix of server-side and client-side homeservers) Experimenting with node discovery from DNS to DHTs and other mechanisms (e.g. gossip mechanisms) Experimenting with smarter bandwidth-efficient routing algorithms than full-mesh (e.g. combinations of spanning trees, overlapping spanning trees, gossip mechanisms) Making Matrix\u0026rsquo;s low-bandwidth CoAP transport production grade Experimenting with metadata-protecting relay mechanisms rather than using full homeservers for server-side relaying.\nhttps://archive.fosdem.org/2020/schedule/event/dip_p2p_matrix/\nBluesky https://gitlab.com/bluesky-community1/decentralized-ecosystem\n","date":1629273330,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629273330,"objectID":"baf4e7dfadca6ea843ac68df1db6087a","permalink":"https://wubigo.com/post/decentralized-social-network-federated-vs-p2p-protocol/","publishdate":"2021-08-18T15:55:30+08:00","relpermalink":"/post/decentralized-social-network-federated-vs-p2p-protocol/","section":"post","summary":"Matrix Traditionally Matrix decentralises communication by replicating conversation history over a mesh of servers, so that no single server has ownership of a given conversation. Meanwhile, users connect to their given homeserver from clients via plain HTTPS + DNS. This has the significant disadvantage that for a user to have full control and ownership over their communication, they need to run their own server - which comes with a cost, and requires you to be a proficient sysadmin.","tags":["IM"],"title":"Decentralized Social Network: Federated VS P2P Protocol","type":"post"},{"authors":null,"categories":[],"content":" 软件项目开发时间线很难做到精确的评估，但在项目管理的时候，\n我们必须要提供这个数字。\n重要特征：时间和不确定性 一个只包含时间的评估隐含了一定程度的不确定性。 如果你告诉我一个任务需要花10天时间，\n我会假设进度在控制之中。但如果你告诉我一个任务花10到15天，那这个期限就有很高的风险。\n项目工期评估技巧  任务分解 评估不确定性 分别做最好和最坏情况的评估 任务跟踪并优化  任务分解    复杂度 时间(人天)     小 1   中等 3   大 5   特别大 10    不用追求一步到位完成工时评估， 开始的时候做粗略的评估，\n该评估可能包含一些复杂度比较大的任务。\n评估不确定性 20-30人日与5-45人日的工时评估是完全不同的，即便两者的中位数都是25人天\n   不确定性级别 乘数效应     低 1.1   中等 1.5   大 2   特别大 5    计算工时    任务 复杂度 不确定性 期望工时 最坏情况     计量支付私有化部署 低 大 20 40    这个任务预计20天完成，最坏情况需求40天。或者工时可以表示为20-40人日\n评估跟踪 最后一定要跟踪实际完成时间\n   任务 复杂度 不确定性 期望工时 最坏情况 实际工时     计量支付私有化部署 低 大 20 40 35    ","date":1625187473,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625187473,"objectID":"886c20cdf840e683d9b54e54ecbf3b1b","permalink":"https://wubigo.com/post/software-estimation/","publishdate":"2021-07-02T08:57:53+08:00","relpermalink":"/post/software-estimation/","section":"post","summary":"软件项目开发时间线很难做到精确的评估，但在项目管理的时候，\n我们必须要提供这个数字。\n重要特征：时间和不确定性 一个只包含时间的评估隐含了一定程度的不确定性。 如果你告诉我一个任务需要花10天时间，\n我会假设进度在控制之中。但如果你告诉我一个任务花10到15天，那这个期限就有很高的风险。\n项目工期评估技巧  任务分解 评估不确定性 分别做最好和最坏情况的评估 任务跟踪并优化  任务分解    复杂度 时间(人天)     小 1   中等 3   大 5   特别大 10    不用追求一步到位完成工时评估， 开始的时候做粗略的评估，\n该评估可能包含一些复杂度比较大的任务。\n评估不确定性 20-30人日与5-45人日的工时评估是完全不同的，即便两者的中位数都是25人天\n   不确定性级别 乘数效应     低 1.1   中等 1.5   大 2   特别大 5    计算工时    任务 复杂度 不确定性 期望工时 最坏情况     计量支付私有化部署 低 大 20 40    这个任务预计20天完成，最坏情况需求40天。或者工时可以表示为20-40人日","tags":[],"title":"软件产品开发工时评估技巧","type":"post"},{"authors":null,"categories":[],"content":" 应用优化 Eliminating unnecessary data transfers is, of course, the single best optimization—e.g., eliminating unnecessary resources or ensuring that the minimum number of bits is transferred by applying the appropriate compression algorithm. Following that, locating the bits closer to the client, by geo-distributing servers around the world—e.g., using a CDN—will help reduce latency of network roundtrips and significantly improve TCP performance. Finally, where possible, existing TCP connections should be reused to minimize overhead imposed by slow-start and other congestion mechanisms\n No bit is faster than one that is not sent; send fewer bits. We can’t make the bits travel faster, but we can move the bits closer. TCP connection reuse is critical to improve performance.  TCP性能优化检查清单 • Upgrade server kernel to latest version (Linux: 3.2+). • Ensure that cwnd size is set to 10. • Disable slow-start after idle. • Ensure that window scaling is enabled. • Eliminate redundant data transfers. • Compress transferred data. • Position servers closer to the user to reduce roundtrip times. • Reuse established TCP connections whenever possible.\nUDP性能优化检查清单 • Application must tolerate a wide range of Internet path conditions. • Application should control rate of transmission. • Application should perform congestion control over all traffic. • Application should use bandwidth similar to TCP. • Application should back off retransmission counters following loss. • Application should not send datagrams that exceed path MTU. • Application should handle datagram loss, duplication, and reordering. • Application should be robust to delivery delays up to 2 minutes. • Application should enable IPv4 UDP checksum, and must enable IPv6 checksum. • Application may use keepalives when needed (minimum interval 15 seconds).\nTLS性能优化检查清单 • Get best performance from TCP. • Upgrade TLS libraries to latest release, and (re)build servers against them. • Enable and configure session caching and stateless resumption. • Monitor your session caching hit rates and adjust configuration accordingly. • Terminate TLS sessions closer to the user to minimize roundtrip latencies. • Configure your TLS record size to fit into a single TCP segment. • Ensure that your certificate chain does not overflow the initial congestion window. • Remove unnecessary certificates from your chain; minimize the depth. • Disable TLS compression on your server. • Configure SNI support on your server. • Configure OCSP stapling on your server. • Append HTTP Strict Transport Security header.\n","date":1622933784,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622933784,"objectID":"f9089e130b722bd9f2d5492c20f12d8e","permalink":"https://wubigo.com/post/high-performance-browser-networking/","publishdate":"2021-06-06T06:56:24+08:00","relpermalink":"/post/high-performance-browser-networking/","section":"post","summary":"应用优化 Eliminating unnecessary data transfers is, of course, the single best optimization—e.g., eliminating unnecessary resources or ensuring that the minimum number of bits is transferred by applying the appropriate compression algorithm. Following that, locating the bits closer to the client, by geo-distributing servers around the world—e.g., using a CDN—will help reduce latency of network roundtrips and significantly improve TCP performance. Finally, where possible, existing TCP connections should be reused to minimize overhead imposed by slow-start and other congestion mechanisms","tags":[],"title":"Web性能优化指南","type":"post"},{"authors":null,"categories":[],"content":"招聘的出发点是发现我们的弱点，而不是寻找天才。\n合理设计的招聘的流程是寻找到合适的人来补齐团队的短板，而不是最好的人。\n“我们寻找天才”，“候选人不到我们的标准”， 我们设计“公正”的测试去客观的\n评估求职者是否与我们认为的“优秀”匹配。\n   寻找天才 补齐短板     寻找最优秀的人 寻找能释放我们潜能的人   能做我们正在做的人 能做我们做不到的人   与我们认为的优秀匹配的人 拥有我们欠缺的能力的人   统一的面试流程 团队挑选候选人   给候选者打分 与候选者协作   文化匹配 欠缺的部分    ","date":1620893610,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620893610,"objectID":"4b9dbbfbd3acb49b7154a5f655b6ce55","permalink":"https://wubigo.com/post/hire-for-weaknesses/","publishdate":"2021-05-13T16:13:30+08:00","relpermalink":"/post/hire-for-weaknesses/","section":"post","summary":"招聘的出发点是发现我们的弱点，而不是寻找天才。\n合理设计的招聘的流程是寻找到合适的人来补齐团队的短板，而不是最好的人。\n“我们寻找天才”，“候选人不到我们的标准”， 我们设计“公正”的测试去客观的\n评估求职者是否与我们认为的“优秀”匹配。\n   寻找天才 补齐短板     寻找最优秀的人 寻找能释放我们潜能的人   能做我们正在做的人 能做我们做不到的人   与我们认为的优秀匹配的人 拥有我们欠缺的能力的人   统一的面试流程 团队挑选候选人   给候选者打分 与候选者协作   文化匹配 欠缺的部分    ","tags":["STARTUP"],"title":"招聘的出发点是补齐短板","type":"post"},{"authors":null,"categories":[],"content":"A secure messaging system is a system where no one but the user and their intended recipients can read the messages or otherwise analyze their contents to infer what they are talking about. Despite messages passing through a server, an end-to-end encrypted message will not allow the server to know the contents of a message. When that same server has a channel for revealing information about the contents of a significant portion of messages, that’s not end-to-end encryption\n","date":1620257779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620257779,"objectID":"2a26d2450a3cd390cfc59c80dbf8c6d2","permalink":"https://wubigo.com/post/secure-messaging-system/","publishdate":"2021-05-06T07:36:19+08:00","relpermalink":"/post/secure-messaging-system/","section":"post","summary":"A secure messaging system is a system where no one but the user and their intended recipients can read the messages or otherwise analyze their contents to infer what they are talking about. Despite messages passing through a server, an end-to-end encrypted message will not allow the server to know the contents of a message. When that same server has a channel for revealing information about the contents of a significant portion of messages, that’s not end-to-end encryption","tags":["IM","P2P"],"title":"Secure Messaging System","type":"post"},{"authors":null,"categories":[],"content":" 摄像头的区别 USB vs RTSP vs ONVIF If we use USB cameras we do not need the camera url and some functions like moving, zooming etc. will not be available. In the case of RTSP cameras and ONVIF cameras there are more functions available. The difference between the RTSP camera and the ONVIF camera is the following: the RTSP camera has only one stream in comparsion with the ONVIF camera having multiple streams at a time\n安装cam2ip https://github.com/gen2brain/cam2ip\n下载OPENCV版 https://github.com/gen2brain/cam2ip/releases/download/1.6/cam2ip-1.6-64bit-cv2.zip\n运行\ncmd:\\\u0026gt;cam2ip-1.6-64bit-cv\\cam2ip Listening on :56000  打开浏览器：\nhttp://localhost:56000/\nhtml jpeg mjpeg\nchrome/inspect/network/img\ndata:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/  display Base64 images in HTML \u0026lt;div\u0026gt; \u0026lt;img src=\u0026quot;data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAAAAUA AAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO 9TXL0Y4OHwAAAABJRU5ErkJggg==\u0026quot; alt=\u0026quot;Red dot\u0026quot; /\u0026gt; \u0026lt;/div\u0026gt;  使用anycam管理IP摄像头 通过RTSP方式管理webcam:\n输入：localhost:56000\n连接方式：MJPEG\nhttp://localhost:56000/mjpeg\n参考 [1]turn-your-old-rtsp-ip-camera-into-an-onvif-ip-webcam\n[2]Turn any webcam into an IP camera\n","date":1615426480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615426480,"objectID":"1f3fc2d3cac482f470eb4a0366dfe1eb","permalink":"https://wubigo.com/post/webcam-to-ip-camera/","publishdate":"2021-03-11T09:34:40+08:00","relpermalink":"/post/webcam-to-ip-camera/","section":"post","summary":"摄像头的区别 USB vs RTSP vs ONVIF If we use USB cameras we do not need the camera url and some functions like moving, zooming etc. will not be available. In the case of RTSP cameras and ONVIF cameras there are more functions available. The difference between the RTSP camera and the ONVIF camera is the following: the RTSP camera has only one stream in comparsion with the ONVIF camera having multiple streams at a time","tags":["IIOT","RTSP","ONVIF","CV","IOT"],"title":"Webcam to IP Camera","type":"post"},{"authors":null,"categories":[],"content":" Once the face is detected, the AI then provides the information on\nits size, pose, and location\nThe state-of-the-art face detection software uses pattern detection technology.\nNo personal data is collected, and no images are stored.\n人脸检测基本方法 #Import Libraries #Import Classifier for Face and Eye Detection #Convert Image to Grayscale #Give coordinates to detect face and eyes location from ROI #Webcam setup for Face Detection #When everything is done, release the capture\n","date":1611403126,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611403126,"objectID":"12e4059257a447a3e6427ce1523ef2a8","permalink":"https://wubigo.com/post/face-detection-notes/","publishdate":"2021-01-23T19:58:46+08:00","relpermalink":"/post/face-detection-notes/","section":"post","summary":"Once the face is detected, the AI then provides the information on\nits size, pose, and location\nThe state-of-the-art face detection software uses pattern detection technology.\nNo personal data is collected, and no images are stored.\n人脸检测基本方法 #Import Libraries #Import Classifier for Face and Eye Detection #Convert Image to Grayscale #Give coordinates to detect face and eyes location from ROI #Webcam setup for Face Detection #When everything is done, release the capture","tags":["AI","CV"],"title":"面部生物识别学习笔记","type":"post"},{"authors":null,"categories":[],"content":"2020年12月31日20:30，武汉光谷国际网球中心，罗振宇“时间的朋友”跨年演讲如约而至。\n今年的演讲主题是——“长大以后”。\n第一部分：开场\n— 1 —\n有一件事，在我心里憋了半年，跟谁都没说。因为我一定要把它带到2020年12月31号的这个晚上，在这里，讲给武汉人听。\n你看，这里有一堆糖果，但这不是一堆普通的糖果。过去的很多年里，这堆糖果曾经出现在世界各地的很多著名展览馆里。它是一个艺术作品。创造这个作品的人，叫冈萨雷斯。\n糖果的重量不多不少，正好是79.4公斤。这是他的爱人生前的体重。在人生最黯淡的时刻，艺术家冈萨雷斯，选择用一种特殊的方式，来纪念他的爱人。\n糖果的周围，没有护栏。来参观的人，可以随意拿走一些做纪念，吃了也行，带走也行。\n但是，每一天的展览结束，冈萨雷斯都会给这堆糖果重新称重，然后补齐被拿走的部分。就这样日复一日，从不间断。这堆糖果的重量，始终是79.4公斤。\n生命是那么甜蜜，就像糖果。但生命终会流逝，就像糖果总会被人拿走。\n但最重要的是不管发生什么，爱他的人，总会让他一次次重生。\n这个故事像极了过去一年间武汉人所经历的那些美好、流逝和重生，也见证了爱的力量。在这里，向你们，向1500万武汉人致敬。\n— 2 —\n经过了这一年，我有一个最大的感慨，三个字：不容易。\n我们所有人都知道，能聚在这儿有多么的不容易。可是你们并不知道一个秘密：在此之前，我们为跨年演讲准备了一个后手，一个B计划。\n在整个2020年的上半年，我的朋友们都在替我操心，如果到年底，大型场馆活动仍然不能开放，你的跨年演讲还办不办？怎么办？是不是一个人拿一手机直播就算了？\n我从一开始就告诉他们：办，肯定得办，而且肯定还是在体育馆里办。哪怕我一个人面对空无一人的体育馆讲完全场，也要办。因为今年我们太需要一点确定性了。\n我们同事说，为了避免我一个人站在场里太孤独、太悲壮，可以在现场每个座位上摆一棵金橘树。\n连这些树的名字都起好了，去年我们不是说了一个词，叫“躬身入局”吗？这些金橘树，就叫“躬身入局橘”。\n还好，今天还是有些人能来到这里，不是金橘树。欢迎你们。\n— 3 —\n2020年，有多少个时刻，我们不得不去设想各种各样的可能性，做各种各样的思想准备。我们终于感受到了什么叫压力测试，什么叫留后手，什么叫底线思维。\n我们想不到，我们买一只口罩都曾经那么难。\n华为想不到，有人会把事情做得那么绝，买一颗芯片都那么难。留学海外的孩子想不到，回一趟家居然那么难。\n家长想不到，今年的整整一个学期，孩子去学校上个学居然会那么难。\n中国的电影院想不到，开个门会那么难。他们更想不到的是，在这种情况下，中国居然还成为了全球第一大票房市场。\n日本人想不到，这个夏天会那么难。2020年东京奥运会居然要推迟到2021年，而且可能还叫2020东京奥运会。这是奥运史上的第一次，多么魔幻的一次。\n美国人巴菲特想不到，炒个股会那么难，活了89年，遇上了美股10天4次熔断。\n过去这一年，这张见证历史的清单，我们还可以列很长。\n世界从此不一样了。今年，我们无数次凝视深渊，也无数次被深渊凝视。\n我下面这段话，不带任何情绪，但是今天开场，必须把它摆到桌面上。这是2020年中国人感受到的一重很重要的事实——\n这40多年来，他们说全球化好，那我们就改革开放，一步步拥抱全球化。然后，他们说要脱钩。\n他们说要产业升级，我们以世界工厂的姿态承接了那些转出的产能。然后，他们说我们抢了他们的工作。\n他们说中国人不创新，老跟在他们后面学，我们努力了很久，真创新了。然后，他们说我们有威胁，不卖芯片给我们。\n这就好比，他们三缺一，叫我去陪他们打麻将，我认认真真打，也就刚和了两把，他们说他们三个人就要改玩斗地主，又不让我上桌了。\n这到底是为什么？\n— 4 —\n原因其实就四个字，我们拿来作为今年演讲的主题：\n长大以后。\n中国现在有多大？我们以全世界五分之一的人口，生产了全世界53%的钢铁，57%的水泥，71%的彩电，76%的光伏板，78%的空调，86%的微波炉，88%的手机，90%的电脑。这么大的体量，确实很难让别人再用40年前的眼光来看我们。\n长大以后意味着什么？每个人其实都有体验。\n今天我在光谷，在武汉，说一个我自己的经历吧。\n整整30年前，1990年，父母送我来武汉上大学。就在距离我们演讲现场5公里外的华中科技大学，当天晚上，我们一家三口，就住在学校的招待所。\n我爸告诉我：“明天你就要正式报到了，你就是大学生了。打明儿起，社会不再把你看成是孩子了。今天，你说错了一句话，我们可以出面，说一句孩子还小，不懂事。明天，你成了大学生了，你说错了话，那你就是说错了，你要自己想办法去负责。”\n当时我同时有两种感觉，一种是说不清什么理由的豪迈；另一种是很明显的恐慌。我还是那个我，但是别人对我的期待，对我的态度，一夜之间就完全不同了——原因只是因为长大。\n过去我们常说，规模是一切问题的解药。很多问题把规模做大，自然就解决了。\n但是，很多时候，规模也是问题的根源。\n长大了，很多问题都是新的，我们要独自面对。2020年弥漫着一种情绪，就是觉得世界变了，此前的经验没用了。\n就像诺贝尔文学奖得主埃利亚斯·卡内蒂说的：“旧的答案分崩离析，新的答案还没有着落。”\n2020年很多人都熟悉这样的感受，就好像脚下一路走过来的跳板猛然被抽走，面前却是波涛万顷的汹涌大海。\n我倒是觉得，答案比我们想象的要多得多。即使有了这样的2020年，不确定性在增加，但世界的确定性也比我们想象的要多得多。\n— 5 —\n上海有一个投资人叫王益和。我辗转听到了他的故事。他筹到了一笔钱，想搞扶贫。方式很简单，不是直接给农民钱，而是买了20万棵金丝楠木的树苗，免费送给四川深山里的农民，让他们种在房前屋后、山上路边。\n金丝楠木是极其珍贵的木材，但是长得特别慢，需要几十年后才能成材。所以，过去只有人砍，没有人种，所以野生的金丝楠木现在非常罕见。\n王益和把树苗免费送给村民，就一个条件，种下去后，短期内不许卖。村民在房前屋后种上这么100棵，几十年后，怎么也值几十万。这就是村民的绿色银行了。更有意思的是，这几十年的时间，你该干嘛干嘛，不用为这些树操心。\n听到这个故事，我的第一反应是震惊的。这个世界上居然就有这种事，只要你肯等，而且不需要你多做什么，那个长期确定性的收益居然就有。\n好了，我知道刚才讲的金丝楠木已经在你的心里种下了一个问题：咱得琢磨琢磨，是什么类型的事具有这样的特点？不用操心，时间越久，价值越大。\n想来想去，还真有一个地方也是这样。学校给我带来的最大的收益——而且时间越长就看得越清楚的那部分收益——是同学。\n就像我自己，1990年到1994年，在武汉的华中科技大学新闻系上学。当时一看周围，一群灰头土脸的同学，十几岁的毛孩子，“谁的青春不迷茫”。\n但是，毕业之后，将近30年，大家分头奋斗。突然有一天就发现，我的同学们都成长为了各行各业的中坚力量。大家互相帮忙、守望相助的能力都在提高。\n我的同学，他们的成就是他们的奋斗所得，但是我居然就能分享到他们成功带来的光芒，而且这件事还是确定的。当然我的奋斗对他们来说，也是一样的。\n这个效应就用我们在场的所有人都熟悉的那个词来命名吧，就叫“时间的朋友效应”。\n什么是“时间的朋友”？说白了，就是在时光里，我也在长，你也在长，咱们从一开始结下的那一点点缘分也在长。\n而在这个过程中，我们只需要做两件事：第一，大家分头努力，各自向好。这说的不就是同学的关系吗？\n第二，我们彼此认同、守望相助。认同一个东西，就是我们都是同一个共同体中的一份子，这说的不就是母校吗？只要做到这两点，“时间的朋友效应“就会出现。\n— 6 —\n“时间的朋友效应”随处都在，它是如此地普遍，以至于我们常常忽略了它的存在，我们经常低估了我们身边的人，低估了这个时代人才的密度。\n著名的索尔维会议合影，摄于1927年。这张照片里面有爱因斯坦、居里夫人、普朗克、玻尔、薛定谔、海森堡、狄拉克、洛伦兹等等。\n真有这样的时刻啊，这么多顶级的大脑，居然生活在同一个时代，他们可以在一起开会，一起研究，一起争论，一起通信，好幸福啊。\n其实，开会的时候，他们并不会意识到自己身处在一个人类历史上怎样的群体当中。\n这才是身处大历史中的人真实的处境。\n你身边其实充满了确定性，只不过，现在，你还不知道它的样子。\n还记得三十年前，我在武汉上大学的时候，有一次从学校骑自行车去磨山植物园。那是一次雨后，我一个人站在竹林里面，第一次听到万物生长，也就是竹子拔节的声音。\n我第一次意识到，原来生命生长，是有声音的。噼噼啪啪，喧闹得很，我知道，那是长大的声音。\n第二部分：本土时代\n— 1 —\n我这一年，心里一直在搞一场辩论赛。辩论的主题是：中国经济未来会好吗？\n我的观点是，中国未来经济还会持续增长。我的反方\u0026ndash;一个假想的怀疑主义者，说这事儿未必，有点看不清。\n我坚信中国未来会好，原因很简单，我是一个乐观主义者。不是有那么一句话吗？悲观主义是个骗子，它吸引我们的注意力，却不让我们参与改变。\n我心里那个反方，那个怀疑主义者，开始陈述他的理由：说中国规模大、人口多，可现在中国不也在讨论老龄化问题，不也在说人口红利没有了吗？\n你怎么知道未来中国还是全球供应链的中心呢？你怎么知道中国未来会不会被印度代替呢？\n我也很紧张，就请教了战略专家徐弃郁。徐弃郁老师给我看了一份智库报告。\n这是美国的彼得森国际经济研究所2020年10月份做给美国人看的报告。这家智库成立于1981年，在国际贸易和投资问题上是公认的最权威的美国智库之一。\n这份报告指出了一个事实，2008-2018年这10年间，中国制造业份额总体烈火烹油一般地在增长，但确实是有一部分低端制造业在向海外转移。\n那印度接住了这部分转移的产能吗？没有。实际上，同期印度制造业增长的份额连1%都不到。\n— 2 —\n到底是为什么呢？我先讲一个故事。\n有一家很大的中国制造业企业，前几年，派了一个得力干将前往印度建厂。在他们工厂里，有个印度本地的小伙子干得挺好。\n咱们中国厂长赶紧提拔了他。结果出事了，底下人不仅不服管，还经常打骂领导。为什么？\n原因咱们中国人可能想不到\u0026ndash;种姓制度。这个印度小伙子是低种姓，而他的手下有高种姓，根本指挥不动。\n见客户时，这个小伙子甚至不能进五星级酒店，不能和客户同桌吃饭，那这工作还怎么开展？公司只好派他去分管一个边缘部门，但有些印度员工还是意见很大。最终这个小伙子只能黯然离开。\n你看，能不能成为全球供应链中心，不是简单的规模大小、人口多少的问题，还有人的组织方式问题。印度的社会组织方式还没来得及现代化。\n徐弃郁老师说，印度要放弃那些传统真的非常难，它缺乏把自己“嵌入”全球供应链的战略决心，它要在全球供应链中取代中国的可能性微乎其微。\n经济学家何帆老师，今年在他写作《变量3》的过程中，我去请教了他。\n他说，今年他的观察重点，确实也集中在中国社会内部的组织方式。\n何帆老师说一个国家，其实就跟一座桥一样。桥修得越大，跑的车、过的人就越多。\n但问题是，桥越大，桥的内部结构就越重要，桥内部的结构化、组织化水平就决定了它的承载能力。否则，桥的自重都会把它压垮。更别提跑什么车，过什么人了。\n— 3 —\n那中国内部结构到底怎样？中国人是以什么方式组织起来的？我们来看个例子。\n联想的武汉工厂是湖北出口额最大的一家制造工厂。\n武汉封城那天，这家工厂也停产了。但是因为电子设备是疫情期间的战略必需品，所以早在2月底，这家工厂就开始筹备复工复产了。\n其中最关键的环节就是招工。最多的时候，一天招工的人数超过了一千人。到了3月31号，这家工厂万人到岗，开动马力生产。\n回头看这家工厂的故事，有几点事实令我感到非常惊讶：\n第一，电子工厂招的工人，要能认识英文字母，必须得能看懂机器上的英文指示；也必须认识物料的编号，才能避免上错料；还要有一定的数学能力，因为交接班必须要做数据盘点。\n武汉封城期间，居然能招到这么多高素质的工人。厂长齐岳告诉我，在如此短的时间内招到如此多符合条件的工人，这是中国制造业多年的本钱、多年的积淀啊。\n在全中国，曾经干过电子产品组装的成熟工大概有上千万人，仅在湖北一省就有几十万。这在其他国家根本不可想象。联想在海外的工厂都只有千人规模，因为海外的劳动力状况支撑不起武汉这样万人规模的大厂。\n第二，在疫情期间啊，招来这么多人，得给工人做核酸检测。在当时的武汉，这么大规模的核酸检测是没问题的。\n但联想在海外的工厂就千难万难了：找核酸试剂难，找护士难，还得自己协调车和场地，现场遇到问题也不知道怎么解决，发现了发热的员工更不知道怎么处理……\n齐岳厂长跟我感慨，在国外，会感觉一个企业是悬浮于社会的，遇到事只能以个体的方式去解决，缺乏社会支持，寸步难行。\n第三，在武汉，社区防疫措施也非常完善。所以社区和工厂构成了一个没有缺环的防疫链条。\n而联想在印度、墨西哥和巴西的工厂，这一套就行不通。员工在工厂里戴着口罩，一出工厂门就把口罩摘了，工厂内部的防疫措施做得再好也白搭。\n那现在这家工厂怎么样了呢？我能告诉你的是，这家一万多人的工厂，开足马力生产至今，零感染；4月就实现100%满产；6月还上了新的生产线。\n不止这一家工厂，全武汉都这样。一季度GDP掉了40%，二季度解封单季打平，三季度强势转正，前三季度，武汉的GDP在全国的大城市里，排进了全国前10。\n武汉正在从“风暴眼”变成“风向标”。\n— 4 —\n中国社会的任何一个节点都处在其他节点的层层包裹当中。是一种超强的社会组织能力。\n下面，我们就稍微花点时间，一层一层来看这个网络的样子。\n我们先来看一下，它的基层的小网络。中国城市的每个小区都被编织进了一个叫居委会的网络里。\n我给你介绍一位居委会干部，这是武汉市洪山区东湖风景区街道东湖新城社区的陶久娣。\n疫情期间，她手里只有12个人，所在的社区却有12765个居民。\n疫情期间，每天早上一睁眼，这些人吃饭、就医、因为憋在家里搞出来的心理问题等等，都是陶久娣需要面对的问题。\n但陶久娣不是一个人在战斗，她有自己的网络。12个人忙不过来，社区物业管理公司的39人，就可以调过来。\n有72名志愿者也加入了她的网络。再不够呢？还有下沉干部。疫情期间，来了42人，都是政府、国企、事业单位的工作人员。\n居委会之外还有更大的社会网络，基层政府、派出所、周边商业设施等等。\n中国像这样的居委会有多少个？我查到的最近的数，有109620个。还有和居委会同等功能的村委会，有533073个。\n这些居委会和村委会是中国社会最小的组织。疫情期间，全中国每个居委会、村委会都是这样运作的。\n一个居委会，在疫情期间，能承担起这么大责任，不是因为它有多大的权力，而是因为它是一个在中国社会基层随时能看得到的节点。资源能汇聚到它那儿，也能分配得出去。\n— 5 —\n今年，我的朋友，《读库》的创始人六哥张立宪，把库房搬到了江苏省南通市。他经常跟我说，当地政府给他们的服务太好了。\n但是请注意，这个“好”，并不是给钱、给地、给资源，而是利用当地的网络，帮助他们解决问题。解决问题可以没有边界，但是给资源，那是有边界的。\n2020年6月6号，读库新仓库在南通开业大吉。六哥特别喜欢6这个数字，凡事都得赶个6。\n有人开玩笑说，南通开发区的服务这么好，不就是个牌儿吗？能不能帮个忙，发个话，把读库的2号库改成6号啊？\n没想到，一直跑前跑后的南通开发区社会事业局的唐进华局长一下子就严肃了：“我怎么服务都行，但这个园区的产权不是开发区政府，是人家企业的。我得查一下合同，应该是没有权力干涉人家的标牌管理，可不能胡来。”\n六哥当着所有的客人，立即竖大拇指。你看，服务归服务，边界归边界。这是一个多么具体的例子，这就是当代中国地方政府的样子。\n过去我们总以为，政府是掌握着大量的资源，他们拥有权力，在做资源分配。但其实，从这几个例子来看，地方政府更像是网络中的枢纽节点，他们不是在聚积资源分配的权力，而是在培养网络连接的能力。\n— 6 —\n上面说的几个故事，这些现象说明了什么？\n投资家李录给了我一个启发，那就是要重新认识中国的地方政府和企业之间的关系。李录说，中国的地方政府提供的其实是 “总部式服务”。\n“总部式服务”什么意思？你就想，你在中国任何一个城市，想创办一家企业，中国的地方政府就像一个企业的总部那样，为你提供土地、修桥造路、六通一平，组织劳动力，优化税收制度，甚至购买你公司生产出的第一批产品。\n而你需要做的呢？就是把你擅长的活儿干好，把业务做大，给当地多解决就业，多贡献税收。在中国做企业，没有一家是从零开始的，咱们都是在享受这个网络基础设施。\n— 7 —\n回到我们开头提的问题，中国的优势到底在哪里？过去我们提到中国的优势，只是一个空泛的概念：比如说规模大。\n站在中国外部看中国，很容易把中国理解成一个庞大的科层制管理体系，就像一张巨大的树状图。科长被处长管着，处长被局长管着，局长被部长管着。\n但生活在中国，我们知道真实的情况不是这样的。中国是一张被组织起来的网络。这张网络规模庞大、层次丰富、密密匝匝，既有强度，又有弹性。这是我们这代中国人信心和底气的由来。\n更重要的是，它还是一张对外开放的网络。我们是全球网络的枢纽。每年接入到咱们这个枢纽的，是10亿吨铁矿石、5亿吨原油、3亿吨煤。\n而中国2019年向世界输出价值14500亿美元的机电产品、7300亿美元的高新技术产品、1200亿美元的服装、540亿美元的家具、530多亿美元的钢材。\n长大以后，中国拥有的不仅仅是庞大的规模，而且是一个超大规模的网络。\n我们中国人是这么想的：我干的事，我走的路，我要和很多人在一起，我要和伙伴在一起，我要和邻居在一起，我要和世界在一起。中国人跟全球所有人一样，希望经济发展，希望安居乐业。\n如果你觉得网络这个词还有点过于新潮的话，我们其实还有一句老话，2500年来，我们一直都相信这个四个字：吾道不孤。\n第三部分：个人财富\n— 1 —\n2020年，我有一个感受，其实越来越多的人看好国家的未来，但与此同时也有很多人对个人的未来感到担心。\n原因有很多了，2020年个人遇到了很具体的挫折，技术变迁对自己生活方式的威胁等等。\n今年流行一句特别扎心的话，叫“困在系统里”。\n假设你是一个送外卖的小哥。表面看，你骑着自己心爱的小摩托，穿行在城市的大街小巷。你有一个很有古典美感的称呼，叫“骑手”。\n曾经有这么一个外卖平台的招聘广告，说：“假如你不自由，就来送外卖。骑车在春天的风中，很暖和。”\n但实际上，外卖小哥都知道，你能接到什么单，到哪取餐，取什么餐，送到哪，怎么走，都是系统告诉你。胆敢超时一分钟，会面对什么处罚，系统会告诉你。\n而且，处罚不只是你一个人来承当，而是整个团队来承受。你会在何种程度上拖累别人，系统也会告诉你。假如你轻松完成任务，系统会知道，给你的时间，给多了。系统就会自动优化，以后再派单的时候把这个时间扣出来。\n2016年，中国外卖小哥三公里送餐平均时限1小时。到2017年，这个时间变成了45分钟。2018年，38分钟。这就是系统和你的真实关系。\n— 2 —\n其实这样困在数字化系统里的，何止外卖小哥？还记得年初疫情的时候，那些在家办公的日子吗？在家线上办公，意味着更自由了吗？\n不，意味着早上起来，你还没洗漱利落的时候，一个接一个的消息就开始轰炸了，一直到晚上都不会消停。\n其实老板们也不堪其扰，一份文件流转到你手里了，没及时批复，被你耽误了，你的下属都看得见。每个人，包括老板，所有人不仅困在家里，还被困在系统里。\n这个系统叫，数字化。\n让我们再重新打量“数字化”这三个字，过去，数字化代表的是更清晰、更透明、更高效。但是，现在，它已经多了另外一种味道，代表着紧张、压迫、身不由己。\n那么，哪副面孔才是数字化的真实面孔？金融学者香帅就回答了这个问题。\n第一，数字化趋势不可逆，而且被疫情大大提前了，至少加速了6年时间。\n第二，数字化的未来，是一个分化的未来。\n有的人就是会被困在数字化系统里，甚至会被系统替代掉。而有的人，则会因为数字化系统而变得更强大。\n那问题来了，这个分化的分水岭，是什么？\n香帅老师提供了一个有趣的答案。\n数字化对你来说是蜜糖还是毒药，只取决于你是对人负责，还是对事负责。\n这话听着特别抽象，甚至很难分辨清楚。但是今年，我听到了一个绝佳的故事。你听完之后，马上就能深深地理解这句话。\n— 3 —\n北京十一联盟总校的校长李希贵校长，在他上任后，问了图书管理员一个问题：假设有学生因为太爱看书，把书直接给拿走了，你会怎么办？\n图书管理员说，这不是违反校纪吗？性质很恶劣，一定得严肃处理。\n校长说，你觉得这是在对你的工作负责，但是，你却因为负责，站到了一个爱看书的孩子的对立面。你不觉得这有点荒谬吗？\n接下来，他提了一个灵魂拷问：身为一名图书管理员，你到底是对书负责？还是对人负责？\n你要是对书负责，那当然，书得要干干净净的，不许在上面乱涂乱画，乱折页，最好别碰，干脆别看，按照规定、按时归还。\n但在你捍卫书的过程中，不知不觉，就站到了读书人的对立面。\n更严重的是，这样的一个图书管理员，你做这些事需要的职业技能，在你上岗以后，最多一个星期，就能完全掌握，然后一辈子就用这一点技能混日子。你不困在系统里，谁困在系统里呢？\n好，我们再看十一学校后来是怎么做的。大量的书送到班级，这绝对不只是给书换了位置。你再去琢磨那个图书管理员，这时他已经变成了另外一个人。\n他服务的不再是书，而是每一个教室里老师和学生。不同的班，上着不同的课；不同的老师，推荐着不同的书。\n为了能配合好每个老师，这个图书管理员需要不断地跟所有老师交流，动态调整这些书。\n从此以后，他就忙起来了，再也没有一刻安生日子了。\n这个人的职业生涯的天花板从此被掀开了。几年打磨之后，用图书管理员这个概念，再也无法描述他。\n在学校里，他不只是管书的，他可以成为专业的阅读指导老师。如果他愿意，他甚至可以凭这门日渐精进的本事，去创个业。比如开个书店，甚至是创办一个出版机构。\n你到底是对事负责，还是对人负责？\n我们每一个感觉多多少少被困在系统里的人，都应该拿这句话来问自己。\n— 4 —\n薄世宁医生告诉了我一个信息，2020年，一款颅内肿瘤诊断软件已经在北京天坛医院上岗。它看片子的能力有多强？准确率超过90%，完胜大多数人类医生。\n在这样的医疗数字化趋势下，一个医生，如果只会看片子，只会对病负责，而不会对病人负责，那数字化对他当然就是一种威胁啊，是迫在眉睫的威胁啊。\n同样是在医院，护士原本好像没有医生那么厉害。但是，今年我有个同事告诉我，他因为出国回来，被隔离了一段时间。在这期间，他和外面一切的交流，是隔着玻璃的。\n那天来了个护士，拍了拍窗户，举起一张A4纸，上面写着：“你今天有没有不舒服？”我的同事摇了摇头。于是护士又把那张纸反过来，纸上什么也没写，只是画了一张笑脸。她在表明：我关心你，我希望你开心些。\n一听这个故事，你就知道这位护士不会被淘汰，不会被困在数字化系统里。做护士，她会是个好护士；不做护士，不论是做幼教、做销售，开个店，她都会挺出色的一个人。\n— 5 —\n换个角度，从系统的角度来看这件事。系统的嘴脸，真的那么狰狞吗？\n其实20多年前，类似的故事早就发生过。阳光下没有新鲜事。\n国际象棋大师卡斯帕罗夫23前年输给了超级计算机深蓝。但是，那又如何？他说，机器的胜利一如既往是人类的胜利。\n就在第二年，他又杀回来了，而且还发起了一种新的比赛形式，就是把机器变成人的辅助，一个棋手带着机器，对战其他的棋手和机器。就像以前，是人和人赛跑，后来汽车跑得比人快，那又怎样？人会沮丧吗? 会说我怎么跑不赢汽车吗？不会呀，一定是发明了一种人加汽车的比赛，叫赛车。\n还记得王朔小说里那个名场面吗？\n“谁敢惹我？”\n“我敢惹你。”\n“那谁敢惹咱俩？”\n你看，遇到这类问题，我们人类一直都是这么做的。叫背靠系统，面对人。系统只是我背后的支撑，面前这个活生生的人，才是目标。\n— 6 —\n关于系统，我的朋友，数据科学家吴明辉，给我开了一个脑洞，他说：未来数字化能达到什么程度呢？\n比如，你看这张图。不严谨地说，这叫把你在单位的人缘可视化。严谨地说，这叫社会网络数字化。\n也就是说，你在你公司用的系统里正常工作，你肯定会和不同的同事交流、互动。只要这些行为发生在系统里，通过图表呈现出来。\n比如一个人，虽然职位不高，他跑动积极，成了一个网络超级节点，显示出他很有领导力。老板心里知道，这得提拔啊。\n另一个人，对内交流只占40%，对外交流占60%，他对外交流的比重，比他同部门的同事要高好多。那老板心里就有数了，这个人对公司很重要，因为他对外承担了公司的脸面。\n在未来的数字化系统里，一个人的价值，你的友好度、协作度和建设性，是能够随时随地被所有人看见的。\n不是有心理学家说过吗，一个人终其一生的追求，是被看见。好的系统，让人真的被看见。\n长大以后，我们必须学会和系统协同进化。再看一眼康德这句话吧，这是在大分化时代，值得我们反复琢磨的一句话：人是目的，不是手段。\n第四部分：科技创新\n— 1 —\n2020年，有一个问题我一直想知道答案。中国作为世界工厂，年产17亿台手机，3.4亿台个人电脑。我们可是占了全球半导体需求的一半以上。\n但是我们能生产的芯片，不过是全球份额的5%。所以人家当然就有能力说，这个不卖给你，那个不卖给你，卡我们的脖子。\n也不单是芯片的事，高精度的数控机床，很多新材料，都有可能被卡脖子，那生产怎么继续？出口怎么维持？世界工厂的地位，将来还在不在？\n这个事对我们来说到底严重不严重？我们自己能做出高端芯片吗？\n我四处打听得来的答案，还是有点让我意外的。虽然好多人觉得特别担忧，但是在行内人看来，答案没有什么争议，就两条：第一，挺难的；第二，肯定行。\n清华大学的李铁夫老师告诉我的。他说，芯片问题不是个科学问题，而是个工程学问题。\n什么意思？所谓科学问题，就是一个问题提出来，哪条路能通？不知道。甚至，这个问题，从根儿上有没有解？也不知道。\n比如说，中国就有个实验室，在地下2400米，寻找宇宙里的暗物质。这样的科学研究，什么时候出成果？能不能有结果？有了成果有没有用啊？不知道。这叫科学问题。\n而所谓的工程学问题，就是这个问题肯定是可以解决的，技术路线也是清晰的，只是暂时不知道怎么做到而已。\n比如说，你知道有一个地方叫香格里拉，你知道它确实就在那儿，你也知道有人就去过，只不过你现在既没钱，也没地图，你不知道怎么去，这就是个工程学问题。\n高端芯片就是这样的工程学问题。堆够了时间和资源，理论上，完全可以解决。\n— 2 —\n毕竟我不懂芯片，未来的事情，别人说得再斩钉截铁，我也只好将信将疑。\n作为文科生，我们想问题倒是有另外一个套路，就是往回看，看看历史上有没有发生过类似的事，而类似的事又是什么结果。\n就像马克·吐温说过的：“历史不会重复，但总在押韵。”\n那我就问了，在历史上，有没有过这样的例子？一个强权，要用自己的政治意志，通过封锁，把一个重要的全球合作伙伴，驱赶出全球化体系，干所谓的“脱钩”？\n你还别说，我就找着这么一位：拿破仑。200年前的一位霸主。\n很多人都知道，拿破仑的悲惨的结局。1812年，他率领大军远征俄国。结果，他的60多万大军，长途跋涉近千公里，葬送在俄罗斯冰天雪地的荒原里。这是拿破仑败亡的开始。\n但拿破仑为什么要打俄罗斯？要知道，就在5年前，法国和俄国之间是签了和平条约的。那为什么还会翻脸？还会有1812年的这场远征呢？\n我听到过历史学界的一个说法：当时拿破仑打遍欧洲无敌手，几乎已经靠武力搞定了整个欧洲大陆。放眼四望，只剩下一个对手，那就是英国。拿破仑是看不起英国人的。\n他有一句名言说：“英国无非就是一个小店主的国家。”意思是，英国人除了会做买卖，会搞点全球贸易，啥也不是。隔着英吉利海峡，拿破仑想出来的最狠毒的招数，就是大陆封锁，和英国脱钩。\n拿破仑先占一个道德制高点，指责英国“不承认全体文明国家所普遍遵守的国际法规则”。\n拿破仑的干法，可比今天某些国家还要狠。任何一艘来自英国及其殖民地的船，都不允许进入欧洲大陆的任何港口。凡是来自英国的工业品，只要上了欧洲的岸，当即没收，当场烧掉。\n你看，拿破仑的目标很明显，要把英国赶出当时整个文明世界的贸易体系，要把英国活活地饿死。\n但是结果怎么样呢？英国还没有崩溃，拿破仑在欧洲大陆的盟友，倒是先背叛了。\n沙皇虽然跟拿破仑签了盟约，表面上是盟友，但俄罗斯当年穷，就靠向英国出口木材、粮食、皮毛，换回必需的工业品。这是一个非常现实的需求。断了和英国的贸易往来，俄国的贵族们都快破产了。\n既然明面上不能干，就只好背地里偷着干。结果俄国成了拿破仑大陆封锁政策的一个巨大的窟窿。大量的英国货物涌入俄国，再从俄国的西部边境经过波兰、奥地利、普鲁士，输入整个欧洲大陆。\n拿破仑忍意识到，如果要把大陆封锁进行到底，就必须彻底征服俄罗斯，而不是仅仅跟沙皇签一纸表面上的协定。后来发生的事情我们就都知道了。\n你看，这个局面有意思吧，越是咬牙切齿要脱钩的人，越是以为自己的势力范围固若铁桶的人，其实内部全是窟窿。无所不在的市场经济规律，还是要尊重一下的。\n更有意思的后果是，在拿破仑推行大陆封锁政策的那些年里，英国的出口非但没有受损，反而一度还增加了。\n我看到的材料说，1805年，大陆封锁政策颁布的前夕，英国出口商品的价值约5100万英镑。到了1810年，拿破仑封锁正欢的时候，这个数字增长到了约6300万英镑。\n其实，正是因为世界市场被拿破仑这样不正常地切断，英国制造业的优势反而进一步被放大了。\n这是我们文科生知道的历史。也是距离今天最近的一次，在全球市场上，有人试图通过自己的政治意志，利用自己的大国地位，把某一个国家从世界的合作体系里排除出去，所发生的事情。\n你听出来了，我意有所指。今天被“卡脖子”的中国扮演的是什么角色？对应到刚才那个故事，恰恰是当年英国那个角色。\n很多人对今天的中国可能还有一个误解，以为中国身在亚洲大陆，所以还是那个满脑子黄土地思维的大陆国家。\n错了，今天的中国是120多个国家和地区的第一大贸易伙伴。在今天的全球化系统中，不是英国，也不是美国，是中国在高举全球自由贸易的大旗，在扮演无所不在的蓝色海洋角色。\n所以，结论很明显，只要中国保持对外开放，只要中国人没有脱钩的意愿，谁也不可能把我们驱赶出去。\n我们还要反过来对那些高喊脱钩的人说一句：拿破仑，了解一下？\n话说回来，拿破仑也好，美国人也罢，他们犯不犯这样的错误，那是他们的事。而科技创新，自己不受制于别人，这是我们这一代中国人实实在在的任务。\n现在，我们做得到底怎么样？除了芯片，我们还有哪些短板？能不能补上？\n— 3 —\n这些问题，我今年是逢人就请教。科技创新，我们做得到底怎么样？除了芯片，我们还有哪些短板？能不能补上？\n你别说，还真有几个视角，挺有意思，这些视角跟我原来想的不太一样。正在发生的中国科技创新，也了解一下？\n你还记得9月15日，消息传来，美国针对华为的“芯片禁令”生效，那时我们的感受吗？我们作为普通吃瓜群众，我们着急，我们愤怒，我们束手无策。\n确实，过去这100多年，绝大部分时候，中国人搞科技就是跟在别人的后面学，就是把别人的成果引进，所以，自然是一小部分精英做的事情。\n但是今天，中国是长大以后的中国，成年之后的科技体系有多种多样的点亮方式。\n为了了解科技创新，我委托了几组同事分头去采访了一些重头项目。比如大家可能都听说了，华龙一号核电机组，全世界最先进的核电技术，今年11月刚刚并网发电。\n但是我们同事采访回来告诉我，你可别搞错了，华龙一号可不是一个设备，它是一组技术。研发这么一套东西，需要5300多家公司的合作。\n那些技术难点并不是什么神秘的天外飞仙，而是需要无数的人花时间，动脑筋，一点一点地去解决。\n给你举个例子，汽轮机低压转子重量285吨，安装的精度只有2丝，相当于一根头发的三分之一。\n神秘吗？不神秘。难吗？当然很难啊，极难。一次安装涉及近300道工序，上千项作业条目，近5万项数据，才装得上去啊。\n中国创新的真实场景，就是这样，在一间间的工厂，一条条的生产线上，由中国的2亿工人，每年毕业的将近300万理工科大学生，在解决一个个具体的挑战的过程中，自下而上地实现的。\n意识到这一点，我对中国创新的担心确实就少了一点。为啥？因为它不是一小拨中国人，看着几座孤零零的、拿不下来的科技碉堡束手无策。而是一个个具体的挑战，被拆解成庞大的工作量，然后靠资金、资源、人才、时间、耐心、决心，一口一口地吃下来。\n日拱一卒，功不唐捐。这不正是我们中国人擅长的事吗？\n— 4 —\n过去我们有个大印象，中国科技在整体水平上可能还行，但是尖端领域欠债太多。\n你看芯片就是典型吧，14纳米我们可以量产，但是7纳米、5纳米、3纳米，我们现在就做不到啊。这个印象不能说是错的。请注意，我们的存量欠债确实很多，但是追赶速度特别快。\n很多人都知道一个常识，中国被卡脖子的那些技术，往往都是因为材料的问题。比如，制造芯片的高纯度硅材料，还有光刻胶材料，等等。\n中国材料科学的现状是什么样呢？\n我请教了孙亚非老师，他说：现在材料学领域的顶尖论文中，要想找到一篇没有华人作者的文章，很难。\n业界有一个估计，中国科学家在材料学前沿领域的贡献在一半左右。\n他列出了未来具有超强影响力的20种新材料。那这里面有多少种材料的前沿研究有中国科学家的参与呢？13种。\n这才是真相。下回我们要是看到哪个科技领域我们是落后的，我们不仅要看到差距，还要看到正在追赶的速度。意识到这一点，我的担心又缓解了一点。\n— 5 —\n今年我遇到的更有意思的视角，是梁宁老师带给我的。\n她说，2020年，很多事情还没有定论，但是有一件事情是铁板钉钉的，那就是关于自主创新的战略决心。这是这一年我们拿到的最珍贵的果实。\n你还记得前些年吗？在很多领域，要不要搞自主创新，这事是有争议的。有人说，自己做，不如买，买的便宜。也有人说，自己做，太难，机会过去了，不太可能。还有人说，加入全球化，跟他们互相依赖，不就行了吗？\n但是到了2020年，这些争议消失了。自主创新成了共识。请注意，共识这个东西一点也不虚。共识的背后，意味着一张张的订单，一笔笔的投资，一个个的政策，放在做好准备的人的面前。\n这是一张在今年流传甚广的卡脖子清单，你翻过来一看，这是什么清单？这不就是我们这代人的任务清单吗？\n今年我在向红杉资本的沈南鹏请教的时候，我还有点忧虑，没想到他特别兴奋地跟我说，今后几年的市场机会都特别好。我还以为他讲的是投资项目。他说不仅是投资，是这一代理工科专业毕业的大学生，机会特别好。\n你想，放在前几年，他们毕业，最好的选择无非就是去那些大厂。而今年呢，你就看这张表嘛，无数个新的机会窗口打开了。那些卡脖子的点，都需要国产替代。\n那就意味着一个个发展神速的创业公司一定会诞生。所以，这一代大学生还犹豫什么呀？根据自己的专业所学，加入呀！\n— 6 —\n越了解中国创新的现状，我们就越知道，中国创新的水位是在不断抬升的，我们就越没有那么恐慌。\n你就想，中国是全世界唯一拥有联合国产业分类当中全部工业门类的国家。中国科技虽然有短板，但是并没有缺环。几乎所有的被卡脖子的领域，中国都有国产替代企业，不信你去看看创业板的上市公司名单。\n再加上庞大的科研经费投入。再加上全民自下而上的创新热情。这样一个庞然大物，过去，力量可能还比较分散。而2020年，突然有人告诉它，打这儿，打这儿，把靶心都给画好了。你说会发生什么？\n这是一张在今年流传甚广的卡脖子清单，你翻过来一看，这是什么清单？这不就是我们这代人的任务清单吗？\n这就是长大以后的中国创新的样子：长大以后，我们能够把卡脖子清单，转化为机会清单。\n法国作家加缪的一句话，也许可以把我们此刻这种复杂的感受呈现出来，他说，“在隆冬，我终于知道我身上有一个不可战胜的夏天。”\n第五部分：线下的价值\n— 1 —\n2020年有一个最大的共识，就是\u0026rdquo;数字化\u0026rdquo;。\n所有的数字化平台，今年都出现了不同程度的上涨。那真是我们熟悉的那句歌词：你问我将要去何方？他们都指着数字化的方向。\n今年，无数人跟我说，罗胖，还是你们好，得到APP的业务在线上，不受影响啊。我只能尴尬而不失礼貌地笑一笑，外表十分淡定，内心慌得一塌糊涂。\n你们无法理解一个“年近半百的过气网红“的感受啊。线上世界的残酷，只有我们线上世界的人懂。\n我们动不动就说，所有行业都值得用新平台重做一遍。但是重做一遍的方式呢？好像也没什么新鲜，就是在新平台上开个号，然后攒粉丝，然后等待遥遥无期的变现。然后等待下一个新平台再开个号，再攒粉丝。\n可怜我老汉一把年纪，真的追不动了。线上的问题，其实我们也都懂。来得快，去得也快，价值成长很快，但是沉淀和固化很难。\n更重要的是，一代平台，除了头部玩家，大多数人都在吃瓜。\n好吧，新时代那么多平台来来去去，老汉我跑不动了。望着远去的马拉松队伍，我找了个花坛坐下来，突然听到了这么一句话，来自英国摇滚巨星大卫·鲍伊：“音乐这东西将变得像自来水或电一样方便可用……你们得做好要办很多巡演的准备。”\n对呀，音乐这个产业，这么多年，被数字化按在地上摩擦。就在过去几十年，他们为版权，打了多少次官司，那些明星骂了多少次街。\n最后呢？就是鲍伊说的呀，线上的音乐真的像自来水和电一样，既方便，又便宜。那什么在为音乐人挣钱呢？换句话说，什么是真正为音乐产业创造价值的环节呢？反而变成了线下。\n过去40年间，演唱会的平均票价上涨了400%以上。今天的那些全球摇滚巨星80%以上的收入来自于演唱会。\n— 2 —\n这事儿对我启发挺大的。所以今年我们做了一点反方向的尝试，我们往线下跑，搞了一个《启发俱乐部》。\n就在北京SKP商场门口的得到学习中心。每周三晚上8点，我在这儿讲一场，线下卖票，线上直播。这个现场很小，不到100个观众，这规模是怎么也扩大不了的。\n那为什么我们还要这么干呢？刚开始，我只有一个朴素的直觉，我在这儿一直讲，讲十年，也不止是周三我一个人讲，我的目标是每天晚上都有老师在这讲，把它给填满。那十几、二十年后，它应该就是一个不错的景观。\n你想，北京SKP，什么所在？今年可成了全世界销售收入最大的商场。我们这帮人要在这个全人类消费文化制高点的门口，偏偏搞出这么一个课堂。一直在这儿讲，咱就成了个学习钉子户，我想想也挺兴奋的。\n我就喜欢看你，明明想要买包包，却不得不和我一起终身学习的样子。\n— 3 —\n问题又来了，虽然我们刚才说，二十年后它有可能成为一个景观，但为什么？除了我们自己的努力之外，是哪些因素，有可能把它造就为一个景观呢？\n这个问题一直困扰我。直到有一天，发生了这么一件事\u0026ndash;当天，我听说有一对观众是一起来的，那是一对父子，而且那天是孩子的生日。孩子是个初中生的样子。\n我很开心，就问那个爸爸，您是我们得到APP的重度用户吧？\n结果爸爸回答，其实不是。APP刚装上。然后我就问那孩子，那你是我们的重度用户吧？\n孩子说，我是跟我爸来的，我也不是。\n那个爸爸说，我就是想让孩子看看，一个人好好读书，而且能把读书当成自己的工作，是个什么样子。那个孩子说，我爸爸特别希望我来，而且我生日这天，我也想和我爸一起聊聊。\n当天，我迎来了两位消费者。1760块钱的GDP，真实地被创造出来了。但他们消费的，并不是我，甚至都不是我当天讲的内容。\n我提供的只是一个工具。他们用这个工具来定义、巩固和强化他们父子之间的关系，特别是在孩子生日这一天。\n当天这个场景让我看到了一些真相。SKP门口的得到学习中心，和里面的《启发俱乐部》，提供的是一个意义的容器，一个意义的存钱罐。\n我往里面存了一点意义，被南来北往的人看到了，他们也跟着往里存自己的意义。\n意义越聚越多，这个存钱罐就越显眼，就能汇集更多人的意义。它就会变得越来越大。我只是这个存钱罐儿的发起者，并不是拥有者。\n这个价值扩展的过程，是坚定的、可持续的，甚至，它是可以超越一代人的生命限制的。\n法国巴黎的花神咖啡馆。文艺青年都知道，这是萨特、波伏娃、加缪那一代人讨论问题、写稿子的地方。\n今天很多到巴黎旅游的人，都要去看一看，喝杯咖啡，虽然萨特和波伏娃早就不在了，但是每一位新到访的游客，都会在这个存钱罐里多存一笔意义，让这个存钱罐越长越大啊。\n— 4 —\n身为一个中国人，我们太理解这种效应了。岳麓书院、西湖、泰山，都是这么被造就的线下场景。\n就拿武汉的黄鹤楼来说，三国时期，这个意义存钱罐就已经立在这儿，当然刚开始往里存东西的人很少了。\n直到唐代，大诗人崔颢，往里面存了一句千古名句：“黄鹤一去不复返，白云千载空悠悠。”\n然后李白来了，看了这一首，写得好啊，服了，说：“眼前有景道不得，崔颢题诗在上头。”他虽然没写诗，但同样往这个意义容器里存了一笔。\n李白出去转一圈，又不服了，接着回来又存了更大一笔：“故人西辞黄鹤楼，烟花三月下扬州。”\n这一互动，这个头一开，可就不得了了。这一千多年，来过黄鹤楼的人，写诗或者吟诗，这都是在往里面一笔一笔地存储意义啊。\n一直到它成为中国的文化地标，成为著名品牌。\n意识到“意义存钱罐”这个效应，可以打消我们心里的很多傲慢。在数字时代，价值创造，不仅是一个人想出一个好主意，然后在数字世界里，拼命地传播、扩展、增长、破圈的过程。它更需要一个大家都看得见的意义容器。\n在真实世界里，制造一个机会，让走过路过的人能把自己的意义放进来，包括那些喜欢你的、无视你的，甚至是讨厌你的人，那个最初的价值才会被固化下来。\n如果没有这个意义容器，价值就会随风飘散。炒作没有价值，就是这个原因。\n— 5 —\n一个画家，画了一幅新的作品，邀请那个最有钱、最识货的收藏家，来看看喜欢不喜欢。如果喜欢，一手交钱，一手交货，是不是这事儿就成了？\n当然不是。\n一幅好的作品，必须交给画廊，画廊必须去办画展，画展必须开放给成千上万买不起这幅画的围观群众，来参观，来打卡。在这个过程的最后宣布，某某藏家收藏了这幅画。\n这个过程当中，那些看似没必要的环节，比如那些买不起的围观者，贡献了什么价值？更进一步地问，那个藏家花一大笔钱，他买下来的究竟是什么？\n你会发现，一幅画的价值链条里，赞叹的人、批评的人、哀叹自己买不起的人、拍下来发个朋友圈打卡炫耀的人、写篇文章赚稿费的人，所有这些参与了但是没有最后买的人，都通过画展这个\u0026rdquo;意义存钱罐\u0026rdquo;，对这幅画的价值做了贡献。\n而最后那个藏家，是把这一切价值打包买走，可不只是买走了那幅画本身啊。\n一套价值的创造过程，包含了非常复杂的社会参与，不只是线上，更有线下的社会参与。\n2020年，挺难的，很多做线下生意的人有点灰心。我就是想给大家打打气。你看，我也参与到线下来，线下不仅没有过时，而且是这个时代最重要的价值创造环节之一。\n— 6 —\n不过，话说回来，咱们别停留在这个结论上。毕竟咱们处在一个数字化时代，咱们不能对线上的价值视而不见。到底是做线上还是做线下？\n有些人就会陷入到做线上还是做线下的纠结中去。不是有句话说，小孩子才做选择，长大以后，我们说，我们都要。\n这几年，小红书APP带火了两个词，一个叫种草，一个叫拔草。\n种草，是在数字世界里对某个东西、某个场景产生向往，。不管它是一个餐厅、一个咖啡馆，还是一家书店。\n而拔草呢，是回到现实生活的真实场景里去亲自体验，去吃饭、去打卡、去买书，然后把自己的体验在数字世界继续记录、标记。\n这个过程，不就是从线下到线上，又返回线下的过程吗？正所谓：拔草复种草，春风吹又生。\n还记得电影《哈利·波特》里面的一个场景吗？\n在第九站台和第十站台之间，有一个9 ¾站台。它是连接魔法世界和真实世界的通道。在那边，是巫师的世界；在这边，是麻瓜的世界。\n在现在这个时代，你要想做成一件事，就需要找到一个9 ¾站台，让你能在数字世界和现实世界之间反复穿梭。\n有人会说，数字世界那么好，你和任何事物之间的距离都是一键可达，任何资源的存在都是无始无终，天长地久，为什么一定要穿梭到现实世界呢？为什么不仅得有线上的灵魂，还得有线下的肉身呢？\n数字世界有千般好处，但是它有一个缺点，就是太丰富了，以至于这种富足感，让选择的动力丧失了，意义就被稀释了。而回到线下，你不得不面对各种各样的边界，选择就回来了。\n凡是选择，必有理由。每一次给出理由，都是在为自己创造意义。同时，也是把你自己的意义放进了那个更大的意义存钱罐中去。\n举个例子，如果你是一个歌星，现在这个时代，你的歌在数字世界里随处可见，如果只有这个，你红的时间可能很短。\n但如果你有能力到各个城市开演唱会，你的歌迷就开始面对选择：要不要去？买什么价位的票？跟谁一起去？在这个选择的过程中，歌迷就给这场演唱会、这个歌星，赋予了他自己的意义。\n更好的例子其实是奥运会。刚开始，它只是法国教育家顾拜旦头脑中的一个观念。\n但每隔四年，它都要下凡，就是来到现实世界真的闹腾一回。它带来了一系列的边界，既有时间的：四年一回；又有空间的：这一届只在某个城市。\n好了，所有人都要选择了：运动员要选择，要不要在这一届冲刺金牌；观众要选择，要不要去看比赛和旅游；城市要选择，怎么借机建设自己的基础设施；主办国要选择，怎么推广自己的文化；各大品牌要选择，怎么做广告；最新科技要选择，怎么利用奥运会来展示自己，等等等等。\n你看，人人都在做选择。一百多年，每四年发生一次的线下聚集，让当初的一个念头，滚动发展成了一个庞大的社会事实。\n— 7 —\n回到我刚开始的那个疑问，在这个数字化狂飙突进的时代，我们又想上车，又怕被撕得粉碎，那怎么办呢？\n我们这一代人，可能不得不同时完成两件事。第一件事，学会利用那些已有的意义容器，收获自己的价值；第二件事，我们必须考虑，为这个世界创造一些新的、可以留传后世的意义容器。\n长大以后，我们得为世界创造新的意义容器。\n简单地说，做任何事，我都要问自己，你的9 ¾站台在哪里？你的黄鹤楼又在哪里？\n第六部分：社会创新\n— 1 —\n2020年，有一个很新的挑战，摆在我们所有做事的人面前。过去我们都假设，我们生活在一个地大物博的国家，我们身处在一个全球化的时代。做手头的事，资源不够，可以通过各种各样的手段，调用远方的资源，乃至是全球的资源来解决问题。\n但是今年呢？我们突然意识到：居然还有这样的处境，我们不得不在一个国家内，一个城市内，甚至是一个小区内，就地取材，解决问题。\n将来疫情过去了，2020年也是给我们出了一道挺有意思的思考题：我们能不能以资源有限为前提去实现目标？\n今年教育专家沈祖芸老师就在她的《全球教育报告》里讲了一个精彩的案例。我激动地跟着沈老师，干脆去了一趟。\n这是一所山村小学，坐落在北京怀柔区的深山里，叫九渡河小学，这是一所地地道道的山村小学。\n要知道，现在中国的山村小学，要说硬件，其实并不差。山村小学真正的问题其实是：没有好的师资力量。整个学校原本只有23个老师，都是当地的老师。\n可想而知，要把一个外地毕业的大学生，把一个名师派到山区去教书，留在那里，有多难。如果名师才是学校资源的话，那九渡河小学这样的难题就无解。\n2020年1月17号，来了个新校长，于海龙校长。他干了一件事，在周边的几个山村贴了很多告示，招辅导老师。\n告示贴出来，一周时间，80多人报名。你可能会有点奇怪，村里能有什么辅导老师呢？\n有，是剪纸的、做豆腐的、做灯笼的、养蜜蜂的、养鱼的、养蚕的、榨油的、厨师等等。\n就是这样一些普通的村民，大家有的，也就无非就是平日里农家谋生的本事。你可能有点泄气，以为于校长能搞什么创新呢，搞了半天好像也就加了几门兴趣课。\n学会磨豆腐倒是没什么不好，但是作为一个家长，送孩子来学校，不是为了让他长大磨豆腐呀，孩子将来可是要凭本事考学的，学会磨豆腐，文化课咋办呢？这不解决问题呀！\n精彩的来了。于校长要求孩子，不仅要把做豆腐出来，还要卖出去，你看，做豆腐就变成了一个很现实的挑战。在这个挑战下，学校里原本的各种文化课程就能被融进来了。\n怎么把豆腐做出来？科学课的内容就进来了；怎么把各项成本核算清楚，各项收入记录明白，算术课的内容基本上就涵盖进来了；别忘了，这些豆腐还要卖掉，卖给其他学校的食堂和周边的餐馆。那么就要写文案，打动人，语文课就涵盖进来了。\n豆腐课的引入，只是引入了一个壳，一个挑战，而国家课程标准要求的那些知识点，被于校长装进了这个壳里。孩子能学到的东西，甚至比课程标准要求的还要多。\n比如说，国家课程标准要求孩子六年级要学百分数。但是磨豆腐需要计算黄豆和水的比例，所以孩子们提前就学会了。百分数对孩子们而言，不再是抽象的概念，而是具体的生活经验。\n国家课程标准规定，三四年级科学课要了解重量单位的换算关系，要学会使用仪器测量质量、体积和温度。这些内容，孩子们一边做豆腐，一边就全都掌握了。\n类似的课程还有很多：木工课、烘焙课、瓷器课、传统手工课、养殖课、蔬菜栽培课……这些课都提供了挑战，知识的来源，可以是课本，可以是网络，可以是身边的辅导老师，这些课提供的只是要解决的那些问题，问题可是学生们自己的。\n问题是最好的老师。\n带着真问题，去满世界寻找解决方案，这是学习本来该有的样子。没想到吧？在这个山村小学，它真切地实现了。\n— 2 —\n在九渡河小学一年级的教室里，我还吃惊地发现了另外一件事，这里居然不教汉语拼音。\n班上的老师，他告诉我：\u0026rdquo;学汉语拼音，学写字，别着急。让孩子从口头作文开始，他要表达清楚一件事。他自然就要寻找把事情记录下来的工具。好了，他急着找工具了，那时候再教写字。写字有困难，再跟孩子说，有一个可以帮助你写字、认字的工具，叫汉语拼音，你要不要学啊？孩子自然就会愿意学汉语拼音。从挑战开始，再递给孩子解决工具，客观效果呢，就是至始至终，学习热情都不会被扑灭。\u0026rdquo;\n其实，这个故事给我最大的刺激不在这里。仅仅是学校在周边的村子里找到了资源吗？四周的村子也有收获。\n磨豆腐、做木工、种菜、养鸡，这些事对于村民来说，原来只是生计和收入。而现在，九渡河小学的一个邀请，让他们成了老师。\n你可以想象，他们的生命被这个新身份点燃了，这在乡土社会是何等的荣耀。\n我和他们中的一些人聊过，他们跟我说，从来没有像今天这样，热爱这门手艺。\n你看，不仅是学校有了资源，周边的村子，周边的人，也因此被照亮，有了光彩。学校和村子之间的关系，也因此被改善了。\n有一句非洲谚语说：养大一个孩子需要一个村子。学习成长这件事，和周边的环境、和社区是一体的。\n九渡河小学，让我们看到了另外一种解决问题的方法。孩子在社区里长大，整个社区滋养孩子，而孩子给社区带来希望。这个良性的教育图景，我们在九渡河和它周边的土地上看到了。\n我在九渡河小学大门口，看到了墙上写的校训，其中有两句：“脚下有根，眼里有光。”\n为什么眼里有光？因为孩子是在解决具体的问题，眼里的光是因为他们真的在找答案。根在哪里？根就在附近的村子里，在附近的人那里。\n所以教育专家沈祖芸说了这么一句话，教育资源其实无处不在。资源就在你身边。\n— 3 —\n假设你是青海一个光伏电站的厂长，因为当地风沙大，经常小石子吹起来，造成光伏板破损率比较高，你作为厂长，你怎么把这个指标控制好？\n青海塔拉滩光伏电站，就遇到了这个问题。\n那怎么防止风沙呢？解决方案是种草固沙。草一长出来，光伏电板容易磨损报废的问题就解决了大半。\n但新的问题又出现了：因为光伏板得定期清洗，水流到了板子下面的草上，光伏板又遮蔽着阳光，给牧草创造了完美的生长环境，牧草疯长，甚至遮蔽了光伏电板。\n这下怎么办？人工除草？电厂很大啊，54平方公里，成本太高了。\n工作人员的办法是，既然是草，那就放羊入场吃吧！就这样，一个高科技的发电站，入场了2000多只羊。效果很好，除草的效率非常高，羊吃得也很开心，胖乎乎的。\n问题是：羊群总是专在一个地方吃，眼见土地又有被吃秃的趋势，于是工作人员规划出了放羊路线，请牧民来到光伏电厂放羊，羊吃不到的地方就请牧民们手动除草，工资另算。\n好了，原本这里只是一个高科技光伏发电，结果因为发生问题，解决问题，解决方案又带来新的问题，接着解决问题，顺便改善了荒漠化，又发展了当地的养殖事业，还给牧民扶了贫，一举四得。\n所以你看，还是前面说的，想要解决问题，别忘了，资源可能就在你身边。\n— 4 —\n接下来这个故事来自我们得到大学成都校区5期的一个同学，他叫蔡平。几年前，他是中国和尼泊尔边境的一名边境警察。他所在的亚热派出所仅有13个人，却管着相当于六万七千多个足球场大小的面积。\n边防警察嘛，要执行公务，制止偷渡、走私、越过边境线的情况，那么地广人稀的地方，没有当地牧民的帮助是不可能的。\n那怎么去团结牧民呢？且不说文化隔阂非常严重，双方就连语言都说不通。\n那怎么办呢？边防警察的办法竟然是：种树！他们一群边防警察，想尽办法，在青藏高原上，种活了两棵树。\n你可能会想，种两棵树有什么了不起的？但你想，那可是在5300米的青藏高原上，全是高山冻土，很多牧民祖祖辈辈都没有见过一棵树。\n这树一种，就是四五年，四五年都失败了。树经常是夏天活着，冬天就死了。终于在第五年的夏天，他们发现，在众多试种的树苗里，有两棵种在扎东寺的高山柳树没有死！\n扎东寺是前往阿里神山朝拜的必经之路，老百姓现在只要一进庙里，就会看到这两颗树。\n对于当地牧民来说，这就是一个奇迹啊。这些边防警察做成了一件不可能的事。能在自己生命当中，实现奇迹的人，当然，就有了威信。\n树种活不久，就开始有牧民自愿加入边防警察的防护队伍，警民一同破获了很多起偷越边境和走私的案件。\n你会发现，正如投资家苏世民所说的那样：“处于困境中的人往往只关注自己的问题，而解决问题的途径通常在于你如何解决别人的问题。”\n你解决了身边的人的问题，他们就成了你解决自己问题的资源。\n— 5 —\n带着这个思路，这几年我们干了一件还挺让我们骄傲的事，这个项目就是得到大学。你想，如果你是一个干事儿的人，平时你都沉浸在解决自己的问题当中，如果你遇到了难题，你自己解决不了，怎么办？\n得到大学的打法就是不要向远处找答案，而是把这个城市里各行各业的实干家都找到，让这些实干家聚在一起，彼此贡献问题，彼此互为解决方案。\n得到大学不是一个小部分成功者的俱乐部，它是一个正在干事的人的共同体。它最大的特点就是同学的多样性。\n你在自己的班级里，大概率会遇到这样的同学：公务员、教师、工程师、科学家、艺术家、演员、创业者、设计师、医生、律师……他们来自各行各业，他们的平均年龄35岁，都正是干事的时候。\n到今天为止，得到大学已经聚集了9300名校友。想加入这个解决问题的网络，成为这个实干家共同体的一员。得到大学，终身学习者的俱乐部欢迎你们。\n— 6 —\n上面这三个故事，有一个共同的内核：不远求。\n我们这代人，往往习惯于把目光投向远方，做一个宏大的计划，然后用四面八方海量的资源去堆出一个奇迹。但是，2020年，一场疫情来了，逼着我们把注意力收回到我们的身边。\n这一年，我们花了更多的时间和家人相处，花了更多的心思尝试在一个小区里解决问题。我们发现，原来体育馆可以变成方舱，原来汽车厂可以生产口罩，原来解决问题的资源就在身边。\n这也许是\u0026rdquo;长大以后\u0026rdquo;才会有的视角吧。\n大人的责任，就是不论资源多么有限，摆在面前的问题总得去解决。那句话大家都知道，成年人的世界，没有容易二字。\n何帆老师在今年的《变量3》那本书里，有一个特别有趣的提醒，当你真的准备解决问题的时候，你应该把自己想象为“变形金刚”。\n没错，当挑战清晰地出现的时候，你会惊喜地发现你自己就是变形金刚，资源就在你的附近，甚至就在你的身上，资源的变形可能无限多。资源的创新空间无穷大。\n第七部分：新变量\n— 1 —\n我们这代人有一个特别奇妙的体验：一个趋势，当它刚冒头的时候，特别不起眼。但是，当它真的长大以后，很多人又后悔没有上车。\n还记得十年前劝你买房的人吗？如果你当初爱答不理，那你现在可就是高攀不起。类似的事情，在我们这代人过去几十年的生命中，是不是反复在发生？\n那我们就来利用2020年这最后一点时间，来看看，我们身边现在有没有这种看起来很小，将来会变得很大的新变量。\n其中有一个很有意思。可能今年很多人都注意到了一条新闻，中国向世界宣布要在2060年实现“碳中和”。\n简单说，就是到2060年的时候，中国这片国土上，温室气体二氧化碳排放量大大下降，即使有排放，排出的部分，也会被植树造林之类的手段消化掉。\n乍一看，这件事特别遥远。这条新闻在很多人眼前可能一飘也就过去了。毕竟，2060年，40年开外啊，谁知道40年之后怎么样。\n但是，为了准备跨年演讲，我还是专门找学者打探了一下，这个目标是咋算出来的？靠谱不靠谱？没成想，这一问，问出了一个意想不到的角度。\n专家说了，这不是算出来的，这是定出来的，这就是一个战略决心。先定目标，再拆任务，再看怎么干。\n在中国，你说是算出来的靠谱？还是定出来的靠谱？至少，2020年发生的几件事都在告诉我们：在中国，先定目标，再拆任务，是一个更靠谱的途径。\n比如，今年12月17日凌晨，嫦娥五号成功带回了月球的土壤样本。咱也不知道人家是怎么做的。总之，都是中国人，跟着高兴呗。\n但是，令人震惊的是，有人翻出了16年前的一张报纸。结果发现，整个探月计划，早在2004年就制定了，叫“绕、落、回”三步走。\n但是，假如你回到16年前，看到这个新闻，你怎么想？16年，好遥远啊。谁知道这16年里会发生什么？还有多少技术需要攻克？充满了不确定。\n而且整个计划也太简单了吧，就三个字，绕、落、回。感觉就像把大象装进冰箱里分三步，打开门，装进去，关上门。\n结果你猜怎么着？就在此后的16年里，整个计划被严丝合缝、半点不差地执行着。今年，完成了。现在，让我们再看一眼这三个字，绕、落、回。多么简单，又是多么坚定。\n— 2 —\n2020年，中国打赢了扶贫攻坚战。这事我们都知道，挺伟大，也挺不容易的。但是，如果你从历史进程中去看这件事，也会发现一个令人震惊的视角。\n要知道，定下2020扶贫攻坚战打完这个目标，是哪一年？不过是5年前，2015年啊。那一年，中国仍然有7000多万农村贫困人口、832个贫困县。\n谁都知道，剩到最后的，一定是最难啃的骨头。但是5年过去了，我们居然就做到了。\n你看，中国人做事，往往就是这样。先定一个难度极高的目标，高到看起来几乎不可能实现。但是，一旦实现，就是一代人，甚至是几代人的红利。\n有了这个超大目标之后，再汇集一切的资源、人才、精力，用一代人，甚至几代人的努力，把它实现。万维钢老师，\n对这种现象，做了一个特别有意思的概括。他说这叫“奇迹业”。\n好，我们带着“奇迹业“的视角，回头再来看看碳中和这件事。你就会发现，它不是一个远在40年后的目标，它是我们这代人生命当中，一个实实在在的大变量。\n你信不信它会发生，这会极大地影响你在未来几十年的很多选择。想想你自己面对的那些命题——不管是职业选择、投资选择，还是你孩子面对的专业选择，你来感受一下，未来40年的变化，将在何种程度上，让你的选择发生偏转。\n如果2060年，中国实现碳中和。那么，核能的装机容量是现在的5倍，风电的装机容量是现在的12倍，而太阳能会是现在的70倍。一个巨大的产业发展空间打开了。你信，还是不信？\n如果2060年，中国实现碳中和。那意味着我们现在看到的所有的燃油车，都将退出历史。那么大的汽车产业，将完全换一套产业链，换一套规则和玩法，重来一遍。你信，还是不信？\n如果2060年，中国实现碳中和。那意味着中国的森林一年生长量要达到10亿立方米，这比现在翻了一倍啊，森林覆盖率要稳定在26%以上，中国的生态环境会发生一次飞跃。你信，还是不信？\n如果2060年，中国实现碳中和。那意味着整个的中国中西部地区，会成为最主要的能源输出地之一。中西部地区的价值、在中国经济版图上的角色，会被重新定义。你信，还是不信？\n如果2060年，中国实现碳中和。那意味着中国会摆脱对外部能源进口的依赖。所谓的马六甲困境，也就是能源运输卡脖子的问题，也就不复存在。中国的国际地缘政治环境，也会迎来一个新局面。你信，还是不信？\n如果2060年，中国实现碳中和。中国的能源产业，将从资源属性，切换到制造业属性。你知道这意味着什么？制造业啊，那是中国人的基本功，到了咱们的主场啊。\n凭借制造业那个可以不断优化，持续迭代，效率越来越高，成本越来越低的特性，将释放一个多么巨大的经济增长空间。你信，还是不信？\n请注意，这是一副摊在桌上的明牌。我们这代人全部都在牌桌上。40年之后，我87岁，我其实挺有信心我还能看到这一天。而今年出生的孩子，那个时候正当40，风华正茂。他们这一生，注定会被这个新变量影响。\n除了碳中和，我们身边这样的变量到处都是。\n比如，自动驾驶，你信不信它在不久的将来会变成实用技术？再比如，数字货币，你信不信它在不久的将来会完成普及？\n无论你的答案是什么，这些变量都对我们这代人的生活产生山呼海啸般的影响。\n你要是让我选择，我选择信。我选择坚定地想象一个变量长大以后的样子。\n过去未去，未来已来。\n2021，值得期待。\n第八部分：尾声\n— 1 —\n作为一名知识服务者，别的忙也帮不上，得到的使命是“让知识成为每个人的力量”。所以，在这新年即将到来之际，我代表得到App送给大家一份礼物。\n我们公司的CEO脱不花为大家做了一门课程，叫做《怎样成为高效学习的人》，这门课把我们这几年从各路高手身上看到的那些高效的学习方式做了一个归纳，希望对你有用。\n最后，跟大家分享几个今年我特别受启发、受震动的时刻。\n我是有个习惯，有些看不懂的现象，要定期向朋友们请教请教。\n今年，螺蛳粉突然蹿红。我就好奇去跟投资人李丰请教他们是不是有什么做得特别好的地方？\n他说，嗨，这事要真分析起来，原因可多了。你可以说，是因为柳州市政府的长期努力，产业链工业化成熟了；你可以说是因为老百姓口味的变化，想吃点新鲜的、奇怪的、还原度高的东西。\n但你有没有想过，还有一个原因更有解释力？这两年，外卖补贴取消了。\n一顿外卖，从十几块变成了二三十。而对消费者来说，你要么吃二三十的外卖，要么吃三五块钱的方便面。这就导致出现了一个空白区间，也就是那些价格在十块左右，品质又能媲美外卖的产品。\n一盒螺蛳粉大概十几块钱，刚好满足这个条件。说白了，螺狮粉的崛起，也许只是因为中国市场的板块扰动，突然裂开了一个空间，给它机会，让它长了出来。也许就这么简单。\n和李丰的这次谈话，提醒了我一个特别重要的事：看待一个现象的时候，对因果关系的追寻，要保持谦卑。\n世界足够复杂，不是用单一因果关系能描述的，所以，每当我觉得自己找到了一个结论，我都应该问自己一句：你以为你以为的就是你以为的吗？\n— 2 —\n今年我的第二个启发时刻是跟中国大妈有关的。过去提起中国大妈，很多人只想到广场舞，各地旅游景点的拍照狂人。但是今年，媒体界的前辈，张力奋老师，给我开了个脑洞。\n有一年，他陪父亲在欧洲坐游轮观光，碰到健身房里有一群60多岁的中国大妈正在摆弄跑步机。因为不知道怎么操作，一位大妈突然从跑步机上摔下来。张力奋老师赶紧跑上前，想扶她起来。\n但是，大妈只是拍了拍衣服，马上站起来，像没事人一样，热情地招呼同伴上去试一试。那真叫天地无畏。旅行中你如果遇到这样的中国大妈，你会怎么看？是不是还觉得有点尴尬？\n但是张力奋老师换了个视角，他这样描述中国大妈：“少年时学业荒废，年轻时经受磨难，下乡插过队，中年刚过下岗失业，承担家务，养儿育女，赡养老人。\n她们生存力极强，甘冒风险，甚至不知风险为何物。与丈夫相比，她们霸气、强悍，将埋怨与坎坷炼成了无所畏惧，神经粗壮，超常乐观。她们文化虽低，不通英文，却是到外面世界看看愿望最强烈的中国人。”\n怎么样？你有没有从中读出一种不一样的中国大妈？我知道，这仍然只是一个侧面。她们一定还有更多的、更动人、更令人惊喜的模样。\n这段话对我影响很大，它时刻提醒我，每当我对事情有一个负面看法，可能只是因为我缺乏一个建设性的视角。\n— 3 —\n第三个特别重要的启发时刻来自我的一个同事马想。\n有一天，他突然悄咪咪地跟我说，他要结婚了，我第一反应肯定是恭喜啊。结果，他愁眉苦脸地说，其实我没想好要不要结婚，我还年轻，但是出于各方面的压力，这婚好像不结不行了。\n我一想，这不行啊，作为老大哥，我难道不得搭救他一把吗？于是，我赶紧就扑上去一个劲儿地劝他：“马想啊马想你得想啊，没想好就千万不要结啊，婚姻对人的生活质量影响特别大，一定要慎重，不能被迫……”\n我劝了半个小时，最后，我给了他一个这辈子我觉得最重要的人生建议：“我的经验是这样，你不要想对方的优点，你得想想对方的缺点，你想想，如果那个缺点，她这一辈子都改不了，你能不能接受？如果你能接受，你再做决定。”\n他眉头紧锁地说，这姑娘的确有缺点。我赶紧问，啥缺点？马想说，缺点就是她对我太好了。\n草率了，幼稚了，那个时候我还不知道有\u0026rdquo;凡尔赛文学\u0026rdquo;这个词儿。气得我摔门而出，特别崩溃——好嘛，我掏心掏肺说半天，原来你是来给我撒狗粮的。\n我越想越气，大半夜我给脱不花发了一条微信说，以后你要再发现我在别人没有明确求助的情况下，主动给别人提建议，你就抽我。\n这事儿你听起来挺搞笑的，但是对我来说，这是一个很重要的教训：每当我觉得自己是一个过来人的时候，就得想想人家让我过来了吗。\n— 4 —\n三个启发时刻都讲完了，其实说的是一回事。\n第一件事说的是，不要给别人轻易下结论。\n第二件事说的是，尤其不要随便给别人下负面结论。\n第三件事说的是，就算带着纯然的善意，也不要轻易对他人下结论。\n这是一个人长大以后才会有的反思。但是，长大也不意味着失去了锋芒。\n我今年找到了一个特别好的姿势，长大以后该有的姿势，那就是：“我有一个启发。”\n“我有一个启发”，它意味着，你把力量指向自己，你带着自己的问题找寻自己的答案，你跟世界是和解的，但你并没有饶过自己，你在不断地给自己出题。\n其实，它的关键点只有一个：这个世界每天都在给人出卷子，而我们做的这张卷子上，有没有我给自己出的题目？\n过去一年，大家可能都有这样的感受，每每一觉醒来都要感慨：“今天又见证了历史。”但是如果，我们每天都在\u0026rdquo;见证历史\u0026rdquo;，那就意味着，我们把自己的生命力全部投放在了发感慨、作结论上面。\n那我们自己的题目呢？没有自己的题目，我们怎么可能受到启发呢？\n— 5 —\n2020年终于过去了。\n如果跳到20年之后，再来回看这一年，一定会发生一个有趣的视角翻转。2020年让我们大吃一惊的那些事件，会往后退，成了一面生命的背景墙而已。\n而那些我们给自己出的题目，才决定了我们生命中2020的样子。\n也许是我们的某项手艺精进了，也许是我们和某些人的关系发展了，也许是我们的视野扩大了，也许是我们在自己关心的事业中的贡献增长了。\n此时此刻，我知道我的卷子上，就有好多题目：\n明年我的女儿就上小学了，我怎么能帮我的女儿做好上学的准备？\n我今年48岁了，怎么能再多带几个学生，让更多年轻人能加入这一行？\n这一次我给你讲了九渡河小学，下一次我再站到台上能告诉你什么样有趣的创新？\n我去年植发了，今年要不要把近视眼手术做了？这事儿得跟家里人抽空讨论一下。\n2021，我们都知道，必然还将会是波澜壮阔的一年。不知道世界会给我们出一张什么样的卷子。但是无论如何，我们得有自己的题目。\n是些我们给自己出的题目，在造就时光中的我们。当我们把所有\u0026rdquo;我辈中人\u0026rdquo;的题目聚在一起，这就是我们这代人的样子，也是我们这代人面对世界的方式。\n用什么心境开始我辈中人的2021？\n每个人都有自己的卷子。这让我想起了作家E·B·怀特的那句话，这句话特别简单，但是当我们面对大变局的时代，这句话可能表达了一种最佳的态度：面对复杂，保持欢喜。\n时间的朋友，我们明年再见。\n","date":1609494510,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609494510,"objectID":"ce8a51943e55e0b38261d00a03805116","permalink":"https://wubigo.com/post/time-is-friend-by-luozhenyu/","publishdate":"2021-01-01T17:48:30+08:00","relpermalink":"/post/time-is-friend-by-luozhenyu/","section":"post","summary":"2020年12月31日20:30，武汉光谷国际网球中心，罗振宇“时间的朋友”跨年演讲如约而至。\n今年的演讲主题是——“长大以后”。\n第一部分：开场\n— 1 —\n有一件事，在我心里憋了半年，跟谁都没说。因为我一定要把它带到2020年12月31号的这个晚上，在这里，讲给武汉人听。\n你看，这里有一堆糖果，但这不是一堆普通的糖果。过去的很多年里，这堆糖果曾经出现在世界各地的很多著名展览馆里。它是一个艺术作品。创造这个作品的人，叫冈萨雷斯。\n糖果的重量不多不少，正好是79.4公斤。这是他的爱人生前的体重。在人生最黯淡的时刻，艺术家冈萨雷斯，选择用一种特殊的方式，来纪念他的爱人。\n糖果的周围，没有护栏。来参观的人，可以随意拿走一些做纪念，吃了也行，带走也行。\n但是，每一天的展览结束，冈萨雷斯都会给这堆糖果重新称重，然后补齐被拿走的部分。就这样日复一日，从不间断。这堆糖果的重量，始终是79.4公斤。\n生命是那么甜蜜，就像糖果。但生命终会流逝，就像糖果总会被人拿走。\n但最重要的是不管发生什么，爱他的人，总会让他一次次重生。\n这个故事像极了过去一年间武汉人所经历的那些美好、流逝和重生，也见证了爱的力量。在这里，向你们，向1500万武汉人致敬。\n— 2 —\n经过了这一年，我有一个最大的感慨，三个字：不容易。\n我们所有人都知道，能聚在这儿有多么的不容易。可是你们并不知道一个秘密：在此之前，我们为跨年演讲准备了一个后手，一个B计划。\n在整个2020年的上半年，我的朋友们都在替我操心，如果到年底，大型场馆活动仍然不能开放，你的跨年演讲还办不办？怎么办？是不是一个人拿一手机直播就算了？\n我从一开始就告诉他们：办，肯定得办，而且肯定还是在体育馆里办。哪怕我一个人面对空无一人的体育馆讲完全场，也要办。因为今年我们太需要一点确定性了。\n我们同事说，为了避免我一个人站在场里太孤独、太悲壮，可以在现场每个座位上摆一棵金橘树。\n连这些树的名字都起好了，去年我们不是说了一个词，叫“躬身入局”吗？这些金橘树，就叫“躬身入局橘”。\n还好，今天还是有些人能来到这里，不是金橘树。欢迎你们。\n— 3 —\n2020年，有多少个时刻，我们不得不去设想各种各样的可能性，做各种各样的思想准备。我们终于感受到了什么叫压力测试，什么叫留后手，什么叫底线思维。\n我们想不到，我们买一只口罩都曾经那么难。\n华为想不到，有人会把事情做得那么绝，买一颗芯片都那么难。留学海外的孩子想不到，回一趟家居然那么难。\n家长想不到，今年的整整一个学期，孩子去学校上个学居然会那么难。\n中国的电影院想不到，开个门会那么难。他们更想不到的是，在这种情况下，中国居然还成为了全球第一大票房市场。\n日本人想不到，这个夏天会那么难。2020年东京奥运会居然要推迟到2021年，而且可能还叫2020东京奥运会。这是奥运史上的第一次，多么魔幻的一次。\n美国人巴菲特想不到，炒个股会那么难，活了89年，遇上了美股10天4次熔断。\n过去这一年，这张见证历史的清单，我们还可以列很长。\n世界从此不一样了。今年，我们无数次凝视深渊，也无数次被深渊凝视。\n我下面这段话，不带任何情绪，但是今天开场，必须把它摆到桌面上。这是2020年中国人感受到的一重很重要的事实——\n这40多年来，他们说全球化好，那我们就改革开放，一步步拥抱全球化。然后，他们说要脱钩。\n他们说要产业升级，我们以世界工厂的姿态承接了那些转出的产能。然后，他们说我们抢了他们的工作。\n他们说中国人不创新，老跟在他们后面学，我们努力了很久，真创新了。然后，他们说我们有威胁，不卖芯片给我们。\n这就好比，他们三缺一，叫我去陪他们打麻将，我认认真真打，也就刚和了两把，他们说他们三个人就要改玩斗地主，又不让我上桌了。\n这到底是为什么？\n— 4 —\n原因其实就四个字，我们拿来作为今年演讲的主题：\n长大以后。\n中国现在有多大？我们以全世界五分之一的人口，生产了全世界53%的钢铁，57%的水泥，71%的彩电，76%的光伏板，78%的空调，86%的微波炉，88%的手机，90%的电脑。这么大的体量，确实很难让别人再用40年前的眼光来看我们。\n长大以后意味着什么？每个人其实都有体验。\n今天我在光谷，在武汉，说一个我自己的经历吧。\n整整30年前，1990年，父母送我来武汉上大学。就在距离我们演讲现场5公里外的华中科技大学，当天晚上，我们一家三口，就住在学校的招待所。\n我爸告诉我：“明天你就要正式报到了，你就是大学生了。打明儿起，社会不再把你看成是孩子了。今天，你说错了一句话，我们可以出面，说一句孩子还小，不懂事。明天，你成了大学生了，你说错了话，那你就是说错了，你要自己想办法去负责。”\n当时我同时有两种感觉，一种是说不清什么理由的豪迈；另一种是很明显的恐慌。我还是那个我，但是别人对我的期待，对我的态度，一夜之间就完全不同了——原因只是因为长大。\n过去我们常说，规模是一切问题的解药。很多问题把规模做大，自然就解决了。\n但是，很多时候，规模也是问题的根源。\n长大了，很多问题都是新的，我们要独自面对。2020年弥漫着一种情绪，就是觉得世界变了，此前的经验没用了。\n就像诺贝尔文学奖得主埃利亚斯·卡内蒂说的：“旧的答案分崩离析，新的答案还没有着落。”\n2020年很多人都熟悉这样的感受，就好像脚下一路走过来的跳板猛然被抽走，面前却是波涛万顷的汹涌大海。\n我倒是觉得，答案比我们想象的要多得多。即使有了这样的2020年，不确定性在增加，但世界的确定性也比我们想象的要多得多。\n— 5 —\n上海有一个投资人叫王益和。我辗转听到了他的故事。他筹到了一笔钱，想搞扶贫。方式很简单，不是直接给农民钱，而是买了20万棵金丝楠木的树苗，免费送给四川深山里的农民，让他们种在房前屋后、山上路边。\n金丝楠木是极其珍贵的木材，但是长得特别慢，需要几十年后才能成材。所以，过去只有人砍，没有人种，所以野生的金丝楠木现在非常罕见。\n王益和把树苗免费送给村民，就一个条件，种下去后，短期内不许卖。村民在房前屋后种上这么100棵，几十年后，怎么也值几十万。这就是村民的绿色银行了。更有意思的是，这几十年的时间，你该干嘛干嘛，不用为这些树操心。\n听到这个故事，我的第一反应是震惊的。这个世界上居然就有这种事，只要你肯等，而且不需要你多做什么，那个长期确定性的收益居然就有。\n好了，我知道刚才讲的金丝楠木已经在你的心里种下了一个问题：咱得琢磨琢磨，是什么类型的事具有这样的特点？不用操心，时间越久，价值越大。\n想来想去，还真有一个地方也是这样。学校给我带来的最大的收益——而且时间越长就看得越清楚的那部分收益——是同学。\n就像我自己，1990年到1994年，在武汉的华中科技大学新闻系上学。当时一看周围，一群灰头土脸的同学，十几岁的毛孩子，“谁的青春不迷茫”。\n但是，毕业之后，将近30年，大家分头奋斗。突然有一天就发现，我的同学们都成长为了各行各业的中坚力量。大家互相帮忙、守望相助的能力都在提高。","tags":["演讲"],"title":"时间的朋友-罗振宇(2020)","type":"post"},{"authors":null,"categories":[],"content":"函数计算最大的卖点是只用交付业务代码，业务代码在预定义的被托管的可执行环境执行。\n由可执行环境管理基础架构，网络，操作系统。\n但问题是可执行环境包含一个特定的运行时。有可能业务需要的类库在该运行时并不存在。\n而函数服务容器支持任意容器镜像作为函数服务的可执行环境能很好的解决这一问题。\n","date":1609412381,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609412381,"objectID":"afd085fe6832d4d5e40554b4b04567b3","permalink":"https://wubigo.com/post/container-support-for-lambda/","publishdate":"2020-12-31T18:59:41+08:00","relpermalink":"/post/container-support-for-lambda/","section":"post","summary":"函数计算最大的卖点是只用交付业务代码，业务代码在预定义的被托管的可执行环境执行。\n由可执行环境管理基础架构，网络，操作系统。\n但问题是可执行环境包含一个特定的运行时。有可能业务需要的类库在该运行时并不存在。\n而函数服务容器支持任意容器镜像作为函数服务的可执行环境能很好的解决这一问题。","tags":["LAMBDA"],"title":"函数服务容器化","type":"post"},{"authors":null,"categories":[],"content":" 项目 2020承接的是一个银行数据中心智能运营平台项目，这个项目需求之前由公司的售前\n和银行的运维部门沟通了一年多确定下来。我以技术负责人的身份在年初加入这个项目。\n加入不到半个月，客户就要求我们提供智能运营平台项目产品架构说明书和产品开发规划\n","date":1609410867,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609410867,"objectID":"cad937b94ea814b26a97d6cfe327f087","permalink":"https://wubigo.com/post/year-end-review-2020/","publishdate":"2020-12-31T18:34:27+08:00","relpermalink":"/post/year-end-review-2020/","section":"post","summary":"项目 2020承接的是一个银行数据中心智能运营平台项目，这个项目需求之前由公司的售前\n和银行的运维部门沟通了一年多确定下来。我以技术负责人的身份在年初加入这个项目。\n加入不到半个月，客户就要求我们提供智能运营平台项目产品架构说明书和产品开发规划","tags":["REVIEW"],"title":"2020年终总结","type":"post"},{"authors":null,"categories":[],"content":" requirements on distributed transactions  database sharding\n service-oriented transformation and more cross-service transactions\n  three scenarios for distributed transactions  Cross-database distributed transactions Cross-service distributed transactions Hybrid distributed transactions  five distributed transaction solutions  XA Specification TCC Saga Local-Message-Based Distributed Transactions Transactional-Message-Based Distributed Transactions  https://www.alibabacloud.com/blog/an-in-depth-analysis-of-distributed-transaction-solutions\n","date":1607322521,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607322521,"objectID":"efeb1a4f15dec2d6210f5c71c9d68cf9","permalink":"https://wubigo.com/post/an-in-depth-analysis-of-distributed-transaction-solutions/","publishdate":"2020-12-07T14:28:41+08:00","relpermalink":"/post/an-in-depth-analysis-of-distributed-transaction-solutions/","section":"post","summary":"requirements on distributed transactions  database sharding\n service-oriented transformation and more cross-service transactions\n  three scenarios for distributed transactions  Cross-database distributed transactions Cross-service distributed transactions Hybrid distributed transactions  five distributed transaction solutions  XA Specification TCC Saga Local-Message-Based Distributed Transactions Transactional-Message-Based Distributed Transactions  https://www.alibabacloud.com/blog/an-in-depth-analysis-of-distributed-transaction-solutions","tags":[],"title":"Distributed Transaction Solutions","type":"post"},{"authors":null,"categories":[],"content":"Kubernetes is removing the \u0026ldquo;dockershim\u0026rdquo;, which is special in-process support the kubelet has for docker.\nHowever, the kubelet still has the CRI (container runtime interface) to support arbitrary runtimes. containerd is currently supported via the CRI, as is every runtime except docker. Docker is being moved from having special-case support to being the same in terms of support as other runtimes.\nDoes that mean using docker as your runtime is deprecated? I don\u0026rsquo;t think so. You just have to use docker via a CRI layer instead of via the in-process dockershim layer. Since there hasn\u0026rsquo;t been a need until now for an out-of-process cri-\u0026gt;docker-api translation layer, there isn\u0026rsquo;t a well supported one I don\u0026rsquo;t think, but now that they\u0026rsquo;ve announced the intent to remove dockershim, I have no doubt that there will be a supported cri -\u0026gt; docker layer before long.\nMaybe the docker project will add built-in support for exposing a CRI interface and save us an extra daemon (as containerd did).\nIn short, the title\u0026rsquo;s misleading from my understanding. The Kubelet is removing the special-cased dockershim, but k8s distributions that ship with docker as the runtime should be able to run a cri-\u0026gt;docker layer to retain docker support.\nFor more info on this, see the discussion on this pr: https://github.com/kubernetes/kubernetes/pull/94624\n","date":1606961472,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606961472,"objectID":"1e6bfffff1f1bd57fe988a9b1e3095b3","permalink":"https://wubigo.com/post/k8s-deprecate-dockershim/","publishdate":"2020-12-03T10:11:12+08:00","relpermalink":"/post/k8s-deprecate-dockershim/","section":"post","summary":"Kubernetes is removing the \u0026ldquo;dockershim\u0026rdquo;, which is special in-process support the kubelet has for docker.\nHowever, the kubelet still has the CRI (container runtime interface) to support arbitrary runtimes. containerd is currently supported via the CRI, as is every runtime except docker. Docker is being moved from having special-case support to being the same in terms of support as other runtimes.\nDoes that mean using docker as your runtime is deprecated?","tags":["k8S","DOCKER"],"title":"Deprecate Dockershim","type":"post"},{"authors":null,"categories":[],"content":" “The act of storing data in Elasticsearch is called indexing, but before we can index a document, we need to decide where to store it”\n“Relational DB ⇒ Databases ⇒ Tables ⇒ Rows ⇒ Columns Elasticsearch ⇒ Indices ⇒ Types ⇒ Documents ⇒ Fields”\n“_index Where the document lives\n_type The class of object that the document represents\n_id The unique identifier for the document”\nActually, in Elasticsearch, our data is stored and indexed in shards, while an index is just a logical namespace that groups together one or more shards. However, this is an internal detail; our application shouldn’t care about shards at all. As far as our application is concerned, our documents live in an index. Elasticsearch takes care of the details.\nIn a relational database, we usually store objects of the same class in the same table, because they share the same data structure. For the same reason, in Elasticsearch we use the same type for documents that represent the same class of thing, because they share the same data structure. Every type has its own mapping or schema definition, which defines the data structure for documents of that type, much like the columns in a database table. Documents of all types can be stored in the same index, but the mapping for the type tells Elasticsearch how the data in each document should be indexed\nAn index is just a logical namespace that points to one or more physical shards. A shard is a low-level worker unit that holds just a slice of all the data in the index\nreindex create a new index with the new settings and copy all of your documents\nfrom the old index to the new index\n","date":1606181184,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606181184,"objectID":"1aca184545d11cc1bb1ff887de578b7f","permalink":"https://wubigo.com/post/elastic-search-notes/","publishdate":"2020-11-24T09:26:24+08:00","relpermalink":"/post/elastic-search-notes/","section":"post","summary":"“The act of storing data in Elasticsearch is called indexing, but before we can index a document, we need to decide where to store it”\n“Relational DB ⇒ Databases ⇒ Tables ⇒ Rows ⇒ Columns Elasticsearch ⇒ Indices ⇒ Types ⇒ Documents ⇒ Fields”\n“_index Where the document lives\n_type The class of object that the document represents\n_id The unique identifier for the document”\nActually, in Elasticsearch, our data is stored and indexed in shards, while an index is just a logical namespace that groups together one or more shards.","tags":["ES"],"title":"Elastic Search Notes","type":"post"},{"authors":null,"categories":[],"content":" Create an external identity provider in AWS IAM/Access management/identity_providers/\ncreate a SAML type identity_providers\nSet up an external identity provider in AWS AWS SSO/Settings\nConfigure SAML SSO in your own identity provider Create AWS IAM role Access Management/SAML 2.0 Federation\nset the provider you created above as the SAML provider. Select Allow programmatic and AWS Management Console access.\nOn the Attach Permission Policies page, select the appropriate policies to attach to the role. These define the permissions that users granted this role will have with AWS. For example, to grant your users read-only access to IAM, filter for and select the IAMReadOnlyAccess policy.\nReview the Trusted entities and Policies information, then click Create Role\nMap AWS role to a user context.samlConfiguration.mappings = { 'https://aws.amazon.com/SAML/Attributes/Role': 'awsRole', 'https://aws.amazon.com/SAML/Attributes/RoleSessionName': 'awsRoleSession' };  ","date":1605946231,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605946231,"objectID":"9956dae3cafe5060cdd5f7b5297d5b32","permalink":"https://wubigo.com/post/aws-sso-connect-to-external-identity-provider/","publishdate":"2020-11-21T16:10:31+08:00","relpermalink":"/post/aws-sso-connect-to-external-identity-provider/","section":"post","summary":"Create an external identity provider in AWS IAM/Access management/identity_providers/\ncreate a SAML type identity_providers\nSet up an external identity provider in AWS AWS SSO/Settings\nConfigure SAML SSO in your own identity provider Create AWS IAM role Access Management/SAML 2.0 Federation\nset the provider you created above as the SAML provider. Select Allow programmatic and AWS Management Console access.\nOn the Attach Permission Policies page, select the appropriate policies to attach to the role.","tags":["SSO","IAM"],"title":"AWS SSO Connect to External SAML Identity Provider","type":"post"},{"authors":null,"categories":[],"content":" 安装ECK(禁用TLS) [](/post/elastic-cloud-on-k8s/)\n安装helm3 安装skywalking8 git clone git@github.com:wubigo/skywalking-kubernetes.git cd skywalking-kubernetes/chart helm repo add elastic https://helm.elastic.co helm dep up skywalking export SKYWALKING_RELEASE_NAME=skywalking export SKYWALKING_RELEASE_NAMESPACE=default   配置ES  skywalking/values-my-es.yaml\noap: image: tag: 8.1.0-es7 # Set the right tag according to the existing Elasticsearch version storageType: elasticsearch7 ui: image: tag: 8.1.0 elasticsearch: enabled: false config: host: 10.101.24.19 port: http: 9200 user: \u0026quot;elastic\u0026quot; #[optional] password: \u0026quot;8FfgPZu0985bAm2x4243ncxJ\u0026quot; # [optional]  helm install \u0026quot;${SKYWALKING_RELEASE_NAME}\u0026quot; skywalking -n \u0026quot;${SKYWALKING_RELEASE_NAMESPACE}\u0026quot; \\ -f ./skywalking/values-my-es.yaml  检查 kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 4h31m quickstart-es-default ClusterIP None \u0026lt;none\u0026gt; 9200/TCP 123m quickstart-es-http ClusterIP 10.101.24.19 \u0026lt;none\u0026gt; 9200/TCP 123m quickstart-es-transport ClusterIP None \u0026lt;none\u0026gt; 9300/TCP 123m skywalking-oap ClusterIP 10.105.155.189 \u0026lt;none\u0026gt; 12800/TCP,11800/TCP 8m4s skywalking-ui ClusterIP 10.111.140.232 \u0026lt;none\u0026gt; 80/TCP 8m4s  kubectl port-forward service/skywalking-ui 8080:80 curl http://localhost:8080  查询 对于Trace 信息， 数据的采集与存储是一个典型的写多读少的业务场景。\nElasticSearch 存储主要分为两大部分：服务/操作索引和 Span 索引。\n利用 ElasticSearch 会对数据进行分片，分 index 的存储，防止历史数据丢失，\n方便对历史问题进行回溯。对于超过 30 天的数据进行归档，转存到 ES 之外以备\n不时之需。ElasticSearch 只对相对较“热”的数据提供检索服务\nhttps://github.com/imperialwicket/elasticsearch-logstash-index-mgmt\n","date":1604476699,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604476699,"objectID":"e197142ca243247686bd46a63111aa67","permalink":"https://wubigo.com/post/skywalking-on-k8s/","publishdate":"2020-11-04T15:58:19+08:00","relpermalink":"/post/skywalking-on-k8s/","section":"post","summary":"安装ECK(禁用TLS) [](/post/elastic-cloud-on-k8s/)\n安装helm3 安装skywalking8 git clone git@github.com:wubigo/skywalking-kubernetes.git cd skywalking-kubernetes/chart helm repo add elastic https://helm.elastic.co helm dep up skywalking export SKYWALKING_RELEASE_NAME=skywalking export SKYWALKING_RELEASE_NAMESPACE=default   配置ES  skywalking/values-my-es.yaml\noap: image: tag: 8.1.0-es7 # Set the right tag according to the existing Elasticsearch version storageType: elasticsearch7 ui: image: tag: 8.1.0 elasticsearch: enabled: false config: host: 10.101.24.19 port: http: 9200 user: \u0026quot;elastic\u0026quot; #[optional] password: \u0026quot;8FfgPZu0985bAm2x4243ncxJ\u0026quot; # [optional]  helm install \u0026quot;${SKYWALKING_RELEASE_NAME}\u0026quot; skywalking -n \u0026quot;${SKYWALKING_RELEASE_NAMESPACE}\u0026quot; \\ -f .","tags":[],"title":"Skywalking on K8s","type":"post"},{"authors":null,"categories":[],"content":" 什么是ECK Built on the Kubernetes Operator pattern, Elastic Cloud on Kubernetes (ECK) extends the basic Kubernetes orchestration capabilities to support the setup and management of Elasticsearch, Kibana and APM Server on Kubernetes  install Elasticsearch CRD kubectl apply -f https://download.elastic.co/downloads/eck/1.2.1/all-in-one.yaml kubectl -n elastic-system logs -f statefulset.apps/elastic-operator  创建PV(两种方法任选其一)  hostPath  localPath.yaml\napiVersion: v1 kind: PersistentVolume metadata: name: es-pv-volume labels: type: local spec: storageClassName: local-hdd capacity: storage: 200Gi accessModes: - ReadWriteOnce hostPath: path: \u0026quot;/mnt/data\u0026quot;   Local volume  Local volumes do not currently support dynamic provisioning 创建目录/mnt/pv\nsc.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer  local-pv.yaml\napiVersion: v1 kind: PersistentVolume metadata: name: local-hdd spec: capacity: storage: 200Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-hdd local: path: /mnt/pv/ nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - ssh  单节点 es.yml\napiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.9.3 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: local-hdd  禁用TLS es-no-tls.yml\napiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.9.3 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: local-hdd http: tls: selfSignedCertificate: disabled: true  kubectl get elasticsearch NAME HEALTH NODES VERSION PHASE AGE quickstart green 1 7.9.3 Ready 50m  检查 kubectl get service quickstart-es-http   密码\nkubectl get secret quickstart-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'  http/s\nkubectl port-forward service/quickstart-es-http 9200 curl -u \u0026quot;elastic:$PASSWORD\u0026quot; -k \u0026quot;https://localhost:9200\u0026quot; curl -u \u0026quot;elastic:8FfgPZu0985bAm2x4243ncxJ\u0026quot; -k \u0026quot;https://10.101.24.19:9200\u0026quot;   安装kibana kibana.yml\napiVersion: kibana.k8s.elastic.co/v1 kind: Kibana metadata: name: quickstart spec: version: 7.9.3 count: 1 elasticsearchRef: name: quickstart namespace: default http: tls: selfSignedCertificate: disabled: true  kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 10h quickstart-es-default ClusterIP None \u0026lt;none\u0026gt; 9200/TCP 8h quickstart-es-http ClusterIP 10.101.24.19 \u0026lt;none\u0026gt; 9200/TCP 8h quickstart-es-transport ClusterIP None \u0026lt;none\u0026gt; 9300/TCP 8h quickstart-kb-http ClusterIP 10.110.209.226 \u0026lt;none\u0026gt; 5601/TCP 15m kubectl port-forward service/quickstart-kb-http 5601  安装filebeat Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an\nagent on your servers, Filebeat monitors the log files or locations that you specify,\ncollects log events, and forwards them either to Elasticsearch or Logstash for indexing.\ncurl -L -O https://raw.githubusercontent.com/elastic/beats/7.9/deploy/kubernetes/filebeat-kubernetes.yaml  --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config labels: k8s-app: filebeat data: filebeat.yml: |- # filebeat.inputs: # - type: container # paths: # - /var/log/containers/*.log # processors: # - add_kubernetes_metadata: # host: ${NODE_NAME} # matchers: # - logs_path: # logs_path: \u0026quot;/var/log/containers/\u0026quot; filebeat.autodiscover: providers: - type: kubernetes node: ${NODE_NAME} hints.enabled: true hints.default_config: type: container paths: - /var/log/containers/*${data.kubernetes.container.id}.log processors: - add_cloud_metadata: - add_host_metadata: - add_locale: ~ cloud.id: ${ELASTIC_CLOUD_ID} cloud.auth: ${ELASTIC_CLOUD_AUTH} output.elasticsearch: hosts: ['http://${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}'] username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} ssl.enabled: false ssl.certificate_authorities: - /mnt/elastic/tls.crt setup.dashboards.enabled: true setup.kibana: host: \u0026quot;http://${KIBANA_HOST}:5601\u0026quot; username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} protocol: \u0026quot;http\u0026quot; ssl.enabled: false filebeat.config: modules: path: ${path.config}/modules.d/*.yml reload.enabled: false filebeat.modules: - module: system syslog: enabled: true var.paths: [\u0026quot;/var/log/messages\u0026quot;] var.convert_timezone: true auth: enabled: true var.paths: [\u0026quot;/var/log/secure\u0026quot;] var.convert_timezone: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat labels: k8s-app: filebeat spec: selector: matchLabels: k8s-app: filebeat template: metadata: labels: k8s-app: filebeat spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule serviceAccountName: filebeat terminationGracePeriodSeconds: 30 hostNetwork: true dnsPolicy: ClusterFirstWithHostNet containers: - name: filebeat image: docker.elastic.co/beats/filebeat:7.9.3 args: [ \u0026quot;-c\u0026quot;, \u0026quot;/etc/filebeat.yml\u0026quot;, \u0026quot;-e\u0026quot;, ] env: - name: ELASTICSEARCH_HOST value: quickstart-es-http - name: ELASTICSEARCH_PORT value: \u0026quot;9200\u0026quot; - name: ELASTICSEARCH_USERNAME value: elastic - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elastic name: quickstart-es-elastic-user - name: KIBANA_HOST value: quickstart-kb-http - name: ELASTIC_CLOUD_ID value: - name: ELASTIC_CLOUD_AUTH value: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName securityContext: runAsUser: 0 # If using Red Hat OpenShift uncomment this: #privileged: true resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: varlog mountPath: /var/log readOnly: true - name: es-certs mountPath: /mnt/elastic/tls.crt readOnly: true subPath: tls.crt - name: localtime mountPath: /etc/localtime readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: varlog hostPath: path: /var/log # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate - name: es-certs secret: secretName: quickstart-es-http-certs-public - name: localtime hostPath: path: /etc/localtime type: File --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: default roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: filebeat labels: k8s-app: filebeat rules: - apiGroups: [\u0026quot;\u0026quot;] # \u0026quot;\u0026quot; indicates the core API group resources: - namespaces - pods verbs: - get - watch - list --- apiVersion: v1 kind: ServiceAccount metadata: name: filebeat labels: k8s-app: filebeat ---  ","date":1604311405,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604311405,"objectID":"32c5e9778a9ee71664040e53c1ad7b27","permalink":"https://wubigo.com/post/elastic-cloud-on-k8s/","publishdate":"2020-11-02T18:03:25+08:00","relpermalink":"/post/elastic-cloud-on-k8s/","section":"post","summary":"什么是ECK Built on the Kubernetes Operator pattern, Elastic Cloud on Kubernetes (ECK) extends the basic Kubernetes orchestration capabilities to support the setup and management of Elasticsearch, Kibana and APM Server on Kubernetes  install Elasticsearch CRD kubectl apply -f https://download.elastic.co/downloads/eck/1.2.1/all-in-one.yaml kubectl -n elastic-system logs -f statefulset.apps/elastic-operator  创建PV(两种方法任选其一)  hostPath  localPath.yaml\napiVersion: v1 kind: PersistentVolume metadata: name: es-pv-volume labels: type: local spec: storageClassName: local-hdd capacity: storage: 200Gi accessModes: - ReadWriteOnce hostPath: path: \u0026quot;/mnt/data\u0026quot;   Local volume  Local volumes do not currently support dynamic provisioning 创建目录/mnt/pv","tags":[],"title":"ECK notes","type":"post"},{"authors":null,"categories":[],"content":" Linux systems which use a GUI often have a network manager running, which uses a dnsmasq instance running on a loopback address such as 127.0.0.1 or 127.0.1.1 to cache DNS requests, and adds this entry to /etc/resolv.conf. The dnsmasq service speeds up DNS look-ups and also provides DHCP services\nsudo cat /run/resolvconf/resolv.conf sudo cat /run/dnsmasq/resolv.conf sudo cat /etc/systemd/resolved.conf  Disable the local DNS cache /etc/NetworkManager/NetworkManager.conf\n#dns=dnsmasq  systemctl restart network-manager  sudo systemctl disable dnsmasq  ubuntu 20.04 双网卡或局域网加无线网主机，路由指向通一个路由器\nls -l /etc/resolv.conf /etc/resolv.conf -\u0026gt; /run/systemd/resolve/stub-resolv.conf unlink /etc/resolv.conf ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf sudo ip route del default dev wlp2s0  https://www.tecmint.com/set-permanent-dns-nameservers-in-ubuntu-debian/\n更新IP地址 To renew or release an IP address for the eth0 interface, enter:\nsudo dhclient -r eth0 sudo dhclient eth0  ","date":1603979954,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603979954,"objectID":"f1f4bebd67d7185d85a5bf6a3ce60177","permalink":"https://wubigo.com/post/ubuntu-dns-client/","publishdate":"2020-10-29T21:59:14+08:00","relpermalink":"/post/ubuntu-dns-client/","section":"post","summary":"Linux systems which use a GUI often have a network manager running, which uses a dnsmasq instance running on a loopback address such as 127.0.0.1 or 127.0.1.1 to cache DNS requests, and adds this entry to /etc/resolv.conf. The dnsmasq service speeds up DNS look-ups and also provides DHCP services\nsudo cat /run/resolvconf/resolv.conf sudo cat /run/dnsmasq/resolv.conf sudo cat /etc/systemd/resolved.conf  Disable the local DNS cache /etc/NetworkManager/NetworkManager.conf\n#dns=dnsmasq  systemctl restart network-manager  sudo systemctl disable dnsmasq  ubuntu 20.","tags":[],"title":"在Ubuntu上禁用DNS本地缓存 ","type":"post"},{"authors":null,"categories":[],"content":" 安装docker 检查dns sudo cat /etc/resolv.conf # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.8.3.1 nameserver 114.114.114.114 nameserver 8.8.8.8 nameserver 114.114.114.114  详细说明\nLetting iptables see bridged traffic cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF sudo sysctl --system  安装kubeadm kubeadm install mirror in china\n安装 kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers kubectl apply -f https://docs.projectcalico.org/v3.11/manifests/calico.yaml kubectl -n kube-system set env daemonset/calico-node FELIX_IGNORELOOSERPF=true kubeadm join 10.8.3.222:6443 --token awon9z.bcw8z \\ --discovery-token-ca-cert-hash sha256:7b90bca7225915f07179fd2ad31820533  检查DNS kubectl run -it busybox --image=busybox --restart=Never -- sh If you don't see a command prompt, try pressing enter. / # nslookup kubernetes Server: 10.96.0.10 Address: 10.96.0.10:53  为calico的KDD安装calicoctl(calico/ctl:v3.11.3 POD) kubectl apply -f https://docs.projectcalico.org/manifests/calicoctl.yaml alias calicoctl=\u0026quot;kubectl exec -i -n kube-system calicoctl -- /calicoctl\u0026quot; calicoctl get node  calicoctl node status用法提示\n calicoctl的版本必须和calico-node版本一致才能正常工作\n 必须在calico-node所在的节点上运行该命令\ncurl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.11.3/calicoctl export DATASTORE_TYPE=kubernetes export KUBECONFIG=~/.kube/config sudo ./calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+------------+-------------+ | 10.8.3.211 | node-to-node mesh | up | 2020-10-30 | Established | +--------------+-------------------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. bigo@bigo-s3:~$ sudo ./calicoctl version Client Version: v3.11.3 Git commit: 05f36cc8 no etcd endpoints specified bigo@bigo-s3:~$ alias calicoctl=\u0026quot;kubectl exec -i -n kube-system calicoctl -- /calicoctl\u0026quot; bigo@bigo-s3:~$ calicoctl version Client Version: v3.16.4 Git commit: 51418082 Cluster Version: v3.11.3 Cluster Type: k8s,bgp,kdd bigo@bigo-s3:~$ calicoctl node status Calico process is not running. command terminated with exit code 1   https://docs.projectcalico.org/getting-started/clis/calicoctl/install\nubuntu 20 安装完后确保没有使用本地DNS缓存\nunlink /etc/resolv.conf ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf  [FATAL][1783] int_dataplane.go 1035: Kernel's RPF check is set to 'loose'. This would allow endpoints to spoof their IP address. Calico requires net.ipv4.conf.all.rp_filter to be set to 0 or 1. If you require loose RPF and you are not concerned about spoofing, this check can be disabled by setting the IgnoreLooseRPF configuration parameter to 'true'.  kubectl -n kube-system set env daemonset/calico-node FELIX_IGNORELOOSERPF=true  确保br_netfilter模块已经加载\nlsmod | grep br_netfilter sudo modprobe br_netfilter  启用IP转发\nsysctl -w net.ipv4.ip_forward = 1  重新安装 kubeadm reset   刷新所有的链（-F），删除所有的非默认链（-X）\niptables -F iptables -X iptables -nvL  kubeadm token create --print-join-command   ","date":1603960713,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603960713,"objectID":"09a810383bac9bc720cac27c6b9ebf06","permalink":"https://wubigo.com/post/k8s-setup-with-kubeadm/","publishdate":"2020-10-29T16:38:33+08:00","relpermalink":"/post/k8s-setup-with-kubeadm/","section":"post","summary":"安装docker 检查dns sudo cat /etc/resolv.conf # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.8.3.1 nameserver 114.114.114.114 nameserver 8.8.8.8 nameserver 114.114.114.114  详细说明\nLetting iptables see bridged traffic cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF sudo sysctl --system  安装kubeadm kubeadm install mirror in china","tags":[],"title":"Setup K8s With Kubeadm","type":"post"},{"authors":null,"categories":[],"content":" 安装kubectl curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update apt-get install -y kubectl=1.18.3-00   Installing bash completion on Linux\nkubectl completion bash \u0026gt; ~/.kube/kubectl.bash.inc printf \u0026quot; # Kubectl shell completion source '$HOME/.kube/kubectl.bash.inc' \u0026quot; \u0026gt;\u0026gt; $HOME/.bashrc source $HOME/.bashrc   git clone git@github.com:GoogleCloudPlatform/microservices-demo.git  cat /etc/docker/daemon.json { \u0026quot;insecure-registries\u0026quot; : [\u0026quot;10.8.5.211\u0026quot;] }  microservices-demo/src/shippingservice/Dockerfile\nRUN go env -w GOPROXY=https://goproxy.cn,direct  microservices-demo/src/recommendationservice/Dockerfile\nENV DISABLE_DEBUGGER=1 ENV DISABLE_PROFILER=1  skaffold run --default-repo=10.8.5.211/library  kubectl port-forward deployment/frontend 8080:8080  ","date":1603935366,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603935366,"objectID":"4d9777670e2efdc466c0ce0601d100d2","permalink":"https://wubigo.com/post/skafflod/","publishdate":"2020-10-29T09:36:06+08:00","relpermalink":"/post/skafflod/","section":"post","summary":"安装kubectl curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update apt-get install -y kubectl=1.18.3-00   Installing bash completion on Linux\nkubectl completion bash \u0026gt; ~/.kube/kubectl.bash.inc printf \u0026quot; # Kubectl shell completion source '$HOME/.kube/kubectl.bash.inc' \u0026quot; \u0026gt;\u0026gt; $HOME/.bashrc source $HOME/.bashrc   git clone git@github.com:GoogleCloudPlatform/microservices-demo.git  cat /etc/docker/daemon.json { \u0026quot;insecure-registries\u0026quot; : [\u0026quot;10.8.5.211\u0026quot;] }  microservices-demo/src/shippingservice/Dockerfile\nRUN go env -w GOPROXY=https://goproxy.cn,direct  microservices-demo/src/recommendationservice/Dockerfile\nENV DISABLE_DEBUGGER=1 ENV DISABLE_PROFILER=1  skaffold run --default-repo=10.","tags":[],"title":"容器工具--Skafflod","type":"post"},{"authors":null,"categories":[],"content":" 开发环境准备 git version 2.34.1.windows.1 cmd\u0026gt;docker version Client: Cloud integration: v1.0.22 Version: 20.10.12 API version: 1.41 Go version: go1.16.12 Git commit: e91ed57 Built: Mon Dec 13 11:44:07 2021 OS/Arch: windows/amd64 Context: default Experimental: true go version go1.17.5 windows/amd64  golangci-lint 在windows上启动git Bash执行如下sh命令安装golangci-lint\n# binary will be $(go env GOPATH)/bin/golangci-lint curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.44.2 golangci-lint --version  安装make choco install mingw --version 8.1.0 cmd\u0026gt;make -v GNU Make 4.3 Built for Windows32  安装coreutils(可选) Git Bash有部分coreutils工具，如果从Git Bash构建，可以不用安装coreutils\nchoco install gnuwin32-coreutils.install  https://github.com/dapr/dapr/blob/master/docs/development/setup-dapr-development-env.md\n代码 cd %GOPATH%/src # Clone dapr mkdir -p github.com\\dapr\\dapr git clone https://github.com/dapr/dapr.git github.com\\dapr\\dapr # Clone component-contrib mkdir -p github.com\\dapr\\components-contrib git clone https://github.com/dapr/components-contrib.git github.com\\dapr\\components-contrib  用Git Bash构建 MINGW64 /d/code/go/src/github.com/dapr/dapr $which head $make  Proto客户端生成  安装protoc  https://github.com/protocolbuffers/protobuf/releases/download/v3.14.0/protoc-3.14.0-win64.zip\n 安装Go PB插件protoc-gen-go, protoc-gen-go-grpc\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest   或者用make工具安装PB插件\nmake init-proto  dapr初始化 ","date":1601259964,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601259964,"objectID":"cc8a78ec346a471fbf1619298b56fa47","permalink":"https://wubigo.com/post/dapr-component-dev-setup/","publishdate":"2020-09-28T10:26:04+08:00","relpermalink":"/post/dapr-component-dev-setup/","section":"post","summary":"开发环境准备 git version 2.34.1.windows.1 cmd\u0026gt;docker version Client: Cloud integration: v1.0.22 Version: 20.10.12 API version: 1.41 Go version: go1.16.12 Git commit: e91ed57 Built: Mon Dec 13 11:44:07 2021 OS/Arch: windows/amd64 Context: default Experimental: true go version go1.17.5 windows/amd64  golangci-lint 在windows上启动git Bash执行如下sh命令安装golangci-lint\n# binary will be $(go env GOPATH)/bin/golangci-lint curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.44.2 golangci-lint --version  安装make choco install mingw --version 8.1.0 cmd\u0026gt;make -v GNU Make 4.","tags":["PaaS","EDA","DDD"],"title":"Dapr组件开发环境搭建","type":"post"},{"authors":null,"categories":[],"content":" 安装 $lsb_release -a Distributor ID: Ubuntu Description: Ubuntu 20.04.1 LTS Release: 20.04 Codename: focal   sudo apt install wireguard wireguard-dkms -y wget algo/configs/localhost/wireguard/desktop.conf /etc/wireguard/wg0.conf sudo modprobe wireguard sudo ln -s /usr/bin/resolvectl /usr/local/bin/resolvconf sudo wg-quick up wg0  ","date":1598848899,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598848899,"objectID":"f0800c4665aae64a9f62bddc85828f0f","permalink":"https://wubigo.com/post/wireguard-vpn-client-for-ubuntu/","publishdate":"2020-08-31T12:41:39+08:00","relpermalink":"/post/wireguard-vpn-client-for-ubuntu/","section":"post","summary":" 安装 $lsb_release -a Distributor ID: Ubuntu Description: Ubuntu 20.04.1 LTS Release: 20.04 Codename: focal   sudo apt install wireguard wireguard-dkms -y wget algo/configs/localhost/wireguard/desktop.conf /etc/wireguard/wg0.conf sudo modprobe wireguard sudo ln -s /usr/bin/resolvectl /usr/local/bin/resolvconf sudo wg-quick up wg0  ","tags":["VPN"],"title":"Wireguad Vpn Client for Ubuntu","type":"post"},{"authors":null,"categories":[],"content":" 重新安装 Ensure that Docker Desktop is set to Linux containers mode when you run Dapr in self hosted mode\nd:\\code\u0026gt;dapr uninstall Removing Dapr from your machine... Removing directory: C:\\Users\\wubigo\\.dapr\\bin Removing container: dapr_placement Dapr has been removed successfully d:\\code\u0026gt;dapr init Making the jump to hyperspace... Installing runtime version 1.6.0 Downloading binaries and setting up components... Downloaded binaries and completed components set up. daprd binary has been installed to C:\\Users\\wubigo\\.dapr\\bin. dapr_placement container is running. dapr_redis container is running. dapr_zipkin container is running. Use `docker ps` to check running containers. d:\\code\u0026gt;dapr -v CLI version: 1.6.0 Runtime version: 1.6.0 d:\\code\u0026gt;docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6dc4fde182b2 daprio/dapr:1.6.0 \u0026quot;./placement\u0026quot; dapr_placement 303f02455ea0 openzipkin/zipkin \u0026quot;start-zipkin\u0026quot; dapr_zipkin 6b1ccff548ba redis \u0026quot;docker-entrypoint.s…\u0026quot; dapr_redis  源代码 默认的日志级别 github.com/dapr/kit@logger\\options.go\nconst ( defaultJSONOutput = false defaultOutputLevel = \u0026quot;debug\u0026quot; undefinedAppID = \u0026quot;\u0026quot; )  ","date":1598338007,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598338007,"objectID":"4cc22a93915c119baf13cce017c139d9","permalink":"https://wubigo.com/post/dapr-notes/","publishdate":"2020-08-25T14:46:47+08:00","relpermalink":"/post/dapr-notes/","section":"post","summary":"重新安装 Ensure that Docker Desktop is set to Linux containers mode when you run Dapr in self hosted mode\nd:\\code\u0026gt;dapr uninstall Removing Dapr from your machine... Removing directory: C:\\Users\\wubigo\\.dapr\\bin Removing container: dapr_placement Dapr has been removed successfully d:\\code\u0026gt;dapr init Making the jump to hyperspace... Installing runtime version 1.6.0 Downloading binaries and setting up components... Downloaded binaries and completed components set up. daprd binary has been installed to C:\\Users\\wubigo\\.dapr\\bin. dapr_placement container is running.","tags":["CLOUD","DOCKER"],"title":"Dapr Notes","type":"post"},{"authors":null,"categories":[],"content":"There is a very important difference between the root and the alias directives. This difference exists in the way the path specified in the root or the alias is processed.\nIn case of the root directive, full path is appended to the root including the location part, whereas in case of the alias directive, only the portion of the path NOT including the location part is appended to the alias.\nlocation /beta { root /var/www/html } location /beta { alias /var/www/html/beta }  https://stackoverflow.com/questions/10631933/nginx-static-file-serving-confusion-with-root-alias\n","date":1597908318,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597908318,"objectID":"ed9f255716946b4c1025a1140641907d","permalink":"https://wubigo.com/post/nginx-root-vs-alias/","publishdate":"2020-08-20T15:25:18+08:00","relpermalink":"/post/nginx-root-vs-alias/","section":"post","summary":"There is a very important difference between the root and the alias directives. This difference exists in the way the path specified in the root or the alias is processed.\nIn case of the root directive, full path is appended to the root including the location part, whereas in case of the alias directive, only the portion of the path NOT including the location part is appended to the alias.","tags":["WEB"],"title":"Nginx Root vs Alias","type":"post"},{"authors":null,"categories":[],"content":" Inline Versus Overlay Virtual Networks In the inline model, every hop between the source and destination is aware of the virtual network the packet belongs to and uses this information to do lookups in the forwarding table. In the overlay network model, only the edges of the network keep track of the virtual networks; the core of the network is unaware of virtual networks. VLAN and VRF are examples of the inline model of virtual networks, whereas MPLS, VXLAN, and other IP-based VPNs are examples of the overlay model. I’ve encountered customer deployments where 32 to 64 VRFs were used\nThe primary benefits of the inline model are transparency and reduced packet header overhead. However, requiring every node along the path be aware of virtual networks makes the model very unscalable and inefficient. It scales poorly because, as we get to the core of the network, the core devices must keep track of every single virtual network to forward packets properly. It is inefficient because any change to the virtual network affects every node in the network. The more moving parts there are, the more possibility there is of introducing an error and causing unexpected problems. Anybody who has deployed VLANs is well aware of these two problems. The same holds for VRFs, as well\n","date":1597737477,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597737477,"objectID":"69329a70ce9f3470e741c2736561f0c9","permalink":"https://wubigo.com/post/cloud-native-data-center-network/","publishdate":"2020-08-18T15:57:57+08:00","relpermalink":"/post/cloud-native-data-center-network/","section":"post","summary":"Inline Versus Overlay Virtual Networks In the inline model, every hop between the source and destination is aware of the virtual network the packet belongs to and uses this information to do lookups in the forwarding table. In the overlay network model, only the edges of the network keep track of the virtual networks; the core of the network is unaware of virtual networks. VLAN and VRF are examples of the inline model of virtual networks, whereas MPLS, VXLAN, and other IP-based VPNs are examples of the overlay model.","tags":["SDN"],"title":"Cloud Native Data Center Network","type":"post"},{"authors":null,"categories":[],"content":" 产品战略 产品战略包括五大部分：胜出的渴望，产品方向，如何设计产品，产品的核心能力，团队管理系统\n产品战略就是选择\n","date":1596279011,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596279011,"objectID":"0c8d271c126024ec9be1e9da31d306e1","permalink":"https://wubigo.com/post/product-strategy/","publishdate":"2020-08-01T18:50:11+08:00","relpermalink":"/post/product-strategy/","section":"post","summary":"产品战略 产品战略包括五大部分：胜出的渴望，产品方向，如何设计产品，产品的核心能力，团队管理系统\n产品战略就是选择","tags":["MBA"],"title":"产品战略","type":"post"},{"authors":null,"categories":[],"content":"Isolate represents an isolated instance of the V8 engine. V8 isolates have completely separate states. Objects from one isolate must not be used in other isolates. When V8 is initialized a default isolate is implicitly created and entered. The embedder can create additional isolates and use them in parallel in multiple threads. An isolate can be entered by at most one thread at any given time. The Locker/Unlocker API must be used to synchronize.\nhttps://v8docs.nodesource.com/node-0.8/d5/dda/classv8_1_1_isolate.html\n","date":1596007720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596007720,"objectID":"9f1c8ab4d3fd7cdf343a9c09faa62e4d","permalink":"https://wubigo.com/post/v8-isolates/","publishdate":"2020-07-29T15:28:40+08:00","relpermalink":"/post/v8-isolates/","section":"post","summary":"Isolate represents an isolated instance of the V8 engine. V8 isolates have completely separate states. Objects from one isolate must not be used in other isolates. When V8 is initialized a default isolate is implicitly created and entered. The embedder can create additional isolates and use them in parallel in multiple threads. An isolate can be entered by at most one thread at any given time. The Locker/Unlocker API must be used to synchronize.","tags":["SLS","NODEJS"],"title":"V8 Isolates","type":"post"},{"authors":null,"categories":[],"content":" Setting Up the Hosted UI with the Amazon Cognito Unless required by your authorization flow, clear the option Generate client secret. The client secret is used by applications that have a server-side component that can secure the client secret\n","date":1595720363,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595720363,"objectID":"6727961fedeb44871b5cbbafbccfd018","permalink":"https://wubigo.com/post/api-gateway-lambda-authorizer/","publishdate":"2020-07-26T07:39:23+08:00","relpermalink":"/post/api-gateway-lambda-authorizer/","section":"post","summary":"Setting Up the Hosted UI with the Amazon Cognito Unless required by your authorization flow, clear the option Generate client secret. The client secret is used by applications that have a server-side component that can secure the client secret","tags":["SLS"],"title":"网关授权之函数实现","type":"post"},{"authors":null,"categories":[],"content":" Lambda Proxy vs Lambda Integration https://github.com/vaquarkhan/vaquarkhan/wiki/Lambda-Proxy-vs-Lambda-Integration-in-AWS-API-Gateway\nPYTHON https://realpython.com/code-evaluation-with-aws-lambda-and-api-gateway/\nJAVA https://www.baeldung.com/aws-lambda-api-gateway\ngit clone https://github.com/eugenp/tutorials.git cd tutorials/aws-lambda mvn clean package shade:shade aws s3 cp ./target/aws-lambda-0.1.0-SNAPSHOT.jar s3://wubigo/  从S3上传文件到lambad\n handler\ncom.baeldung.lambda.apigateway.APIDemoHandler::handleRequest   NODEJS https://itnext.io/how-to-build-a-serverless-app-with-s3-and-lambda-in-15-minutes-b14eecd4ea89\n","date":1595075683,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595075683,"objectID":"efdc85cef9b987e626299e89059ca169","permalink":"https://wubigo.com/post/lambda-proxy-vs-lambda-integration/","publishdate":"2020-07-18T20:34:43+08:00","relpermalink":"/post/lambda-proxy-vs-lambda-integration/","section":"post","summary":"Lambda Proxy vs Lambda Integration https://github.com/vaquarkhan/vaquarkhan/wiki/Lambda-Proxy-vs-Lambda-Integration-in-AWS-API-Gateway\nPYTHON https://realpython.com/code-evaluation-with-aws-lambda-and-api-gateway/\nJAVA https://www.baeldung.com/aws-lambda-api-gateway\ngit clone https://github.com/eugenp/tutorials.git cd tutorials/aws-lambda mvn clean package shade:shade aws s3 cp ./target/aws-lambda-0.1.0-SNAPSHOT.jar s3://wubigo/  从S3上传文件到lambad\n handler\ncom.baeldung.lambda.apigateway.APIDemoHandler::handleRequest   NODEJS https://itnext.io/how-to-build-a-serverless-app-with-s3-and-lambda-in-15-minutes-b14eecd4ea89","tags":["SLS"],"title":"Lambda Proxy vs Lambda Integration","type":"post"},{"authors":null,"categories":[],"content":" 函数计算有很多使用场景，今天介绍定时任务调度\n例如每周六生成业务报表\n事件类别  资源生命周期事件 HTTP请求 消息队列 调度  调度事件 define event rules that self-trigger regularly and configure a target action to do some regular work. So you can define an Amazon Lambda function or AWS Step Functions state machine as scheduled targets. Hence, when this event is triggered at the specified time or interval you defined, your function or state machine is executed. These types of events are called scheduled Amazon CloudWatch Events\n定义调度  cron 表达式 rate 表达式  函数调用网关接口类型  HTTP API：A lightweight, low-latency RESTful API(Gateway version 2 API) REST API：A customizable, feature-rich RESTful API WebSocket API  HTTP API features Automatic deployments – When you modify routes or integrations, changes deploy automatically to stages that have automatic deployment enabled.\nDefault stage – You can create a default stage ($default) to serve requests at the root path of your API\u0026rsquo;s URL. For named stages, you must include the stage name at the beginning of the path.\nCORS configuration – You can configure your API to add CORS headers to outgoing responses, instead of adding them manually in your function code.\nREST APIs are the classic RESTful APIs that API Gateway has supported since launch. REST APIs currently have more customization, integration, and management features.\nREST API features Integration types – REST APIs support custom Lambda integrations. With a custom integration, you can send just the body of the request to the function, or apply a transform template to the request body before sending it to the function.\nAccess control – REST APIs support more options for authentication and authorization.\nMonitoring and tracing – REST APIs support AWS X-Ray tracing and additional logging options.\n","date":1594812227,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594812227,"objectID":"afbaa62604cdbf03e84b00932db90d7d","permalink":"https://wubigo.com/post/lambda-schedule-event/","publishdate":"2020-07-15T19:23:47+08:00","relpermalink":"/post/lambda-schedule-event/","section":"post","summary":"函数计算有很多使用场景，今天介绍定时任务调度\n例如每周六生成业务报表\n事件类别  资源生命周期事件 HTTP请求 消息队列 调度  调度事件 define event rules that self-trigger regularly and configure a target action to do some regular work. So you can define an Amazon Lambda function or AWS Step Functions state machine as scheduled targets. Hence, when this event is triggered at the specified time or interval you defined, your function or state machine is executed. These types of events are called scheduled Amazon CloudWatch Events","tags":["SLS"],"title":"Lambda之任务定时调度","type":"post"},{"authors":null,"categories":[],"content":" PYTHON 镜像  ERROR: Could not install packages due to an EnvironmentError: HTTPSConnectionPool(host=\u0026lsquo;files.pythonhosted.org\u0026rsquo;, port=443)\n $HOME/.config/pip/pip.conf\n[global] trusted-host=mirrors.aliyun.com index-url=http://mirrors.aliyun.com/pypi/simple/  WINDOWS 10 Python 3.6.8   File \u0026quot;C:\\code\\venv3\\lib\\site-packages\\pip\\_vendor\\distlib\\scripts.py\u0026quot;, line 383, in _get_launcher raise ValueError(msg) ValueError: Unable to find resource t64.exe in package pip._vendor.distlib  python -m pip uninstall pip python -m ensurepip python -m pip install -U pip  ","date":1594727096,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594727096,"objectID":"c59b9c788338a992b1e41696f3b7afdc","permalink":"https://wubigo.com/post/python3-notes/","publishdate":"2020-07-14T19:44:56+08:00","relpermalink":"/post/python3-notes/","section":"post","summary":" PYTHON 镜像  ERROR: Could not install packages due to an EnvironmentError: HTTPSConnectionPool(host=\u0026lsquo;files.pythonhosted.org\u0026rsquo;, port=443)\n $HOME/.config/pip/pip.conf\n[global] trusted-host=mirrors.aliyun.com index-url=http://mirrors.aliyun.com/pypi/simple/  WINDOWS 10 Python 3.6.8   File \u0026quot;C:\\code\\venv3\\lib\\site-packages\\pip\\_vendor\\distlib\\scripts.py\u0026quot;, line 383, in _get_launcher raise ValueError(msg) ValueError: Unable to find resource t64.exe in package pip._vendor.distlib  python -m pip uninstall pip python -m ensurepip python -m pip install -U pip  ","tags":["PYTHON"],"title":"Python3 Notes","type":"post"},{"authors":null,"categories":[],"content":" python3  改变操作系统的地区为美国  否则报UnicodeDecodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t decode\npython-3.6.8-amd64\npy -3.6 -m pip install virtualenv py -3.6 -m virtualenv venv3 .\\venv3\\script\\activate pip install zappa git clone https://github.com/Miserlou/Zappa.git cd zappy/example pip install flask zappa deploy dev_event   检查状态\nzappa status dev_api   FAQ  IllegalLocationConstraintException\nget this error if you're trying to create a bucket with a name that's already been taken   https://stackoverflow.com/questions/49174673/aws-s3api-create-bucket-bucket-make-exception\n--create-bucket-configuration LocationConstraint=eu-west-1  ","date":1594706533,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594706533,"objectID":"c611507cbd69c0e067f26afb32726130","permalink":"https://wubigo.com/post/serveless-python-zappy/","publishdate":"2020-07-14T14:02:13+08:00","relpermalink":"/post/serveless-python-zappy/","section":"post","summary":" python3  改变操作系统的地区为美国  否则报UnicodeDecodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t decode\npython-3.6.8-amd64\npy -3.6 -m pip install virtualenv py -3.6 -m virtualenv venv3 .\\venv3\\script\\activate pip install zappa git clone https://github.com/Miserlou/Zappa.git cd zappy/example pip install flask zappa deploy dev_event   检查状态\nzappa status dev_api   FAQ  IllegalLocationConstraintException\nget this error if you're trying to create a bucket with a name that's already been taken   https://stackoverflow.com/questions/49174673/aws-s3api-create-bucket-bucket-make-exception\n--create-bucket-configuration LocationConstraint=eu-west-1  ","tags":["SLS"],"title":"Serveless Python Zappa","type":"post"},{"authors":null,"categories":[],"content":" 在公有云厂商的新加坡或香港地区免费申请一台免费云主机(可选) 利用terraform创建一台云主机\ngit clone git@github.com:wubigo/iaas.git cd iaas\\aws\\ec2 .\\apply Apply complete! Resources: 2 added, 0 changed, 0 destroyed. Outputs: instance_id = i-069i247af71871dd9 public_ip = 13.113.195.66 ssh ubuntu@13.113.195.66 ubuntu@ip-10-12-0-247:~$ lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 20.04 LTS Release: 20.04 Codename: focal  也可以直接在web控制台根据向导创建\n搭建VPN服务器 git clone https://github.com/wubigo/algo.git sudo apt install -y python3-virtualenv cd algo python3 -m virtualenv --python=\u0026quot;$(command -v python3)\u0026quot; .env \u0026amp;\u0026amp; source .env/bin/activate \u0026amp;\u0026amp; python3 -m pip install -U pip virtualenv \u0026amp;\u0026amp; python3 -m pip install -r requirements.txt ansible-playbook main.yml -e \u0026quot;provider=local server_name=algo ondemand_cellular=false ondemand_wifi=false dns_adblocking=true ssh_tunneling=true store_pki=true region=ams3 do_token=token\u0026quot; \u0026quot;# Congratulations! #\u0026quot; \u0026quot;# Your Algo server is running. #\u0026quot; \u0026quot;# Config files and certificates are in the ./configs/ directory. #\u0026quot; \u0026quot;# Go to https://whoer.net/ after connecting #\u0026quot; \u0026quot;# and ensure that all your traffic passes through the VPN. #\u0026quot; \u0026quot;# Local DNS resolver 172.16.0.1 #\u0026quot; \u0026quot;# The p12 and SSH keys password for new users is XXXXXXXX #\u0026quot; \u0026quot;# The CA key password is XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX #\u0026quot; \u0026quot;# Shell access: ssh -F configs/\u0026lt;server_ip\u0026gt;/ssh_config \u0026lt;hostname\u0026gt; #\u0026quot;  在ansible运行过程中会提示让你输入云主机的公网IP或DNS主机名\n配置WINDOWS客户端 Certificates and configuration files that users will need are placed in the configs directory. Make sure to secure these files since many contain private keys. All files are saved under a subdirectory named with the IP address of your new Algo VPN server.\n 下载wireguard on windows  wireguard-amd64-0.1.1.msi\n import the conf file from /configs/algo/configs/localhost/wireguard/laptop.conf generated by ansible\n update dns to \u0026lsquo;1.1.1.1\u0026rsquo; if the local dns doesn\u0026rsquo;t work\n  停止VPN服务 systemctl stop wg-quick@wg0  更新配置 sudo rm -rf /etc/wireguard/* rm -rf configs/*  请确保etc/wireguard/*下的配置文件被成功删除\n","date":1593868990,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593868990,"objectID":"b5e3ceca775035df4155846bc994802c","permalink":"https://wubigo.com/post/set-up-a-personal-vpn-in-the-cloud/","publishdate":"2020-07-04T21:23:10+08:00","relpermalink":"/post/set-up-a-personal-vpn-in-the-cloud/","section":"post","summary":"在公有云厂商的新加坡或香港地区免费申请一台免费云主机(可选) 利用terraform创建一台云主机\ngit clone git@github.com:wubigo/iaas.git cd iaas\\aws\\ec2 .\\apply Apply complete! Resources: 2 added, 0 changed, 0 destroyed. Outputs: instance_id = i-069i247af71871dd9 public_ip = 13.113.195.66 ssh ubuntu@13.113.195.66 ubuntu@ip-10-12-0-247:~$ lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 20.04 LTS Release: 20.04 Codename: focal  也可以直接在web控制台根据向导创建\n搭建VPN服务器 git clone https://github.com/wubigo/algo.git sudo apt install -y python3-virtualenv cd algo python3 -m virtualenv --python=\u0026quot;$(command -v python3)\u0026quot; .env \u0026amp;\u0026amp; source .env/bin/activate \u0026amp;\u0026amp; python3 -m pip install -U pip virtualenv \u0026amp;\u0026amp; python3 -m pip install -r requirements.","tags":["VPN"],"title":"五分钟搭建VPN云服务","type":"post"},{"authors":null,"categories":[],"content":"The size of the market for multiparty, multicloud data and code aggregation is enormous and only grows larger as companies capture every last bit of data. Vendia’s serverless-based technology offers benefits such as ease of experimentation, no operational heavy lifting and a pay-as-you-go pricing model, making it both very consumable and highly disruptive\n","date":1593725266,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593725266,"objectID":"873dda75bc0772bd69b6a7bc7a3359da","permalink":"https://wubigo.com/post/multi-cloud-serverless-platform/","publishdate":"2020-07-03T05:27:46+08:00","relpermalink":"/post/multi-cloud-serverless-platform/","section":"post","summary":"The size of the market for multiparty, multicloud data and code aggregation is enormous and only grows larger as companies capture every last bit of data. Vendia’s serverless-based technology offers benefits such as ease of experimentation, no operational heavy lifting and a pay-as-you-go pricing model, making it both very consumable and highly disruptive","tags":["CMP"],"title":"Multi Cloud Serverless Platform","type":"post"},{"authors":null,"categories":[],"content":" config climc service-config-edit region2  or\nkubectl describe cm default-region -n onecloud  region.conf\nlog_level: debug log_verbose_level: 10 enable_host_health_check: false enable_ssl: false port: 30888 port_v2: 30888  依赖的模块  HTTP web framework\ngithub.com/gin-gonic/gin  ORM\nhttps://github.com/go-gorm/gorm   region endpoint climc endpoint-show b0d33d8b370c42418cb3c6e51442c072 +--------------------+----------------------------------+ | Field | Value | +--------------------+----------------------------------+ | can_delete | false | | can_update | true | | created_at | 2020-06-23T03:03:20.000000Z | | deleted | false | | enabled | true | | id | b0d33d8b370c42418cb3c6e51442c072 | | interface | public | | is_emulated | false | | name | compute_v2-public | | public_key_bit_len | 0 | | region_id | region0 | | service_id | fb54a285aa3e4c848298148596011aa1 | | service_name | region2 | | service_type | compute_v2 | | update_version | 0 | | updated_at | 2020-06-23T03:03:20.000000Z | | url | https://10.8.3.247:30888 | +--------------------+----------------------------------+  https://10.8.3.247:30888/stats  WEB CONTROLLER pkg\\appsrv\\dispatcher\\dispatcher.go\nAddModelDispatcher  ","date":1593492552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593492552,"objectID":"f9b2679919e936f695810d3bb0ecc48d","permalink":"https://wubigo.com/post/cmp-onecloud-region-code/","publishdate":"2020-06-30T12:49:12+08:00","relpermalink":"/post/cmp-onecloud-region-code/","section":"post","summary":"config climc service-config-edit region2  or\nkubectl describe cm default-region -n onecloud  region.conf\nlog_level: debug log_verbose_level: 10 enable_host_health_check: false enable_ssl: false port: 30888 port_v2: 30888  依赖的模块  HTTP web framework\ngithub.com/gin-gonic/gin  ORM\nhttps://github.com/go-gorm/gorm   region endpoint climc endpoint-show b0d33d8b370c42418cb3c6e51442c072 +--------------------+----------------------------------+ | Field | Value | +--------------------+----------------------------------+ | can_delete | false | | can_update | true | | created_at | 2020-06-23T03:03:20.000000Z | | deleted | false | | enabled | true | | id | b0d33d8b370c42418cb3c6e51442c072 | | interface | public | | is_emulated | false | | name | compute_v2-public | | public_key_bit_len | 0 | | region_id | region0 | | service_id | fb54a285aa3e4c848298148596011aa1 | | service_name | region2 | | service_type | compute_v2 | | update_version | 0 | | updated_at | 2020-06-23T03:03:20.","tags":["CMP"],"title":"Cmp Onecloud Region Code","type":"post"},{"authors":null,"categories":[],"content":"","date":1592960256,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592960256,"objectID":"f6883a490c3cd88bbe02dc72e8770f04","permalink":"https://wubigo.com/post/i-just-hit-100k-yr-on-github-sponsors/","publishdate":"2020-06-24T08:57:36+08:00","relpermalink":"/post/i-just-hit-100k-yr-on-github-sponsors/","section":"post","summary":"","tags":["OPENSOURCE"],"title":"I-Just-Hit-$100k-yr-On-GitHub-Sponsors","type":"post"},{"authors":null,"categories":[],"content":" 系统设置 /etc/yum.conf\nobsoletes=0  obsoletes=1: 安装k8s 1.15.8会报一个依赖错误\n安装ansible sudo yum install epel-release sudo yum install python-pip pip install ansible  安装kubelet（可选） OCADM创建K8S的前提条件是kebelet正常工作\n安装kubelet\nOCBOOT git clone https://github.com/yunionio/ocboot.git cd ocboot ./run.py ./config-allinone.yml  allinone.yml\nmariadb_node: use_local: true hostname: 192.168.137.190 user: root db_user: root db_password: qwe123 primary_master_node: #master_node: use_local: true hostname: 192.168.137.190 user: root db_host: 192.168.137.190 db_user: root db_password: qwe123 onecloud_user: demo onecloud_user_password: demo@123 controlplane_host: 192.168.137.190 controlplane_port: \u0026quot;6443\u0026quot; as_host: true registry_mirrors: - https://lje6zxpk.mirror.aliyuncs.com   安装制定版本\nonecloud_version: v3.2.1   https://github.com/yunionio/ocboot/blob/master/onecloud/roles/common/tasks/main.yml\nurl: https://iso.yunion.cn/yumrepo-3.2/yunion.repo  HOST kubectl logs -n onecloud default-host-4fgxx \u0026gt; /tmp/host.log cat /var/lib/dhclient/dhclient-b17b6243-80ba-4e51-bb07-753bfb756270-ens33.lease  OCBOOT ansible-playbook -e ANSIBLE_HOST_KEY_CHECKING=False -i /root/ocboot/onecloud/zz_generated.hosts /root/ocboot/onecloud/zz_generated.site.yml -vv TASK [primary-master-node : Update onecloud user demo password to demo@123] [oc-upload-config] storing the configuration used in ConfigMap \u0026quot;ocadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy [oc-addons] Applied addon: calico [oc-addons] Applied addon: local-path-provisioner [oc-addons] Applied addon: traefik [oc-addons] Applied addon: onecloud-operator  ocboot -\u0026gt; ocadm -\u0026gt; 部署k8s 和 onecloud-operator\nocadm的addon阶段里面定义了onecloud-operator\noperator收到一个 OnecloudCluster 实例创建的请求，就开始搭建 onecloud 的 keystone、region、glance、web前端等各个服务\nkubectl -n kube-system get cm ocadm-config -oyaml  ISO安装 /opt/yunion/upgrade/config.yml\ncat /opt/yunion/upgrade/config.yml |grep db  disable host-agent ocadm node disable-host-agent ocadm baremetal disable  说明  operator 本身是由ocadm 的工具安装的  ocadm init 命令就会安装 operator\n代码\n ocboot安装k8s  ansible任务\u0026ldquo;Use ocadm init first master node\u0026rdquo; 部署 k8s 集群，并安装对应的 operator 等插件\n [root@localhost ocboot]# ansible-playbook -e ANSIBLE_HOST_KEY_CHECKING=False -i /root/ocboot/onecloud/zz_generated.hosts /root/ocboot/onecloud/zz_generated.site.yml -vv ansible-playbook 2.9.9 config file = /etc/ansible/ansible.cfg configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible-playbook python version = 2.7.5 (default, Apr 2 2020, 13:16:51) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] Using /etc/ansible/ansible.cfg as config file PLAYBOOK: zz_generated.site.yml ******************************************************************************************************* 2 plays in /root/ocboot/onecloud/zz_generated.site.yml PLAY [mariadb_node] ******************************************************************************************************************* TASK [Gathering Facts] **************************************************************************************************************** task path: /root/ocboot/onecloud/zz_generated.site.yml:1 ok: [127.0.0.1] META: ran handlers TASK [mariadb : Install mariadb] ****************************************************************************************************** task path: /root/ocboot/onecloud/roles/mariadb/tasks/main.yml:1 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;results\u0026quot;: [\u0026quot;1:mariadb-server-5.5.65-1.el7.x86_64 providing mariadb-server is already installed\u0026quot;]} TASK [mariadb : Install MySQL-python] ************************************************************************************************* task path: /root/ocboot/onecloud/roles/mariadb/tasks/main.yml:6 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;results\u0026quot;: [\u0026quot;MySQL-python-1.2.5-1.el7.x86_64 providing MySQL-python is already installed\u0026quot;]} TASK [mariadb : Copy my.cnf] ********************************************************************************************************** task path: /root/ocboot/onecloud/roles/mariadb/tasks/main.yml:11 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;checksum\u0026quot;: \u0026quot;e079b173a071cae24cec85746edacb4983f3b1d0\u0026quot;, \u0026quot;dest\u0026quot;: \u0026quot;/etc/my.cnf\u0026quot;, \u0026quot;gid\u0026quot;: 0, \u0026quot;group\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;0644\u0026quot;, \u0026quot;owner\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/etc/my.cnf\u0026quot;, \u0026quot;size\u0026quot;: 674, \u0026quot;state\u0026quot;: \u0026quot;file\u0026quot;, \u0026quot;uid\u0026quot;: 0} TASK [mariadb : Enable mariadb] ******************************************************************************************************* task path: /root/ocboot/onecloud/roles/mariadb/tasks/main.yml:16 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;enabled\u0026quot;: true, \u0026quot;name\u0026quot;: \u0026quot;mariadb\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;started\u0026quot;, \u0026quot;status\u0026quot;: {\u0026quot;ActiveEnterTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:47 EDT\u0026quot;, \u0026quot;ActiveEnterTimestampMonotonic\u0026quot;: \u0026quot;14040115\u0026quot;, \u0026quot;ActiveExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ActiveState\u0026quot;: \u0026quot;active\u0026quot;, \u0026quot;After\u0026quot;: \u0026quot;tmp.mount system.slice systemd-journald.socket syslog.target network.target -.mount basic.target\u0026quot;, \u0026quot;AllowIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;AmbientCapabilities\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;AssertResult\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;AssertTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:43 EDT\u0026quot;, \u0026quot;AssertTimestampMonotonic\u0026quot;: \u0026quot;9998771\u0026quot;, \u0026quot;Before\u0026quot;: \u0026quot;multi-user.target shutdown.target\u0026quot;, \u0026quot;BlockIOAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;BlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CPUAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUQuotaPerSecUSec\u0026quot;: \u0026quot;infinity\u0026quot;, \u0026quot;CPUSchedulingPolicy\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingPriority\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingResetOnFork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CanIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CanReload\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CanStart\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CanStop\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CapabilityBoundingSet\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;ConditionResult\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;ConditionTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:43 EDT\u0026quot;, \u0026quot;ConditionTimestampMonotonic\u0026quot;: \u0026quot;9998771\u0026quot;, \u0026quot;Conflicts\u0026quot;: \u0026quot;shutdown.target\u0026quot;, \u0026quot;ControlGroup\u0026quot;: \u0026quot;/system.slice/mariadb.service\u0026quot;, \u0026quot;ControlPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DefaultDependencies\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Delegate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;MariaDB database server\u0026quot;, \u0026quot;DevicePolicy\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;ExecMainCode\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainPID\u0026quot;: \u0026quot;1188\u0026quot;, \u0026quot;ExecMainStartTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:44 EDT\u0026quot;, \u0026quot;ExecMainStartTimestampMonotonic\u0026quot;: \u0026quot;10605707\u0026quot;, \u0026quot;ExecMainStatus\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecStart\u0026quot;: \u0026quot;{ path=/usr/bin/mysqld_safe ; argv[]=/usr/bin/mysqld_safe --basedir=/usr ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;ExecStartPost\u0026quot;: \u0026quot;{ path=/usr/libexec/mariadb-wait-ready ; argv[]=/usr/libexec/mariadb-wait-ready $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;ExecStartPre\u0026quot;: \u0026quot;{ path=/usr/libexec/mariadb-prepare-db-dir ; argv[]=/usr/libexec/mariadb-prepare-db-dir %n ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;FailureAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;FileDescriptorStoreMax\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;FragmentPath\u0026quot;: \u0026quot;/usr/lib/systemd/system/mariadb.service\u0026quot;, \u0026quot;Group\u0026quot;: \u0026quot;mysql\u0026quot;, \u0026quot;GuessMainPID\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;IOScheduling\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;mariadb.service\u0026quot;, \u0026quot;IgnoreOnIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreOnSnapshot\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreSIGPIPE\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;InactiveEnterTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;InactiveExitTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:43 EDT\u0026quot;, \u0026quot;InactiveExitTimestampMonotonic\u0026quot;: \u0026quot;10006574\u0026quot;, \u0026quot;JobTimeoutAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;JobTimeoutUSec\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;KillMode\u0026quot;: \u0026quot;control-group\u0026quot;, \u0026quot;KillSignal\u0026quot;: \u0026quot;15\u0026quot;, \u0026quot;LimitAS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCORE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCPU\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitDATA\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitFSIZE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitLOCKS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitMEMLOCK\u0026quot;: \u0026quot;65536\u0026quot;, \u0026quot;LimitMSGQUEUE\u0026quot;: \u0026quot;819200\u0026quot;, \u0026quot;LimitNICE\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitNOFILE\u0026quot;: \u0026quot;4096\u0026quot;, \u0026quot;LimitNPROC\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitRSS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitRTPRIO\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitRTTIME\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitSIGPENDING\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitSTACK\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LoadState\u0026quot;: \u0026quot;loaded\u0026quot;, \u0026quot;MainPID\u0026quot;: \u0026quot;1188\u0026quot;, \u0026quot;MemoryAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;MemoryCurrent\u0026quot;: \u0026quot;105254912\u0026quot;, \u0026quot;MemoryLimit\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MountFlags\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Names\u0026quot;: \u0026quot;mariadb.service\u0026quot;, \u0026quot;NeedDaemonReload\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Nice\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;NoNewPrivileges\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NonBlocking\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NotifyAccess\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;OOMScoreAdjust\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;OnFailureJobMode\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;PermissionsStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateDevices\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateNetwork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateTmp\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;ProtectHome\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectSystem\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStart\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStop\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RemainAfterExit\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Requires\u0026quot;: \u0026quot;system.slice basic.target -.mount\u0026quot;, \u0026quot;RequiresMountsFor\u0026quot;: \u0026quot;/var/tmp\u0026quot;, \u0026quot;Restart\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RestartUSec\u0026quot;: \u0026quot;100ms\u0026quot;, \u0026quot;Result\u0026quot;: \u0026quot;success\u0026quot;, \u0026quot;RootDirectoryStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RuntimeDirectoryMode\u0026quot;: \u0026quot;0755\u0026quot;, \u0026quot;SameProcessGroup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SecureBits\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;SendSIGHUP\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SendSIGKILL\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Slice\u0026quot;: \u0026quot;system.slice\u0026quot;, \u0026quot;StandardError\u0026quot;: \u0026quot;inherit\u0026quot;, \u0026quot;StandardInput\u0026quot;: \u0026quot;null\u0026quot;, \u0026quot;StandardOutput\u0026quot;: \u0026quot;journal\u0026quot;, \u0026quot;StartLimitAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;StartLimitBurst\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;StartLimitInterval\u0026quot;: \u0026quot;10000000\u0026quot;, \u0026quot;StartupBlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StartupCPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StatusErrno\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;StopWhenUnneeded\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SubState\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;SyslogLevelPrefix\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;SyslogPriority\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;SystemCallErrorNumber\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;TTYReset\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVHangup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVTDisallocate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksCurrent\u0026quot;: \u0026quot;20\u0026quot;, \u0026quot;TasksMax\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TimeoutStartUSec\u0026quot;: \u0026quot;5min\u0026quot;, \u0026quot;TimeoutStopUSec\u0026quot;: \u0026quot;5min\u0026quot;, \u0026quot;TimerSlackNSec\u0026quot;: \u0026quot;50000\u0026quot;, \u0026quot;Transient\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;simple\u0026quot;, \u0026quot;UMask\u0026quot;: \u0026quot;0022\u0026quot;, \u0026quot;UnitFilePreset\u0026quot;: \u0026quot;disabled\u0026quot;, \u0026quot;UnitFileState\u0026quot;: \u0026quot;enabled\u0026quot;, \u0026quot;User\u0026quot;: \u0026quot;mysql\u0026quot;, \u0026quot;WantedBy\u0026quot;: \u0026quot;multi-user.target\u0026quot;, \u0026quot;WatchdogTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:44 EDT\u0026quot;, \u0026quot;WatchdogTimestampMonotonic\u0026quot;: \u0026quot;10605863\u0026quot;, \u0026quot;WatchdogUSec\u0026quot;: \u0026quot;0\u0026quot;}} TASK [mariadb : Change root password] ************************************************************************************************* task path: /root/ocboot/onecloud/roles/mariadb/tasks/main.yml:22 [WARNING]: Module did not set no_log for update_password ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;failed_when_result\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;unable to connect to database, check login_user and login_password are correct or /root/.my.cnf has the credentials. Exception message: (1045, \\\u0026quot;Access denied for user 'root'@'localhost' (using password: NO)\\\u0026quot;)\u0026quot;} TASK [mariadb : Allow remote root access with grant priv] ***************************************************************************** task path: /root/ocboot/onecloud/roles/mariadb/tasks/main.yml:30 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;User unchanged\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;root\u0026quot;} TASK [mariadb : Remove all anonymous user accounts] *********************************************************************************** task path: /root/ocboot/onecloud/roles/mariadb/tasks/main.yml:40 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;User doesn't exist\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;\u0026quot;} TASK [mariadb : Remove test database] ************************************************************************************************* task path: /root/ocboot/onecloud/roles/mariadb/tasks/main.yml:48 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;db\u0026quot;: \u0026quot;test\u0026quot;, \u0026quot;db_list\u0026quot;: [\u0026quot;test\u0026quot;]} META: ran handlers META: ran handlers PLAY [primary_master_node] ************************************************************************************************************ TASK [Gathering Facts] **************************************************************************************************************** task path: /root/ocboot/onecloud/zz_generated.site.yml:4 ok: [127.0.0.1] META: ran handlers TASK [Do common setup] **************************************************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:1 TASK [common : Install yum utils] ***************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:3 [DEPRECATION WARNING]: Invoking \u0026quot;yum\u0026quot; only once while using a loop via squash_actions is deprecated. Instead of using a loop to supply multiple items and specifying `name: \u0026quot;{{ item }}\u0026quot;`, please use `name: ['yum-utils', 'bash-completion', 'epel-release']` and remove the loop. This feature will be removed in version 2.11. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. ok: [127.0.0.1] =\u0026gt; (item=[u'yum-utils', u'bash-completion', u'epel-release']) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;changes\u0026quot;: {\u0026quot;installed\u0026quot;: [], \u0026quot;updated\u0026quot;: []}, \u0026quot;item\u0026quot;: [\u0026quot;yum-utils\u0026quot;, \u0026quot;bash-completion\u0026quot;, \u0026quot;epel-release\u0026quot;], \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;obsoletes\u0026quot;: {\u0026quot;kubernetes-cni\u0026quot;: {\u0026quot;dist\u0026quot;: \u0026quot;x86_64\u0026quot;, \u0026quot;repo\u0026quot;: \u0026quot;@kubernetes\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.7.5-0\u0026quot;}}, \u0026quot;rc\u0026quot;: 0, \u0026quot;results\u0026quot;: [\u0026quot;All packages providing yum-utils are up to date\u0026quot;, \u0026quot;All packages providing bash-completion are up to date\u0026quot;, \u0026quot;All packages providing epel-release are up to date\u0026quot;, \u0026quot;\u0026quot;]} TASK [common : Add yunion rpm repository] ********************************************************************************************* task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:10 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;checksum_dest\u0026quot;: \u0026quot;2f42e4a4f882cad83d59c75b5f6f3df278d6a574\u0026quot;, \u0026quot;checksum_src\u0026quot;: \u0026quot;2f42e4a4f882cad83d59c75b5f6f3df278d6a574\u0026quot;, \u0026quot;dest\u0026quot;: \u0026quot;/etc/yum.repos.d/yunion.repo\u0026quot;, \u0026quot;elapsed\u0026quot;: 0, \u0026quot;gid\u0026quot;: 0, \u0026quot;group\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;md5sum\u0026quot;: \u0026quot;7f952d8ebe445afc837adc4dec8e5a4c\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;0644\u0026quot;, \u0026quot;msg\u0026quot;: \u0026quot;OK (165 bytes)\u0026quot;, \u0026quot;owner\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;size\u0026quot;: 165, \u0026quot;src\u0026quot;: \u0026quot;/root/.ansible/tmp/ansible-tmp-1592572330.52-54744-188320608425950/tmp70g6X8\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;file\u0026quot;, \u0026quot;status_code\u0026quot;: 200, \u0026quot;uid\u0026quot;: 0, \u0026quot;url\u0026quot;: \u0026quot;https://iso.yunion.cn/yumrepo-3.1/yunion.repo\u0026quot;} TASK [common : Install yunion common packages] **************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:16 [DEPRECATION WARNING]: Invoking \u0026quot;yum\u0026quot; only once while using a loop via squash_actions is deprecated. Instead of using a loop to supply multiple items and specifying `name: \u0026quot;{{ item }}\u0026quot;`, please use `name: ['epel-release', 'libaio', 'jq', 'libusb', 'lvm2', 'nc', 'ntp', 'fetchclient', 'fuse', 'fuse-devel', 'fuse-libs', 'oniguruma', 'pciutils', 'spice', 'spice-protocol', 'sysstat', 'tcpdump', 'telegraf-1.5.18-1', 'usbredir', 'yunion-qemu-2.12.1', 'yunion-ocadm', 'yunion-climc', 'yunion-executor-server', 'kernel-3.10.0-1062.4.3.el7.yn20191203', 'kernel-devel-3.10.0-1062.4.3.el7.yn20191203', 'kernel- headers-3.10.0-1062.4.3.el7.yn20191203', 'kmod-openvswitch-2.9.6-1.el7', 'openvswitch-2.9.6-1.el7', 'net-tools']` and remove the loop. This feature will be removed in version 2.11. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. ok: [127.0.0.1] =\u0026gt; (item=[u'epel-release', u'libaio', u'jq', u'libusb', u'lvm2', u'nc', u'ntp', u'fetchclient', u'fuse', u'fuse-devel', u'fuse-libs', u'oniguruma', u'pciutils', u'spice', u'spice-protocol', u'sysstat', u'tcpdump', u'telegraf-1.5.18-1', u'usbredir', u'yunion-qemu-2.12.1', u'yunion-ocadm', u'yunion-climc', u'yunion-executor-server', u'kernel-3.10.0-1062.4.3.el7.yn20191203', u'kernel-devel-3.10.0-1062.4.3.el7.yn20191203', u'kernel-headers-3.10.0-1062.4.3.el7.yn20191203', u'kmod-openvswitch-2.9.6-1.el7', u'openvswitch-2.9.6-1.el7', u'net-tools']) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;item\u0026quot;: [\u0026quot;epel-release\u0026quot;, \u0026quot;libaio\u0026quot;, \u0026quot;jq\u0026quot;, \u0026quot;libusb\u0026quot;, \u0026quot;lvm2\u0026quot;, \u0026quot;nc\u0026quot;, \u0026quot;ntp\u0026quot;, \u0026quot;fetchclient\u0026quot;, \u0026quot;fuse\u0026quot;, \u0026quot;fuse-devel\u0026quot;, \u0026quot;fuse-libs\u0026quot;, \u0026quot;oniguruma\u0026quot;, \u0026quot;pciutils\u0026quot;, \u0026quot;spice\u0026quot;, \u0026quot;spice-protocol\u0026quot;, \u0026quot;sysstat\u0026quot;, \u0026quot;tcpdump\u0026quot;, \u0026quot;telegraf-1.5.18-1\u0026quot;, \u0026quot;usbredir\u0026quot;, \u0026quot;yunion-qemu-2.12.1\u0026quot;, \u0026quot;yunion-ocadm\u0026quot;, \u0026quot;yunion-climc\u0026quot;, \u0026quot;yunion-executor-server\u0026quot;, \u0026quot;kernel-3.10.0-1062.4.3.el7.yn20191203\u0026quot;, \u0026quot;kernel-devel-3.10.0-1062.4.3.el7.yn20191203\u0026quot;, \u0026quot;kernel-headers-3.10.0-1062.4.3.el7.yn20191203\u0026quot;, \u0026quot;kmod-openvswitch-2.9.6-1.el7\u0026quot;, \u0026quot;openvswitch-2.9.6-1.el7\u0026quot;, \u0026quot;net-tools\u0026quot;], \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;results\u0026quot;: [\u0026quot;epel-release-7-12.noarch providing epel-release is already installed\u0026quot;, \u0026quot;libaio-0.3.109-13.el7.x86_64 providing libaio is already installed\u0026quot;, \u0026quot;jq-1.6-1.el7.x86_64 providing jq is already installed\u0026quot;, \u0026quot;1:libusb-0.1.4-3.el7.x86_64 providing libusb is already installed\u0026quot;, \u0026quot;7:lvm2-2.02.186-7.el7.x86_64 providing lvm2 is already installed\u0026quot;, \u0026quot;2:nmap-ncat-6.40-19.el7.x86_64 providing nc is already installed\u0026quot;, \u0026quot;ntp-4.2.6p5-29.el7.centos.x86_64 providing ntp is already installed\u0026quot;, \u0026quot;fetchclient-1.0.0-3.el7.centos.x86_64 providing fetchclient is already installed\u0026quot;, \u0026quot;fuse-2.9.2-11.el7.x86_64 providing fuse is already installed\u0026quot;, \u0026quot;fuse-devel-2.9.2-11.el7.x86_64 providing fuse-devel is already installed\u0026quot;, \u0026quot;fuse-libs-2.9.2-11.el7.x86_64 providing fuse-libs is already installed\u0026quot;, \u0026quot;oniguruma-5.9.5-3.el7.x86_64 providing oniguruma is already installed\u0026quot;, \u0026quot;pciutils-3.5.1-3.el7.x86_64 providing pciutils is already installed\u0026quot;, \u0026quot;spice-0.13.3-1.el7.centos.x86_64 providing spice is already installed\u0026quot;, \u0026quot;spice-protocol-0.12.14-1.el7.noarch providing spice-protocol is already installed\u0026quot;, \u0026quot;sysstat-10.1.5-19.el7.x86_64 providing sysstat is already installed\u0026quot;, \u0026quot;14:tcpdump-4.9.2-4.el7_7.1.x86_64 providing tcpdump is already installed\u0026quot;, \u0026quot;telegraf-1.5.18-1.x86_64 providing telegraf-1.5.18-1 is already installed\u0026quot;, \u0026quot;usbredir-0.7.1-3.el7.x86_64 providing usbredir is already installed\u0026quot;, \u0026quot;yunion-qemu-2.12.1-2.12.1-1.el7.centos.x86_64 providing yunion-qemu-2.12.1 is already installed\u0026quot;, \u0026quot;yunion-ocadm-3.1.9_20200609.1-1.x86_64 providing yunion-ocadm is already installed\u0026quot;, \u0026quot;yunion-climc-v3.1.9.20200609.1-20060920.x86_64 providing yunion-climc is already installed\u0026quot;, \u0026quot;yunion-executor-server-2.10.20190527.0-19120916.x86_64 providing yunion-executor-server is already installed\u0026quot;, \u0026quot;kernel-3.10.0-1062.4.3.el7.yn20191203.x86_64 providing kernel-3.10.0-1062.4.3.el7.yn20191203 is already installed\u0026quot;, \u0026quot;kernel-devel-3.10.0-1062.4.3.el7.yn20191203.x86_64 providing kernel-devel-3.10.0-1062.4.3.el7.yn20191203 is already installed\u0026quot;, \u0026quot;kernel-headers-3.10.0-1062.4.3.el7.yn20191203.x86_64 providing kernel-headers-3.10.0-1062.4.3.el7.yn20191203 is already installed\u0026quot;, \u0026quot;kmod-openvswitch-2.9.6-1.el7.x86_64 providing kmod-openvswitch-2.9.6-1.el7 is already installed\u0026quot;, \u0026quot;openvswitch-2.9.6-1.el7.x86_64 providing openvswitch-2.9.6-1.el7 is already installed\u0026quot;, \u0026quot;net-tools-2.0-0.25.20131004git.el7.x86_64 providing net-tools is already installed\u0026quot;]} TASK [common : Turn off selinux] ****************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:49 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;configfile\u0026quot;: \u0026quot;/etc/selinux/config\u0026quot;, \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;policy\u0026quot;: \u0026quot;targeted\u0026quot;, \u0026quot;reboot_required\u0026quot;: false, \u0026quot;state\u0026quot;: \u0026quot;disabled\u0026quot;} TASK [common : Gather service facts] ************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:53 ok: [127.0.0.1] =\u0026gt; {\u0026quot;ansible_facts\u0026quot;: {\u0026quot;services\u0026quot;: {\u0026quot;NetworkManager-dispatcher.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;NetworkManager-dispatcher.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;NetworkManager-wait-online.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;NetworkManager-wait-online.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;NetworkManager.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;NetworkManager.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;arp-ethers.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;arp-ethers.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;auditd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;auditd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;autovt@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;autovt@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;blk-availability.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;blk-availability.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;brandbot.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;brandbot.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;chrony-dnssrv@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;chrony-dnssrv@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;chrony-wait.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;chrony-wait.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;chronyd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;chronyd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;conntrackd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;conntrackd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;console-getty.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;console-getty.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;console-shell.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;console-shell.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;container-getty@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;container-getty@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;containerd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;containerd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;cpupower.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;cpupower.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;crond.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;crond.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;dbus-org.freedesktop.hostname1.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dbus-org.freedesktop.hostname1.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dbus-org.freedesktop.import1.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dbus-org.freedesktop.import1.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dbus-org.freedesktop.locale1.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dbus-org.freedesktop.locale1.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dbus-org.freedesktop.login1.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dbus-org.freedesktop.login1.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dbus-org.freedesktop.machine1.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dbus-org.freedesktop.machine1.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dbus-org.freedesktop.timedate1.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dbus-org.freedesktop.timedate1.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dbus.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dbus.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;debug-shell.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;debug-shell.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;dm-event.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dm-event.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;docker.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;docker.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;dracut-cmdline.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dracut-cmdline.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dracut-initqueue.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dracut-initqueue.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dracut-mount.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dracut-mount.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dracut-pre-mount.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dracut-pre-mount.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dracut-pre-pivot.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dracut-pre-pivot.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dracut-pre-trigger.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dracut-pre-trigger.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dracut-pre-udev.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dracut-pre-udev.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;dracut-shutdown.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;dracut-shutdown.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;ebtables.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;ebtables.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;emergency.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;emergency.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;firewalld.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;firewalld.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;fstrim.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;fstrim.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;getty@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;getty@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;getty@tty1.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;getty@tty1.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;unknown\u0026quot;}, \u0026quot;halt-local.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;halt-local.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;initrd-cleanup.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;initrd-cleanup.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;initrd-parse-etc.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;initrd-parse-etc.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;initrd-switch-root.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;initrd-switch-root.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;initrd-udevadm-cleanup-db.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;initrd-udevadm-cleanup-db.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;iprdump.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;iprdump.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;iprinit.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;iprinit.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;iprupdate.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;iprupdate.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;ipvsadm.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;ipvsadm.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;irqbalance.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;irqbalance.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;kdump.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;kdump.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;kmod-static-nodes.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;kmod-static-nodes.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;kubelet.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;kubelet.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;lvm2-lvmetad.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;lvm2-lvmetad.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;lvm2-lvmpolld.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;lvm2-lvmpolld.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;lvm2-monitor.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;lvm2-monitor.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;lvm2-pvscan@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;lvm2-pvscan@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;lvm2-pvscan@8:2.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;lvm2-pvscan@8:2.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;unknown\u0026quot;}, \u0026quot;mariadb.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;mariadb.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;messagebus.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;messagebus.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;microcode.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;microcode.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;netconsole\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;netconsole\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;sysv\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;network\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;network\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;sysv\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;network.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;network.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;unknown\u0026quot;}, \u0026quot;ntpd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;ntpd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;ntpdate.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;ntpdate.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;openvswitch.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;openvswitch.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;ovs-delete-transient-ports.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;ovs-delete-transient-ports.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;ovs-vswitchd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;ovs-vswitchd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;ovsdb-server.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;ovsdb-server.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;plymouth-halt.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;plymouth-halt.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;plymouth-kexec.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;plymouth-kexec.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;plymouth-poweroff.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;plymouth-poweroff.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;plymouth-quit-wait.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;plymouth-quit-wait.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;plymouth-quit.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;plymouth-quit.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;plymouth-read-write.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;plymouth-read-write.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;plymouth-reboot.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;plymouth-reboot.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;plymouth-start.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;plymouth-start.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;plymouth-switch-root.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;plymouth-switch-root.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;polkit.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;polkit.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;postfix.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;postfix.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;quotaon.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;quotaon.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;rc-local.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rc-local.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;rdisc.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rdisc.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;rescue.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rescue.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;rhel-autorelabel-mark.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rhel-autorelabel-mark.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;rhel-autorelabel.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rhel-autorelabel.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;rhel-configure.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rhel-configure.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;rhel-dmesg.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rhel-dmesg.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;rhel-domainname.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rhel-domainname.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;rhel-import-state.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rhel-import-state.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;rhel-loadmodules.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rhel-loadmodules.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;rhel-readonly.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rhel-readonly.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;rsyncd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rsyncd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;rsyncd@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rsyncd@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;rsyslog.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;rsyslog.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;selinux-policy-migrate-local-changes@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;selinux-policy-migrate-local-changes@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;selinux-policy-migrate-local-changes@targeted.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;selinux-policy-migrate-local-changes@targeted.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;unknown\u0026quot;}, \u0026quot;serial-getty@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;serial-getty@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;sshd-keygen.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;sshd-keygen.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;sshd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;sshd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;sshd@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;sshd@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;svnserve.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;svnserve.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;sysstat.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;sysstat.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;systemd-ask-password-console.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-ask-password-console.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-ask-password-plymouth.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-ask-password-plymouth.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-ask-password-wall.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-ask-password-wall.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-backlight@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-backlight@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-binfmt.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-binfmt.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-bootchart.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-bootchart.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;systemd-firstboot.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-firstboot.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-fsck-root.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-fsck-root.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-fsck@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-fsck@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-halt.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-halt.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-hibernate-resume@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-hibernate-resume@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-hibernate.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-hibernate.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-hostnamed.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-hostnamed.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-hwdb-update.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-hwdb-update.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-hybrid-sleep.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-hybrid-sleep.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-importd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-importd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-initctl.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-initctl.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-journal-catalog-update.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-journal-catalog-update.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-journal-flush.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-journal-flush.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-journald.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-journald.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-kexec.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-kexec.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-localed.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-localed.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-logind.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-logind.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-machine-id-commit.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-machine-id-commit.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-machined.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-machined.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-modules-load.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-modules-load.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-nspawn@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-nspawn@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;systemd-poweroff.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-poweroff.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-quotacheck.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-quotacheck.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-random-seed.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-random-seed.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-readahead-collect.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-readahead-collect.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;systemd-readahead-done.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-readahead-done.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;indirect\u0026quot;}, \u0026quot;systemd-readahead-drop.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-readahead-drop.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;systemd-readahead-replay.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-readahead-replay.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;systemd-reboot.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-reboot.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-remount-fs.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-remount-fs.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-rfkill@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-rfkill@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-shutdownd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-shutdownd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-suspend.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-suspend.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-sysctl.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-sysctl.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-timedated.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-timedated.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-tmpfiles-clean.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-tmpfiles-clean.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-tmpfiles-setup-dev.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-tmpfiles-setup-dev.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-tmpfiles-setup.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-tmpfiles-setup.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-udev-settle.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-udev-settle.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-udev-trigger.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-udev-trigger.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-udevd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-udevd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-update-done.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-update-done.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-update-utmp-runlevel.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-update-utmp-runlevel.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-update-utmp.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-update-utmp.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-user-sessions.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-user-sessions.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;systemd-vconsole-setup.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;systemd-vconsole-setup.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;tcsd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;tcsd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;teamd@.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;teamd@.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;static\u0026quot;}, \u0026quot;telegraf.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;telegraf.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;tuned.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;tuned.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;vgauthd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;vgauthd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;vmtoolsd-init.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;vmtoolsd-init.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;vmtoolsd.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;vmtoolsd.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}, \u0026quot;wpa_supplicant.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;wpa_supplicant.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;disabled\u0026quot;}, \u0026quot;yunion-executor.service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;yunion-executor.service\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;enabled\u0026quot;}}}, \u0026quot;changed\u0026quot;: false} TASK [common : Disable systemd services] ********************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:56 ok: [127.0.0.1] =\u0026gt; (item=firewalld) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;enabled\u0026quot;: false, \u0026quot;item\u0026quot;: \u0026quot;firewalld\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;firewalld\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: {\u0026quot;ActiveEnterTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ActiveExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ActiveState\u0026quot;: \u0026quot;inactive\u0026quot;, \u0026quot;After\u0026quot;: \u0026quot;polkit.service system.slice basic.target dbus.service\u0026quot;, \u0026quot;AllowIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;AmbientCapabilities\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;AssertResult\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;AssertTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Before\u0026quot;: \u0026quot;shutdown.target docker.service network-pre.target\u0026quot;, \u0026quot;BlockIOAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;BlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;BusName\u0026quot;: \u0026quot;org.fedoraproject.FirewallD1\u0026quot;, \u0026quot;CPUAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUQuotaPerSecUSec\u0026quot;: \u0026quot;infinity\u0026quot;, \u0026quot;CPUSchedulingPolicy\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingPriority\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingResetOnFork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CanIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CanReload\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CanStart\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CanStop\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CapabilityBoundingSet\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;ConditionResult\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ConditionTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Conflicts\u0026quot;: \u0026quot;shutdown.target ebtables.service ip6tables.service iptables.service ipset.service\u0026quot;, \u0026quot;ControlPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DefaultDependencies\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Delegate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;firewalld - dynamic firewall daemon\u0026quot;, \u0026quot;DevicePolicy\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;Documentation\u0026quot;: \u0026quot;man:firewalld(1)\u0026quot;, \u0026quot;EnvironmentFile\u0026quot;: \u0026quot;/etc/sysconfig/firewalld (ignore_errors=yes)\u0026quot;, \u0026quot;ExecMainCode\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainStartTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainStatus\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecReload\u0026quot;: \u0026quot;{ path=/bin/kill ; argv[]=/bin/kill -HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;ExecStart\u0026quot;: \u0026quot;{ path=/usr/sbin/firewalld ; argv[]=/usr/sbin/firewalld --nofork --nopid $FIREWALLD_ARGS ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;FailureAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;FileDescriptorStoreMax\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;FragmentPath\u0026quot;: \u0026quot;/usr/lib/systemd/system/firewalld.service\u0026quot;, \u0026quot;GuessMainPID\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;IOScheduling\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;firewalld.service\u0026quot;, \u0026quot;IgnoreOnIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreOnSnapshot\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreSIGPIPE\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;InactiveEnterTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;InactiveExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;JobTimeoutAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;JobTimeoutUSec\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;KillMode\u0026quot;: \u0026quot;mixed\u0026quot;, \u0026quot;KillSignal\u0026quot;: \u0026quot;15\u0026quot;, \u0026quot;LimitAS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCORE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCPU\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitDATA\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitFSIZE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitLOCKS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitMEMLOCK\u0026quot;: \u0026quot;65536\u0026quot;, \u0026quot;LimitMSGQUEUE\u0026quot;: \u0026quot;819200\u0026quot;, \u0026quot;LimitNICE\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitNOFILE\u0026quot;: \u0026quot;4096\u0026quot;, \u0026quot;LimitNPROC\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitRSS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitRTPRIO\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitRTTIME\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitSIGPENDING\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitSTACK\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LoadState\u0026quot;: \u0026quot;loaded\u0026quot;, \u0026quot;MainPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;MemoryAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;MemoryCurrent\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MemoryLimit\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MountFlags\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Names\u0026quot;: \u0026quot;firewalld.service\u0026quot;, \u0026quot;NeedDaemonReload\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Nice\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;NoNewPrivileges\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NonBlocking\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NotifyAccess\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;OOMScoreAdjust\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;OnFailureJobMode\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;PermissionsStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateDevices\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateNetwork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateTmp\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectHome\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectSystem\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStart\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStop\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RemainAfterExit\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Requires\u0026quot;: \u0026quot;system.slice basic.target\u0026quot;, \u0026quot;Restart\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RestartUSec\u0026quot;: \u0026quot;100ms\u0026quot;, \u0026quot;Result\u0026quot;: \u0026quot;success\u0026quot;, \u0026quot;RootDirectoryStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RuntimeDirectoryMode\u0026quot;: \u0026quot;0755\u0026quot;, \u0026quot;SameProcessGroup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SecureBits\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;SendSIGHUP\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SendSIGKILL\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Slice\u0026quot;: \u0026quot;system.slice\u0026quot;, \u0026quot;StandardError\u0026quot;: \u0026quot;null\u0026quot;, \u0026quot;StandardInput\u0026quot;: \u0026quot;null\u0026quot;, \u0026quot;StandardOutput\u0026quot;: \u0026quot;null\u0026quot;, \u0026quot;StartLimitAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;StartLimitBurst\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;StartLimitInterval\u0026quot;: \u0026quot;10000000\u0026quot;, \u0026quot;StartupBlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StartupCPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StatusErrno\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;StopWhenUnneeded\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SubState\u0026quot;: \u0026quot;dead\u0026quot;, \u0026quot;SyslogLevelPrefix\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;SyslogPriority\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;SystemCallErrorNumber\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;TTYReset\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVHangup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVTDisallocate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksCurrent\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TasksMax\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TimeoutStartUSec\u0026quot;: \u0026quot;1min 30s\u0026quot;, \u0026quot;TimeoutStopUSec\u0026quot;: \u0026quot;1min 30s\u0026quot;, \u0026quot;TimerSlackNSec\u0026quot;: \u0026quot;50000\u0026quot;, \u0026quot;Transient\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;dbus\u0026quot;, \u0026quot;UMask\u0026quot;: \u0026quot;0022\u0026quot;, \u0026quot;UnitFilePreset\u0026quot;: \u0026quot;enabled\u0026quot;, \u0026quot;UnitFileState\u0026quot;: \u0026quot;disabled\u0026quot;, \u0026quot;Wants\u0026quot;: \u0026quot;network-pre.target\u0026quot;, \u0026quot;WatchdogTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;WatchdogUSec\u0026quot;: \u0026quot;0\u0026quot;}} ok: [127.0.0.1] =\u0026gt; (item=NetworkManager) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;enabled\u0026quot;: false, \u0026quot;item\u0026quot;: \u0026quot;NetworkManager\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;NetworkManager\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;status\u0026quot;: {\u0026quot;ActiveEnterTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ActiveExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ActiveState\u0026quot;: \u0026quot;inactive\u0026quot;, \u0026quot;After\u0026quot;: \u0026quot;systemd-journald.socket network-pre.target system.slice dbus.service basic.target\u0026quot;, \u0026quot;AllowIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;AmbientCapabilities\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;AssertResult\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;AssertTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Before\u0026quot;: \u0026quot;NetworkManager-wait-online.service network.service network.target shutdown.target\u0026quot;, \u0026quot;BlockIOAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;BlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;BusName\u0026quot;: \u0026quot;org.freedesktop.NetworkManager\u0026quot;, \u0026quot;CPUAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUQuotaPerSecUSec\u0026quot;: \u0026quot;infinity\u0026quot;, \u0026quot;CPUSchedulingPolicy\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingPriority\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingResetOnFork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CanIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CanReload\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CanStart\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CanStop\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CapabilityBoundingSet\u0026quot;: \u0026quot;539309282\u0026quot;, \u0026quot;ConditionResult\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ConditionTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Conflicts\u0026quot;: \u0026quot;shutdown.target\u0026quot;, \u0026quot;ControlPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DefaultDependencies\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Delegate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;Network Manager\u0026quot;, \u0026quot;DevicePolicy\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;Documentation\u0026quot;: \u0026quot;man:NetworkManager(8)\u0026quot;, \u0026quot;ExecMainCode\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainStartTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainStatus\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecReload\u0026quot;: \u0026quot;{ path=/usr/bin/dbus-send ; argv[]=/usr/bin/dbus-send --print-reply --system --type=method_call --dest=org.freedesktop.NetworkManager /org/freedesktop/NetworkManager org.freedesktop.NetworkManager.Reload uint32:0 ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;ExecStart\u0026quot;: \u0026quot;{ path=/usr/sbin/NetworkManager ; argv[]=/usr/sbin/NetworkManager --no-daemon ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;FailureAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;FileDescriptorStoreMax\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;FragmentPath\u0026quot;: \u0026quot;/usr/lib/systemd/system/NetworkManager.service\u0026quot;, \u0026quot;GuessMainPID\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;IOScheduling\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;NetworkManager.service\u0026quot;, \u0026quot;IgnoreOnIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreOnSnapshot\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreSIGPIPE\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;InactiveEnterTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;InactiveExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;JobTimeoutAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;JobTimeoutUSec\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;KillMode\u0026quot;: \u0026quot;process\u0026quot;, \u0026quot;KillSignal\u0026quot;: \u0026quot;15\u0026quot;, \u0026quot;LimitAS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCORE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCPU\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitDATA\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitFSIZE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitLOCKS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitMEMLOCK\u0026quot;: \u0026quot;65536\u0026quot;, \u0026quot;LimitMSGQUEUE\u0026quot;: \u0026quot;819200\u0026quot;, \u0026quot;LimitNICE\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitNOFILE\u0026quot;: \u0026quot;4096\u0026quot;, \u0026quot;LimitNPROC\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitRSS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitRTPRIO\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitRTTIME\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitSIGPENDING\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitSTACK\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LoadState\u0026quot;: \u0026quot;loaded\u0026quot;, \u0026quot;MainPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;MemoryAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;MemoryCurrent\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MemoryLimit\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MountFlags\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Names\u0026quot;: \u0026quot;NetworkManager.service\u0026quot;, \u0026quot;NeedDaemonReload\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Nice\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;NoNewPrivileges\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NonBlocking\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NotifyAccess\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;OOMScoreAdjust\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;OnFailureJobMode\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;PermissionsStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateDevices\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateNetwork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateTmp\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectHome\u0026quot;: \u0026quot;read-only\u0026quot;, \u0026quot;ProtectSystem\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;RefuseManualStart\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStop\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RemainAfterExit\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RequiredBy\u0026quot;: \u0026quot;NetworkManager-wait-online.service\u0026quot;, \u0026quot;Requires\u0026quot;: \u0026quot;system.slice basic.target\u0026quot;, \u0026quot;Restart\u0026quot;: \u0026quot;on-failure\u0026quot;, \u0026quot;RestartUSec\u0026quot;: \u0026quot;100ms\u0026quot;, \u0026quot;Result\u0026quot;: \u0026quot;success\u0026quot;, \u0026quot;RootDirectoryStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RuntimeDirectoryMode\u0026quot;: \u0026quot;0755\u0026quot;, \u0026quot;SameProcessGroup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SecureBits\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;SendSIGHUP\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SendSIGKILL\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Slice\u0026quot;: \u0026quot;system.slice\u0026quot;, \u0026quot;StandardError\u0026quot;: \u0026quot;inherit\u0026quot;, \u0026quot;StandardInput\u0026quot;: \u0026quot;null\u0026quot;, \u0026quot;StandardOutput\u0026quot;: \u0026quot;journal\u0026quot;, \u0026quot;StartLimitAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;StartLimitBurst\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;StartLimitInterval\u0026quot;: \u0026quot;10000000\u0026quot;, \u0026quot;StartupBlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StartupCPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StatusErrno\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;StopWhenUnneeded\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SubState\u0026quot;: \u0026quot;dead\u0026quot;, \u0026quot;SyslogLevelPrefix\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;SyslogPriority\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;SystemCallErrorNumber\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;TTYReset\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVHangup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVTDisallocate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksCurrent\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TasksMax\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TimeoutStartUSec\u0026quot;: \u0026quot;1min 30s\u0026quot;, \u0026quot;TimeoutStopUSec\u0026quot;: \u0026quot;1min 30s\u0026quot;, \u0026quot;TimerSlackNSec\u0026quot;: \u0026quot;50000\u0026quot;, \u0026quot;Transient\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;dbus\u0026quot;, \u0026quot;UMask\u0026quot;: \u0026quot;0022\u0026quot;, \u0026quot;UnitFilePreset\u0026quot;: \u0026quot;enabled\u0026quot;, \u0026quot;UnitFileState\u0026quot;: \u0026quot;disabled\u0026quot;, \u0026quot;Wants\u0026quot;: \u0026quot;network.target\u0026quot;, \u0026quot;WatchdogTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;WatchdogUSec\u0026quot;: \u0026quot;0\u0026quot;}} TASK [common : Load br_netfilter] ***************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:66 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;name\u0026quot;: \u0026quot;br_netfilter\u0026quot;, \u0026quot;params\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;present\u0026quot;} TASK [common : Load br_netfilter at boot] ********************************************************************************************* task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:71 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;checksum\u0026quot;: \u0026quot;c3394aff3e3a3e3920e68205b8e839b3d725648e\u0026quot;, \u0026quot;dest\u0026quot;: \u0026quot;/etc/modules-load.d/kubernetes.conf\u0026quot;, \u0026quot;gid\u0026quot;: 0, \u0026quot;group\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;0644\u0026quot;, \u0026quot;owner\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/etc/modules-load.d/kubernetes.conf\u0026quot;, \u0026quot;size\u0026quot;: 12, \u0026quot;state\u0026quot;: \u0026quot;file\u0026quot;, \u0026quot;uid\u0026quot;: 0} TASK [common : Change sysctl setting] ************************************************************************************************* task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:80 ok: [127.0.0.1] =\u0026gt; (item=net.bridge.bridge-nf-call-iptables) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;item\u0026quot;: \u0026quot;net.bridge.bridge-nf-call-iptables\u0026quot;} ok: [127.0.0.1] =\u0026gt; (item=net.bridge.bridge-nf-call-ip6tables) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;item\u0026quot;: \u0026quot;net.bridge.bridge-nf-call-ip6tables\u0026quot;} ok: [127.0.0.1] =\u0026gt; (item=net.ipv4.ip_forward) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;item\u0026quot;: \u0026quot;net.ipv4.ip_forward\u0026quot;} TASK [common : Change sysctl fs.inotify.max_user_watches] ***************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:90 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false} TASK [common : Change sysctl fs.inotify.max_user_instances] *************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:96 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false} TASK [common : Turn off swap mount] *************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:103 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;dump\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;fstab\u0026quot;: \u0026quot;/etc/fstab\u0026quot;, \u0026quot;fstype\u0026quot;: \u0026quot;swap\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;swap\u0026quot;, \u0026quot;opts\u0026quot;: \u0026quot;defaults\u0026quot;, \u0026quot;passno\u0026quot;: \u0026quot;0\u0026quot;} TASK [common : Turn off swap now] ***************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:108 skipping: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;skip_reason\u0026quot;: \u0026quot;Conditional result was False\u0026quot;} TASK [common : Config ipvs] *********************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:112 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;checksum\u0026quot;: \u0026quot;6cd1b849237cd98710c45881e193b51928da107b\u0026quot;, \u0026quot;dest\u0026quot;: \u0026quot;/etc/sysconfig/modules/ipvs.modules\u0026quot;, \u0026quot;gid\u0026quot;: 0, \u0026quot;group\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;0755\u0026quot;, \u0026quot;owner\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/etc/sysconfig/modules/ipvs.modules\u0026quot;, \u0026quot;size\u0026quot;: 337, \u0026quot;state\u0026quot;: \u0026quot;file\u0026quot;, \u0026quot;uid\u0026quot;: 0} TASK [common : Load ipvs modules] ***************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:119 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;/etc/sysconfig/modules/ipvs.modules\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.086708\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:12:25.258452\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:12:25.171744\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stdout_lines\u0026quot;: []} TASK [common : Add docker rpm repository] ********************************************************************************************* task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:125 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;checksum_dest\u0026quot;: \u0026quot;f5df3864590b9dcc53e2dc95bbbf0bacba2878bf\u0026quot;, \u0026quot;checksum_src\u0026quot;: \u0026quot;f5df3864590b9dcc53e2dc95bbbf0bacba2878bf\u0026quot;, \u0026quot;dest\u0026quot;: \u0026quot;/etc/yum.repos.d/docker-ce.repo\u0026quot;, \u0026quot;elapsed\u0026quot;: 0, \u0026quot;gid\u0026quot;: 0, \u0026quot;group\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;md5sum\u0026quot;: \u0026quot;494b8cf832d408308d9c356c523292ab\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;0644\u0026quot;, \u0026quot;msg\u0026quot;: \u0026quot;OK (2640 bytes)\u0026quot;, \u0026quot;owner\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;size\u0026quot;: 2640, \u0026quot;src\u0026quot;: \u0026quot;/root/.ansible/tmp/ansible-tmp-1592572345.49-55208-29542343234615/tmpEz4Z4A\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;file\u0026quot;, \u0026quot;status_code\u0026quot;: 200, \u0026quot;uid\u0026quot;: 0, \u0026quot;url\u0026quot;: \u0026quot;http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\u0026quot;} TASK [common : Install docker] ******************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:131 [DEPRECATION WARNING]: Invoking \u0026quot;yum\u0026quot; only once while using a loop via squash_actions is deprecated. Instead of using a loop to supply multiple items and specifying `name: \u0026quot;{{ item }}\u0026quot;`, please use `name: ['docker-ce-19.03.9', 'docker-ce-cli-19.03.9', 'containerd.io']` and remove the loop. This feature will be removed in version 2.11. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. ok: [127.0.0.1] =\u0026gt; (item=[u'docker-ce-19.03.9', u'docker-ce-cli-19.03.9', u'containerd.io']) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;item\u0026quot;: [\u0026quot;docker-ce-19.03.9\u0026quot;, \u0026quot;docker-ce-cli-19.03.9\u0026quot;, \u0026quot;containerd.io\u0026quot;], \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;results\u0026quot;: [\u0026quot;3:docker-ce-19.03.9-3.el7.x86_64 providing docker-ce-19.03.9 is already installed\u0026quot;, \u0026quot;1:docker-ce-cli-19.03.9-3.el7.x86_64 providing docker-ce-cli-19.03.9 is already installed\u0026quot;, \u0026quot;containerd.io-1.2.13-3.2.el7.x86_64 providing containerd.io is already installed\u0026quot;]} TASK [common : Make /etc/docker dir] ************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:138 [WARNING]: Consider using the file module with state=directory rather than running 'mkdir'. If you need to use command because file is insufficient you can add 'warn: false' to this command task or set 'command_warnings=False' in ansible.cfg to get rid of this message. changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;mkdir -p /etc/docker\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.010111\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:12:27.301673\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:12:27.291562\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stdout_lines\u0026quot;: []} TASK [common : Config docker file] **************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:141 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;checksum\u0026quot;: \u0026quot;764b1b752176a0b8432bc31283b18af42499601a\u0026quot;, \u0026quot;dest\u0026quot;: \u0026quot;/etc/docker/daemon.json\u0026quot;, \u0026quot;gid\u0026quot;: 0, \u0026quot;group\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;0644\u0026quot;, \u0026quot;owner\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/etc/docker/daemon.json\u0026quot;, \u0026quot;size\u0026quot;: 337, \u0026quot;state\u0026quot;: \u0026quot;file\u0026quot;, \u0026quot;uid\u0026quot;: 0} TASK [common : Restart docker service] ************************************************************************************************ task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:144 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;enabled\u0026quot;: true, \u0026quot;name\u0026quot;: \u0026quot;docker\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;started\u0026quot;, \u0026quot;status\u0026quot;: {\u0026quot;ActiveEnterTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:12 EDT\u0026quot;, \u0026quot;ActiveEnterTimestampMonotonic\u0026quot;: \u0026quot;10481250914\u0026quot;, \u0026quot;ActiveExitTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:12 EDT\u0026quot;, \u0026quot;ActiveExitTimestampMonotonic\u0026quot;: \u0026quot;10480910143\u0026quot;, \u0026quot;ActiveState\u0026quot;: \u0026quot;active\u0026quot;, \u0026quot;After\u0026quot;: \u0026quot;docker.socket firewalld.service basic.target system.slice containerd.service systemd-journald.socket network-online.target\u0026quot;, \u0026quot;AllowIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;AmbientCapabilities\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;AssertResult\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;AssertTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:12 EDT\u0026quot;, \u0026quot;AssertTimestampMonotonic\u0026quot;: \u0026quot;10480929019\u0026quot;, \u0026quot;Before\u0026quot;: \u0026quot;shutdown.target multi-user.target\u0026quot;, \u0026quot;BindsTo\u0026quot;: \u0026quot;containerd.service\u0026quot;, \u0026quot;BlockIOAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;BlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CPUAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUQuotaPerSecUSec\u0026quot;: \u0026quot;infinity\u0026quot;, \u0026quot;CPUSchedulingPolicy\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingPriority\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingResetOnFork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CanIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CanReload\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CanStart\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CanStop\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CapabilityBoundingSet\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;ConditionResult\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;ConditionTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:12 EDT\u0026quot;, \u0026quot;ConditionTimestampMonotonic\u0026quot;: \u0026quot;10480929019\u0026quot;, \u0026quot;Conflicts\u0026quot;: \u0026quot;shutdown.target\u0026quot;, \u0026quot;ConsistsOf\u0026quot;: \u0026quot;docker.socket\u0026quot;, \u0026quot;ControlGroup\u0026quot;: \u0026quot;/system.slice/docker.service\u0026quot;, \u0026quot;ControlPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DefaultDependencies\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Delegate\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;Docker Application Container Engine\u0026quot;, \u0026quot;DevicePolicy\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;Documentation\u0026quot;: \u0026quot;https://docs.docker.com\u0026quot;, \u0026quot;ExecMainCode\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainPID\u0026quot;: \u0026quot;49020\u0026quot;, \u0026quot;ExecMainStartTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:12 EDT\u0026quot;, \u0026quot;ExecMainStartTimestampMonotonic\u0026quot;: \u0026quot;10480940693\u0026quot;, \u0026quot;ExecMainStatus\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecReload\u0026quot;: \u0026quot;{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;ExecStart\u0026quot;: \u0026quot;{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;FailureAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;FileDescriptorStoreMax\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;FragmentPath\u0026quot;: \u0026quot;/usr/lib/systemd/system/docker.service\u0026quot;, \u0026quot;GuessMainPID\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;IOScheduling\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;docker.service\u0026quot;, \u0026quot;IgnoreOnIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreOnSnapshot\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreSIGPIPE\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;InactiveEnterTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:12 EDT\u0026quot;, \u0026quot;InactiveEnterTimestampMonotonic\u0026quot;: \u0026quot;10480920291\u0026quot;, \u0026quot;InactiveExitTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:12 EDT\u0026quot;, \u0026quot;InactiveExitTimestampMonotonic\u0026quot;: \u0026quot;10480940781\u0026quot;, \u0026quot;JobTimeoutAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;JobTimeoutUSec\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;KillMode\u0026quot;: \u0026quot;process\u0026quot;, \u0026quot;KillSignal\u0026quot;: \u0026quot;15\u0026quot;, \u0026quot;LimitAS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCORE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCPU\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitDATA\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitFSIZE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitLOCKS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitMEMLOCK\u0026quot;: \u0026quot;65536\u0026quot;, \u0026quot;LimitMSGQUEUE\u0026quot;: \u0026quot;819200\u0026quot;, \u0026quot;LimitNICE\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitNOFILE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitNPROC\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitRSS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitRTPRIO\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitRTTIME\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitSIGPENDING\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitSTACK\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LoadState\u0026quot;: \u0026quot;loaded\u0026quot;, \u0026quot;MainPID\u0026quot;: \u0026quot;49020\u0026quot;, \u0026quot;MemoryAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;MemoryCurrent\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MemoryLimit\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MountFlags\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Names\u0026quot;: \u0026quot;docker.service\u0026quot;, \u0026quot;NeedDaemonReload\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Nice\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;NoNewPrivileges\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NonBlocking\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NotifyAccess\u0026quot;: \u0026quot;main\u0026quot;, \u0026quot;OOMScoreAdjust\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;OnFailureJobMode\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;PermissionsStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateDevices\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateNetwork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateTmp\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectHome\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectSystem\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStart\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStop\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RemainAfterExit\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Requires\u0026quot;: \u0026quot;basic.target docker.socket system.slice\u0026quot;, \u0026quot;Restart\u0026quot;: \u0026quot;always\u0026quot;, \u0026quot;RestartUSec\u0026quot;: \u0026quot;2s\u0026quot;, \u0026quot;Result\u0026quot;: \u0026quot;success\u0026quot;, \u0026quot;RootDirectoryStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RuntimeDirectoryMode\u0026quot;: \u0026quot;0755\u0026quot;, \u0026quot;SameProcessGroup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SecureBits\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;SendSIGHUP\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SendSIGKILL\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Slice\u0026quot;: \u0026quot;system.slice\u0026quot;, \u0026quot;StandardError\u0026quot;: \u0026quot;inherit\u0026quot;, \u0026quot;StandardInput\u0026quot;: \u0026quot;null\u0026quot;, \u0026quot;StandardOutput\u0026quot;: \u0026quot;journal\u0026quot;, \u0026quot;StartLimitAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;StartLimitBurst\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;StartLimitInterval\u0026quot;: \u0026quot;60000000\u0026quot;, \u0026quot;StartupBlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StartupCPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StatusErrno\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;StopWhenUnneeded\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SubState\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;SyslogLevelPrefix\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;SyslogPriority\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;SystemCallErrorNumber\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;TTYReset\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVHangup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVTDisallocate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksCurrent\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TasksMax\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TimeoutStartUSec\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;TimeoutStopUSec\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;TimerSlackNSec\u0026quot;: \u0026quot;50000\u0026quot;, \u0026quot;Transient\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TriggeredBy\u0026quot;: \u0026quot;docker.socket\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;notify\u0026quot;, \u0026quot;UMask\u0026quot;: \u0026quot;0022\u0026quot;, \u0026quot;UnitFilePreset\u0026quot;: \u0026quot;disabled\u0026quot;, \u0026quot;UnitFileState\u0026quot;: \u0026quot;enabled\u0026quot;, \u0026quot;WantedBy\u0026quot;: \u0026quot;multi-user.target\u0026quot;, \u0026quot;Wants\u0026quot;: \u0026quot;network-online.target\u0026quot;, \u0026quot;WatchdogTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:12 EDT\u0026quot;, \u0026quot;WatchdogTimestampMonotonic\u0026quot;: \u0026quot;10481250856\u0026quot;, \u0026quot;WatchdogUSec\u0026quot;: \u0026quot;0\u0026quot;}} TASK [common : Add kubernetes rpm repository] ***************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:150 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;checksum\u0026quot;: \u0026quot;2547dde5142d0fc2e1e245df8ad221b5b0cfc34e\u0026quot;, \u0026quot;dest\u0026quot;: \u0026quot;/etc/yum.repos.d/kubernetes.repo\u0026quot;, \u0026quot;gid\u0026quot;: 0, \u0026quot;group\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;0755\u0026quot;, \u0026quot;owner\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/etc/yum.repos.d/kubernetes.repo\u0026quot;, \u0026quot;size\u0026quot;: 272, \u0026quot;state\u0026quot;: \u0026quot;file\u0026quot;, \u0026quot;uid\u0026quot;: 0} TASK [common : Install k8s packages] ************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:158 [DEPRECATION WARNING]: Invoking \u0026quot;yum\u0026quot; only once while using a loop via squash_actions is deprecated. Instead of using a loop to supply multiple items and specifying `name: \u0026quot;{{ item }}\u0026quot;`, please use `name: ['bridge-utils', 'ipvsadm', 'conntrack-tools', 'jq', 'kubelet-1.15.8-0', 'kubectl-1.15.8-0', 'kubeadm-1.15.8-0']` and remove the loop. This feature will be removed in version 2.11. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. ok: [127.0.0.1] =\u0026gt; (item=[u'bridge-utils', u'ipvsadm', u'conntrack-tools', u'jq', u'kubelet-1.15.8-0', u'kubectl-1.15.8-0', u'kubeadm-1.15.8-0']) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;item\u0026quot;: [\u0026quot;bridge-utils\u0026quot;, \u0026quot;ipvsadm\u0026quot;, \u0026quot;conntrack-tools\u0026quot;, \u0026quot;jq\u0026quot;, \u0026quot;kubelet-1.15.8-0\u0026quot;, \u0026quot;kubectl-1.15.8-0\u0026quot;, \u0026quot;kubeadm-1.15.8-0\u0026quot;], \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;results\u0026quot;: [\u0026quot;bridge-utils-1.5-9.el7.x86_64 providing bridge-utils is already installed\u0026quot;, \u0026quot;ipvsadm-1.27-8.el7.x86_64 providing ipvsadm is already installed\u0026quot;, \u0026quot;conntrack-tools-1.4.4-7.el7.x86_64 providing conntrack-tools is already installed\u0026quot;, \u0026quot;jq-1.6-1.el7.x86_64 providing jq is already installed\u0026quot;, \u0026quot;kubelet-1.15.8-0.x86_64 providing kubelet-1.15.8-0 is already installed\u0026quot;, \u0026quot;kubectl-1.15.8-0.x86_64 providing kubectl-1.15.8-0 is already installed\u0026quot;, \u0026quot;kubeadm-1.15.8-0.x86_64 providing kubeadm-1.15.8-0 is already installed\u0026quot;]} TASK [common : Enable kubelet] ******************************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:170 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;enabled\u0026quot;: true, \u0026quot;name\u0026quot;: \u0026quot;kubelet\u0026quot;, \u0026quot;status\u0026quot;: {\u0026quot;ActiveEnterTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:56 EDT\u0026quot;, \u0026quot;ActiveEnterTimestampMonotonic\u0026quot;: \u0026quot;10525198841\u0026quot;, \u0026quot;ActiveExitTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:11:16 EDT\u0026quot;, \u0026quot;ActiveExitTimestampMonotonic\u0026quot;: \u0026quot;10605261077\u0026quot;, \u0026quot;ActiveState\u0026quot;: \u0026quot;inactive\u0026quot;, \u0026quot;After\u0026quot;: \u0026quot;basic.target systemd-journald.socket system.slice\u0026quot;, \u0026quot;AllowIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;AmbientCapabilities\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;AssertResult\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;AssertTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:56 EDT\u0026quot;, \u0026quot;AssertTimestampMonotonic\u0026quot;: \u0026quot;10525190197\u0026quot;, \u0026quot;Before\u0026quot;: \u0026quot;shutdown.target multi-user.target\u0026quot;, \u0026quot;BlockIOAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;BlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CPUAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUQuotaPerSecUSec\u0026quot;: \u0026quot;infinity\u0026quot;, \u0026quot;CPUSchedulingPolicy\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingPriority\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingResetOnFork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CanIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CanReload\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CanStart\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CanStop\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CapabilityBoundingSet\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;ConditionResult\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;ConditionTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:56 EDT\u0026quot;, \u0026quot;ConditionTimestampMonotonic\u0026quot;: \u0026quot;10525190197\u0026quot;, \u0026quot;Conflicts\u0026quot;: \u0026quot;shutdown.target\u0026quot;, \u0026quot;ControlPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DefaultDependencies\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Delegate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;kubelet: The Kubernetes Node Agent\u0026quot;, \u0026quot;DevicePolicy\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;Documentation\u0026quot;: \u0026quot;https://kubernetes.io/docs/\u0026quot;, \u0026quot;DropInPaths\u0026quot;: \u0026quot;/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\u0026quot;, \u0026quot;Environment\u0026quot;: \u0026quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\u0026quot;, \u0026quot;EnvironmentFile\u0026quot;: \u0026quot;/etc/sysconfig/kubelet (ignore_errors=yes)\u0026quot;, \u0026quot;ExecMainCode\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;ExecMainExitTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:11:16 EDT\u0026quot;, \u0026quot;ExecMainExitTimestampMonotonic\u0026quot;: \u0026quot;10605269793\u0026quot;, \u0026quot;ExecMainPID\u0026quot;: \u0026quot;49741\u0026quot;, \u0026quot;ExecMainStartTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:56 EDT\u0026quot;, \u0026quot;ExecMainStartTimestampMonotonic\u0026quot;: \u0026quot;10525198693\u0026quot;, \u0026quot;ExecMainStatus\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecStart\u0026quot;: \u0026quot;{ path=/usr/bin/kubelet ; argv[]=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;FailureAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;FileDescriptorStoreMax\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;FragmentPath\u0026quot;: \u0026quot;/usr/lib/systemd/system/kubelet.service\u0026quot;, \u0026quot;GuessMainPID\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;IOScheduling\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;kubelet.service\u0026quot;, \u0026quot;IgnoreOnIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreOnSnapshot\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreSIGPIPE\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;InactiveEnterTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:11:16 EDT\u0026quot;, \u0026quot;InactiveEnterTimestampMonotonic\u0026quot;: \u0026quot;10605270045\u0026quot;, \u0026quot;InactiveExitTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 09:09:56 EDT\u0026quot;, \u0026quot;InactiveExitTimestampMonotonic\u0026quot;: \u0026quot;10525198841\u0026quot;, \u0026quot;JobTimeoutAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;JobTimeoutUSec\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;KillMode\u0026quot;: \u0026quot;control-group\u0026quot;, \u0026quot;KillSignal\u0026quot;: \u0026quot;15\u0026quot;, \u0026quot;LimitAS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCORE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCPU\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitDATA\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitFSIZE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitLOCKS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitMEMLOCK\u0026quot;: \u0026quot;65536\u0026quot;, \u0026quot;LimitMSGQUEUE\u0026quot;: \u0026quot;819200\u0026quot;, \u0026quot;LimitNICE\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitNOFILE\u0026quot;: \u0026quot;4096\u0026quot;, \u0026quot;LimitNPROC\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitRSS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitRTPRIO\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitRTTIME\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitSIGPENDING\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitSTACK\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LoadState\u0026quot;: \u0026quot;loaded\u0026quot;, \u0026quot;MainPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;MemoryAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;MemoryCurrent\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MemoryLimit\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MountFlags\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Names\u0026quot;: \u0026quot;kubelet.service\u0026quot;, \u0026quot;NeedDaemonReload\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Nice\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;NoNewPrivileges\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NonBlocking\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NotifyAccess\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;OOMScoreAdjust\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;OnFailureJobMode\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;PermissionsStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateDevices\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateNetwork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateTmp\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectHome\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectSystem\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStart\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStop\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RemainAfterExit\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Requires\u0026quot;: \u0026quot;basic.target system.slice\u0026quot;, \u0026quot;Restart\u0026quot;: \u0026quot;always\u0026quot;, \u0026quot;RestartUSec\u0026quot;: \u0026quot;10s\u0026quot;, \u0026quot;Result\u0026quot;: \u0026quot;success\u0026quot;, \u0026quot;RootDirectoryStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RuntimeDirectoryMode\u0026quot;: \u0026quot;0755\u0026quot;, \u0026quot;SameProcessGroup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SecureBits\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;SendSIGHUP\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SendSIGKILL\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Slice\u0026quot;: \u0026quot;system.slice\u0026quot;, \u0026quot;StandardError\u0026quot;: \u0026quot;inherit\u0026quot;, \u0026quot;StandardInput\u0026quot;: \u0026quot;null\u0026quot;, \u0026quot;StandardOutput\u0026quot;: \u0026quot;journal\u0026quot;, \u0026quot;StartLimitAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;StartLimitBurst\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;StartLimitInterval\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;StartupBlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StartupCPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StatusErrno\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;StopWhenUnneeded\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SubState\u0026quot;: \u0026quot;dead\u0026quot;, \u0026quot;SyslogLevelPrefix\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;SyslogPriority\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;SystemCallErrorNumber\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;TTYReset\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVHangup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVTDisallocate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksCurrent\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TasksMax\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TimeoutStartUSec\u0026quot;: \u0026quot;1min 30s\u0026quot;, \u0026quot;TimeoutStopUSec\u0026quot;: \u0026quot;1min 30s\u0026quot;, \u0026quot;TimerSlackNSec\u0026quot;: \u0026quot;50000\u0026quot;, \u0026quot;Transient\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;simple\u0026quot;, \u0026quot;UMask\u0026quot;: \u0026quot;0022\u0026quot;, \u0026quot;UnitFilePreset\u0026quot;: \u0026quot;disabled\u0026quot;, \u0026quot;UnitFileState\u0026quot;: \u0026quot;enabled\u0026quot;, \u0026quot;WantedBy\u0026quot;: \u0026quot;multi-user.target\u0026quot;, \u0026quot;WatchdogTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;WatchdogUSec\u0026quot;: \u0026quot;0\u0026quot;}} TASK [common : Export yunion bin path in /etc/profile] ******************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:174 ok: [127.0.0.1] =\u0026gt; {\u0026quot;backup\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;} TASK [common : Enable yunion systemd services] **************************************************************************************** task path: /root/ocboot/onecloud/roles/common/tasks/main.yml:183 ok: [127.0.0.1] =\u0026gt; (item=yunion-executor) =\u0026gt; {\u0026quot;ansible_loop_var\u0026quot;: \u0026quot;item\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;enabled\u0026quot;: true, \u0026quot;item\u0026quot;: \u0026quot;yunion-executor\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;yunion-executor\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;started\u0026quot;, \u0026quot;status\u0026quot;: {\u0026quot;ActiveEnterTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:40 EDT\u0026quot;, \u0026quot;ActiveEnterTimestampMonotonic\u0026quot;: \u0026quot;6497667\u0026quot;, \u0026quot;ActiveExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ActiveState\u0026quot;: \u0026quot;active\u0026quot;, \u0026quot;After\u0026quot;: \u0026quot;basic.target -.mount system.slice systemd-journald.socket\u0026quot;, \u0026quot;AllowIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;AmbientCapabilities\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;AssertResult\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;AssertTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:40 EDT\u0026quot;, \u0026quot;AssertTimestampMonotonic\u0026quot;: \u0026quot;6472564\u0026quot;, \u0026quot;Before\u0026quot;: \u0026quot;shutdown.target multi-user.target\u0026quot;, \u0026quot;BlockIOAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;BlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CPUAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUQuotaPerSecUSec\u0026quot;: \u0026quot;infinity\u0026quot;, \u0026quot;CPUSchedulingPolicy\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingPriority\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CPUSchedulingResetOnFork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;CanIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CanReload\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;CanStart\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CanStop\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;CapabilityBoundingSet\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;ConditionResult\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;ConditionTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:40 EDT\u0026quot;, \u0026quot;ConditionTimestampMonotonic\u0026quot;: \u0026quot;6472564\u0026quot;, \u0026quot;Conflicts\u0026quot;: \u0026quot;shutdown.target\u0026quot;, \u0026quot;ControlGroup\u0026quot;: \u0026quot;/system.slice/yunion-executor.service\u0026quot;, \u0026quot;ControlPID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DefaultDependencies\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Delegate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;Yunion Command Executor\u0026quot;, \u0026quot;DevicePolicy\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;Documentation\u0026quot;: \u0026quot;https://docs.yunion.cn\u0026quot;, \u0026quot;ExecMainCode\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainExitTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecMainPID\u0026quot;: \u0026quot;745\u0026quot;, \u0026quot;ExecMainStartTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:40 EDT\u0026quot;, \u0026quot;ExecMainStartTimestampMonotonic\u0026quot;: \u0026quot;6497314\u0026quot;, \u0026quot;ExecMainStatus\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ExecStart\u0026quot;: \u0026quot;{ path=/opt/yunion/bin/executor-server ; argv[]=/opt/yunion/bin/executor-server ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }\u0026quot;, \u0026quot;FailureAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;FileDescriptorStoreMax\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;FragmentPath\u0026quot;: \u0026quot;/usr/lib/systemd/system/yunion-executor.service\u0026quot;, \u0026quot;Group\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;GuessMainPID\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;IOScheduling\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;yunion-executor.service\u0026quot;, \u0026quot;IgnoreOnIsolate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreOnSnapshot\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;IgnoreSIGPIPE\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;InactiveEnterTimestampMonotonic\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;InactiveExitTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:40 EDT\u0026quot;, \u0026quot;InactiveExitTimestampMonotonic\u0026quot;: \u0026quot;6497667\u0026quot;, \u0026quot;JobTimeoutAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;JobTimeoutUSec\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;KillMode\u0026quot;: \u0026quot;process\u0026quot;, \u0026quot;KillSignal\u0026quot;: \u0026quot;15\u0026quot;, \u0026quot;LimitAS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCORE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitCPU\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitDATA\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitFSIZE\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitLOCKS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitMEMLOCK\u0026quot;: \u0026quot;65536\u0026quot;, \u0026quot;LimitMSGQUEUE\u0026quot;: \u0026quot;819200\u0026quot;, \u0026quot;LimitNICE\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitNOFILE\u0026quot;: \u0026quot;500000\u0026quot;, \u0026quot;LimitNPROC\u0026quot;: \u0026quot;500000\u0026quot;, \u0026quot;LimitRSS\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitRTPRIO\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;LimitRTTIME\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LimitSIGPENDING\u0026quot;: \u0026quot;31119\u0026quot;, \u0026quot;LimitSTACK\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;LoadState\u0026quot;: \u0026quot;loaded\u0026quot;, \u0026quot;MainPID\u0026quot;: \u0026quot;745\u0026quot;, \u0026quot;MemoryAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;MemoryCurrent\u0026quot;: \u0026quot;21422080\u0026quot;, \u0026quot;MemoryLimit\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;MountFlags\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Names\u0026quot;: \u0026quot;yunion-executor.service\u0026quot;, \u0026quot;NeedDaemonReload\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Nice\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;NoNewPrivileges\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NonBlocking\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;NotifyAccess\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;OOMScoreAdjust\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;OnFailureJobMode\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;PermissionsStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateDevices\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateNetwork\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;PrivateTmp\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectHome\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;ProtectSystem\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStart\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RefuseManualStop\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RemainAfterExit\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Requires\u0026quot;: \u0026quot;basic.target -.mount system.slice\u0026quot;, \u0026quot;RequiresMountsFor\u0026quot;: \u0026quot;/opt/yunion/bin\u0026quot;, \u0026quot;Restart\u0026quot;: \u0026quot;always\u0026quot;, \u0026quot;RestartUSec\u0026quot;: \u0026quot;30s\u0026quot;, \u0026quot;Result\u0026quot;: \u0026quot;success\u0026quot;, \u0026quot;RootDirectoryStartOnly\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;RuntimeDirectoryMode\u0026quot;: \u0026quot;0755\u0026quot;, \u0026quot;SameProcessGroup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SecureBits\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;SendSIGHUP\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SendSIGKILL\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;Slice\u0026quot;: \u0026quot;system.slice\u0026quot;, \u0026quot;StandardError\u0026quot;: \u0026quot;inherit\u0026quot;, \u0026quot;StandardInput\u0026quot;: \u0026quot;null\u0026quot;, \u0026quot;StandardOutput\u0026quot;: \u0026quot;journal\u0026quot;, \u0026quot;StartLimitAction\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;StartLimitBurst\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;StartLimitInterval\u0026quot;: \u0026quot;10000000\u0026quot;, \u0026quot;StartupBlockIOWeight\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StartupCPUShares\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;StatusErrno\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;StopWhenUnneeded\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;SubState\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;SyslogLevelPrefix\u0026quot;: \u0026quot;yes\u0026quot;, \u0026quot;SyslogPriority\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;SystemCallErrorNumber\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;TTYReset\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVHangup\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TTYVTDisallocate\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksAccounting\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;TasksCurrent\u0026quot;: \u0026quot;6\u0026quot;, \u0026quot;TasksMax\u0026quot;: \u0026quot;18446744073709551615\u0026quot;, \u0026quot;TimeoutStartUSec\u0026quot;: \u0026quot;1min 30s\u0026quot;, \u0026quot;TimeoutStopUSec\u0026quot;: \u0026quot;1min 30s\u0026quot;, \u0026quot;TimerSlackNSec\u0026quot;: \u0026quot;50000\u0026quot;, \u0026quot;Transient\u0026quot;: \u0026quot;no\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;simple\u0026quot;, \u0026quot;UMask\u0026quot;: \u0026quot;0022\u0026quot;, \u0026quot;UnitFilePreset\u0026quot;: \u0026quot;disabled\u0026quot;, \u0026quot;UnitFileState\u0026quot;: \u0026quot;enabled\u0026quot;, \u0026quot;User\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;WantedBy\u0026quot;: \u0026quot;multi-user.target\u0026quot;, \u0026quot;WatchdogTimestamp\u0026quot;: \u0026quot;Fri 2020-06-19 04:14:40 EDT\u0026quot;, \u0026quot;WatchdogTimestampMonotonic\u0026quot;: \u0026quot;6497589\u0026quot;, \u0026quot;WatchdogUSec\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;WorkingDirectory\u0026quot;: \u0026quot;/opt/yunion/bin\u0026quot;}} TASK [primary-master-node : Get default gateway] ************************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:5 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;ip route get 1 | head -n 1 | awk '{print $3}'\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.009565\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:12:33.110610\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:12:33.101045\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;192.168.137.2\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;192.168.137.2\u0026quot;]} TASK [primary-master-node : set default gateway var] ********************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:9 ok: [127.0.0.1] =\u0026gt; {\u0026quot;ansible_facts\u0026quot;: {\u0026quot;default_gateway\u0026quot;: \u0026quot;192.168.137.2\u0026quot;}, \u0026quot;changed\u0026quot;: false} TASK [primary-master-node : Get default gateway 192.168.137.2 local ip address] ******************************************************* task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:13 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;ip route get 1 | head -n 1 | awk '{print $7}'\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.008940\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:12:33.673240\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:12:33.664300\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;192.168.137.190\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;192.168.137.190\u0026quot;]} TASK [primary-master-node : set default ip var] *************************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:17 ok: [127.0.0.1] =\u0026gt; {\u0026quot;ansible_facts\u0026quot;: {\u0026quot;default_ip\u0026quot;: \u0026quot;192.168.137.190\u0026quot;}, \u0026quot;changed\u0026quot;: false} TASK [primary-master-node : Get default ip address 192.168.137.190 masklen] *********************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:21 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;ip route list | grep 192.168.137.190 | head -n 1 | awk '{print $1}' | cut -d '/' -f 2\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.011152\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:12:34.263700\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:12:34.252548\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;24\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;24\u0026quot;]} TASK [primary-master-node : set default ip masklen] *********************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:25 ok: [127.0.0.1] =\u0026gt; {\u0026quot;ansible_facts\u0026quot;: {\u0026quot;default_masklen\u0026quot;: \u0026quot;24\u0026quot;}, \u0026quot;changed\u0026quot;: false} TASK [primary-master-node : Get cluster token] **************************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:29 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;/opt/yunion/bin/ocadm token list | cut -d ' ' -f1 | sed -n '2p'\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.137109\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:12:34.818033\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:12:34.680924\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;failed to load admin kubeconfig: open /etc/kubernetes/admin.conf: no such file or directory\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [\u0026quot;failed to load admin kubeconfig: open /etc/kubernetes/admin.conf: no such file or directory\u0026quot;], \u0026quot;stdout\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stdout_lines\u0026quot;: []} TASK [primary-master-node : Pull ocadm images on node] ******************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:33 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: [\u0026quot;/opt/yunion/bin/ocadm\u0026quot;, \u0026quot;config\u0026quot;, \u0026quot;images\u0026quot;, \u0026quot;pull\u0026quot;, \u0026quot;--image-repository\u0026quot;, \u0026quot;registry.cn-beijing.aliyuncs.com/yunionio\u0026quot;], \u0026quot;delta\u0026quot;: \u0026quot;0:00:17.093107\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:12:52.289511\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:12:35.196404\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/kube-apiserver:v1.15.8\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/kube-apiserver:v1.15.8\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/kube-controller-manager:v1.15.8\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/kube-controller-manager:v1.15.8\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/kube-scheduler:v1.15.8\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/kube-scheduler:v1.15.8\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/kube-proxy:v1.15.8\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/kube-proxy:v1.15.8\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/pause:3.1\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/pause:3.1\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/etcd:3.3.10\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/etcd:3.3.10\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/coredns:1.3.1\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/coredns:1.3.1\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/calico-kube-controllers:v3.7.2\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/calico-kube-controllers:v3.7.2\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/calico-node:v3.7.2\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/calico-node:v3.7.2\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/calico-cni:v3.7.2\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/calico-cni:v3.7.2\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/local-path-provisioner:v0.0.11\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/local-path-provisioner:v0.0.11\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/traefik:v1.7\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/traefik:v1.7\\n[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/onecloud-operator:v3.1.9-20200609.1\\n[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/onecloud-operator:v3.1.9-20200609.1\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/kube-apiserver:v1.15.8\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/kube-apiserver:v1.15.8\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/kube-controller-manager:v1.15.8\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/kube-controller-manager:v1.15.8\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/kube-scheduler:v1.15.8\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/kube-scheduler:v1.15.8\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/kube-proxy:v1.15.8\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/kube-proxy:v1.15.8\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/pause:3.1\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/pause:3.1\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/etcd:3.3.10\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/etcd:3.3.10\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/coredns:1.3.1\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/coredns:1.3.1\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/calico-kube-controllers:v3.7.2\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/calico-kube-controllers:v3.7.2\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/calico-node:v3.7.2\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/calico-node:v3.7.2\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/calico-cni:v3.7.2\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/calico-cni:v3.7.2\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/local-path-provisioner:v0.0.11\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/local-path-provisioner:v0.0.11\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/traefik:v1.7\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/traefik:v1.7\u0026quot;, \u0026quot;[config/images] Pulling registry.cn-beijing.aliyuncs.com/yunionio/onecloud-operator:v3.1.9-20200609.1\u0026quot;, \u0026quot;[config/images] Pulled registry.cn-beijing.aliyuncs.com/yunionio/onecloud-operator:v3.1.9-20200609.1\u0026quot;]} TASK [primary-master-node : Check node is init] *************************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:39 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;cmd\u0026quot;: \u0026quot;test -f /etc/kubernetes/kubelet.conf\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.005267\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:12:52.666920\u0026quot;, \u0026quot;failed_when_result\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;non-zero return code\u0026quot;, \u0026quot;rc\u0026quot;: 1, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:12:52.661653\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stdout_lines\u0026quot;: []} TASK [primary-master-node : construct controlplane endpoint] ************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:47 ok: [127.0.0.1] =\u0026gt; {\u0026quot;ansible_facts\u0026quot;: {\u0026quot;controlplane_endpoint\u0026quot;: \u0026quot;192.168.137.190:6443\u0026quot;}, \u0026quot;changed\u0026quot;: false} TASK [primary-master-node : construct init args] ************************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:51 ok: [127.0.0.1] =\u0026gt; {\u0026quot;ansible_facts\u0026quot;: {\u0026quot;init_args\u0026quot;: \u0026quot;init --mysql-host 192.168.137.190 --mysql-user root --mysql-password qwe123 --control-plane-endpoint 192.168.137.190:6443\u0026quot;}, \u0026quot;changed\u0026quot;: false} TASK [primary-master-node : construct image repository] ******************************************************************************* task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:55 skipping: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;skip_reason\u0026quot;: \u0026quot;Conditional result was False\u0026quot;} TASK [primary-master-node : construct onecloud version] ******************************************************************************* task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:61 skipping: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;skip_reason\u0026quot;: \u0026quot;Conditional result was False\u0026quot;} TASK [primary-master-node : init node as onecloud host agent] ************************************************************************* task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:67 ok: [127.0.0.1] =\u0026gt; {\u0026quot;ansible_facts\u0026quot;: {\u0026quot;init_args\u0026quot;: \u0026quot;init --mysql-host 192.168.137.190 --mysql-user root --mysql-password qwe123 --control-plane-endpoint 192.168.137.190:6443 --enable-host-agent\u0026quot;}, \u0026quot;changed\u0026quot;: false} TASK [primary-master-node : Use ocadm init first master node] ************************************************************************* task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:73 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: [\u0026quot;/opt/yunion/bin/ocadm\u0026quot;, \u0026quot;init\u0026quot;, \u0026quot;--mysql-host\u0026quot;, \u0026quot;192.168.137.190\u0026quot;, \u0026quot;--mysql-user\u0026quot;, \u0026quot;root\u0026quot;, \u0026quot;--mysql-password\u0026quot;, \u0026quot;qwe123\u0026quot;, \u0026quot;--control-plane-endpoint\u0026quot;, \u0026quot;192.168.137.190:6443\u0026quot;, \u0026quot;--enable-host-agent\u0026quot;], \u0026quot;delta\u0026quot;: \u0026quot;0:00:29.169449\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:13:22.805060\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:12:53.635611\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\\t[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 19.03.9. Latest validated version: 18.09\\nI0619 09:13:22.772341 55811 addons.go:60] [enable-host] Enable host for node localhost.localdomain\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [\u0026quot;\\t[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 19.03.9. Latest validated version: 18.09\u0026quot;, \u0026quot;I0619 09:13:22.772341 55811 addons.go:60] [enable-host] Enable host for node localhost.localdomain\u0026quot;], \u0026quot;stdout\u0026quot;: \u0026quot;[init] Using Kubernetes and Onecloud version: v1.15.8 \u0026amp; v3.1.9-20200609.1\\n[preflight] Running pre-flight checks\\n[preflight] Pulling images required for setting up a OneCloud on Kubernetes cluster\\n[preflight] This might take a minute or two, depending on the speed of your internet connection\\n[preflight] You can also perform this action in beforehand using 'ocadm config images pull'\\n[kubelet-start] Writing kubelet environment file with flags to file \\\u0026quot;/var/lib/kubelet/kubeadm-flags.env\\\u0026quot;\\n[kubelet-start] Writing kubelet configuration to file \\\u0026quot;/var/lib/kubelet/config.yaml\\\u0026quot;\\n[kubelet-start] Activating the kubelet service\\n[certs] Using certificateDir folder \\\u0026quot;/etc/kubernetes/pki\\\u0026quot;\\n[certs] Generating \\\u0026quot;ca\\\u0026quot; certificate and key\\n[certs] Generating \\\u0026quot;apiserver\\\u0026quot; certificate and key\\n[certs] apiserver serving cert is signed for DNS names [localhost.localdomain kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.137.190 192.168.137.190]\\n[certs] Generating \\\u0026quot;apiserver-kubelet-client\\\u0026quot; certificate and key\\n[certs] Generating \\\u0026quot;front-proxy-ca\\\u0026quot; certificate and key\\n[certs] Generating \\\u0026quot;front-proxy-client\\\u0026quot; certificate and key\\n[certs] Generating \\\u0026quot;etcd/ca\\\u0026quot; certificate and key\\n[certs] Generating \\\u0026quot;apiserver-etcd-client\\\u0026quot; certificate and key\\n[certs] Generating \\\u0026quot;etcd/server\\\u0026quot; certificate and key\\n[certs] etcd/server serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.137.190 127.0.0.1 ::1]\\n[certs] Generating \\\u0026quot;etcd/peer\\\u0026quot; certificate and key\\n[certs] etcd/peer serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.137.190 127.0.0.1 ::1]\\n[certs] Generating \\\u0026quot;etcd/healthcheck-client\\\u0026quot; certificate and key\\n[certs] Generating \\\u0026quot;sa\\\u0026quot; key and public key\\n[kubeconfig] Using kubeconfig folder \\\u0026quot;/etc/kubernetes\\\u0026quot;\\n[kubeconfig] Writing \\\u0026quot;admin.conf\\\u0026quot; kubeconfig file\\n[kubeconfig] Writing \\\u0026quot;kubelet.conf\\\u0026quot; kubeconfig file\\n[kubeconfig] Writing \\\u0026quot;controller-manager.conf\\\u0026quot; kubeconfig file\\n[kubeconfig] Writing \\\u0026quot;scheduler.conf\\\u0026quot; kubeconfig file\\n[control-plane] Using manifest folder \\\u0026quot;/etc/kubernetes/manifests\\\u0026quot;\\n[control-plane] Creating static Pod manifest for \\\u0026quot;kube-apiserver\\\u0026quot;\\n[control-plane] Creating static Pod manifest for \\\u0026quot;kube-controller-manager\\\u0026quot;\\n[control-plane] Creating static Pod manifest for \\\u0026quot;kube-scheduler\\\u0026quot;\\n[etcd] Creating static Pod manifest for local etcd in \\\u0026quot;/etc/kubernetes/manifests\\\u0026quot;\\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \\\u0026quot;/etc/kubernetes/manifests\\\u0026quot;. This can take up to 4m0s\\n[apiclient] All control plane components are healthy after 16.004632 seconds\\n[upload-config] Storing the configuration used in ConfigMap \\\u0026quot;kubeadm-config\\\u0026quot; in the \\\u0026quot;kube-system\\\u0026quot; Namespace\\n[kubelet] Creating a ConfigMap \\\u0026quot;kubelet-config-1.15\\\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster\\n[upload-certs] Storing the certificates in Secret \\\u0026quot;kubeadm-certs\\\u0026quot; in the \\\u0026quot;kube-system\\\u0026quot; Namespace\\n[upload-certs] Using certificate key:\\n70775d2d9aa973c41daf09d14610e75af2e46865adb77ced17e1be7d20269839\\n[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the label \\\u0026quot;node-role.kubernetes.io/master=''\\\u0026quot;\\n[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\\n[bootstrap-token] Using token: 36mkjj.zf2i3ppe5szzp6q2\\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\\n[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\\n[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\\n[bootstrap-token] Creating the \\\u0026quot;cluster-info\\\u0026quot; ConfigMap in the \\\u0026quot;kube-public\\\u0026quot; namespace\\n[oc-upload-config] storing the configuration used in ConfigMap \\\u0026quot;ocadm-config\\\u0026quot; in the \\\u0026quot;kube-system\\\u0026quot; Namespace\\n[addons] Applied essential addon: CoreDNS\\n[addons] Applied essential addon: kube-proxy\\n[oc-addons] Applied addon: calico\\n[oc-addons] Applied addon: local-path-provisioner\\n[oc-addons] Applied addon: traefik\\n[oc-addons] Applied addon: onecloud-operator\\n\\nYour Kubernetes and Onecloud control-plane has initialized successfully!\\n\\nTo start using your cluster, you need to run the following as a regular user:\\n\\n mkdir -p $HOME/.kube\\n sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\\n sudo chown $(id -u):$(id -g) $HOME/.kube/config\\n\\nYou can now join any number of the control-plane node running the following command on each as root:\\n\\n ocadm join 192.168.137.190:6443 --token 36mkjj.zf2i3ppe5szzp6q2 \\\\\\n --discovery-token-ca-cert-hash sha256:32fe2be8fa02f1a435fca186e9b48419e842e40c21db42cdba45f8508a63062e \\\\\\n --control-plane --certificate-key 70775d2d9aa973c41daf09d14610e75af2e46865adb77ced17e1be7d20269839\\n\\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\\n\\\u0026quot;ocadm init phase upload-certs --experimental-upload-certs\\\u0026quot; to reload certs afterward.\\n\\nThen you can join any number of worker nodes by running the following on each as root:\\n\\nocadm join 192.168.137.190:6443 --token 36mkjj.zf2i3ppe5szzp6q2 \\\\\\n --discovery-token-ca-cert-hash sha256:32fe2be8fa02f1a435fca186e9b48419e842e40c21db42cdba45f8508a63062e \u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;[init] Using Kubernetes and Onecloud version: v1.15.8 \u0026amp; v3.1.9-20200609.1\u0026quot;, \u0026quot;[preflight] Running pre-flight checks\u0026quot;, \u0026quot;[preflight] Pulling images required for setting up a OneCloud on Kubernetes cluster\u0026quot;, \u0026quot;[preflight] This might take a minute or two, depending on the speed of your internet connection\u0026quot;, \u0026quot;[preflight] You can also perform this action in beforehand using 'ocadm config images pull'\u0026quot;, \u0026quot;[kubelet-start] Writing kubelet environment file with flags to file \\\u0026quot;/var/lib/kubelet/kubeadm-flags.env\\\u0026quot;\u0026quot;, \u0026quot;[kubelet-start] Writing kubelet configuration to file \\\u0026quot;/var/lib/kubelet/config.yaml\\\u0026quot;\u0026quot;, \u0026quot;[kubelet-start] Activating the kubelet service\u0026quot;, \u0026quot;[certs] Using certificateDir folder \\\u0026quot;/etc/kubernetes/pki\\\u0026quot;\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;ca\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;apiserver\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] apiserver serving cert is signed for DNS names [localhost.localdomain kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.137.190 192.168.137.190]\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;apiserver-kubelet-client\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;front-proxy-ca\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;front-proxy-client\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;etcd/ca\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;apiserver-etcd-client\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;etcd/server\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] etcd/server serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.137.190 127.0.0.1 ::1]\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;etcd/peer\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] etcd/peer serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.137.190 127.0.0.1 ::1]\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;etcd/healthcheck-client\\\u0026quot; certificate and key\u0026quot;, \u0026quot;[certs] Generating \\\u0026quot;sa\\\u0026quot; key and public key\u0026quot;, \u0026quot;[kubeconfig] Using kubeconfig folder \\\u0026quot;/etc/kubernetes\\\u0026quot;\u0026quot;, \u0026quot;[kubeconfig] Writing \\\u0026quot;admin.conf\\\u0026quot; kubeconfig file\u0026quot;, \u0026quot;[kubeconfig] Writing \\\u0026quot;kubelet.conf\\\u0026quot; kubeconfig file\u0026quot;, \u0026quot;[kubeconfig] Writing \\\u0026quot;controller-manager.conf\\\u0026quot; kubeconfig file\u0026quot;, \u0026quot;[kubeconfig] Writing \\\u0026quot;scheduler.conf\\\u0026quot; kubeconfig file\u0026quot;, \u0026quot;[control-plane] Using manifest folder \\\u0026quot;/etc/kubernetes/manifests\\\u0026quot;\u0026quot;, \u0026quot;[control-plane] Creating static Pod manifest for \\\u0026quot;kube-apiserver\\\u0026quot;\u0026quot;, \u0026quot;[control-plane] Creating static Pod manifest for \\\u0026quot;kube-controller-manager\\\u0026quot;\u0026quot;, \u0026quot;[control-plane] Creating static Pod manifest for \\\u0026quot;kube-scheduler\\\u0026quot;\u0026quot;, \u0026quot;[etcd] Creating static Pod manifest for local etcd in \\\u0026quot;/etc/kubernetes/manifests\\\u0026quot;\u0026quot;, \u0026quot;[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \\\u0026quot;/etc/kubernetes/manifests\\\u0026quot;. This can take up to 4m0s\u0026quot;, \u0026quot;[apiclient] All control plane components are healthy after 16.004632 seconds\u0026quot;, \u0026quot;[upload-config] Storing the configuration used in ConfigMap \\\u0026quot;kubeadm-config\\\u0026quot; in the \\\u0026quot;kube-system\\\u0026quot; Namespace\u0026quot;, \u0026quot;[kubelet] Creating a ConfigMap \\\u0026quot;kubelet-config-1.15\\\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster\u0026quot;, \u0026quot;[upload-certs] Storing the certificates in Secret \\\u0026quot;kubeadm-certs\\\u0026quot; in the \\\u0026quot;kube-system\\\u0026quot; Namespace\u0026quot;, \u0026quot;[upload-certs] Using certificate key:\u0026quot;, \u0026quot;70775d2d9aa973c41daf09d14610e75af2e46865adb77ced17e1be7d20269839\u0026quot;, \u0026quot;[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the label \\\u0026quot;node-role.kubernetes.io/master=''\\\u0026quot;\u0026quot;, \u0026quot;[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\u0026quot;, \u0026quot;[bootstrap-token] Using token: 36mkjj.zf2i3ppe5szzp6q2\u0026quot;, \u0026quot;[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\u0026quot;, \u0026quot;[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\u0026quot;, \u0026quot;[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\u0026quot;, \u0026quot;[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\u0026quot;, \u0026quot;[bootstrap-token] Creating the \\\u0026quot;cluster-info\\\u0026quot; ConfigMap in the \\\u0026quot;kube-public\\\u0026quot; namespace\u0026quot;, \u0026quot;[oc-upload-config] storing the configuration used in ConfigMap \\\u0026quot;ocadm-config\\\u0026quot; in the \\\u0026quot;kube-system\\\u0026quot; Namespace\u0026quot;, \u0026quot;[addons] Applied essential addon: CoreDNS\u0026quot;, \u0026quot;[addons] Applied essential addon: kube-proxy\u0026quot;, \u0026quot;[oc-addons] Applied addon: calico\u0026quot;, \u0026quot;[oc-addons] Applied addon: local-path-provisioner\u0026quot;, \u0026quot;[oc-addons] Applied addon: traefik\u0026quot;, \u0026quot;[oc-addons] Applied addon: onecloud-operator\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;Your Kubernetes and Onecloud control-plane has initialized successfully!\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;To start using your cluster, you need to run the following as a regular user:\u0026quot;, \u0026quot;\u0026quot;, \u0026quot; mkdir -p $HOME/.kube\u0026quot;, \u0026quot; sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\u0026quot;, \u0026quot; sudo chown $(id -u):$(id -g) $HOME/.kube/config\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;You can now join any number of the control-plane node running the following command on each as root:\u0026quot;, \u0026quot;\u0026quot;, \u0026quot; ocadm join 192.168.137.190:6443 --token 36mkjj.zf2i3ppe5szzp6q2 \\\\\u0026quot;, \u0026quot; --discovery-token-ca-cert-hash sha256:32fe2be8fa02f1a435fca186e9b48419e842e40c21db42cdba45f8508a63062e \\\\\u0026quot;, \u0026quot; --control-plane --certificate-key 70775d2d9aa973c41daf09d14610e75af2e46865adb77ced17e1be7d20269839\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;Please note that the certificate-key gives access to cluster sensitive data, keep it secret!\u0026quot;, \u0026quot;As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\u0026quot;, \u0026quot;\\\u0026quot;ocadm init phase upload-certs --experimental-upload-certs\\\u0026quot; to reload certs afterward.\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;Then you can join any number of worker nodes by running the following on each as root:\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;ocadm join 192.168.137.190:6443 --token 36mkjj.zf2i3ppe5szzp6q2 \\\\\u0026quot;, \u0026quot; --discovery-token-ca-cert-hash sha256:32fe2be8fa02f1a435fca186e9b48419e842e40c21db42cdba45f8508a63062e \u0026quot;]} TASK [primary-master-node : Wait 500 seconds for primary master to response: 192.168.137.190:6443] ************************************ task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:77 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;elapsed\u0026quot;: 1, \u0026quot;match_groupdict\u0026quot;: {}, \u0026quot;match_groups\u0026quot;: [], \u0026quot;path\u0026quot;: null, \u0026quot;port\u0026quot;: 6443, \u0026quot;search_regex\u0026quot;: null, \u0026quot;state\u0026quot;: \u0026quot;started\u0026quot;} TASK [primary-master-node : Export KUBECONFIG in master's ~/.bashrc] ****************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:85 ok: [127.0.0.1] =\u0026gt; {\u0026quot;backup\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;} TASK [primary-master-node : Source kubectl bash completion in master' /etc/profile] *************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:94 ok: [127.0.0.1] =\u0026gt; {\u0026quot;backup\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;\u0026quot;} TASK [primary-master-node : Wait onecloud CRD created] ******************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:103 FAILED - RETRYING: Wait onecloud CRD created (30 retries left). FAILED - RETRYING: Wait onecloud CRD created (29 retries left). changed: [127.0.0.1] =\u0026gt; {\u0026quot;attempts\u0026quot;: 3, \u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;kubectl get crd | grep onecloudcluster\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.307379\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:13:47.494261\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:13:47.186882\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;onecloudclusters.onecloud.yunion.io 2020-06-19T13:13:47Z\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;onecloudclusters.onecloud.yunion.io 2020-06-19T13:13:47Z\u0026quot;]} TASK [primary-master-node : Check onecloud cluster exists] **************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:112 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;kubectl get onecloudcluster -n onecloud | grep default | wc -l\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.390113\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:13:48.313265\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:13:47.923152\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;No resources found.\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [\u0026quot;No resources found.\u0026quot;], \u0026quot;stdout\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;0\u0026quot;]} TASK [primary-master-node : Create essential services, wait for a few minutes...] ***************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:118 changed: [127.0.0.1] =\u0026gt; {\u0026quot;attempts\u0026quot;: 1, \u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;/opt/yunion/bin/ocadm cluster create --wait\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:10:40.163163\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:24:28.987060\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:13:48.823897\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;[I 200619 09:13:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: keystone: current status is nil, maybe deployment not created\\n[I 200619 09:13:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: keystone: keystone is upgrading\\n[I 200619 09:14:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: keystone: keystone is upgrading\\n[I 200619 09:14:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: current status is nil, maybe deployment not created\\n[I 200619 09:14:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: current status is nil, maybe deployment not created\\n[I 200619 09:14:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: current status is nil, maybe deployment not created\\n[I 200619 09:14:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: current status is nil, maybe deployment not created\\n[I 200619 09:14:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:15:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:15:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:15:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:15:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:15:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:15:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:16:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:16:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:16:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:16:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:16:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:16:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:17:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:17:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:17:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:17:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:17:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:17:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:18:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:18:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:18:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:18:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:18:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:18:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:19:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:19:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:19:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:19:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:19:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:19:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:20:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:20:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:20:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:20:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:20:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:20:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:21:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:21:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:21:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:21:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:21:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:21:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:22:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:22:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:22:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:22:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:22:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:22:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:23:08 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:23:18 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:23:28 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:23:38 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:23:48 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:23:58 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:24:08 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\\n[I 200619 09:24:18 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [\u0026quot;[I 200619 09:13:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: keystone: current status is nil, maybe deployment not created\u0026quot;, \u0026quot;[I 200619 09:13:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: keystone: keystone is upgrading\u0026quot;, \u0026quot;[I 200619 09:14:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: keystone: keystone is upgrading\u0026quot;, \u0026quot;[I 200619 09:14:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: current status is nil, maybe deployment not created\u0026quot;, \u0026quot;[I 200619 09:14:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: current status is nil, maybe deployment not created\u0026quot;, \u0026quot;[I 200619 09:14:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: current status is nil, maybe deployment not created\u0026quot;, \u0026quot;[I 200619 09:14:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: current status is nil, maybe deployment not created\u0026quot;, \u0026quot;[I 200619 09:14:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:15:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:15:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:15:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:15:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:15:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:15:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:16:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:16:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:16:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:16:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:16:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:16:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:17:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:17:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:17:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:17:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:17:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:17:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:18:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:18:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:18:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:18:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:18:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:18:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:19:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:19:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:19:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:19:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:19:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:19:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:20:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:20:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:20:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:20:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:20:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:20:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:21:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:21:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:21:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:21:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:21:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:21:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:22:09 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:22:19 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:22:29 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:22:39 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:22:49 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:22:59 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:23:08 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:23:18 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:23:28 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:23:38 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:23:48 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:23:58 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:24:08 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;, \u0026quot;[I 200619 09:24:18 onecloud.WaitOnecloudDeploymentUpdated.func1(operator.go:155)] Wait: glance: glance is upgrading\u0026quot;], \u0026quot;stdout\u0026quot;: \u0026quot;Cluster default created\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;Cluster default created\u0026quot;]} TASK [primary-master-node : Check onecloud user demo exists] ************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:126 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;eval $(/opt/yunion/bin/ocadm cluster rcadmin)\\n/opt/yunion/bin/climc user-list | grep -w demo | wc -l\\n\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:01.393779\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:24:31.007207\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:24:29.613428\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;0\u0026quot;]} TASK [primary-master-node : Create onecloud web login user demo] ********************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:132 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;eval $(/opt/yunion/bin/ocadm cluster rcadmin)\\n/opt/yunion/bin/climc user-create --password demo@123 --enabled demo\\n\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:01.503214\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:24:33.089978\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:24:31.586764\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;+-------------------+----------------------------------+\\n| Field | Value |\\n+-------------------+----------------------------------+\\n| allow_web_console | true |\\n| can_delete | true |\\n| can_update | true |\\n| created_at | 2020-06-19T13:24:32.000000Z |\\n| credential_count | 0 |\\n| deleted | false |\\n| domain_id | default |\\n| enable_mfa | true |\\n| enabled | true |\\n| failed_auth_count | 0 |\\n| group_count | 0 |\\n| id | 25b3572d974346328e116ece7417347b |\\n| is_emulated | false |\\n| is_system_account | false |\\n| name | demo |\\n| project_count | 0 |\\n| project_domain | Default |\\n| update_version | 0 |\\n| updated_at | 2020-06-19T13:24:32.000000Z |\\n+-------------------+----------------------------------+\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;+-------------------+----------------------------------+\u0026quot;, \u0026quot;| Field | Value |\u0026quot;, \u0026quot;+-------------------+----------------------------------+\u0026quot;, \u0026quot;| allow_web_console | true |\u0026quot;, \u0026quot;| can_delete | true |\u0026quot;, \u0026quot;| can_update | true |\u0026quot;, \u0026quot;| created_at | 2020-06-19T13:24:32.000000Z |\u0026quot;, \u0026quot;| credential_count | 0 |\u0026quot;, \u0026quot;| deleted | false |\u0026quot;, \u0026quot;| domain_id | default |\u0026quot;, \u0026quot;| enable_mfa | true |\u0026quot;, \u0026quot;| enabled | true |\u0026quot;, \u0026quot;| failed_auth_count | 0 |\u0026quot;, \u0026quot;| group_count | 0 |\u0026quot;, \u0026quot;| id | 25b3572d974346328e116ece7417347b |\u0026quot;, \u0026quot;| is_emulated | false |\u0026quot;, \u0026quot;| is_system_account | false |\u0026quot;, \u0026quot;| name | demo |\u0026quot;, \u0026quot;| project_count | 0 |\u0026quot;, \u0026quot;| project_domain | Default |\u0026quot;, \u0026quot;| update_version | 0 |\u0026quot;, \u0026quot;| updated_at | 2020-06-19T13:24:32.000000Z |\u0026quot;, \u0026quot;+-------------------+----------------------------------+\u0026quot;]} TASK [primary-master-node : Update onecloud user demo password to demo@123] *********************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:138 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;eval $(/opt/yunion/bin/ocadm cluster rcadmin)\\n/opt/yunion/bin/climc user-update --password demo@123 --enabled --allow-web-console demo\\n\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:01.321374\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:24:34.906848\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:24:33.585474\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;+-------------------+----------------------------------+\\n| Field | Value |\\n+-------------------+----------------------------------+\\n| allow_web_console | true |\\n| can_delete | true |\\n| can_update | true |\\n| created_at | 2020-06-19T13:24:32.000000Z |\\n| credential_count | 0 |\\n| deleted | false |\\n| domain_id | default |\\n| enable_mfa | true |\\n| enabled | true |\\n| failed_auth_count | 0 |\\n| group_count | 0 |\\n| id | 25b3572d974346328e116ece7417347b |\\n| is_emulated | false |\\n| is_system_account | false |\\n| name | demo |\\n| project_count | 0 |\\n| project_domain | Default |\\n| update_version | 0 |\\n| updated_at | 2020-06-19T13:24:32.000000Z |\\n+-------------------+----------------------------------+\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;+-------------------+----------------------------------+\u0026quot;, \u0026quot;| Field | Value |\u0026quot;, \u0026quot;+-------------------+----------------------------------+\u0026quot;, \u0026quot;| allow_web_console | true |\u0026quot;, \u0026quot;| can_delete | true |\u0026quot;, \u0026quot;| can_update | true |\u0026quot;, \u0026quot;| created_at | 2020-06-19T13:24:32.000000Z |\u0026quot;, \u0026quot;| credential_count | 0 |\u0026quot;, \u0026quot;| deleted | false |\u0026quot;, \u0026quot;| domain_id | default |\u0026quot;, \u0026quot;| enable_mfa | true |\u0026quot;, \u0026quot;| enabled | true |\u0026quot;, \u0026quot;| failed_auth_count | 0 |\u0026quot;, \u0026quot;| group_count | 0 |\u0026quot;, \u0026quot;| id | 25b3572d974346328e116ece7417347b |\u0026quot;, \u0026quot;| is_emulated | false |\u0026quot;, \u0026quot;| is_system_account | false |\u0026quot;, \u0026quot;| name | demo |\u0026quot;, \u0026quot;| project_count | 0 |\u0026quot;, \u0026quot;| project_domain | Default |\u0026quot;, \u0026quot;| update_version | 0 |\u0026quot;, \u0026quot;| updated_at | 2020-06-19T13:24:32.000000Z |\u0026quot;, \u0026quot;+-------------------+----------------------------------+\u0026quot;]} TASK [primary-master-node : Make onecloud user demo as system admin] ****************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:143 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;eval $(/opt/yunion/bin/ocadm cluster rcadmin)\\n/opt/yunion/bin/climc project-add-user system demo admin\\n\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:01.121861\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:24:36.534888\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:24:35.413027\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stdout_lines\u0026quot;: []} TASK [primary-master-node : Check if admin network adm0 exists] *********************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:148 ok: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;cmd\u0026quot;: \u0026quot;eval $(/opt/yunion/bin/ocadm cluster rcadmin)\\n/opt/yunion/bin/climc network-show adm0 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1\\n\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:01.148059\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:24:38.185217\u0026quot;, \u0026quot;failed_when_result\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;non-zero return code\u0026quot;, \u0026quot;rc\u0026quot;: 1, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:24:37.037158\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stdout_lines\u0026quot;: []} TASK [primary-master-node : Check if network contains 192.168.137.190] **************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:157 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;eval $(/opt/yunion/bin/ocadm cluster rcadmin)\\n/opt/yunion/bin/climc network-list --ip 192.168.137.190 | grep -v Total | wc -l\\n\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:01.293203\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:24:39.964917\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:24:38.671714\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;0\u0026quot;]} TASK [primary-master-node : set admin network count] ********************************************************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:164 ok: [127.0.0.1] =\u0026gt; {\u0026quot;ansible_facts\u0026quot;: {\u0026quot;admin_network_count\u0026quot;: \u0026quot;0\u0026quot;}, \u0026quot;changed\u0026quot;: false} TASK [primary-master-node : Register adm0 network for 192.168.137.190/24, gateway 192.168.137.2] ************************************** task path: /root/ocboot/onecloud/roles/primary-master-node/tasks/main.yml:168 changed: [127.0.0.1] =\u0026gt; {\u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;eval $(/opt/yunion/bin/ocadm cluster rcadmin)\\n/opt/yunion/bin/climc network-create --gateway 192.168.137.2 --server-type baremetal bcast0 adm0 192.168.137.190 192.168.137.190 24\\n/opt/yunion/bin/climc network-private adm0\\n\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:02.829742\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2020-06-19 09:24:43.553069\u0026quot;, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2020-06-19 09:24:40.723327\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], \u0026quot;stdout\u0026quot;: \u0026quot;+----------------------+--------------------------------------+\\n| Field | Value |\\n+----------------------+--------------------------------------+\\n| alloc_timout_seconds | 0 |\\n| bm_nics | 0 |\\n| brand | OneCloud |\\n| can_delete | true |\\n| can_update | true |\\n| cloud_env | onpremise |\\n| cloudregion_id | default |\\n| created_at | 2020-06-19T13:24:42.000000Z |\\n| deleted | false |\\n| domain_id | default |\\n| eip_vnics | 0 |\\n| exit | false |\\n| group_vnics | 0 |\\n| guest_gateway | 192.168.137.2 |\\n| guest_ip6_mask | 0 |\\n| guest_ip_end | 192.168.137.190 |\\n| guest_ip_mask | 24 |\\n| guest_ip_start | 192.168.137.190 |\\n| id | 823cc07c-7c2e-4bd6-85b8-96b207b80ea8 |\\n| ifname_hint | adm0 |\\n| is_emulated | false |\\n| is_public | false |\\n| is_system | false |\\n| lb_vnics | 0 |\\n| name | adm0 |\\n| pending_deleted | false |\\n| ports | 1 |\\n| ports_used | 0 |\\n| project_domain | Default |\\n| project_src | local |\\n| provider | OneCloud |\\n| public_scope | none |\\n| region | Default |\\n| region_id | default |\\n| reserve_vnics | 0 |\\n| server_type | baremetal |\\n| status | available |\\n| tenant | system |\\n| tenant_id | 8fb3820aeee245598b080580af2e20ed |\\n| update_version | 1 |\\n| updated_at | 2020-06-19T13:24:42.000000Z |\\n| vlan_id | 1 |\\n| vnics | 0 |\\n| vpc | Default |\\n| vpc_id | default |\\n| wire | bcast0 |\\n| wire_id | f2b2cd2c-d712-4459-8c42-2ad50d6e5647 |\\n| zone | zone0 |\\n| zone_id | c467362b-74d3-4973-8a5b-71af48da4e34 |\\n+----------------------+--------------------------------------+\\n+----------------------+--------------------------------------+\\n| Field | Value |\\n+----------------------+--------------------------------------+\\n| alloc_timout_seconds | 0 |\\n| bm_nics | 0 |\\n| brand | OneCloud |\\n| can_delete | true |\\n| can_update | true |\\n| cloud_env | onpremise |\\n| cloudregion_id | default |\\n| created_at | 2020-06-19T13:24:42.000000Z |\\n| deleted | false |\\n| domain_id | default |\\n| eip_vnics | 0 |\\n| exit | false |\\n| group_vnics | 0 |\\n| guest_gateway | 192.168.137.2 |\\n| guest_ip6_mask | 0 |\\n| guest_ip_end | 192.168.137.190 |\\n| guest_ip_mask | 24 |\\n| guest_ip_start | 192.168.137.190 |\\n| id | 823cc07c-7c2e-4bd6-85b8-96b207b80ea8 |\\n| ifname_hint | adm0 |\\n| is_emulated | false |\\n| is_public | false |\\n| is_system | false |\\n| lb_vnics | 0 |\\n| name | adm0 |\\n| pending_deleted | false |\\n| ports | 1 |\\n| ports_used | 0 |\\n| project_domain | Default |\\n| project_src | local |\\n| provider | OneCloud |\\n| public_scope | none |\\n| region | Default |\\n| region_id | default |\\n| reserve_vnics | 0 |\\n| server_type | baremetal |\\n| status | available |\\n| tenant | system |\\n| tenant_id | 8fb3820aeee245598b080580af2e20ed |\\n| update_version | 1 |\\n| updated_at | 2020-06-19T13:24:42.000000Z |\\n| vlan_id | 1 |\\n| vnics | 0 |\\n| vpc | Default |\\n| vpc_id | default |\\n| wire | bcast0 |\\n| wire_id | f2b2cd2c-d712-4459-8c42-2ad50d6e5647 |\\n| zone | zone0 |\\n| zone_id | c467362b-74d3-4973-8a5b-71af48da4e34 |\\n+----------------------+--------------------------------------+\u0026quot;, \u0026quot;stdout_lines\u0026quot;: [\u0026quot;+----------------------+--------------------------------------+\u0026quot;, \u0026quot;| Field | Value |\u0026quot;, \u0026quot;+----------------------+--------------------------------------+\u0026quot;, \u0026quot;| alloc_timout_seconds | 0 |\u0026quot;, \u0026quot;| bm_nics | 0 |\u0026quot;, \u0026quot;| brand | OneCloud |\u0026quot;, \u0026quot;| can_delete | true |\u0026quot;, \u0026quot;| can_update | true |\u0026quot;, \u0026quot;| cloud_env | onpremise |\u0026quot;, \u0026quot;| cloudregion_id | default |\u0026quot;, \u0026quot;| created_at | 2020-06-19T13:24:42.000000Z |\u0026quot;, \u0026quot;| deleted | false |\u0026quot;, \u0026quot;| domain_id | default |\u0026quot;, \u0026quot;| eip_vnics | 0 |\u0026quot;, \u0026quot;| exit | false |\u0026quot;, \u0026quot;| group_vnics | 0 |\u0026quot;, \u0026quot;| guest_gateway | 192.168.137.2 |\u0026quot;, \u0026quot;| guest_ip6_mask | 0 |\u0026quot;, \u0026quot;| guest_ip_end | 192.168.137.190 |\u0026quot;, \u0026quot;| guest_ip_mask | 24 |\u0026quot;, \u0026quot;| guest_ip_start | 192.168.137.190 |\u0026quot;, \u0026quot;| id | 823cc07c-7c2e-4bd6-85b8-96b207b80ea8 |\u0026quot;, \u0026quot;| ifname_hint | adm0 |\u0026quot;, \u0026quot;| is_emulated | false |\u0026quot;, \u0026quot;| is_public | false |\u0026quot;, \u0026quot;| is_system | false |\u0026quot;, \u0026quot;| lb_vnics | 0 |\u0026quot;, \u0026quot;| name | adm0 |\u0026quot;, \u0026quot;| pending_deleted | false |\u0026quot;, \u0026quot;| ports | 1 |\u0026quot;, \u0026quot;| ports_used | 0 |\u0026quot;, \u0026quot;| project_domain | Default |\u0026quot;, \u0026quot;| project_src | local |\u0026quot;, \u0026quot;| provider | OneCloud |\u0026quot;, \u0026quot;| public_scope | none |\u0026quot;, \u0026quot;| region | Default |\u0026quot;, \u0026quot;| region_id | default |\u0026quot;, \u0026quot;| reserve_vnics | 0 |\u0026quot;, \u0026quot;| server_type | baremetal |\u0026quot;, \u0026quot;| status | available |\u0026quot;, \u0026quot;| tenant | system |\u0026quot;, \u0026quot;| tenant_id | 8fb3820aeee245598b080580af2e20ed |\u0026quot;, \u0026quot;| update_version | 1 |\u0026quot;, \u0026quot;| updated_at | 2020-06-19T13:24:42.000000Z |\u0026quot;, \u0026quot;| vlan_id | 1 |\u0026quot;, \u0026quot;| vnics | 0 |\u0026quot;, \u0026quot;| vpc | Default |\u0026quot;, \u0026quot;| vpc_id | default |\u0026quot;, \u0026quot;| wire | bcast0 |\u0026quot;, \u0026quot;| wire_id | f2b2cd2c-d712-4459-8c42-2ad50d6e5647 |\u0026quot;, \u0026quot;| zone | zone0 |\u0026quot;, \u0026quot;| zone_id | c467362b-74d3-4973-8a5b-71af48da4e34 |\u0026quot;, \u0026quot;+----------------------+--------------------------------------+\u0026quot;, \u0026quot;+----------------------+--------------------------------------+\u0026quot;, \u0026quot;| Field | Value |\u0026quot;, \u0026quot;+----------------------+--------------------------------------+\u0026quot;, \u0026quot;| alloc_timout_seconds | 0 |\u0026quot;, \u0026quot;| bm_nics | 0 |\u0026quot;, \u0026quot;| brand | OneCloud |\u0026quot;, \u0026quot;| can_delete | true |\u0026quot;, \u0026quot;| can_update | true |\u0026quot;, \u0026quot;| cloud_env | onpremise |\u0026quot;, \u0026quot;| cloudregion_id | default |\u0026quot;, \u0026quot;| created_at | 2020-06-19T13:24:42.000000Z |\u0026quot;, \u0026quot;| deleted | false |\u0026quot;, \u0026quot;| domain_id | default |\u0026quot;, \u0026quot;| eip_vnics | 0 |\u0026quot;, \u0026quot;| exit | false |\u0026quot;, \u0026quot;| group_vnics | 0 |\u0026quot;, \u0026quot;| guest_gateway | 192.168.137.2 |\u0026quot;, \u0026quot;| guest_ip6_mask | 0 |\u0026quot;, \u0026quot;| guest_ip_end | 192.168.137.190 |\u0026quot;, \u0026quot;| guest_ip_mask | 24 |\u0026quot;, \u0026quot;| guest_ip_start | 192.168.137.190 |\u0026quot;, \u0026quot;| id | 823cc07c-7c2e-4bd6-85b8-96b207b80ea8 |\u0026quot;, \u0026quot;| ifname_hint | adm0 |\u0026quot;, \u0026quot;| is_emulated | false |\u0026quot;, \u0026quot;| is_public | false |\u0026quot;, \u0026quot;| is_system | false |\u0026quot;, \u0026quot;| lb_vnics | 0 |\u0026quot;, \u0026quot;| name | adm0 |\u0026quot;, \u0026quot;| pending_deleted | false |\u0026quot;, \u0026quot;| ports | 1 |\u0026quot;, \u0026quot;| ports_used | 0 |\u0026quot;, \u0026quot;| project_domain | Default |\u0026quot;, \u0026quot;| project_src | local |\u0026quot;, \u0026quot;| provider | OneCloud |\u0026quot;, \u0026quot;| public_scope | none |\u0026quot;, \u0026quot;| region | Default |\u0026quot;, \u0026quot;| region_id | default |\u0026quot;, \u0026quot;| reserve_vnics | 0 |\u0026quot;, \u0026quot;| server_type | baremetal |\u0026quot;, \u0026quot;| status | available |\u0026quot;, \u0026quot;| tenant | system |\u0026quot;, \u0026quot;| tenant_id | 8fb3820aeee245598b080580af2e20ed |\u0026quot;, \u0026quot;| update_version | 1 |\u0026quot;, \u0026quot;| updated_at | 2020-06-19T13:24:42.000000Z |\u0026quot;, \u0026quot;| vlan_id | 1 |\u0026quot;, \u0026quot;| vnics | 0 |\u0026quot;, \u0026quot;| vpc | Default |\u0026quot;, \u0026quot;| vpc_id | default |\u0026quot;, \u0026quot;| wire | bcast0 |\u0026quot;, \u0026quot;| wire_id | f2b2cd2c-d712-4459-8c42-2ad50d6e5647 |\u0026quot;, \u0026quot;| zone | zone0 |\u0026quot;, \u0026quot;| zone_id | c467362b-74d3-4973-8a5b-71af48da4e34 |\u0026quot;, \u0026quot;+----------------------+--------------------------------------+\u0026quot;]} META: ran handlers META: ran handlers PLAY RECAP **************************************************************************************************************************** 127.0.0.1 : ok=61 changed=18 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0  ","date":1592574121,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592574121,"objectID":"a92db36e9ac685727664e99fc320ce2f","permalink":"https://wubigo.com/post/cmp-onecloud-all-in-one-setup-from-source-code/","publishdate":"2020-06-19T21:42:01+08:00","relpermalink":"/post/cmp-onecloud-all-in-one-setup-from-source-code/","section":"post","summary":"系统设置 /etc/yum.conf\nobsoletes=0  obsoletes=1: 安装k8s 1.15.8会报一个依赖错误\n安装ansible sudo yum install epel-release sudo yum install python-pip pip install ansible  安装kubelet（可选） OCADM创建K8S的前提条件是kebelet正常工作\n安装kubelet\nOCBOOT git clone https://github.com/yunionio/ocboot.git cd ocboot ./run.py ./config-allinone.yml  allinone.yml\nmariadb_node: use_local: true hostname: 192.168.137.190 user: root db_user: root db_password: qwe123 primary_master_node: #master_node: use_local: true hostname: 192.168.137.190 user: root db_host: 192.168.137.190 db_user: root db_password: qwe123 onecloud_user: demo onecloud_user_password: demo@123 controlplane_host: 192.168.137.190 controlplane_port: \u0026quot;6443\u0026quot; as_host: true registry_mirrors: - https://lje6zxpk.","tags":["CMP"],"title":"从源代码构建Onecloud","type":"post"},{"authors":null,"categories":[],"content":"最近黑客新网也被墙了，WTF\nlynx https://news.ycombinator.com/news 1. Generics and Compile-Time in Rust (pingcap.com) 107 points by Bella-Xiang 4 hours ago | hide | 17 comments 2. Adobe to remove Flash Player from web site after December 2020 (adobe.com) 740 points by michaelhoffman 14 hours ago | hide | 412 comments 3. How many of you know that the team is working on something that no-one wants? (iism.org) 255 points by kiyanwang 8 hours ago | hide | 136 comments 4. Recently minted database technologies that I find intriguing (lucperkins.dev) 331 points by biggestlou 9 hours ago | hide | 107 comments 5. AWS’s Share of Amazon’s Profit (tbray.org) 154 points by nurbel 8 hours ago | hide | 82 comments 6. Model S Long Range Plus: Building the First 400-Mile Electric Vehicle (tesla.com) 181 points by Reedx 5 hours ago | hide | 196 comments 7. Feynman Lectures on the Strong Interactions (columbia.edu) 5 points by chmaynard 1 hour ago | hide | discuss 8. Basics of Pneumatic Logic (hydraulicspneumatics.com) 54 points by Koshkin 5 hours ago | hide | 13 comments 9. -- press space for next page -- Arrow keys: Up and Down to move. Right to follow a link; Left to go back. H)elp O)ptions P)rint G)o M)ain screen Q)uit /=search [delete]=history list  ","date":1592284858,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592284858,"objectID":"21f53956ebebdf1f47646d75175ae863","permalink":"https://wubigo.com/post/lynx-ycombinator/","publishdate":"2020-06-16T13:20:58+08:00","relpermalink":"/post/lynx-ycombinator/","section":"post","summary":"最近黑客新网也被墙了，WTF\nlynx https://news.ycombinator.com/news 1. Generics and Compile-Time in Rust (pingcap.com) 107 points by Bella-Xiang 4 hours ago | hide | 17 comments 2. Adobe to remove Flash Player from web site after December 2020 (adobe.com) 740 points by michaelhoffman 14 hours ago | hide | 412 comments 3. How many of you know that the team is working on something that no-one wants? (iism.org) 255 points by kiyanwang 8 hours ago | hide | 136 comments 4.","tags":["WEB"],"title":"通过lynx浏览黑客新网","type":"post"},{"authors":null,"categories":[],"content":" FIDO Fast Identity Online (FIDO)\nWebAuthn was officially recognized as a W3C web standard in March 2019. Today, WebAuthn is part of the FIDO Alliance’s FIDO2 specifications and the FIDO Alliance runs certification programs to ensure compliance\n","date":1592257009,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592257009,"objectID":"833724ac4e2cf2d874fba6806cf354ff","permalink":"https://wubigo.com/post/webauthn-fido/","publishdate":"2020-06-16T05:36:49+08:00","relpermalink":"/post/webauthn-fido/","section":"post","summary":"FIDO Fast Identity Online (FIDO)\nWebAuthn was officially recognized as a W3C web standard in March 2019. Today, WebAuthn is part of the FIDO Alliance’s FIDO2 specifications and the FIDO Alliance runs certification programs to ensure compliance","tags":["IDP","UAA"],"title":"Webauthn notes","type":"post"},{"authors":null,"categories":[],"content":" 虚机网卡地址 配置静态IP C:\\Documents and Settings\\All Users\\Application Data\\VMware\\vmnetdhcp.conf\nhost VMnet8 { hardware ethernet 00:0C:29:23:AV:67; fixed-address 192.168.137.170; }  重启VMWARE DHCP服务 net stop vmnetdhcp net start vmnetdhcp  ","date":1592093658,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592093658,"objectID":"d13291bfe0e447db7a0e4c7bdcba5823","permalink":"https://wubigo.com/post/vmware-vm-static-ip/","publishdate":"2020-06-14T08:14:18+08:00","relpermalink":"/post/vmware-vm-static-ip/","section":"post","summary":" 虚机网卡地址 配置静态IP C:\\Documents and Settings\\All Users\\Application Data\\VMware\\vmnetdhcp.conf\nhost VMnet8 { hardware ethernet 00:0C:29:23:AV:67; fixed-address 192.168.137.170; }  重启VMWARE DHCP服务 net stop vmnetdhcp net start vmnetdhcp  ","tags":["VM","NETWORK"],"title":"分配Vmware虚机静态IP","type":"post"},{"authors":null,"categories":[],"content":" 自动启动网卡 CENTOS安装完后，网卡默认没有激活，无法分配IP地址\n/etc/sysconfig/network-scripts/ifcfg-enp0s3\nDEVICE=enp0s3 BOOTPROTO=dhcp ONBOOT=yes  systemctl restart network   set the default route on interface\nDEFROUTE=\u0026quot;yes\u0026quot;   删除MariaDB rpm -qa | grep mariadb sudo yum remove -y mariadb mariadb-server rpm -qa | grep mariadb yum remove -y mariadb-libs-5.5.65-1.el7.x86_64 rm -rf /var/log/mariadb rm -f /var/log/mariadb/mariadb.log.rpmsave rm -rf /var/lib/mysql rm -rf /usr/lib64/mysql rm -rf /usr/share/mysql   install\nsudo yum install MariaDB-server sudo systemctl start mariadb.service   安装polipo git clone https://github.com/jech/polipo.git cd polipo git checkout polipo-1.1.1 yum groupinstall 'Development Tools' yum install texinfo make all su -c 'make install'  包管理 yum list available yum repolist yum repo-pkgs kubernetes list  ","date":1592039010,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592039010,"objectID":"9db704b9880294d61d506a51c2ea8ce6","permalink":"https://wubigo.com/post/centos-notes/","publishdate":"2020-06-13T17:03:30+08:00","relpermalink":"/post/centos-notes/","section":"post","summary":"自动启动网卡 CENTOS安装完后，网卡默认没有激活，无法分配IP地址\n/etc/sysconfig/network-scripts/ifcfg-enp0s3\nDEVICE=enp0s3 BOOTPROTO=dhcp ONBOOT=yes  systemctl restart network   set the default route on interface\nDEFROUTE=\u0026quot;yes\u0026quot;   删除MariaDB rpm -qa | grep mariadb sudo yum remove -y mariadb mariadb-server rpm -qa | grep mariadb yum remove -y mariadb-libs-5.5.65-1.el7.x86_64 rm -rf /var/log/mariadb rm -f /var/log/mariadb/mariadb.log.rpmsave rm -rf /var/lib/mysql rm -rf /usr/lib64/mysql rm -rf /usr/share/mysql   install\nsudo yum install MariaDB-server sudo systemctl start mariadb.service   安装polipo git clone https://github.","tags":["LINUX"],"title":"Centos Notes","type":"post"},{"authors":null,"categories":[],"content":" 准备  iptables\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules br_netfilter EOF sudo modprobe br_netfilter lsmod | grep br_netfilter cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system  install container runtime\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026quot; sudo apt-get update apt-cache madison docker-ce sudo apt-get install docker-ce=17.12.1~ce-0~ubuntu sudo usermod -aG docker bigo  setup vpn and proxy\n  VPN and proxy\n install kubeadm\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-cache madison kubectl sudo apt-get install -y kubelet=1.15.12-00 kubeadm=1.15.12-00 kubectl=1.15.12-00 sudo apt-mark hold kubelet kubeadm kubectl  SWAPOFF\nsudo swapoff -a   安装 kubeadm.init.yml\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration clusterName: kubernetes imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.15.12 networking: podSubnet: 10.244.0.0/16   KUBERNETES_RELEASE_VERSION=\u0026quot;$(curl -sSL https://dl.k8s.io/release/stable.txt)\u0026quot; kubeadm config images list kubeadm config images pull --config kubeadm.init.yml sudo kubeadm init --config kubeadm.init.yml kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml kubectl taint nodes --all node-role.kubernetes.io/master- kubectl delete -f https://docs.projectcalico.org/manifests/calico.yaml kubectl delete pod -n kube-system coredns- kubeadm token list   pod-network-cidr should not overlap with your local netowrk\n troubleshooting  Loop (127.0.0.1:46732 -\u0026gt; :53) detected for zone\nkubectl logs -n kube-system coredns-94d74667-9t778 2020-06-14T09:20:33.480Z [INFO] CoreDNS-1.3.1 2020-06-14T09:20:33.480Z [INFO] linux/amd64, go1.11.4, 6b56a9c CoreDNS-1.3.1 linux/amd64, go1.11.4, 6b56a9c 2020-06-14T09:20:33.480Z [INFO] plugin/reload: Running configuration MD5 = 5d5369fbc12f985709b924e721217843 2020-06-14T09:20:33.481Z [FATAL] plugin/loop: Loop (127.0.0.1:46732 -\u0026gt; :53) detected for zone \u0026quot;.\u0026quot;, see https://coredns.io/plugins/loop#troubleshooting. Query: \u0026quot;HINFO 5542328687544567010.7848882517153318095.\u0026quot;   https://coredns.io/plugins/loop#troubleshooting\n方法一：\nkubectl edit -n kube-system cm kubelet-config-1.15 resolvConf: /opt/etc/resolv.conf  方法二：\nhttps://tecadmin.net/disable-local-dns-caching-ubuntu/\n方法三：\nkubectl edit -n kube-system cm coredns forward . /etc/resolv.conf -\u0026gt; forward . 192.168.137.180   系统重启后k8s没有自动启动  在/etc/fstab永久关闭swap\n","date":1590186731,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590186731,"objectID":"a80ee80ee26ed01b53c7618a97e24c74","permalink":"https://wubigo.com/post/kubeadm-note-update/","publishdate":"2020-05-23T06:32:11+08:00","relpermalink":"/post/kubeadm-note-update/","section":"post","summary":"准备  iptables\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules br_netfilter EOF sudo modprobe br_netfilter lsmod | grep br_netfilter cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system  install container runtime\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026quot; sudo apt-get update apt-cache madison docker-ce sudo apt-get install docker-ce=17.12.1~ce-0~ubuntu sudo usermod -aG docker bigo  setup vpn and proxy","tags":["K8S"],"title":"Kubeadm Note Update","type":"post"},{"authors":null,"categories":[],"content":" 软件工程师为了找到工作，花费大量的时间在leetcode上刷题\n然后为面试制作一份完美的简历。\n一旦在创业公司或在FANG巨头中找到自己的心仪的工作，发现\n为了面试所做的技术准备在实际工作中根本排不上用场。\n下面我们来谈谈我们对于由TechLead首创的高效程序员的7个习惯\n的个人看法\n学会阅读别人的代码 能快速识别出一个项目是否值得 避免不必要的会议 代码管理工具git 书写简单的易维护的代码 要事优先并学会拒绝 从运营的角度进行系统设计 https://medium.com/better-programming/7-habits-of-highly-effective-programmers-563ee3b63f33\n","date":1587276921,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587276921,"objectID":"a15aa9b56737af50e066c7716ef116cc","permalink":"https://wubigo.com/post/7-habits-of-highly-effective-programmers/","publishdate":"2020-04-19T14:15:21+08:00","relpermalink":"/post/7-habits-of-highly-effective-programmers/","section":"post","summary":"软件工程师为了找到工作，花费大量的时间在leetcode上刷题\n然后为面试制作一份完美的简历。\n一旦在创业公司或在FANG巨头中找到自己的心仪的工作，发现\n为了面试所做的技术准备在实际工作中根本排不上用场。\n下面我们来谈谈我们对于由TechLead首创的高效程序员的7个习惯\n的个人看法\n学会阅读别人的代码 能快速识别出一个项目是否值得 避免不必要的会议 代码管理工具git 书写简单的易维护的代码 要事优先并学会拒绝 从运营的角度进行系统设计 https://medium.com/better-programming/7-habits-of-highly-effective-programmers-563ee3b63f33","tags":["HABIT","CODE"],"title":"高效程序员的7个习惯","type":"post"},{"authors":null,"categories":[],"content":" 4月8号的更新，出现了如下问题\nhttps://answers.microsoft.com/en-us/windows/forum/all/input-indicator-doesnt-work-well-and-icon-missing\n通过卸载更新把更新删除后，系统恢复正常\n删除本地下载的更新文件 https://winaero.com/blog/delete-downloaded-windows-update-files-in-windows-10/\nC:\\Windows\\SoftwareDistribution\\Download\n","date":1586518964,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586518964,"objectID":"c1032ca0e11cec483a6510c95bc6ae07","permalink":"https://wubigo.com/post/windows-update-corruption/","publishdate":"2020-04-10T19:42:44+08:00","relpermalink":"/post/windows-update-corruption/","section":"post","summary":"4月8号的更新，出现了如下问题\nhttps://answers.microsoft.com/en-us/windows/forum/all/input-indicator-doesnt-work-well-and-icon-missing\n通过卸载更新把更新删除后，系统恢复正常\n删除本地下载的更新文件 https://winaero.com/blog/delete-downloaded-windows-update-files-in-windows-10/\nC:\\Windows\\SoftwareDistribution\\Download","tags":["WINDOWS"],"title":"Windows Update Corruption","type":"post"},{"authors":null,"categories":[],"content":"","date":1586126442,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586126442,"objectID":"156b8c412373e412562d577f77d906fa","permalink":"https://wubigo.com/post/no-code-for-backend/","publishdate":"2020-04-06T06:40:42+08:00","relpermalink":"/post/no-code-for-backend/","section":"post","summary":"","tags":["NO-CODE"],"title":"No-Coding for Backend","type":"post"},{"authors":null,"categories":[],"content":" B2提供免费的10G存储空间, 而且不需要预先设置支付信息例如信用卡认证等。\n而且文件上传下载快速\n安装 pip install --upgrade b2  配置 b2 authorize-account [\u0026lt;applicationKeyId\u0026gt;] [\u0026lt;applicationKey\u0026gt;]  上传 b2 upload-file cmp000 \u0026quot;Commander 8.1.0 Installer (x64).zip\u0026quot; cmp_commander_810_x86.zip Commander 8.1.0 Installer (x64).zip: 100%|███████████████████████████| 835M/835M [06:14\u0026lt;00:00, 2.23MB/s] URL by file name: https://f000.backblazeb2.com/file/cmp000/cmp_commander_810_x86.zip URL by fileId: https://f000.backblazeb2.com/b2api/v2/b2_download_file_by_id?fileId=4_z5fb65c33d39d419a79180f12_f20359e5b85eeada6_d20200405_m102029_c000_v0001066_t0051 { \u0026quot;action\u0026quot;: \u0026quot;upload\u0026quot;, \u0026quot;fileId\u0026quot;: \u0026quot;4_z5fb65c33d39d419a79180f12_f20359e5b85eeada6_d20200405_m102029_c000_v0001066_t0051\u0026quot;, \u0026quot;fileName\u0026quot;: \u0026quot;cmp_commander_810_x86.zip\u0026quot;, \u0026quot;size\u0026quot;: 835119744, \u0026quot;uploadTimestamp\u0026quot;: 1586082029000 }  ","date":1586082606,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586082606,"objectID":"dbc7d7c73f981692f05db542c4aef988","permalink":"https://wubigo.com/post/b2-cmd-line-notes/","publishdate":"2020-04-05T18:30:06+08:00","relpermalink":"/post/b2-cmd-line-notes/","section":"post","summary":" B2提供免费的10G存储空间, 而且不需要预先设置支付信息例如信用卡认证等。\n而且文件上传下载快速\n安装 pip install --upgrade b2  配置 b2 authorize-account [\u0026lt;applicationKeyId\u0026gt;] [\u0026lt;applicationKey\u0026gt;]  上传 b2 upload-file cmp000 \u0026quot;Commander 8.1.0 Installer (x64).zip\u0026quot; cmp_commander_810_x86.zip Commander 8.1.0 Installer (x64).zip: 100%|███████████████████████████| 835M/835M [06:14\u0026lt;00:00, 2.23MB/s] URL by file name: https://f000.backblazeb2.com/file/cmp000/cmp_commander_810_x86.zip URL by fileId: https://f000.backblazeb2.com/b2api/v2/b2_download_file_by_id?fileId=4_z5fb65c33d39d419a79180f12_f20359e5b85eeada6_d20200405_m102029_c000_v0001066_t0051 { \u0026quot;action\u0026quot;: \u0026quot;upload\u0026quot;, \u0026quot;fileId\u0026quot;: \u0026quot;4_z5fb65c33d39d419a79180f12_f20359e5b85eeada6_d20200405_m102029_c000_v0001066_t0051\u0026quot;, \u0026quot;fileName\u0026quot;: \u0026quot;cmp_commander_810_x86.zip\u0026quot;, \u0026quot;size\u0026quot;: 835119744, \u0026quot;uploadTimestamp\u0026quot;: 1586082029000 }  ","tags":["B2","SDS"],"title":"B2命令行工具","type":"post"},{"authors":null,"categories":[],"content":" 云管平台赋能企业在多云环境下，对服务或资源自身和生命周期进行自动化的管理。\n当前行业主要聚焦在可用行，成本管理，云安全。\n平台支持要求  公有云\n 私有云(openstack/VMWARE/K8S)\n  总概 https://solutionsreview.com/cloud-platforms/whats-changed-2020-gartner-magic-quadrant-for-cloud-management-platforms/\n","date":1585978298,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585978298,"objectID":"b9d06e36d79457bf45f01069c4f38ad2","permalink":"https://wubigo.com/post/cmp-2020/","publishdate":"2020-04-04T13:31:38+08:00","relpermalink":"/post/cmp-2020/","section":"post","summary":"云管平台赋能企业在多云环境下，对服务或资源自身和生命周期进行自动化的管理。\n当前行业主要聚焦在可用行，成本管理，云安全。\n平台支持要求  公有云\n 私有云(openstack/VMWARE/K8S)\n  总概 https://solutionsreview.com/cloud-platforms/whats-changed-2020-gartner-magic-quadrant-for-cloud-management-platforms/","tags":["CLOUD"],"title":"云管平台(CMP)行业报告(2020)","type":"post"},{"authors":null,"categories":[],"content":" 上网，最让人无法忍受的是大量的广告影响自己的有限的注意力。\n以前的做法是尽量避免浏览带有大量广告的网站。\n今天介绍几款可以主动屏蔽商业广告的工具。\n基于操作系统 https://github.com/pi-hole/pi-hole\n基于浏览器 Adblock Plus | The world\u0026rsquo;s # 1 free ad blocker\n","date":1585506441,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585506441,"objectID":"87307d9dc1821f036175401a1321dd4e","permalink":"https://wubigo.com/post/ad-blocking/","publishdate":"2020-03-30T02:27:21+08:00","relpermalink":"/post/ad-blocking/","section":"post","summary":"上网，最让人无法忍受的是大量的广告影响自己的有限的注意力。\n以前的做法是尽量避免浏览带有大量广告的网站。\n今天介绍几款可以主动屏蔽商业广告的工具。\n基于操作系统 https://github.com/pi-hole/pi-hole\n基于浏览器 Adblock Plus | The world\u0026rsquo;s # 1 free ad blocker","tags":["INTERNET"],"title":"广告拦截","type":"post"},{"authors":null,"categories":[],"content":"git bash$ winpty docker run -it ubuntu /bin/bash  Prefixing the path with a double slash (//bin//bash) or\nset MSYS_NO_PATHCONV=1  http://stackoverflow.com/questions/7250130/how-to-stop-mingw-and-msys-from-mangling-path-names-given-at-the-command-line#34386471\nhttps://github.com/git-for-windows/git/issues/577#issuecomment-166118846\n","date":1583580700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583580700,"objectID":"c33b3eb3238c6838031a693ba82860bf","permalink":"https://wubigo.com/post/git-bash-notes/","publishdate":"2020-03-07T19:31:40+08:00","relpermalink":"/post/git-bash-notes/","section":"post","summary":"git bash$ winpty docker run -it ubuntu /bin/bash  Prefixing the path with a double slash (//bin//bash) or\nset MSYS_NO_PATHCONV=1  http://stackoverflow.com/questions/7250130/how-to-stop-mingw-and-msys-from-mangling-path-names-given-at-the-command-line#34386471\nhttps://github.com/git-for-windows/git/issues/577#issuecomment-166118846","tags":["GIT"],"title":"Git Bash Notes","type":"post"},{"authors":null,"categories":[],"content":" LevelDB 是一基于内存+SSD的键值存储引擎。\n 内存存储最新的更改和热数据(缓存) SSD持久化所有数据  LevelDB经常使用代理+主从模式构建集群。 与REDIS协议保持兼容，可以被REDIS客户端访问。\n内存结构 LevelDB在内存维护两个跳表:MemTable, 一个只读，一个可写。\n序列号(Sequence number)是全局自增的，每次的修改序列号都会加1。 每个主键保存多个版本的键值。LevelDB用serial number标识键值对 的版本。最大的serial number代表最新的键值对。\n键值对有两种更新操作：\n Put = 1 Delete = 0  当可写的MemTable的大小超过阈值，会把所有的键值转移到只读的MemTable。 同时会创建一个新的可写的MemTable。只读的MemTable会被一个线程异步的 写入SSD。读操作会先查询可写的MemTable，然后查询只读的MemTable，最后 去查询SSD.\n对可写的MemTable支持多线程操作，所以需要并发控制。\n只读的MemTable存在时间很短，被创建后就会把异步写入SSD,然后清空。 当可写的MemTable增长很快，只读的MemTable会很快被填满。如果只读 的MemTable还没有别完全写入SSD，写入线程就会被阻塞。\n内存结构操作日志 最近的MemTable的写入操作都会有一个对应的日志文件记录。 日志文件也有两份，对应两个跳表。\nSSD数据结构 SST LevelDB在SSD上存储了许多SST(Sorted String Table)文件, 每个文件对应一个级别，每个级别有多个SST文件。 SST文件的大小是相同的，不同的是每个级别文件数目不同。 主键在每个SST文件都是有序的。 级别0的文件和其他级别的文件有一个明显的区别：其他级别的SST 文件之间的主键不会重叠，但级别0的SST文件之间的主键可能重叠， 因为级别0的SST文件之间从MemTable直接转存过来。\n为了防止在级别0的SST文件进行键值读取的读放大，LevelDB默认 级别0有4个文件。\nMANIFEST 所有文件的主键范围，级别和其他的元信息存储在MANIFEST文件。 MANIFEST有版本号，通过文件名后缀标识，例如MANIFEST-000031。 每次打开数据库，一个新的版本号的MANIFEST就好被创建，旧版本 的文件就会被删除。\nCURRENT CURRENT文件内容记录了当前版本的MANIFEST文件名字。LevelDB首先读 CURRENT文件内容知道合法的MANIFEST文件。\nLOCK LevelDB数据库目录不允许被多个进程同时访问，当一个进程打开数据库， 一个排他的文件锁就创建。\nlog 日志文件，记录操作日志，例如次要整理和主要整理日志\n数据整理 从只读MemTable到0级的SST文件的转移叫次级整理。从SST文件向 更低级转移交主要整理。\n文件合并 ","date":1582197229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582197229,"objectID":"144864156e51adf019cafd17ef956bb4","permalink":"https://wubigo.com/post/leveldb-main-structure/","publishdate":"2020-02-20T19:13:49+08:00","relpermalink":"/post/leveldb-main-structure/","section":"post","summary":" LevelDB 是一基于内存+SSD的键值存储引擎。\n 内存存储最新的更改和热数据(缓存) SSD持久化所有数据  LevelDB经常使用代理+主从模式构建集群。 与REDIS协议保持兼容，可以被REDIS客户端访问。\n内存结构 LevelDB在内存维护两个跳表:MemTable, 一个只读，一个可写。\n序列号(Sequence number)是全局自增的，每次的修改序列号都会加1。 每个主键保存多个版本的键值。LevelDB用serial number标识键值对 的版本。最大的serial number代表最新的键值对。\n键值对有两种更新操作：\n Put = 1 Delete = 0  当可写的MemTable的大小超过阈值，会把所有的键值转移到只读的MemTable。 同时会创建一个新的可写的MemTable。只读的MemTable会被一个线程异步的 写入SSD。读操作会先查询可写的MemTable，然后查询只读的MemTable，最后 去查询SSD.\n对可写的MemTable支持多线程操作，所以需要并发控制。\n只读的MemTable存在时间很短，被创建后就会把异步写入SSD,然后清空。 当可写的MemTable增长很快，只读的MemTable会很快被填满。如果只读 的MemTable还没有别完全写入SSD，写入线程就会被阻塞。\n内存结构操作日志 最近的MemTable的写入操作都会有一个对应的日志文件记录。 日志文件也有两份，对应两个跳表。\nSSD数据结构 SST LevelDB在SSD上存储了许多SST(Sorted String Table)文件, 每个文件对应一个级别，每个级别有多个SST文件。 SST文件的大小是相同的，不同的是每个级别文件数目不同。 主键在每个SST文件都是有序的。 级别0的文件和其他级别的文件有一个明显的区别：其他级别的SST 文件之间的主键不会重叠，但级别0的SST文件之间的主键可能重叠， 因为级别0的SST文件之间从MemTable直接转存过来。\n为了防止在级别0的SST文件进行键值读取的读放大，LevelDB默认 级别0有4个文件。\nMANIFEST 所有文件的主键范围，级别和其他的元信息存储在MANIFEST文件。 MANIFEST有版本号，通过文件名后缀标识，例如MANIFEST-000031。 每次打开数据库，一个新的版本号的MANIFEST就好被创建，旧版本 的文件就会被删除。\nCURRENT CURRENT文件内容记录了当前版本的MANIFEST文件名字。LevelDB首先读 CURRENT文件内容知道合法的MANIFEST文件。\nLOCK LevelDB数据库目录不允许被多个进程同时访问，当一个进程打开数据库， 一个排他的文件锁就创建。\nlog 日志文件，记录操作日志，例如次要整理和主要整理日志\n数据整理 从只读MemTable到0级的SST文件的转移叫次级整理。从SST文件向 更低级转移交主要整理。\n文件合并 ","tags":["KVS","CACHE"],"title":"LevelDB结构解析","type":"post"},{"authors":null,"categories":[],"content":" 跳表介绍  Skip lists are a data structure that can be used in place of balanced trees. Skip lists use probabilistic balancing rather than strictly enforced balancing and as a result the algorithms for insertion and deletion in skip lists are much simpler and significantly faster than equivalent algorithms for balanced trees.\n跳表是一种可以用来代替平衡树的数据结构，跳表使用概率平衡而不是严格执行的平衡， 因此，与等效树的等效算法相比，跳表中插入和删除的算法要简单得多，并且速度要快得多。\n 跳表是由包含多个级别的链表组成，最低级别的链表存储了所有的主键并且按序链接\njava跳表 https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentSkipListMap.html\n跳表结构  多条链构成，是关键字升序排列的数据结构； 包含多个级别，一个head引用指向最高的级别，最低（底部）的级别，包含所有的key； 每一个级别都是其更低级别的子集，并且是有序的； 如果关键字key在级别level=i中出现，则level\u0026lt;=i的链表中都会包含该关键字key；  ConcurrentSkipListMap和TreeMap类似，它们虽然都是有序的哈希表。但是，第一，它们的线程安全机制不同，TreeMap是非线程安全的，而ConcurrentSkipListMap是线程安全的。第二，ConcurrentSkipListMap是通过跳表实现的，而TreeMap是通过红黑树实现的\n跳表分为许多层(level)，每一层都可以看作是数据的索引，这些索引的意义就是加快跳表查找数据速度。每一层的数据都是有序的，上一层数据是下一层数据的子集，并且第一层(level 1)包含了全部的数据；层次越高，跳跃性越大，包含的数据越少。跳表包含一个表头，它查找数据时，是从上往下，从左往右进行查找。\nConcurrentSkipListMap优点：\n ConcurrentSkipListMap 的key是有序的。\n ConcurrentSkipListMap 支持更高的并发。ConcurrentSkipListMap 的存取时间是log（N），和线程数几乎无关\n  使用场景举例 在其他线程向集合插入数据时安全的获取一个只读快照\n实时流数据处理：最近5分钟订阅的最新消息列表\nConcurrentSkipListMap\u0026lt;Instant, String\u0026gt; events = new ConcurrentSkipListMap\u0026lt;\u0026gt;( Comparator.comparingLong(v -\u0026gt; v.toEpochMilli()));  参考 [1] Skip Lists: A Probabilistic Alternative to Balanced Trees\n","date":1582163906,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582163906,"objectID":"f32dcf8ece2d2c925712694f93c3d562","permalink":"https://wubigo.com/post/leveldb%E4%B9%8B%E8%B7%B3%E8%A1%A8/","publishdate":"2020-02-20T09:58:26+08:00","relpermalink":"/post/leveldb%E4%B9%8B%E8%B7%B3%E8%A1%A8/","section":"post","summary":"跳表介绍  Skip lists are a data structure that can be used in place of balanced trees. Skip lists use probabilistic balancing rather than strictly enforced balancing and as a result the algorithms for insertion and deletion in skip lists are much simpler and significantly faster than equivalent algorithms for balanced trees.\n跳表是一种可以用来代替平衡树的数据结构，跳表使用概率平衡而不是严格执行的平衡， 因此，与等效树的等效算法相比，跳表中插入和删除的算法要简单得多，并且速度要快得多。\n 跳表是由包含多个级别的链表组成，最低级别的链表存储了所有的主键并且按序链接\njava跳表 https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentSkipListMap.html\n跳表结构  多条链构成，是关键字升序排列的数据结构； 包含多个级别，一个head引用指向最高的级别，最低（底部）的级别，包含所有的key； 每一个级别都是其更低级别的子集，并且是有序的； 如果关键字key在级别level=i中出现，则level\u0026lt;=i的链表中都会包含该关键字key；  ConcurrentSkipListMap和TreeMap类似，它们虽然都是有序的哈希表。但是，第一，它们的线程安全机制不同，TreeMap是非线程安全的，而ConcurrentSkipListMap是线程安全的。第二，ConcurrentSkipListMap是通过跳表实现的，而TreeMap是通过红黑树实现的\n跳表分为许多层(level)，每一层都可以看作是数据的索引，这些索引的意义就是加快跳表查找数据速度。每一层的数据都是有序的，上一层数据是下一层数据的子集，并且第一层(level 1)包含了全部的数据；层次越高，跳跃性越大，包含的数据越少。跳表包含一个表头，它查找数据时，是从上往下，从左往右进行查找。\nConcurrentSkipListMap优点：","tags":["KVS","CACHE"],"title":"键值存储引擎基础数据结构之跳表","type":"post"},{"authors":null,"categories":[],"content":" IN VPC 更新前VPC\n更新后VPC\nWhile Hyperplane still uses a cross account network interface, it provides the following benefits for Lambda within a VPC:\n Reduced latency when a function is invoked by using pre-created network interfaces. The network interface is created when the Lambda function is initially created. Network interfaces are shared across functions with the same security group:subnet combination Function scaling is no longer bound to the number of network interfaces  While the new changes make it more conducive for developers to connect Lambda functions to VPCs, the basic architecture doesn’t change in terms of your VPC.\n Your Lambda functions still need the IAM permissions required to create and delete network interfaces in your VPC. You still control the subnet and security group configurations of these network interfaces. You can continue to apply normal network security controls and follow best practices on VPC configuration. You still have to use a NAT device(for example VPC NAT Gateway) to give a function internet access or use VPC endpoints to connect to services outside of your VPC. Nothing changes about the types of resources that your functions can access inside of your VPCs.  参考 https://lumigo.io/blog/to-vpc-or-not-to-vpc-in-aws-lambda/\nhttps://medium.com/@emaildelivery/serverless-pitfalls-issues-you-may-encounter-running-a-start-up-on-aws-lambda-f242b404f41c\nhttps://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/\n","date":1577658829,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577658829,"objectID":"bc321c35db17778feaefcd7527fcef66","permalink":"https://wubigo.com/post/serverless-vpc/","publishdate":"2019-12-30T06:33:49+08:00","relpermalink":"/post/serverless-vpc/","section":"post","summary":"IN VPC 更新前VPC\n更新后VPC\nWhile Hyperplane still uses a cross account network interface, it provides the following benefits for Lambda within a VPC:\n Reduced latency when a function is invoked by using pre-created network interfaces. The network interface is created when the Lambda function is initially created. Network interfaces are shared across functions with the same security group:subnet combination Function scaling is no longer bound to the number of network interfaces  While the new changes make it more conducive for developers to connect Lambda functions to VPCs, the basic architecture doesn’t change in terms of your VPC.","tags":["SERVERLESS","SLS"],"title":"函数计算VPC支持更新","type":"post"},{"authors":null,"categories":[],"content":"Machine learning is the study of powerful techniques that can learn from experience. As a machine learning algorithm accumulates more experience, typically in the form of observational data or interactions with an environment, its performance improves\n","date":1576219056,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576219056,"objectID":"9549a06d9acb9bacc290937d2254a527","permalink":"https://wubigo.com/post/machine-learning-notes/","publishdate":"2019-12-13T14:37:36+08:00","relpermalink":"/post/machine-learning-notes/","section":"post","summary":"Machine learning is the study of powerful techniques that can learn from experience. As a machine learning algorithm accumulates more experience, typically in the form of observational data or interactions with an environment, its performance improves","tags":["ML"],"title":"Machine Learning Notes","type":"post"},{"authors":null,"categories":[],"content":" 计算 尽管函数计算和容器的快速崛起，EC2依然是AWS的业务焦点，\n主要的新功能包括：\n 基于Nitro平台的针对HPC和机器学习的负载的实例\n 基于定制芯片Inferencia针对机器学习实例\n 标准实例支持100Gb网络带宽\n  网络  传输网关支持多播\n 加速的网络到网络的VPN链接\n  存储和数据分析  S3 Access Points和数据湖\n ES搜索支持S3\n 联合查询支持关系数据库，REDSHIFT数据仓库，S3数据湖，而不需要移动数据\n AQUA查询加速器\n 数据湖导出(REDSHIFT数据仓库查询结果能直接导出到S3，并以Parquet格式存放)\n 托管的Cassandra服务\n  函数计算  Provisioned Concurrency for Lambda Functions\n EKS正式支持FARGATE\n RDS PROXY\n VPC支持更新\n  先锋基金IT迁移架构图 私有云架构包括4千万行的单体应用，hadoop数据仓库(20PB)和PaaS(2015年) 数据仓库和PaaS迁移到AWS PaaS实施基于EDA架构的改造 PaaS迁移到Fargate ","date":1575776253,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575776253,"objectID":"f6dcdf6a8ac6b35027a93018cf80a2ca","permalink":"https://wubigo.com/post/aws-reinvent-2019-keynote/","publishdate":"2019-12-08T11:37:33+08:00","relpermalink":"/post/aws-reinvent-2019-keynote/","section":"post","summary":" 计算 尽管函数计算和容器的快速崛起，EC2依然是AWS的业务焦点，\n主要的新功能包括：\n 基于Nitro平台的针对HPC和机器学习的负载的实例\n 基于定制芯片Inferencia针对机器学习实例\n 标准实例支持100Gb网络带宽\n  网络  传输网关支持多播\n 加速的网络到网络的VPN链接\n  存储和数据分析  S3 Access Points和数据湖\n ES搜索支持S3\n 联合查询支持关系数据库，REDSHIFT数据仓库，S3数据湖，而不需要移动数据\n AQUA查询加速器\n 数据湖导出(REDSHIFT数据仓库查询结果能直接导出到S3，并以Parquet格式存放)\n 托管的Cassandra服务\n  函数计算  Provisioned Concurrency for Lambda Functions\n EKS正式支持FARGATE\n RDS PROXY\n VPC支持更新\n  先锋基金IT迁移架构图 私有云架构包括4千万行的单体应用，hadoop数据仓库(20PB)和PaaS(2015年) 数据仓库和PaaS迁移到AWS PaaS实施基于EDA架构的改造 PaaS迁移到Fargate ","tags":["AWS","CLOUD"],"title":"AWS Re:Invent 2019主题汇总-P1","type":"post"},{"authors":null,"categories":[],"content":" Provisioned Concurrency for Lambda Functions To provide customers with improved control over their mission-critical app performance on serverless, AWS introduces Provisioned Concurrency, which is a Lambda feature and works with any trigger. For example, you can use it with WebSockets APIs, GraphQL resolvers, or IoT Rules. This feature gives you more control when building serverless applications that require low latency, such as web and mobile apps, games, or any service that is part of a complex transaction.\nThis is a feature that keeps functions initialized and hyper-ready to respond in double-digit milliseconds. This addition is helpful for implementing interactive services, such as web and mobile backends, latency-sensitive microservices, or synchronous APIs.\nOn enabling Provisioned Concurrency for a function, the Lambda service will initialize the requested number of execution environments so they can be ready to respond to invocations.\nTo know more Provisioned Concurrency in detail, read the Provisioned Concurrency for Lambda Functions\n","date":1575774066,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575774066,"objectID":"369ab3782cadacee3e3cbfe41f008f18","permalink":"https://wubigo.com/post/lambda-provisioned-concurrency/","publishdate":"2019-12-08T11:01:06+08:00","relpermalink":"/post/lambda-provisioned-concurrency/","section":"post","summary":"Provisioned Concurrency for Lambda Functions To provide customers with improved control over their mission-critical app performance on serverless, AWS introduces Provisioned Concurrency, which is a Lambda feature and works with any trigger. For example, you can use it with WebSockets APIs, GraphQL resolvers, or IoT Rules. This feature gives you more control when building serverless applications that require low latency, such as web and mobile apps, games, or any service that is part of a complex transaction.","tags":["LAMBDA","SERVERLESS"],"title":"Lambda Provisioned Concurrency","type":"post"},{"authors":null,"categories":[],"content":"aws sts get-caller-identity aws s3control list-access-points --account-id 46569194568 aws s3control create-access-point --name my-access-point --account-id 46569194568 --bucket wubigo aws s3control get-access-point --account-id \u0026quot;46569194568\u0026quot; --name my-access-point { \u0026quot;Name\u0026quot;: \u0026quot;my-access-point\u0026quot;, \u0026quot;PublicAccessBlockConfiguration\u0026quot;: { \u0026quot;IgnorePublicAcls\u0026quot;: true, \u0026quot;BlockPublicPolicy\u0026quot;: true, \u0026quot;BlockPublicAcls\u0026quot;: true, \u0026quot;RestrictPublicBuckets\u0026quot;: true }, \u0026quot;CreationDate\u0026quot;: \u0026quot;2019-12-04T14:24:38Z\u0026quot;, \u0026quot;Bucket\u0026quot;: \u0026quot;wubigo\u0026quot;, \u0026quot;NetworkOrigin\u0026quot;: \u0026quot;Internet\u0026quot; }  ","date":1575469238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575469238,"objectID":"ab48a8832e073dbb44caa5d57c02350e","permalink":"https://wubigo.com/post/aws-s3-access-point/","publishdate":"2019-12-04T22:20:38+08:00","relpermalink":"/post/aws-s3-access-point/","section":"post","summary":"aws sts get-caller-identity aws s3control list-access-points --account-id 46569194568 aws s3control create-access-point --name my-access-point --account-id 46569194568 --bucket wubigo aws s3control get-access-point --account-id \u0026quot;46569194568\u0026quot; --name my-access-point { \u0026quot;Name\u0026quot;: \u0026quot;my-access-point\u0026quot;, \u0026quot;PublicAccessBlockConfiguration\u0026quot;: { \u0026quot;IgnorePublicAcls\u0026quot;: true, \u0026quot;BlockPublicPolicy\u0026quot;: true, \u0026quot;BlockPublicAcls\u0026quot;: true, \u0026quot;RestrictPublicBuckets\u0026quot;: true }, \u0026quot;CreationDate\u0026quot;: \u0026quot;2019-12-04T14:24:38Z\u0026quot;, \u0026quot;Bucket\u0026quot;: \u0026quot;wubigo\u0026quot;, \u0026quot;NetworkOrigin\u0026quot;: \u0026quot;Internet\u0026quot; }  ","tags":["AWS","S3"],"title":"Aws S3 Access Point","type":"post"},{"authors":null,"categories":[],"content":" JAVA 这两年最重要的项目就是GRAAL的正式版发布。\nGRAAL能做什么？\n 让解释性程序例如JAVA, JS 运行的更快: AOT编译为宿主二进制可执行文件,  启动时间小于100ms， 像C, GO, ERLANG一样的执行速度\n 更低的内存占用：只占用传统的JVM应用20%的内存\n  听起来是不是该项目为函数计算做准备的？\n是，但不完全是。\nGRAAL的官方目标是提供一个统一的虚拟机执行平台，支持如下运行环境：\n JavaScrip Python Ruby R JVM 语言（Java, Scala, Groovy, Kotlin, Clojure） LLVM语言 (C , C++)  而且不同语言之间零成本互相调用\n安装 wget https://github.com/oracle/graal/releases/download/vm-19.2.1/graalvm-ce-linux-amd64-19.2.1.tar.gz tar zxvf graalvm-ce-linux-amd64-19.2.1.tar.gz export PATH=$PATH:$GRAAL_HOME/bin   检查\njs --version GraalVM JavaScript (GraalVM CE Native 19.2.1)  安装native-image\ngu install native-image  gu available Downloading: Component catalog from www.graalvm.org ComponentId Version Component name Origin -------------------------------------------------------------------------------- llvm-toolchain 19.2.1 LLVM.org toolchain github.com native-image 19.2.1 Native Image github.com python 19.2.1 Graal.Python github.com R 19.2.1 FastR github.com ruby 19.2.1 TruffleRuby github.com   使用Polyglot Shell polyglot --jvm --shell  创建JAVA编写的可执行二进制文件  安装glibc-devel, zlib-devel (头文件C库 and zlib) 和 gcc\nsudo apt-get install libz-dev   HelloWorld.java\npublic class HelloWorld { public static void main(String... args) { System.out.println(\u0026quot;Hello World\u0026quot;); } }   编译\njavac HelloWorld.java native-image -cp . HelloWorld Build on Server(pid: 20375, port: 45977) [helloworld:20375] classlist: 199.03 ms [helloworld:20375] (cap): 1,866.60 ms [helloworld:20375] setup: 5,938.57 ms [helloworld:20375] (typeflow): 17,532.76 ms [helloworld:20375] (objects): 8,477.10 ms [helloworld:20375] (features): 2,365.65 ms [helloworld:20375] analysis: 28,469.52 ms [helloworld:20375] (clinit): 861.90 ms [helloworld:20375] universe: 2,785.89 ms [helloworld:20375] (parse): 9,430.36 ms [helloworld:20375] (inline): 1,623.19 ms [helloworld:20375] (compile): 11,158.60 ms [helloworld:20375] compile: 22,588.75 ms [helloworld:20375] image: 687.25 ms [helloworld:20375] write: 1,153.04 ms [helloworld:20375] [total]: 62,321.01 ms  执行\n./helloworld Hello World   部署到容器 微服务 http://sparkjava.com/\nhttps://quarkus.io/get-started/\nhttps://github.com/spring-projects/spring-framework/wiki/GraalVM-native-image-support\n参考 https://royvanrijn.com/blog/2018/09/part-2-native-microservice-in-graalvm/\n","date":1572315423,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572315423,"objectID":"e513e714d31637dba20991cb752cca29","permalink":"https://wubigo.com/post/java-last-2-years/","publishdate":"2019-10-29T10:17:03+08:00","relpermalink":"/post/java-last-2-years/","section":"post","summary":"JAVA 这两年最重要的项目就是GRAAL的正式版发布。\nGRAAL能做什么？\n 让解释性程序例如JAVA, JS 运行的更快: AOT编译为宿主二进制可执行文件,  启动时间小于100ms， 像C, GO, ERLANG一样的执行速度\n 更低的内存占用：只占用传统的JVM应用20%的内存\n  听起来是不是该项目为函数计算做准备的？\n是，但不完全是。\nGRAAL的官方目标是提供一个统一的虚拟机执行平台，支持如下运行环境：\n JavaScrip Python Ruby R JVM 语言（Java, Scala, Groovy, Kotlin, Clojure） LLVM语言 (C , C++)  而且不同语言之间零成本互相调用\n安装 wget https://github.com/oracle/graal/releases/download/vm-19.2.1/graalvm-ce-linux-amd64-19.2.1.tar.gz tar zxvf graalvm-ce-linux-amd64-19.2.1.tar.gz export PATH=$PATH:$GRAAL_HOME/bin   检查\njs --version GraalVM JavaScript (GraalVM CE Native 19.2.1)  安装native-image\ngu install native-image  gu available Downloading: Component catalog from www.graalvm.org ComponentId Version Component name Origin -------------------------------------------------------------------------------- llvm-toolchain 19.","tags":["JAVA","GRAAL"],"title":"JAVA这两年","type":"post"},{"authors":null,"categories":[],"content":" 第一代 2003年， jBPM 1.0发布。\n运行环境：J2EE\n过程定义语言：jPDL(当时工作流厂商都有各自的过程定义语言和建模工具)\n当时的主流的技术： applets, Swing桌面和EJB\n第二代 2004年，jBPM 2.0发布\n同时jBPM加入JBoss基金会.\n运行环境：任何JAVA环境(POJO实现过程运行时)，不需要应用服务器\n第三代 2005年, jBPM 3.0发布，支持BPEL\n过程定义语言：过程虚拟机\n架构： 与二代相比，架构发生了巨大变化。可以操作的业务功能大范围扩展，不仅通\n过JAVA实现状态机，而且支持建模\nHIBERNETE作为持久机制并同时提供会话对象的概念，\n工作流引擎所有的相关性交互都纳入contextual block范畴\n这为以后的工作流命令设计模式和命令拦截设计模式的广泛应用打下良好的基础\n第四代 2009年， jBPM 4.0 alpha版发布.\n过程虚拟机成功工作流引擎的核心。\n过程定义语言：BPMN, jPDL 和 BPEL\n因为团队人员离开并启动Activiti，正式版没能发布。\n主要改进：\n 无状态的服务API 运行时和历史数据的分离： 保证运行时持久的性能  第五代 2010年, Activiti 1发布\n改变：\n 版权从LGPL转到APACHE.\n 过程定义语言：BPMN(唯一)\n 从性能和扩展性加强PVM\n 多租户支持\n 轻量级架构\n  第六代 2017年，flowable 6.0发布。\n改变：\n 过程模型：放弃PVM,使用原生BPMN， 实现真正的动态过程执行和复杂的过程迁移\n 数据远完全抽象：支持NoSQL\n CMMN支持\n 函数式工作流\n  ","date":1572146762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572146762,"objectID":"e0dfe5b99639eb697878a92a13287890","permalink":"https://wubigo.com/post/digital-evolution-of-open-source-business-process-management/","publishdate":"2019-10-27T11:26:02+08:00","relpermalink":"/post/digital-evolution-of-open-source-business-process-management/","section":"post","summary":"第一代 2003年， jBPM 1.0发布。\n运行环境：J2EE\n过程定义语言：jPDL(当时工作流厂商都有各自的过程定义语言和建模工具)\n当时的主流的技术： applets, Swing桌面和EJB\n第二代 2004年，jBPM 2.0发布\n同时jBPM加入JBoss基金会.\n运行环境：任何JAVA环境(POJO实现过程运行时)，不需要应用服务器\n第三代 2005年, jBPM 3.0发布，支持BPEL\n过程定义语言：过程虚拟机\n架构： 与二代相比，架构发生了巨大变化。可以操作的业务功能大范围扩展，不仅通\n过JAVA实现状态机，而且支持建模\nHIBERNETE作为持久机制并同时提供会话对象的概念，\n工作流引擎所有的相关性交互都纳入contextual block范畴\n这为以后的工作流命令设计模式和命令拦截设计模式的广泛应用打下良好的基础\n第四代 2009年， jBPM 4.0 alpha版发布.\n过程虚拟机成功工作流引擎的核心。\n过程定义语言：BPMN, jPDL 和 BPEL\n因为团队人员离开并启动Activiti，正式版没能发布。\n主要改进：\n 无状态的服务API 运行时和历史数据的分离： 保证运行时持久的性能  第五代 2010年, Activiti 1发布\n改变：\n 版权从LGPL转到APACHE.\n 过程定义语言：BPMN(唯一)\n 从性能和扩展性加强PVM\n 多租户支持\n 轻量级架构\n  第六代 2017年，flowable 6.0发布。\n改变：\n 过程模型：放弃PVM,使用原生BPMN， 实现真正的动态过程执行和复杂的过程迁移\n 数据远完全抽象：支持NoSQL","tags":["BPM","工作流"],"title":"开源的工作流引擎技术演进历史","type":"post"},{"authors":null,"categories":[],"content":"心理学家：21世纪最重要的工作技能\n专注是21世纪最重要的工作技能，\n可是很多人没有意识到这一点。\n1971年的时候心理学家西蒙就说过：“大量的信息\n意味着另一种东西变得很稀缺：注意力”。 几十年之前这是个事实，\n在21世纪，注意力变的更加珍贵。\n工作环境正在发生快速的变化，在不远的将来，\n在这个世界只存在两种人：\n 一种注意力被别人完全控制和操纵的人\n 一种注意力不可被打扰的人\n  研究人员告诉我们专注和注意力人们创新发展的原材料\n下面是一些常见的工作干扰：\n 聊天群\n 会议\n 电话\n 邮件\n 同事\n  ","date":1570868399,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570868399,"objectID":"0cb72b155cad394555a5f8fb31a9ffe2","permalink":"https://wubigo.com/post/no.-1-work-skill-of-the-future/","publishdate":"2019-10-12T16:19:59+08:00","relpermalink":"/post/no.-1-work-skill-of-the-future/","section":"post","summary":"心理学家：21世纪最重要的工作技能\n专注是21世纪最重要的工作技能，\n可是很多人没有意识到这一点。\n1971年的时候心理学家西蒙就说过：“大量的信息\n意味着另一种东西变得很稀缺：注意力”。 几十年之前这是个事实，\n在21世纪，注意力变的更加珍贵。\n工作环境正在发生快速的变化，在不远的将来，\n在这个世界只存在两种人：\n 一种注意力被别人完全控制和操纵的人\n 一种注意力不可被打扰的人\n  研究人员告诉我们专注和注意力人们创新发展的原材料\n下面是一些常见的工作干扰：\n 聊天群\n 会议\n 电话\n 邮件\n 同事\n  ","tags":[],"title":"心理学家：21世纪最重要的工作技能","type":"post"},{"authors":null,"categories":[],"content":"","date":1570786790,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570786790,"objectID":"f711712646ea1553fb09bd8cd950c6b6","permalink":"https://wubigo.com/post/function-computing-rise-like-s3/","publishdate":"2019-10-11T17:39:50+08:00","relpermalink":"/post/function-computing-rise-like-s3/","section":"post","summary":"","tags":[],"title":"函数计算颠覆对象执行环境(像S3对象存储一样)","type":"post"},{"authors":null,"categories":[],"content":" 在windows中使用docker有多种方式：\n docker WIN10 desktop\n WSL\n  本文主要介绍在WSL中使用docker\n前提条件  Windows 10 Version 1803以上(支持cgroups)\n Ubuntu for WSL 16.0.4 LTS(WSL支持的最新版本)\n Docker 17.09\n  安装WSL install WSL\nWSL 命令行和默认 shell cmd:\\\u0026gt;wsl wubigo:/tmp/docker-desktop-root/mnt/host/d/code#/etc# cat /etc/wsl.conf [automount] root = /mnt/host crossDistro = true options = \u0026quot;metadata\u0026quot;  WSL DOCKER磁盘卷路径 cmd:\\\u0026gt;docker volume inspect edgex_consul-config [ { \u0026quot;CreatedAt\u0026quot;: \u0026quot;2022-03-07T08:04:34Z\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Labels\u0026quot;: { \u0026quot;com.docker.compose.project\u0026quot;: \u0026quot;edgex\u0026quot;, \u0026quot;com.docker.compose.version\u0026quot;: \u0026quot;2.2.3\u0026quot;, \u0026quot;com.docker.compose.volume\u0026quot;: \u0026quot;consul-config\u0026quot; }, \u0026quot;Mountpoint\u0026quot;: \u0026quot;/var/lib/docker/volumes/edgex_consul-config/_data\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;edgex_consul-config\u0026quot;, \u0026quot;Options\u0026quot;: null, \u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot; } ]  \\\\wsl$\\docker-desktop-data\\version-pack-data\\community\\docker\\volumes\\\n安装Ubuntu for WSL 16.0.4 LTS install ubuntu in WSL\n安装DOCKER  启动ubuntu in WSL(以管理员身份运行)\nc:\\\u0026gt;wsl -d Ubuntu-16.04  安装docker\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot; $ sudo apt-get install docker-ce=17.09.0~ce-0~ubuntu $ sudo usermod -aG docker $USER  启动docker服务\n  /usr/local/sbin/start_docker.sh\n#!/usr/bin/env bash sudo cgroupfs-mount sudo service docker start  在系统启动的时候运行docker服务 在控制面版\\管理工具\\计划任务创建任务\n操作的参数如下：\n-c \u0026quot;sudo /bin/sh /usr/local/sbin/start_docker.sh\u0026quot;  备份WSL 在备份前保存并关闭所有的WSL进程，否则正在运行的进程被强行关闭\nc:\\\u0026gt;wsl -l c:\\\u0026gt;wsl --export Ubuntu-16.04 Ubuntu-16.wsl.export.tar  升级linux d:\\code\\dapp\u0026gt;wsl -l -v NAME STATE VERSION * Ubuntu Running 1 docker-desktop Running 2 docker-desktop-data Running 2  升级Ubuntu 从WSL 1 到2\nwsl --set-version Ubuntu 2 d:\\code\\dapp\u0026gt;wsl -l -v NAME STATE VERSION * Ubuntu Stopped 2 docker-desktop Running 2 docker-desktop-data Running 2  WSL localhost connection refused 运行之前先备份/etc/hosts\nwsl2-ubuntu-map-win-localhost.sh\nnameserver=$(grep -m 1 nameserver /etc/resolv.conf | awk '{print $2}') # find nameserver [ -n \u0026quot;$nameserver\u0026quot; ] || \u0026quot;unable to find nameserver\u0026quot; || exit 1 # exit immediately if nameserver was not found echo \u0026quot;##### nameserver found: '$nameserver'\u0026quot; localhost_entry=$(grep -v \u0026quot;127.0.0.1\u0026quot; /etc/hosts | grep \u0026quot;\\slocalhost$\u0026quot;) # find localhost entry excluding 127.0.0.1 if [ -n \u0026quot;$localhost_entry\u0026quot; ]; then # if localhost entry was found echo \u0026quot;##### localhost entry found: '$localhost_entry'\u0026quot; sed -i \u0026quot;s/$localhost_entry/$nameserver localhost/g\u0026quot; /etc/hosts # then update localhost entry with the new $nameserver else # else if entry was not found echo \u0026quot;##### localhost entry not found\u0026quot; echo \u0026quot;$nameserver localhost\u0026quot; \u0026gt;\u0026gt; /etc/hosts # then append $nameserver mapping to localhost fi cat /etc/hosts  https://gist.github.com/toryano0820/6ee3bff2474cdf13e70d972da710996a#:~:text=For%20WSL2%3A%20Fixes%20%22Connection%20Refused%22%20issue%20when%20accessing,Remember%20to%20backup%20%22%2Fetc%2Fhosts%22%20just%20in%20case%20%21%21%21\n在windows访问wsl文件系统 \\\\wsl$  REF\n1\n静态IP WSL-ip.bat\nwsl -d Ubuntu -u root ip addr add 192.168.50.16/24 broadcast 192.168.50.255 dev eth0 label eth0:1 netsh interface ip add address \u0026quot;vEthernet (WSL)\u0026quot; 192.168.50.88 255.255.255.0  https://lifesaver.codes/answer/static-ip-on-wsl-2-418\n","date":1570515679,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570515679,"objectID":"8fde9949f2d493404d430487d0ef5aea","permalink":"https://wubigo.com/post/docker-within-wsl/","publishdate":"2019-10-08T14:21:19+08:00","relpermalink":"/post/docker-within-wsl/","section":"post","summary":"在windows中使用docker有多种方式：\n docker WIN10 desktop\n WSL\n  本文主要介绍在WSL中使用docker\n前提条件  Windows 10 Version 1803以上(支持cgroups)\n Ubuntu for WSL 16.0.4 LTS(WSL支持的最新版本)\n Docker 17.09\n  安装WSL install WSL\nWSL 命令行和默认 shell cmd:\\\u0026gt;wsl wubigo:/tmp/docker-desktop-root/mnt/host/d/code#/etc# cat /etc/wsl.conf [automount] root = /mnt/host crossDistro = true options = \u0026quot;metadata\u0026quot;  WSL DOCKER磁盘卷路径 cmd:\\\u0026gt;docker volume inspect edgex_consul-config [ { \u0026quot;CreatedAt\u0026quot;: \u0026quot;2022-03-07T08:04:34Z\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Labels\u0026quot;: { \u0026quot;com.docker.compose.project\u0026quot;: \u0026quot;edgex\u0026quot;, \u0026quot;com.docker.compose.version\u0026quot;: \u0026quot;2.2.3\u0026quot;, \u0026quot;com.docker.compose.volume\u0026quot;: \u0026quot;consul-config\u0026quot; }, \u0026quot;Mountpoint\u0026quot;: \u0026quot;/var/lib/docker/volumes/edgex_consul-config/_data\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;edgex_consul-config\u0026quot;, \u0026quot;Options\u0026quot;: null, \u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot; } ]  \\\\wsl$\\docker-desktop-data\\version-pack-data\\community\\docker\\volumes\\","tags":["DOCKER","WINDOWS"],"title":"在WSL中使用Docker","type":"post"},{"authors":null,"categories":[],"content":" 昨天看到AWS STEP FUNCTIONS支持嵌套的工作流\n，当时就被震惊了。 AWS早些年推出SWF框架提供工作流服务，\n后来工作流服务就没有大的工作，SWF框架的核心开发者一部分\n离职去了UBER开发Cadence。没想到沉寂了多年的AWS会在STEP FUNCTIONS\n支持工作流，看来这个千亿规模的市场又快被颠覆了。\n20多年来，工作流都是超大型企业的配置专利，而STEP function的推出\n可以预计，高大上的工作流服务会很快走进很多中小企业工作台并被普及应用。\n回顾一下自己的工作历史：\n从以前的数据中心服务器SA(2006), 到虚拟主机ESX，\n再到OPENSTACK(2010)搭建混合云，\n然后利用K8S(2014)搭建PaaS，现在SERVERLESS,\n计算架构正在发生快速的演进。本文梳理了算力演进历史和未来\n内容\n 从虚拟主机到容器\n 从容器到unikernel\n 函数计算的蓬勃发展\n  什么是虚拟计算 hypervisor分类  Type-1 裸机  KVM, QEMU, VMWare, Virtualbox\n Type-2 托管  XenServer， Hyper-v， KVM, ESX, Xen\n虚拟机的问题  贵  一台云主机(8G/4Core/500MBps)在2017年的超过6000元/年\n 操作慢  一个普通的镜像在2G左右，再加上JAVA应用，一个镜像需要10G.\n启动，备份非常不便。\n容器计算 |虚拟机| 容器 |:\u0026mdash;|:\u0026mdash;|:\u0026mdash;| 构件| 完整的操作系统和应用| 微内核和应用 虚拟技术| 硬件虚拟化| 操作系统虚拟化\n容器计算带来什么好处  容器镜像小  alpine的容量是2M\n 容器占用的硬件资源更少  一台PC可以启动上百个容器\n 容器启动快  一般几毫秒可以启动\n 容器不用备份  容器创建只需要一个Dockerfile，容器镜像是只读镜像\n 容器和微服务器架构，DevOps, CI/CD天然融合  容器存在的问题  安全  容器共享操作系统内核，具体较低的隔离级别，\n如果内核出问题，其他的容器也会处于风险之中\n 网络  如何在足够的隔离级别和复杂的高效网络连接权衡\n容器编排  K8S\n EKS\n SWARM\n MARATHON\n MAGNM\n FLEET\n  用户轻松在计算集群里面部署，管理，扩展基于容器的应用而\n不用关心容器和服务器的绑定，系统扩容等问题\n容器网络  CNM  DOCKER规范，libnetwork实现\n内置的驱动包括none, host, bridge , overlay, MACvlan\n$ docker network ls NAME DRIVER SCOPE 68343147e103 bridge bridge local 5d7df1d8f633 docker_default bridge local d3990aab14a9 host host local fe4ec77439f4 none null local   CNI  COREOS规范，被K8S, MESOS, CLOUD Foundry采用\njson格式的网络模式定义\ndocker的核心组件  LXC\n AUFS\n  DOCKER的优势  镜像不可修改\n 部署没有第三方依赖\n 注册器简单且容易扩展\n 容易回滚\n  对DOCKER的误解：\n 如果学习了docker就不必学其他的系统知识了\n 每个容器只能有一个进程\n 用容器了就不必使用其他的配置管理工具了\n 必须使用容器才能达到高效敏捷和一致性状态优势\n  Open Containers Initiative UniKernel 函数计算的兴起 No server is easier to manage than no-server\n背景：\n 前端技术演进   强大的原生客户/移动客户端让开发者通过调用各种云服务  编写大规模互联网应用，替换传统的后台服务\n HTTP/S应用接口及基于token的安全认证成为工业标准   后台的定制化开发  基于云服务器定制开发\nServerLess VS PaaS PasS可能是ServerLess的一个迭代\n总结  虚拟机是一种更成熟的技术，更安全\n 容器虚拟化是一个更适合微服务器架构的方案\n 虚拟机和容器并不是互斥而是互补\n 函数计算是一下个\u0026hellip;\n  ","date":1569455317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569455317,"objectID":"6d2763c07718c307f3026de164a600e2","permalink":"https://wubigo.com/post/from-vm-to-container-to-serverless/","publishdate":"2019-09-26T07:48:37+08:00","relpermalink":"/post/from-vm-to-container-to-serverless/","section":"post","summary":"昨天看到AWS STEP FUNCTIONS支持嵌套的工作流\n，当时就被震惊了。 AWS早些年推出SWF框架提供工作流服务，\n后来工作流服务就没有大的工作，SWF框架的核心开发者一部分\n离职去了UBER开发Cadence。没想到沉寂了多年的AWS会在STEP FUNCTIONS\n支持工作流，看来这个千亿规模的市场又快被颠覆了。\n20多年来，工作流都是超大型企业的配置专利，而STEP function的推出\n可以预计，高大上的工作流服务会很快走进很多中小企业工作台并被普及应用。\n回顾一下自己的工作历史：\n从以前的数据中心服务器SA(2006), 到虚拟主机ESX，\n再到OPENSTACK(2010)搭建混合云，\n然后利用K8S(2014)搭建PaaS，现在SERVERLESS,\n计算架构正在发生快速的演进。本文梳理了算力演进历史和未来\n内容\n 从虚拟主机到容器\n 从容器到unikernel\n 函数计算的蓬勃发展\n  什么是虚拟计算 hypervisor分类  Type-1 裸机  KVM, QEMU, VMWare, Virtualbox\n Type-2 托管  XenServer， Hyper-v， KVM, ESX, Xen\n虚拟机的问题  贵  一台云主机(8G/4Core/500MBps)在2017年的超过6000元/年\n 操作慢  一个普通的镜像在2G左右，再加上JAVA应用，一个镜像需要10G.\n启动，备份非常不便。\n容器计算 |虚拟机| 容器 |:\u0026mdash;|:\u0026mdash;|:\u0026mdash;| 构件| 完整的操作系统和应用| 微内核和应用 虚拟技术| 硬件虚拟化| 操作系统虚拟化\n容器计算带来什么好处  容器镜像小  alpine的容量是2M","tags":["ARCHITECTURE"],"title":"虚拟机 -\u003e 容器 -\u003e 函数计算","type":"post"},{"authors":null,"categories":[],"content":" AWS Step Functions VS Adds Support for Nested Workflows\n","date":1569396179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569396179,"objectID":"6ba3521d4c612a9916b380d7328b06ae","permalink":"https://wubigo.com/post/step-functions/","publishdate":"2019-09-25T15:22:59+08:00","relpermalink":"/post/step-functions/","section":"post","summary":"AWS Step Functions VS Adds Support for Nested Workflows","tags":[],"title":"Step Functions 常见问题","type":"post"},{"authors":null,"categories":[],"content":"https://news.ycombinator.com/item?id=20726906\nISP Starter Kit\nhttp://www.wispa.org/Resources/HOW-TO-START-A-WISP\nwireless fiber\n5G mobile broadband\nhttps://www.huawei.com/en/press-events/news/2019/1/huawei-releases-wireless-fiber-solution\nhttps://www.techdirt.com/articles/20190904/08392642916/colorado-town-offers-1-gbps-60-after-years-battling-comcast.shtml\n","date":1568597961,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568597961,"objectID":"63df43e8426dce6be14bd570794ecbb2","permalink":"https://wubigo.com/post/start-your-own-isp/","publishdate":"2019-09-16T09:39:21+08:00","relpermalink":"/post/start-your-own-isp/","section":"post","summary":"https://news.ycombinator.com/item?id=20726906\nISP Starter Kit\nhttp://www.wispa.org/Resources/HOW-TO-START-A-WISP\nwireless fiber\n5G mobile broadband\nhttps://www.huawei.com/en/press-events/news/2019/1/huawei-releases-wireless-fiber-solution\nhttps://www.techdirt.com/articles/20190904/08392642916/colorado-town-offers-1-gbps-60-after-years-battling-comcast.shtml","tags":["ISP","BROADBAND","WISP","NETWORK"],"title":"宽带服务","type":"post"},{"authors":null,"categories":[],"content":" DOCKER DEAMON PROXY  systemd level  /etc/systemd/system/docker.service.d/https-proxy.conf\n[Service] Environment=\u0026quot;http_proxy=http://127.0.0.1:8123\u0026quot; \u0026quot;https_proxy=https://127.0.0.1:8123\u0026quot; \u0026quot;NO_PROXY=registry-1.docker.io\u0026quot;  sudo systemctl daemon-reload sudo systemctl restart docker   service level  /etc/default/docker\nexport http_proxy=\u0026quot;http://127.0.0.1:3128/\u0026quot;   daemon level  /etc/docker/daemon\nDOCKER CLIENT PROXY ~/.docker/config.json\n \u0026quot;proxies\u0026quot;:{ \u0026quot;default\u0026quot;:{} }  ","date":1568329676,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568329676,"objectID":"b847c0c713511dfb1c6a90cc8b40db80","permalink":"https://wubigo.com/post/docker-proxy/","publishdate":"2019-09-13T07:07:56+08:00","relpermalink":"/post/docker-proxy/","section":"post","summary":" DOCKER DEAMON PROXY  systemd level  /etc/systemd/system/docker.service.d/https-proxy.conf\n[Service] Environment=\u0026quot;http_proxy=http://127.0.0.1:8123\u0026quot; \u0026quot;https_proxy=https://127.0.0.1:8123\u0026quot; \u0026quot;NO_PROXY=registry-1.docker.io\u0026quot;  sudo systemctl daemon-reload sudo systemctl restart docker   service level  /etc/default/docker\nexport http_proxy=\u0026quot;http://127.0.0.1:3128/\u0026quot;   daemon level  /etc/docker/daemon\nDOCKER CLIENT PROXY ~/.docker/config.json\n \u0026quot;proxies\u0026quot;:{ \u0026quot;default\u0026quot;:{} }  ","tags":["DOCKER"],"title":"Docker Proxy for daemon and client","type":"post"},{"authors":null,"categories":[],"content":" [TOC]\nAWS领先的设计理念和强大的技术生态\n使你身陷其中，学习你要用它，开发你要\n用它，上线还要用它。 一年下来项目还没有\n正式商用，已经有十几万的研发费用。\n今天向你推荐 localstack（与openstack啥关系？私有云+公有云），\n让你使用AWS免费，至少在项目POC或开发测试阶段免费。\n有了它， 你不用再焦急的等待老板审批公有云的\n计算，存储，数据库资源开发申请。\n是不是这个项目听起来很激动？\n那如何使用localstack呢？\n安装localstack localstack是一个非常活跃的正在快速成长中的项目，\n建议通过源代码安装\n 下载源代码\ngit clone git@github.com:localstack/localstack.git git fetch --all git checkout tags/v0.10.3 -b v0.10.3  启用需要使用的AWS服务\n  修改配置文件，启用你需要使用的AWS服务:ec2,s3,iot,kafka等。\n注意服务的名字必须来自服务名字列表， 否则不识别\n启用服务就是修改下边的配置文件\nlocalstack\\docker-compose.yml\n SERVICES=${SERVICES-ec2,ecs,stepfunctions,iam,lambda,dynamodb,apigateway,s3,sns} DATA_DIR=${DATA_DIR-/tmp/localstack/data } volumes: - \u0026quot;${TMPDIR:-d:/tmp/localstack}:/tmp/localstack\u0026quot;  make sure driver D is shared in docker desktop daemon\ndocker-compose up localstack_1 | Starting mock S3 (http port 4572)... localstack_1 | Starting mock SNS (http port 4575)... localstack_1 | Starting mock IAM (http port 4593)... localstack_1 | Starting mock API Gateway (http port 4567)... localstack_1 | Starting mock DynamoDB (http port 4569)... localstack_1 | Starting mock Lambda service (http port 4574)... localstack_1 | Starting mock CloudWatch Logs (http port 4586)... localstack_1 | Starting mock StepFunctions (http port 4585)...  系统消息显示需要的服务/端口已经启动。\n到目前为至，localstack已经安装完毕。\n记录并保存localstack的操作数据 if volumes in docker settings\nLocalstack is recording all API calls in JSON file.\nWhen the container restarts, it will re-apply these calls -\nthis is how we are able to keep our data between restarts\ndocker-compose.yml\n- DATA_DIR=/tmp/localstack/data  下边，我们来验证公有云服务是否可用。\n安装AWS客户端  安装到虚拟环境\n(venv) d:\\code\\venv\u0026gt;pip install awscli   可以安装到系统环境\n 配置AWS CLI\n(venv) d:\\code\\venv\u0026gt;aws configure AWS Access Key ID [None]: any-id-is-ok AWS Secret Access Key [None]: fake-key Default region name [local]: local Default output format [None]:   验证服务编排  试用stepfunctions服务  上面显示stepfunctions服务在4585端口，下面的端口要和\n配置保持一致\n(venv) d:\\code\\venv\u0026gt;aws stepfunctions --endpoint-url http://localhost:4585 list-activities   创建一个HelloWorld工作流\n (venv) d:\\code\\venv\u0026gt;aws stepfunctions --endpoint-url http://localhost:4585 create-state-machine --definition \u0026quot;{\\\u0026quot;Comment\\\u0026quot;: \\\u0026quot;A Hello World example of the Amazon States Language using a Pass state\\\u0026quot;,\\\u0026quot;StartAt\\\u0026quot;: \\\u0026quot;HelloWorld\\\u0026quot;,\\\u0026quot;States\\\u0026quot;: {\\\u0026quot;HelloWorld\\\u0026quot;: {\\\u0026quot;Type\\\u0026quot;: \\\u0026quot;Pass\\\u0026quot;,\\\u0026quot;End\\\u0026quot;: true}}}\u0026quot; --name \u0026quot;HelloWorld\u0026quot; --role-arn \u0026quot;arn:aws:iam::012345678901:role/DummyRole\u0026quot;  显示创建的工作流\n(venv) d:\\code\\venv\u0026gt;aws stepfunctions --endpoint-url http://localhost:4585 list-state-machines { \u0026quot;stateMachines\u0026quot;: [ { \u0026quot;creationDate\u0026quot;: 1568199315.809, \u0026quot;stateMachineArn\u0026quot;: \u0026quot;arn:aws:states:us-east-1:123456789012:stateMachine:HelloWorld\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;HelloWorld\u0026quot; } ] }   工作流已经创建，你可以启动工作流，\n添加Lambda，部署微服务，添加微服务到到工作流，\n所有公有云的计算，存储，API调用，上行宽带费用\n通过使用localstack一切免费。\n赶快加入项目，贡献你的力量\nhttps://localstack.cloud/\n参考  https://hub.docker.com/r/amazon/aws-stepfunctions-local https://docs.aws.amazon.com/lambda/latest/dg/limits.html  ","date":1568245584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568245584,"objectID":"aeaaca1145fc29cef7b588fe20beb857","permalink":"https://wubigo.com/post/use-public-cloud-for-free/","publishdate":"2019-09-12T07:46:24+08:00","relpermalink":"/post/use-public-cloud-for-free/","section":"post","summary":"[TOC]\nAWS领先的设计理念和强大的技术生态\n使你身陷其中，学习你要用它，开发你要\n用它，上线还要用它。 一年下来项目还没有\n正式商用，已经有十几万的研发费用。\n今天向你推荐 localstack（与openstack啥关系？私有云+公有云），\n让你使用AWS免费，至少在项目POC或开发测试阶段免费。\n有了它， 你不用再焦急的等待老板审批公有云的\n计算，存储，数据库资源开发申请。\n是不是这个项目听起来很激动？\n那如何使用localstack呢？\n安装localstack localstack是一个非常活跃的正在快速成长中的项目，\n建议通过源代码安装\n 下载源代码\ngit clone git@github.com:localstack/localstack.git git fetch --all git checkout tags/v0.10.3 -b v0.10.3  启用需要使用的AWS服务\n  修改配置文件，启用你需要使用的AWS服务:ec2,s3,iot,kafka等。\n注意服务的名字必须来自服务名字列表， 否则不识别\n启用服务就是修改下边的配置文件\nlocalstack\\docker-compose.yml\n SERVICES=${SERVICES-ec2,ecs,stepfunctions,iam,lambda,dynamodb,apigateway,s3,sns} DATA_DIR=${DATA_DIR-/tmp/localstack/data } volumes: - \u0026quot;${TMPDIR:-d:/tmp/localstack}:/tmp/localstack\u0026quot;  make sure driver D is shared in docker desktop daemon\ndocker-compose up localstack_1 | Starting mock S3 (http port 4572)... localstack_1 | Starting mock SNS (http port 4575).","tags":["IAAS","CLOUD","LOCALSTACK","AWS"],"title":"免费使用公有云服务","type":"post"},{"authors":null,"categories":[],"content":" 注册用户 /etc/matrix-synapse/homeserver.yaml\nregistration_shared_secret:  sudo systemctl restart matrix-synapse  配置代理 sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/nginx-selfsigned.key -out /etc/ssl/certs/nginx-selfsigned.crt  server { listen 443 ssl http2; listen [::]:443 ssl http2; # For the federation port listen 8448 ssl http2 default_server; listen [::]:8448 ssl http2 default_server; ssl_certificate /etc/ssl/certs/nginx-selfsigned.crt; ssl_certificate_key /etc/ssl/private/nginx-selfsigned.key; location ~* ^(\\/_matrix|\\/_synapse\\/client) { proxy_pass http://localhost:8008; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $host; # Nginx by default only allows file uploads up to 1M in size # Increase client_max_body_size to match max_upload_size defined in homeserver.yaml client_max_body_size 50M; } }  创建用户 工具 register_new_matrix_user -c /etc/matrix-synapse/homeserver.yaml http://localhost:8008  API nonce curl http://192.168.10.103:8008/_synapse/admin/v1/register {\u0026quot;nonce\u0026quot;:\u0026quot;c6091d1c6fc96580b0fc25820e908ef4613fbec15233063e2aadc1d3a81c8a9170805f5d9f2fa63016dd7138bc8ad409a4da46bf75a2e429c541d0828abf0612\u0026quot;}  MAC import hashlib import hmac def generate_mac(nonce, user, password, admin=False, user_type=None): mac = hmac.new( key=b'registration_shared_secret', digestmod=hashlib.sha1, ) mac.update(nonce.encode('utf8')) mac.update(b\u0026quot;\\x00\u0026quot;) mac.update(user.encode('utf8')) mac.update(b\u0026quot;\\x00\u0026quot;) mac.update(password.encode('utf8')) mac.update(b\u0026quot;\\x00\u0026quot;) mac.update(b\u0026quot;admin\u0026quot; if admin else b\u0026quot;notadmin\u0026quot;) if user_type: mac.update(b\u0026quot;\\x00\u0026quot;) mac.update(user_type.encode('utf8')) return mac.hexdigest() nonce = \u0026quot;6052ab555c5925adfb2441dffa7761c375d3b1c7dd83032eb2ae6ff4d1a3424ebab1fe6f0ccdae3eba2f655caff9de74e706019adee57da2f3b4b3199432dd5d\u0026quot; shared_secret = \u0026quot;registration_shared_secret\u0026quot; mac=generate_mac(nonce, \u0026quot;wuhg\u0026quot;, \u0026quot;wuhg\u0026quot;, True) print(mac)  POST /_synapse/admin/v1/register { \u0026quot;nonce\u0026quot;: \u0026quot;thisisanonce\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;bigo\u0026quot;, \u0026quot;displayname\u0026quot;: \u0026quot;bigo\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pizz\u0026quot;, \u0026quot;admin\u0026quot;: true, \u0026quot;mac\u0026quot;: \u0026quot;mac_digest_here\u0026quot; }  收发消息 托管的WEB https://app.element.io/#/login\nhomeserver：https://192.168.10.10\n安装 wget https://github.com/vector-im/element-web/releases/download/v1.8.1/element-v1.8.1.tar.gz tar element-v1.8.1.tar.gz cp element-v1.8.1 /var/www/html/element cd /var/www/html/element cp config.sample.json config.json  https://192.168.10.10/element\n","date":1566094020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566094020,"objectID":"149de4edd3108e265dabf258bb9d7e53","permalink":"https://wubigo.com/post/matrix-homeserver/","publishdate":"2019-08-18T10:07:00+08:00","relpermalink":"/post/matrix-homeserver/","section":"post","summary":"注册用户 /etc/matrix-synapse/homeserver.yaml\nregistration_shared_secret:  sudo systemctl restart matrix-synapse  配置代理 sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/nginx-selfsigned.key -out /etc/ssl/certs/nginx-selfsigned.crt  server { listen 443 ssl http2; listen [::]:443 ssl http2; # For the federation port listen 8448 ssl http2 default_server; listen [::]:8448 ssl http2 default_server; ssl_certificate /etc/ssl/certs/nginx-selfsigned.crt; ssl_certificate_key /etc/ssl/private/nginx-selfsigned.key; location ~* ^(\\/_matrix|\\/_synapse\\/client) { proxy_pass http://localhost:8008; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $host; # Nginx by default only allows file uploads up to 1M in size # Increase client_max_body_size to match max_upload_size defined in homeserver.","tags":["IM","MATRIX"],"title":"Matrix Homeserver","type":"post"},{"authors":null,"categories":[],"content":" 配置  下载B2 TLS安全证书\nopenssl s_client -showcerts -connect api.backblazeb2.com:443 \u0026gt; b2.crt mv b2.crt .minio/certs/CAs/ set MINIO_ACCESS_KEY=B2_keyID set MINIO_SECRET_KEY=B2_applicationKey minio gateway b2   ","date":1560741974,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560741974,"objectID":"48a2eb1b4a70faa6e3c161fad6070ebb","permalink":"https://wubigo.com/post/minio-as-a-gateway-to-b2/","publishdate":"2019-06-17T11:26:14+08:00","relpermalink":"/post/minio-as-a-gateway-to-b2/","section":"post","summary":" 配置  下载B2 TLS安全证书\nopenssl s_client -showcerts -connect api.backblazeb2.com:443 \u0026gt; b2.crt mv b2.crt .minio/certs/CAs/ set MINIO_ACCESS_KEY=B2_keyID set MINIO_SECRET_KEY=B2_applicationKey minio gateway b2   ","tags":["STORAGE","SDS","MINIO"],"title":"Minio配置为B2存储网关","type":"post"},{"authors":null,"categories":[],"content":"https://www.sdxcentral.com/networking/virtualization/definitions/how-does-micro-segmentation-help-security-explanation/\n","date":1558862784,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558862784,"objectID":"5b4d0919be97be90f51b0fd30369154e","permalink":"https://wubigo.com/post/secucity-micro-segmentation/","publishdate":"2019-05-26T17:26:24+08:00","relpermalink":"/post/secucity-micro-segmentation/","section":"post","summary":"https://www.sdxcentral.com/networking/virtualization/definitions/how-does-micro-segmentation-help-security-explanation/","tags":["SECURITY"],"title":"Secucity Micro Segmentation","type":"post"},{"authors":null,"categories":[],"content":" give the wireless network higher priority than the wired WIRELESS CONNECTION \u0026gt; \u0026quot;Internet Protocol Version 4 (TCP/IPv4) Properties\u0026quot; \u0026gt; advanced TCP/IP setting \u0026gt; Automatic metric  Uncheck it. That will enable a text box named \u0026ldquo;Interface metric\u0026rdquo;. Fill in a number. It needs to be larger than 1 (reserved for loopback) and the number(30) you choose for the wired network.\nWIRED CONNECTION \u0026gt; \u0026quot;Internet Protocol Version 4 (TCP/IPv4) Properties\u0026quot; \u0026gt; advanced TCP/IP setting \u0026gt; Automatic metric  Again Uncheck \u0026ldquo;Automatic metric\u0026rdquo;, and fill in a number in the \u0026ldquo;Interface metric\u0026rdquo; box. It needs to be larger than 1 but smaller than the number above (15).\n","date":1557478676,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557478676,"objectID":"35fa5639b14ce7b07a0f9984c5843689","permalink":"https://wubigo.com/post/wireless-and-wired-connection-both-at-a-same-time-in-windows/","publishdate":"2019-05-10T16:57:56+08:00","relpermalink":"/post/wireless-and-wired-connection-both-at-a-same-time-in-windows/","section":"post","summary":"give the wireless network higher priority than the wired WIRELESS CONNECTION \u0026gt; \u0026quot;Internet Protocol Version 4 (TCP/IPv4) Properties\u0026quot; \u0026gt; advanced TCP/IP setting \u0026gt; Automatic metric  Uncheck it. That will enable a text box named \u0026ldquo;Interface metric\u0026rdquo;. Fill in a number. It needs to be larger than 1 (reserved for loopback) and the number(30) you choose for the wired network.\nWIRED CONNECTION \u0026gt; \u0026quot;Internet Protocol Version 4 (TCP/IPv4) Properties\u0026quot; \u0026gt; advanced TCP/IP setting \u0026gt; Automatic metric  Again Uncheck \u0026ldquo;Automatic metric\u0026rdquo;, and fill in a number in the \u0026ldquo;Interface metric\u0026rdquo; box.","tags":["NETWORK","WINDOWS"],"title":"Wireless and Wired Connection Both at a Same Time in Windows","type":"post"},{"authors":null,"categories":[],"content":"AppInfo 启动类型必须是自动或手动，\n否则，msinstaller， services.msc， regedit\n等都会报错：\nThe Service command cannot be started, either because it is disabled or because it has no enabled devices associated with it  AppInfo\tsvchost.exe\tFacilitates the running of interactive applications with additional administrative privileges.\tUsers will be unable to launch applications with the additional administrative privileges they may require to perform desired user tasks. These tools include regedit. Although safe to disable, this is not recommended since you need to boot into safe mode to enable again.  ","date":1557114714,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557114714,"objectID":"e07f0c24af37e7520af65940e921a17e","permalink":"https://wubigo.com/post/windows-application-information-service/","publishdate":"2019-05-06T11:51:54+08:00","relpermalink":"/post/windows-application-information-service/","section":"post","summary":"AppInfo 启动类型必须是自动或手动，\n否则，msinstaller， services.msc， regedit\n等都会报错：\nThe Service command cannot be started, either because it is disabled or because it has no enabled devices associated with it  AppInfo\tsvchost.exe\tFacilitates the running of interactive applications with additional administrative privileges.\tUsers will be unable to launch applications with the additional administrative privileges they may require to perform desired user tasks. These tools include regedit. Although safe to disable, this is not recommended since you need to boot into safe mode to enable again.","tags":["WINDOWS"],"title":"Windows Application Information Service","type":"post"},{"authors":null,"categories":[],"content":" 微服务认证和授权有很多方案，\n这里比较各种主流方案的优缺点，\n为你的业务系统选择MAA方案提供指南\n   方案 优点 缺点     分布式会话管理 简单，成熟，服务器统一管理 扩展性比较差   客户令牌     单点登录     API网关令牌管理     第三方应用授权     SSL/TLS 双向认证      方案  分布式会话管理  会话信息由服务器存储\n实现方式：\n Sticky session Session replication Centralized session storage   客户令牌  令牌由客户持有\nJWT: 头，负载和签名\n 头\n{ \u0026quot;typ\u0026quot;: \u0026quot;JWT\u0026quot;, \u0026quot;alg\u0026quot;: \u0026quot;HS256\u0026quot; }  负载\n{ \u0026quot;id\u0026quot;: 123, \u0026quot;name\u0026quot;: \u0026quot;hi tico\u0026quot;, \u0026quot;is_admin\u0026quot;: true, \u0026quot;expire\u0026quot;: 1558213420 }  签名\nHMACSHA256( base64UrlEncode(header) + \u0026quot;.\u0026quot; + base64UrlEncode(payload), secret )    单点登录\n API网关令牌管理\n   第三方应用授权   API 令牌  例如github的API个人令牌\n OAUTH  Someone may wonder why an Authorization Code is used to request Access Token, rather than returning the Access Token to the client directly from the authorization server. The reason why OAuth is designed in this way is to pass through the user agent (browser) during the process of redirecting to the client’s Callback URL. If the Access Token is passed directly, there is a risk of being stolen.\nBy using the authorization code, the client directly interacts with the authorization server when applying for the access token, and the authorization server also authorize the client when processing the client’s token request, so it’s prevent others from forging the client’s identity to use the authentication code\n SSL/TLS 双向认证  例如istio citadel私有证书中心\n[1]. https://medium.com/tech-tajawal/microservice-authentication-and-authorization-solutions-e0e5e74b248a\n","date":1556442487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556442487,"objectID":"29e3bf1dd3d55d889c347594eb902323","permalink":"https://wubigo.com/post/microservices-authentication-and-authorization/","publishdate":"2019-04-28T17:08:07+08:00","relpermalink":"/post/microservices-authentication-and-authorization/","section":"post","summary":"微服务认证和授权有很多方案，\n这里比较各种主流方案的优缺点，\n为你的业务系统选择MAA方案提供指南\n   方案 优点 缺点     分布式会话管理 简单，成熟，服务器统一管理 扩展性比较差   客户令牌     单点登录     API网关令牌管理     第三方应用授权     SSL/TLS 双向认证      方案  分布式会话管理  会话信息由服务器存储\n实现方式：\n Sticky session Session replication Centralized session storage   客户令牌  令牌由客户持有\nJWT: 头，负载和签名\n 头\n{ \u0026quot;typ\u0026quot;: \u0026quot;JWT\u0026quot;, \u0026quot;alg\u0026quot;: \u0026quot;HS256\u0026quot; }  负载","tags":["MICROSERVICE","UAA"],"title":"微服务认证和授权（MAA）指南","type":"post"},{"authors":null,"categories":[],"content":"之前一直用pycharm,今天把code升级到1.3.2的时候， 突然提示我安装python扩展，决定试试。 结果发现python的解释器设置有问题， 总是设置为系统的解释器， 而虚拟环境的解释器不起作用。\napt remove --purge python3.5 reboot  结果ubuntu桌面启动不了。好多应用程序例如chrome，virtualbox都消失了， 造成了很大的麻烦。\nCtrl+Alt+F1进入虚拟控制台登录\napt install python3.5 apt install ubuntu-desktop  重新安装chrome和virtualbox\ncd /etc/apt/sources.list.d sudo mv google-chrome.list.save google-chrome.list apt update apt install google-chrome-stable  ","date":1553407449,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553407449,"objectID":"85d87c64d29ba663f9fe082e68adbc96","permalink":"https://wubigo.com/post/linux-python3.5-remove/","publishdate":"2019-03-24T14:04:09+08:00","relpermalink":"/post/linux-python3.5-remove/","section":"post","summary":"之前一直用pycharm,今天把code升级到1.3.2的时候， 突然提示我安装python扩展，决定试试。 结果发现python的解释器设置有问题， 总是设置为系统的解释器， 而虚拟环境的解释器不起作用。\napt remove --purge python3.5 reboot  结果ubuntu桌面启动不了。好多应用程序例如chrome，virtualbox都消失了， 造成了很大的麻烦。\nCtrl+Alt+F1进入虚拟控制台登录\napt install python3.5 apt install ubuntu-desktop  重新安装chrome和virtualbox\ncd /etc/apt/sources.list.d sudo mv google-chrome.list.save google-chrome.list apt update apt install google-chrome-stable  ","tags":["LINUX","PYTHON"],"title":"Linux删除Python3.5","type":"post"},{"authors":null,"categories":[],"content":" 本地流线型开发 本地流线型开发\n集成开发，测试部署 IDE\n","date":1553385716,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553385716,"objectID":"192502c0143ab7c3e4de72aa84030803","permalink":"https://wubigo.com/post/k8s-app-development-toolbox/","publishdate":"2019-03-24T08:01:56+08:00","relpermalink":"/post/k8s-app-development-toolbox/","section":"post","summary":"本地流线型开发 本地流线型开发\n集成开发，测试部署 IDE","tags":["K8S","APP"],"title":"K8s高效应用开发工具集","type":"post"},{"authors":null,"categories":[],"content":" K8S POD Command Override OCR\ndocker Entrypoint vs k8s command     docker k8s     entry ENTRYPOINT command   arguments CMD args    k8s command and args override the default OCR Entrypoint and Cmd\nDockerfile\nFROM alpine:3.8 RUN apk add --no-cache curl ethtool \u0026amp;\u0026amp; rm -rf /var/cache/apk/* CMD [\u0026quot;--version\u0026quot;] ENTRYPOINT [\u0026quot;curl\u0026quot;]  cmd-override-pod.yaml\napiVersion: v1 kind: Pod metadata: name: command-override labels: purpose: override-command spec: containers: - name: command-override-container image: bigo/curl:v1 command: [\u0026quot;curl\u0026quot;] args: [\u0026quot;--help\u0026quot;] restartPolicy: Never  docker run -it bigo/curl:v1 curl 7.61.1 (x86_64-alpine-linux-musl) libcurl/7.61.1 LibreSSL/2.0.0 zlib/1.2.11 libssh2/1.8.0 nghttp2/1.32.0 Release-Date: 2018-09-05  kubectl apply -f cmd-override-pod.yaml kubectl logs command-override Usage: curl [options...] \u0026lt;url\u0026gt; --abstract-unix-socket \u0026lt;path\u0026gt; Connect via abstract Unix domain socket --anyauth Pick any authentication method -a, --append Append to target file when uploading  ","date":1552891750,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552891750,"objectID":"e060b27024638440ab348f813d905de1","permalink":"https://wubigo.com/post/k8s-pod-command-override/","publishdate":"2019-03-18T14:49:10+08:00","relpermalink":"/post/k8s-pod-command-override/","section":"post","summary":"K8S POD Command Override OCR\ndocker Entrypoint vs k8s command     docker k8s     entry ENTRYPOINT command   arguments CMD args    k8s command and args override the default OCR Entrypoint and Cmd\nDockerfile\nFROM alpine:3.8 RUN apk add --no-cache curl ethtool \u0026amp;\u0026amp; rm -rf /var/cache/apk/* CMD [\u0026quot;--version\u0026quot;] ENTRYPOINT [\u0026quot;curl\u0026quot;]  cmd-override-pod.yaml\napiVersion: v1 kind: Pod metadata: name: command-override labels: purpose: override-command spec: containers: - name: command-override-container image: bigo/curl:v1 command: [\u0026quot;curl\u0026quot;] args: [\u0026quot;--help\u0026quot;] restartPolicy: Never  docker run -it bigo/curl:v1 curl 7.","tags":["K8S"],"title":"K8s Pod Command Override","type":"post"},{"authors":null,"categories":[],"content":" Node-level Logging  System component logs      RUN IN CONTAINER(Y/N) Systemd(W/WO) LOGGER LOCATION     kubelet and container runtime  W/O /var/log   kubelet and container runtime  W journald   scheduler Y  /var/log   kube-proxy Y  /var/log    /var/lib/kubelet/pods/\u0026lt;PodUID\u0026gt;/\n/var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;\nls -l /var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;/ lrwxrwxrwx 1 root root 165 3月 30 06:52 0.log -\u0026gt; /var/lib/docker/containers/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021-json.log  Cluster-level logging  Use a node-level logging agent that runs on every node. Include a dedicated sidecar container for logging in an application pod. Push logs directly to a backend from within an application  具体实现\n EFK  ","date":1552789507,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552789507,"objectID":"62a4013c2742426ceb75ec69f930a7ea","permalink":"https://wubigo.com/post/k8s-logging/","publishdate":"2019-03-17T10:25:07+08:00","relpermalink":"/post/k8s-logging/","section":"post","summary":"Node-level Logging  System component logs      RUN IN CONTAINER(Y/N) Systemd(W/WO) LOGGER LOCATION     kubelet and container runtime  W/O /var/log   kubelet and container runtime  W journald   scheduler Y  /var/log   kube-proxy Y  /var/log    /var/lib/kubelet/pods/\u0026lt;PodUID\u0026gt;/\n/var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;\nls -l /var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;/ lrwxrwxrwx 1 root root 165 3月 30 06:52 0.log -\u0026gt; /var/lib/docker/containers/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021-json.log  Cluster-level logging  Use a node-level logging agent that runs on every node.","tags":["K8S"],"title":"K8s Logging","type":"post"},{"authors":null,"categories":[],"content":" git clone git@github.com:wubigo/kubernetes.git git remote add upstream https://github.com/kubernetes/kubernetes.git git fetch --all git checkout tags/v1.13.3 -b v1.13.3 git branch -av|grep 1.13 * fix-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2. remotes/origin/release-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2.  管理POD func (kl *Kubelet) syncPod(o syncPodOptions) error {  ","date":1551669200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551669200,"objectID":"c919bd285f8023e9d2964065904c018d","permalink":"https://wubigo.com/post/k8s-core-development/","publishdate":"2019-03-04T11:13:20+08:00","relpermalink":"/post/k8s-core-development/","section":"post","summary":" git clone git@github.com:wubigo/kubernetes.git git remote add upstream https://github.com/kubernetes/kubernetes.git git fetch --all git checkout tags/v1.13.3 -b v1.13.3 git branch -av|grep 1.13 * fix-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2. remotes/origin/release-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2.  管理POD func (kl *Kubelet) syncPod(o syncPodOptions) error {  ","tags":["K8S"],"title":"K8S CORE DEVELOPMENT","type":"post"},{"authors":null,"categories":[],"content":" 基于腾讯云Go SDK开发\n下载开发工具集 go get -u github.com/tencentcloud/tencentcloud-sdk-go  为集群准备CVM 从本地开发集群K8S读取安全凭证secretId和secretKey配置信息， 然后把安全凭证传送给SDK客户端\nsecretId, secretKey:= K8SClient.Secrets(\u0026quot;namespace=tencent\u0026quot;).Get(\u0026quot;cloud-pass\u0026quot;) credential := CloudCommon.NewCredential(\u0026quot;secretId\u0026quot;, \u0026quot;secretKey\u0026quot;) client, _ := cvm.NewClient(credential, regions.Beijing)  request := cvm.NewAllocateHostsRequest() request.FromJsonString(K8SClient.Configs(\u0026quot;namespace=tencent\u0026quot;).Get(\u0026quot;K8S-TENCENT-PROD\u0026quot;)) response, err := client.AllocateHosts(request)  通过ANSIBLE在CVM搭建K8S集群 Ansible.Hosts().Get(response.ToJsonString())  调用ANSIBLE开始在CVM部署K8S集群\n","date":1551615889,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551615889,"objectID":"ae96d6074f37a8b4118582872daa3c52","permalink":"https://wubigo.com/post/dev-on-tencent-cloud-sdk-in-go/","publishdate":"2019-03-03T20:24:49+08:00","relpermalink":"/post/dev-on-tencent-cloud-sdk-in-go/","section":"post","summary":"基于腾讯云Go SDK开发\n下载开发工具集 go get -u github.com/tencentcloud/tencentcloud-sdk-go  为集群准备CVM 从本地开发集群K8S读取安全凭证secretId和secretKey配置信息， 然后把安全凭证传送给SDK客户端\nsecretId, secretKey:= K8SClient.Secrets(\u0026quot;namespace=tencent\u0026quot;).Get(\u0026quot;cloud-pass\u0026quot;) credential := CloudCommon.NewCredential(\u0026quot;secretId\u0026quot;, \u0026quot;secretKey\u0026quot;) client, _ := cvm.NewClient(credential, regions.Beijing)  request := cvm.NewAllocateHostsRequest() request.FromJsonString(K8SClient.Configs(\u0026quot;namespace=tencent\u0026quot;).Get(\u0026quot;K8S-TENCENT-PROD\u0026quot;)) response, err := client.AllocateHosts(request)  通过ANSIBLE在CVM搭建K8S集群 Ansible.Hosts().Get(response.ToJsonString())  调用ANSIBLE开始在CVM部署K8S集群","tags":["K8S","API","SDK","TENCENT"],"title":"通过SDK操纵公有云","type":"post"},{"authors":null,"categories":[],"content":"转录语音数据集\nmozilla crowdsources the largest dataset of human voices available for use, including 18 different languages, adding up to almost 1,400 hours of recorded voice data from more than 42,000 contributors\nhttps://blog.mozilla.org/blog/2019/02/28/sharing-our-common-voices-mozilla-releases-the-largest-to-date-public-domain-transcribed-voice-dataset/\n","date":1551385179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551385179,"objectID":"7890348034d7b9c1f7a05490f0731485","permalink":"https://wubigo.com/post/speech_recognition-transcribed-voice-dataset/","publishdate":"2019-03-01T04:19:39+08:00","relpermalink":"/post/speech_recognition-transcribed-voice-dataset/","section":"post","summary":"转录语音数据集\nmozilla crowdsources the largest dataset of human voices available for use, including 18 different languages, adding up to almost 1,400 hours of recorded voice data from more than 42,000 contributors\nhttps://blog.mozilla.org/blog/2019/02/28/sharing-our-common-voices-mozilla-releases-the-largest-to-date-public-domain-transcribed-voice-dataset/","tags":["RNN","DEEPLEARNING","SPEECH_RECOGNITION"],"title":"mozilla crowdsources the largest dataset of human voices","type":"post"},{"authors":null,"categories":[],"content":" 简介 CNI是K8S的网络插件实现规范，与docker的CNM并不兼容，在K8S和docker的博弈过程中， K8S把docker作为默认的runtime并没有换来docker对K8S的支持。K8S决定支持CNI规范。 许多网络厂商的产品都提供同时都支持CNM和CNI的产品。\n在容器网络环境，经常看到docker看不到K8S POD的IP网络配置， DOCKER容器有时候和POD无法通信。\nCNI相对CNM是一个轻量级的规范。网络配置是基于JSON格式， 网络插件支持创建和删除指令。POD启动的时候发送创建指令。\nPOD运行时首先为分配一个网络命名空间，并把该网络命名空间制定给容器ID， 然后把CNI配置文件传送给CNI网络驱动。网络驱动连接容器到自己的网络， 并把分配的IP地址通过JSON文件报告给POD运行时POD终止的时候发送删除指令。\n当前CNI指令负责处理IPAM, L2和L3, POD运行时处理端口映射(L4)\nK8S网络基础 K8S网络基础\nCNI插件 CNI实现方式 CNI有很多实现，在这里之列举熟悉的几个实现。并提供详细的说明文档。\n Flannel\n Kube-router\nKube-router\n OpenVSwitch\n Calico\nCalico可以以非封装或非覆盖方式部署以支持高性能，高扩展扩展性数据中心网络需求\nCNI-Calico\n Weave Net\n 网桥\nCNI 网桥\n  ","date":1550996323,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550996323,"objectID":"4dbacfc0727205a4c013414bf43243c1","permalink":"https://wubigo.com/post/k8s-cni/","publishdate":"2019-02-24T16:18:43+08:00","relpermalink":"/post/k8s-cni/","section":"post","summary":" 简介 CNI是K8S的网络插件实现规范，与docker的CNM并不兼容，在K8S和docker的博弈过程中， K8S把docker作为默认的runtime并没有换来docker对K8S的支持。K8S决定支持CNI规范。 许多网络厂商的产品都提供同时都支持CNM和CNI的产品。\n在容器网络环境，经常看到docker看不到K8S POD的IP网络配置， DOCKER容器有时候和POD无法通信。\nCNI相对CNM是一个轻量级的规范。网络配置是基于JSON格式， 网络插件支持创建和删除指令。POD启动的时候发送创建指令。\nPOD运行时首先为分配一个网络命名空间，并把该网络命名空间制定给容器ID， 然后把CNI配置文件传送给CNI网络驱动。网络驱动连接容器到自己的网络， 并把分配的IP地址通过JSON文件报告给POD运行时POD终止的时候发送删除指令。\n当前CNI指令负责处理IPAM, L2和L3, POD运行时处理端口映射(L4)\nK8S网络基础 K8S网络基础\nCNI插件 CNI实现方式 CNI有很多实现，在这里之列举熟悉的几个实现。并提供详细的说明文档。\n Flannel\n Kube-router\nKube-router\n OpenVSwitch\n Calico\nCalico可以以非封装或非覆盖方式部署以支持高性能，高扩展扩展性数据中心网络需求\nCNI-Calico\n Weave Net\n 网桥\nCNI 网桥\n  ","tags":["K8S","CNI","NETWORK"],"title":"K8S CNI操作指引","type":"post"},{"authors":null,"categories":[],"content":"","date":1550911549,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550911549,"objectID":"ad87d4d50ad00942d83b178fa166e755","permalink":"https://wubigo.com/project/vpn/","publishdate":"2019-02-23T16:45:49+08:00","relpermalink":"/project/vpn/","section":"project","summary":"","tags":["VPN","NFV"],"title":"KV-STORE","type":"project"},{"authors":null,"categories":[],"content":" 前言 利用公有云或混合云的架构是企业IT应用落地实施的最高效方式。\n企业在上云的过程中，主要面临以下问题：\n 公有云开发环境搭建：引起这个问题的主要原因有两个\n 开发成本变化：固定成本变成线性成本 网络带宽：宽带问题成为开发中的大难题  遗留应用迁移：公有云迁移并没有一个固定的方案，必须结合遗留应用的 架构并结合公有云厂商的基础服务的特性采用最合理的有效方案。\n 应用架构的快速迭代：容器架构才刚刚落地， 函数计算又异军突起\n  内容 WEB应用架构演进：虚机，容器，函数计算  微服务架构软件交付流程   函数计算架构软件交付流程  函数计算之道 函数计算进行时\n公有云服务本地开发环境搭建 开发者可以免费使用公有云服务进行开发测试\n免费使用公有云服务\n函数计算本地开发环境搭建 搭建一个完全本地的函数计算开发环境\n函数计算的本地开发环境搭建有两种方式：\n 基于localstack+terraform+serverless   开发环境本地搭建参考\n 基于kubeless/knative+k8s   函数计算真实应用场景开发案例 函数计算应用案例\n函数计算云服务 函数计算重要开发项目库    项目 平台 支持语言     Zappa  python   Serverless AWS Lambda, Azure Functions, Google CloudFunctions Node.js, Python, Java, Go, C#, Ruby, Swift, Kotlin, PHP, Scala    ","date":1550911549,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550911549,"objectID":"f440d17ccc61ed1834d85ba75f354e42","permalink":"https://wubigo.com/project/noserver/","publishdate":"2019-02-23T16:45:49+08:00","relpermalink":"/project/noserver/","section":"project","summary":" 前言 利用公有云或混合云的架构是企业IT应用落地实施的最高效方式。\n企业在上云的过程中，主要面临以下问题：\n 公有云开发环境搭建：引起这个问题的主要原因有两个\n 开发成本变化：固定成本变成线性成本 网络带宽：宽带问题成为开发中的大难题  遗留应用迁移：公有云迁移并没有一个固定的方案，必须结合遗留应用的 架构并结合公有云厂商的基础服务的特性采用最合理的有效方案。\n 应用架构的快速迭代：容器架构才刚刚落地， 函数计算又异军突起\n  内容 WEB应用架构演进：虚机，容器，函数计算  微服务架构软件交付流程   函数计算架构软件交付流程  函数计算之道 函数计算进行时\n公有云服务本地开发环境搭建 开发者可以免费使用公有云服务进行开发测试\n免费使用公有云服务\n函数计算本地开发环境搭建 搭建一个完全本地的函数计算开发环境\n函数计算的本地开发环境搭建有两种方式：\n 基于localstack+terraform+serverless   开发环境本地搭建参考\n 基于kubeless/knative+k8s   函数计算真实应用场景开发案例 函数计算应用案例\n函数计算云服务 函数计算重要开发项目库    项目 平台 支持语言     Zappa  python   Serverless AWS Lambda, Azure Functions, Google CloudFunctions Node.js, Python, Java, Go, C#, Ruby, Swift, Kotlin, PHP, Scala    ","tags":[],"title":"函数计算","type":"project"},{"authors":null,"categories":[],"content":"","date":1550911549,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550911549,"objectID":"ad6914935c5459ff42379a3cde5cc777","permalink":"https://wubigo.com/project/sds/","publishdate":"2019-02-23T16:45:49+08:00","relpermalink":"/project/sds/","section":"project","summary":"","tags":["SDS","BIGDATA","STORAGE"],"title":"大数据存储","type":"project"},{"authors":null,"categories":["IT"],"content":" setup for gitlab tee .gitlab-ci.yml \u0026lt;\u0026lt; EOF image: monachus/hugo variables: GIT_SUBMODULE_STRATEGY: recursive pages: script: - hugo artifacts: paths: - public only: - master EOF git init echo \u0026quot;/public\u0026quot; \u0026gt;\u0026gt; .gitignore  post  hugo new post//index.md\n deploy  hugo publish the public to web server\n Configuration Lookup Order confit/_default/\n ./config.toml ./config.yaml ./config.json  confit/_default/config.toml\n hugo build destination\npublishDir  Number of items per page in paginated lists\npaginate = 20  taxonomies\n by tag by author\ntag = \u0026quot;tags\u0026quot; author = \u0026quot;authors\u0026quot;    confit/_default/menus.toml\n Navigation Links widget enable/disabl under content/home/ folder Navigation Links widget display order\nweight = 1   config/_default/languages.toml\n多语言显示\nlanguageCode = \u0026quot;en-us\u0026quot; languageCode = \u0026quot;zh-Hans\u0026quot;  Blog set content/post/_index.md\n post view as Card\nview = 3   content/home/posts.md\n Number of recent posts to list.\ncount = 20   theme https://sourcethemes.com/academic/zh/docs/page-builder/#icons\n","date":1550806707,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550806707,"objectID":"a81ccaae6503c87d521581d175a11183","permalink":"https://wubigo.com/post/blog_on_hugo/","publishdate":"2019-02-22T11:38:27+08:00","relpermalink":"/post/blog_on_hugo/","section":"post","summary":"Decide to gave hugo a shot after many years of being jekyll","tags":["BLOG"],"title":"Blog on hugo way","type":"post"},{"authors":null,"categories":[],"content":"大多数重要的应用程序都是在文件系统的基础上构建，而不是在原始磁盘上 构建。例如LEVELDB.\n文件系统在系统崩溃恢复所表现的行为，我们称之为持久化属性，大概分为 两类：原子操作和顺序操作\n[1] All File Systems Are Not Created Equal\n","date":1550271641,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550271641,"objectID":"41206a470d840d80cae7fde430d5834c","permalink":"https://wubigo.com/post/file-system-notes/","publishdate":"2019-02-16T07:00:41+08:00","relpermalink":"/post/file-system-notes/","section":"post","summary":"大多数重要的应用程序都是在文件系统的基础上构建，而不是在原始磁盘上 构建。例如LEVELDB.\n文件系统在系统崩溃恢复所表现的行为，我们称之为持久化属性，大概分为 两类：原子操作和顺序操作\n[1] All File Systems Are Not Created Equal","tags":["KVS","OS"],"title":"文件系统的持久化特性","type":"post"},{"authors":null,"categories":[],"content":" 在4年前加入GLODON的时候，当时我们正在从传统的CS，单体的中心数据库，私有数据中心 向分布式，混合云架构演进。我们现在构建，部署，运营分布式关系/图像/键值数据库， 分布式检索，基于HDFS和对象存储的大数据分析平台。\n在这个过程中，打心里认为学到的最重要的一个简单概念：日志。有时候我们叫它预写日志， 提交日志或事务日志。日志伴随着计算机出现就一直存在，是分布式数据系统和实时应用 架构的核心。\n你不会完全理解数据库，NoSQL，键值存储，复制，paxos共识，hadoop，版本控制等任务 软件系统，除非完全理解日志。然而大部分程序员其实对日志并不熟悉。本贴将带你一步步 了解日志，包括什么是日志，如何有效的使用日志，通过日志来构建数据集成，实时数据处理 等业务系统。\n第一个部分：什么是日志 日志可能是最简单的存储抽象。它是一个只可追加的，完全按照时间顺序记录的数据序列。 新的记录追加到日志存储的末尾。读取的时候从左向右处理。每一条记录条目分配了一个 唯一的日志序列号。\n日志的存储次序定义了时间的概念，因为左边的日志条目始终比右边的日志存储的时间更早。 日志条目唯一序列号可以看做日志条目时间戳。把这个顺序定义为时间咋一看觉得奇怪，但 这个属性很方便的和特定的物理时钟隔离开来。这个属性很快证明是分布式系统所必须的。\n日志条目的内容和格式不是本贴讨论内容要重点关心的。而且我们也不能无限的追加记录， 因为存储空间的限制。我们晚点开会谈到这一点。\n因此，日志和文件，数据表没有根本不同。文件是字节数组，数据表是记录数组，日志只是 一个特定的类型的文件或数据表：日志数组按时间排序。\n到此，你可能奇怪，我们有啥必要讨论如此简单的东西？只可追加的日志序列到底和数据系统 在哪些方面有关联呢？答案是日志有一个特殊的目的：记录什么时间发生了什么事。对分布式 系统来说，在很多方面，这都是核心问题。\n但在我们继续讨论之前，让我先澄清一个让人困惑的事情。每一个程序员应该对另一种日志定义 非常熟悉并经常使用：非结构化的错误日志或跟踪日志，这些日志由应用程序利用syslog或 log4j写到本地文件系统。这类日志我们叫做应用日志。应用日志是我要描述的日志的退化形式。 他俩最大的区别是应用日志主要是用来运维开发人员读取使用，而我描述的数据日志主用来被 程序访问。\n数据库日志 我不知道数据库日志的真正起源，可能就像二分查找一样，因为太过于简单，发明者都没有意识 到这是一个发明。它最早在IBM的System R中使用，用作各种数据结构和索引在崩溃的后同步。 为了保证数据的原子性和持久性，数据库把要修改的记录在提交前先写入日志。日志记录发生了什么 ，是每个数据表或索引表的历史投影。因为日志被立即写入存储盘，被作为系统崩溃发生后进行 系统恢复有效的数据来源。\n慢慢的，日志的使用范围从ACID的实现细节扩展到数据库间的数据复制机制。实践证明，这些 本地数据库数据的变更记录正是要保持远程的数据库副本同步所必需的。Oracle, MySQL和 PostgreSQL都有日志传送协议，在从库上做数据库复制。\n分布式系统日志 日志解决的两个问题：变更顺序和数据分发，在分布式数据系统中更加重要。对数据变更顺序的 达成一致是这些系统设计的核心问题。\n以日志为中心作为分布式系统的解决思路，源于一个简单的观察结果，我称之为状态机复制规则：\n 如果两个相同的，\u0026rdquo;确定性\u0026rdquo;的程序从相同的状态启动，以相同的顺便获取 相同的输入，他俩会产生相同的输出并以相同的状态结束\n \u0026ldquo;确定性\u0026rdquo;是指程序处理不依赖于时间，不会因为任何其他的带外的输入影响其处理结果。那些 输出结果依赖线程的执行顺序，或者API调用*gettimeofday*，或其他不可重复的调用的程序 都是非\u0026rdquo;确定性\u0026rdquo;。\n机器里面的数据是程序的状态的表示，任务处理完后，无论是数据留在才内存，还是在磁盘。\n以相同的顺序产生的相同输入这一点提醒我们：这就是日志上场了。这是一个直觉概念： \u0026ldquo;确定性\u0026rdquo;的程序会从相同的日志产生相同的结果。\n分布式应用看来一个相当直观思路就是:实现一个分布式一致性的日志，作为输入分发到多个 机器上处理相同的任务的程序。日志的目的就是排除输入流产生的所有非确定性输出，保证 每个处理输入流的程序副本保持在相同的状态。\n如果你理解了这一点，关于这个规则就没有多深奥或多复杂啦，它或多或少的等于说：确实性的 处理过程是确定性的。然而，我认为它只是分布式系统设计更通用的设计工具之一。\n以日志为中心的分布式设计的一个亮点，以时间戳为索引的日志成为程序副本的状态的时钟。 你可以用一个数字来描述一个程序副本：该副本处理的最近日志条目的时间戳。时间戳和日志 一起唯一的快照了副本的完整状态。\n不同应用组的人描述日志的使用可能有所不同，数据库组的人经常区分物理日志和逻辑日志， 物理日志记录每行改变的内容，逻辑日志记录导致改变的SQL命令(CRUD)。\n分布式系统文献经常把处理和复制划分为两大类别：状态机模式和主备模式。状态机模式 经常指双活，对输入请求做日志记录，每一个副本处理每一个请求。主备模式和状态机模式 有稍微的差别，主备模式为推举一个主节点，主节点会按顺便处理请求，并把处理结果的 状态作为日志同步到从节点。\n变更日志101： 参考 [1] What every software engineer should know about real-time data\u0026rsquo;s unifying abstraction\n","date":1550195081,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550195081,"objectID":"897a4f0c6d60249acc93428aeab23370","permalink":"https://wubigo.com/post/log-about-real-time-datas-unifying-abstraction/","publishdate":"2019-02-15T09:44:41+08:00","relpermalink":"/post/log-about-real-time-datas-unifying-abstraction/","section":"post","summary":"在4年前加入GLODON的时候，当时我们正在从传统的CS，单体的中心数据库，私有数据中心 向分布式，混合云架构演进。我们现在构建，部署，运营分布式关系/图像/键值数据库， 分布式检索，基于HDFS和对象存储的大数据分析平台。\n在这个过程中，打心里认为学到的最重要的一个简单概念：日志。有时候我们叫它预写日志， 提交日志或事务日志。日志伴随着计算机出现就一直存在，是分布式数据系统和实时应用 架构的核心。\n你不会完全理解数据库，NoSQL，键值存储，复制，paxos共识，hadoop，版本控制等任务 软件系统，除非完全理解日志。然而大部分程序员其实对日志并不熟悉。本贴将带你一步步 了解日志，包括什么是日志，如何有效的使用日志，通过日志来构建数据集成，实时数据处理 等业务系统。\n第一个部分：什么是日志 日志可能是最简单的存储抽象。它是一个只可追加的，完全按照时间顺序记录的数据序列。 新的记录追加到日志存储的末尾。读取的时候从左向右处理。每一条记录条目分配了一个 唯一的日志序列号。\n日志的存储次序定义了时间的概念，因为左边的日志条目始终比右边的日志存储的时间更早。 日志条目唯一序列号可以看做日志条目时间戳。把这个顺序定义为时间咋一看觉得奇怪，但 这个属性很方便的和特定的物理时钟隔离开来。这个属性很快证明是分布式系统所必须的。\n日志条目的内容和格式不是本贴讨论内容要重点关心的。而且我们也不能无限的追加记录， 因为存储空间的限制。我们晚点开会谈到这一点。\n因此，日志和文件，数据表没有根本不同。文件是字节数组，数据表是记录数组，日志只是 一个特定的类型的文件或数据表：日志数组按时间排序。\n到此，你可能奇怪，我们有啥必要讨论如此简单的东西？只可追加的日志序列到底和数据系统 在哪些方面有关联呢？答案是日志有一个特殊的目的：记录什么时间发生了什么事。对分布式 系统来说，在很多方面，这都是核心问题。\n但在我们继续讨论之前，让我先澄清一个让人困惑的事情。每一个程序员应该对另一种日志定义 非常熟悉并经常使用：非结构化的错误日志或跟踪日志，这些日志由应用程序利用syslog或 log4j写到本地文件系统。这类日志我们叫做应用日志。应用日志是我要描述的日志的退化形式。 他俩最大的区别是应用日志主要是用来运维开发人员读取使用，而我描述的数据日志主用来被 程序访问。\n数据库日志 我不知道数据库日志的真正起源，可能就像二分查找一样，因为太过于简单，发明者都没有意识 到这是一个发明。它最早在IBM的System R中使用，用作各种数据结构和索引在崩溃的后同步。 为了保证数据的原子性和持久性，数据库把要修改的记录在提交前先写入日志。日志记录发生了什么 ，是每个数据表或索引表的历史投影。因为日志被立即写入存储盘，被作为系统崩溃发生后进行 系统恢复有效的数据来源。\n慢慢的，日志的使用范围从ACID的实现细节扩展到数据库间的数据复制机制。实践证明，这些 本地数据库数据的变更记录正是要保持远程的数据库副本同步所必需的。Oracle, MySQL和 PostgreSQL都有日志传送协议，在从库上做数据库复制。\n分布式系统日志 日志解决的两个问题：变更顺序和数据分发，在分布式数据系统中更加重要。对数据变更顺序的 达成一致是这些系统设计的核心问题。\n以日志为中心作为分布式系统的解决思路，源于一个简单的观察结果，我称之为状态机复制规则：\n 如果两个相同的，\u0026rdquo;确定性\u0026rdquo;的程序从相同的状态启动，以相同的顺便获取 相同的输入，他俩会产生相同的输出并以相同的状态结束\n \u0026ldquo;确定性\u0026rdquo;是指程序处理不依赖于时间，不会因为任何其他的带外的输入影响其处理结果。那些 输出结果依赖线程的执行顺序，或者API调用*gettimeofday*，或其他不可重复的调用的程序 都是非\u0026rdquo;确定性\u0026rdquo;。\n机器里面的数据是程序的状态的表示，任务处理完后，无论是数据留在才内存，还是在磁盘。\n以相同的顺序产生的相同输入这一点提醒我们：这就是日志上场了。这是一个直觉概念： \u0026ldquo;确定性\u0026rdquo;的程序会从相同的日志产生相同的结果。\n分布式应用看来一个相当直观思路就是:实现一个分布式一致性的日志，作为输入分发到多个 机器上处理相同的任务的程序。日志的目的就是排除输入流产生的所有非确定性输出，保证 每个处理输入流的程序副本保持在相同的状态。\n如果你理解了这一点，关于这个规则就没有多深奥或多复杂啦，它或多或少的等于说：确实性的 处理过程是确定性的。然而，我认为它只是分布式系统设计更通用的设计工具之一。\n以日志为中心的分布式设计的一个亮点，以时间戳为索引的日志成为程序副本的状态的时钟。 你可以用一个数字来描述一个程序副本：该副本处理的最近日志条目的时间戳。时间戳和日志 一起唯一的快照了副本的完整状态。\n不同应用组的人描述日志的使用可能有所不同，数据库组的人经常区分物理日志和逻辑日志， 物理日志记录每行改变的内容，逻辑日志记录导致改变的SQL命令(CRUD)。\n分布式系统文献经常把处理和复制划分为两大类别：状态机模式和主备模式。状态机模式 经常指双活，对输入请求做日志记录，每一个副本处理每一个请求。主备模式和状态机模式 有稍微的差别，主备模式为推举一个主节点，主节点会按顺便处理请求，并把处理结果的 状态作为日志同步到从节点。\n变更日志101： 参考 [1] What every software engineer should know about real-time data\u0026rsquo;s unifying abstraction","tags":["KVS","CACHE","LSM"],"title":"日志：每个程序员都应该了解的实时数据统一抽象","type":"post"},{"authors":null,"categories":[],"content":"https://git-lfs.github.com/\n","date":1549936588,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549936588,"objectID":"2a72f1b479ea4bb4d3d3037de25eb76c","permalink":"https://wubigo.com/post/git-extension-for-versioning-large-files/","publishdate":"2019-02-12T09:56:28+08:00","relpermalink":"/post/git-extension-for-versioning-large-files/","section":"post","summary":"https://git-lfs.github.com/","tags":["GIT"],"title":"git管理大文件","type":"post"},{"authors":null,"categories":["IT"],"content":" version notes\nsome only works on 1.13\nkubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;13\u0026quot;, GitVersion:\u0026quot;v1.13.3\u0026quot;, GitCommit:\u0026quot;721bfa751924da8d1680787490c54b9179b1fed0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2019-02-16T15:29:34Z\u0026quot;, GoVersion:\u0026quot;go1.11.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Starting with Kubernetes 1.12, the K8S.gcr.io/kube-${ARCH}, K8S.gcr.io/etcd and K8S.gcr.io/pause images don’t require an -${ARCH} suffix\n get all Pending pods\nkubectl get pods --field-selector=status.phase=Pending  images list\nkubeadm config images list -v 4 I0217 07:28:13.305268 14495 interface.go:384] Looking for default routes with IPv4 addresses I0217 07:28:13.307275 14495 interface.go:389] Default route transits interface \u0026quot;enp0s3\u0026quot; I0217 07:28:13.308349 14495 interface.go:196] Interface enp0s3 is up I0217 07:28:13.309611 14495 interface.go:244] Interface \u0026quot;enp0s3\u0026quot; has 2 addresses :[192.168.1.9/24 fe80::a00:27ff:fe75:f493/64]. I0217 07:28:13.310328 14495 interface.go:211] Checking addr 192.168.1.9/24. I0217 07:28:13.311219 14495 interface.go:218] IP found 192.168.1.9 I0217 07:28:13.311961 14495 interface.go:250] Found valid IPv4 address 192.168.1.9 for interface \u0026quot;enp0s3\u0026quot;. I0217 07:28:13.312688 14495 interface.go:395] Found active IP 192.168.1.9 I0217 07:28:13.313427 14495 version.go:163] fetching Kubernetes version from URL: https://dl.K8S.io/release/stable-1.txt I0217 07:28:23.320683 14495 version.go:94] could not fetch a Kubernetes version from the internet: unable to get URL \u0026quot;https://dl.K8S.io/release/stable-1.txt\u0026quot;: Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: net/http: request canceled (Client.Timeout exceeded while awaiting headers) I0217 07:28:23.321520 14495 version.go:95] falling back to the local client version: v1.13.3 I0217 07:28:23.330622 14495 feature_gate.go:206] feature gates: \u0026amp;{map[]} K8S.gcr.io/kube-apiserver:v1.13.3 K8S.gcr.io/kube-controller-manager:v1.13.3 K8S.gcr.io/kube-scheduler:v1.13.3 K8S.gcr.io/kube-proxy:v1.13.3 K8S.gcr.io/pause:3.1 K8S.gcr.io/etcd:3.2.24 K8S.gcr.io/coredns:1.2.6  pull images beforehand\nkubeadm config images pull -v 4   init phase kubeadm config print init-defaults \u0026gt;adm.defaults.yaml git diff adm.defaults.yaml -imageRepository: K8S.gcr.io +imageRepository: mirrorgooglecontainers sudo kubeadm init phase preflight --config=./adm.defaults.yaml -v 4  Self-hosting the Kubernetes control plane As of 1.8, you can experimentally create a self-hosted Kubernetes control plane. This means that key components such as the API server, controller manager, and scheduler run as DaemonSet pods configured via the Kubernetes API instead of static pods configured in the kubelet via static files. To create a self-hosted cluster, pass the flag \u0026ndash;feature-gates=SelfHosting=true to kubeadm init.\n  https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/ https://discuss.kubernetes.io/t/question-about-etcd-cluster-with-kubeadm-in-1-11/1228\nkubectl get configmaps --all-namespaces kubectl describe configmaps kubeadm-config -n kube-system kubectl -n kube-system get deployment coredns -o yaml | \\ sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \\ kubectl apply -f - kubectl scale --current-replicas=2 --replicas=1 deployments.apps/nginx1-14 kubectl logs calico-node-4mb5z -n kube-system  ","date":1549856307,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549856307,"objectID":"3b9aedf05dacf02f125f31d5b45ac452","permalink":"https://wubigo.com/post/kubeamd-cheat-sheet/","publishdate":"2019-02-11T11:38:27+08:00","relpermalink":"/post/kubeamd-cheat-sheet/","section":"post","summary":"version notes\nsome only works on 1.13\nkubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;13\u0026quot;, GitVersion:\u0026quot;v1.13.3\u0026quot;, GitCommit:\u0026quot;721bfa751924da8d1680787490c54b9179b1fed0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2019-02-16T15:29:34Z\u0026quot;, GoVersion:\u0026quot;go1.11.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Starting with Kubernetes 1.12, the K8S.gcr.io/kube-${ARCH}, K8S.gcr.io/etcd and K8S.gcr.io/pause images don’t require an -${ARCH} suffix\n get all Pending pods\nkubectl get pods --field-selector=status.phase=Pending  images list\nkubeadm config images list -v 4 I0217 07:28:13.305268 14495 interface.go:384] Looking for default routes with IPv4 addresses I0217 07:28:13.307275 14495 interface.go:389] Default route transits interface \u0026quot;enp0s3\u0026quot; I0217 07:28:13.","tags":["K8S"],"title":"kubeamd cheat sheet","type":"post"},{"authors":null,"categories":null,"content":" track http redirection  http://wubigo.com/post -\u0026gt; http://wubigo.com/post/ -\u0026gt; https://wubigo.com/post/\ncurl -IL http://wubigo.com/post\n HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post Via: 1.1 varnish X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Content-Length: 0 HTTP/1.1 301 Moved Permanently Strict-Transport-Security: max-age=31556952 Location: http://wubigo.com/post/ Access-Control-Allow-Origin: * X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post/ X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Access-Control-Allow-Origin: * Cache-Control: max-age=600 X-Cache: HIT X-Cache-Hits: 1  main goal  HTTP/2\u0026rsquo;s multiplexed connections, allowing multiple streams of data to reach all the endpoints independently. In contrast, HTTP hosted on Transmission Control Protocol (TCP) can be blocked if any of the multiplexed data streams has an error.\n reduced connection and transport latency, and bandwidth estimation in each direction to avoid congestion. It also moves control of the congestion avoidance algorithms into the application space at both endpoints, rather than the kernel space, which it is claimed will allow these algorithms to improve more rapidly. Additionally, the protocol can be extended with forward error correction (FEC) to further improve performance when errors are expected\n  [1]. revive Gopherspace\n[2]. gopher client\n","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"ef107e3fcd63362281edd05426733f5a","permalink":"https://wubigo.com/post/2019-02-01-http3notes/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/post/2019-02-01-http3notes/","section":"post","summary":"track http redirection  http://wubigo.com/post -\u0026gt; http://wubigo.com/post/ -\u0026gt; https://wubigo.com/post/\ncurl -IL http://wubigo.com/post\n HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post Via: 1.1 varnish X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Content-Length: 0 HTTP/1.1 301 Moved Permanently Strict-Transport-Security: max-age=31556952 Location: http://wubigo.com/post/ Access-Control-Allow-Origin: * X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post/ X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Access-Control-Allow-Origin: * Cache-Control: max-age=600 X-Cache: HIT X-Cache-Hits: 1  main goal  HTTP/2\u0026rsquo;s multiplexed connections, allowing multiple streams of data to reach all the endpoints independently.","tags":["HTTP","WEB"],"title":"HTTP/3","type":"post"},{"authors":null,"categories":[],"content":" 安装mc https://dl.min.io/client/mc/release/windows-amd64/mc.exe\nmc config host add b2 http://192.168.1.3:9000 B2_keyID B2_applicationKey   本地文件同步到b2\nmc cp -r . b2/wubigo/   安装S3CMD https://github.com/s3tools/s3cmd/releases/download/v2.0.2/s3cmd-2.0.2.tar.gz sudo python setup.py install  ~/.s3cfg\n# Setup endpoint host_base = http://192.168.1.3:9000 host_bucket = http://192.168.1.3:9000 bucket_location = us-east-1 use_https = True # Setup access keys access_key = Q3AM3UQ867SPQQA43P2F secret_key = zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG # Enable S3 v4 signature APIs signature_v2 = False   同步本地文件到B2\ns3cmd sync . s3://wubigo/   总结 在不进行任何优化的情况下，s3cmd比mc传输速度快好几倍\n","date":1547790914,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547790914,"objectID":"1b909cbd28da112b3e0fa06f88e02c22","permalink":"https://wubigo.com/post/minio-client/","publishdate":"2019-01-18T13:55:14+08:00","relpermalink":"/post/minio-client/","section":"post","summary":"安装mc https://dl.min.io/client/mc/release/windows-amd64/mc.exe\nmc config host add b2 http://192.168.1.3:9000 B2_keyID B2_applicationKey   本地文件同步到b2\nmc cp -r . b2/wubigo/   安装S3CMD https://github.com/s3tools/s3cmd/releases/download/v2.0.2/s3cmd-2.0.2.tar.gz sudo python setup.py install  ~/.s3cfg\n# Setup endpoint host_base = http://192.168.1.3:9000 host_bucket = http://192.168.1.3:9000 bucket_location = us-east-1 use_https = True # Setup access keys access_key = Q3AM3UQ867SPQQA43P2F secret_key = zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG # Enable S3 v4 signature APIs signature_v2 = False   同步本地文件到B2\ns3cmd sync . s3://wubigo/   总结 在不进行任何优化的情况下，s3cmd比mc传输速度快好几倍","tags":["MINIO","SDS","SDK"],"title":"Minio Client","type":"post"},{"authors":null,"categories":[],"content":" 检查域名使用的名字服务器 https://lookup.icann.org/\n更改名字服务器 更改名字服务器需通过域名注册服务商进行更改\n","date":1547770441,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547770441,"objectID":"67e35fef2be47e6a4031472147c899f4","permalink":"https://wubigo.com/post/domain-nameserver-notes/","publishdate":"2019-01-18T08:14:01+08:00","relpermalink":"/post/domain-nameserver-notes/","section":"post","summary":"检查域名使用的名字服务器 https://lookup.icann.org/\n更改名字服务器 更改名字服务器需通过域名注册服务商进行更改","tags":["DNS","WEB","CDN"],"title":"域名名字服务器配置","type":"post"},{"authors":null,"categories":[],"content":" 端到端VXLAN(unicast) ip a ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ remote 10.12.0.172 \\ local 10.12.2.95 \\ dev eth0 ip -d link show dev vxlan0 ip addr add 192.168.8.101/24 dev vxlan0 ip link set vxlan0 up ip r default via 10.12.0.1 dev eth0 10.12.0.0/21 dev eth0 proto kernel scope link src 10.12.2.95 192.168.8.0/24 dev vxlan0 proto kernel scope link src 192.168.8.101 bridge fdb | grep vxlan0 ip neigh  多播vxlan(multicast) ","date":1546486277,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546486277,"objectID":"e8cef2ac0dd81dbaa0771dc94b8eeec4","permalink":"https://wubigo.com/post/vxlan-on-linux/","publishdate":"2019-01-03T11:31:17+08:00","relpermalink":"/post/vxlan-on-linux/","section":"post","summary":" 端到端VXLAN(unicast) ip a ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ remote 10.12.0.172 \\ local 10.12.2.95 \\ dev eth0 ip -d link show dev vxlan0 ip addr add 192.168.8.101/24 dev vxlan0 ip link set vxlan0 up ip r default via 10.12.0.1 dev eth0 10.12.0.0/21 dev eth0 proto kernel scope link src 10.12.2.95 192.168.8.0/24 dev vxlan0 proto kernel scope link src 192.168.8.101 bridge fdb | grep vxlan0 ip neigh  多播vxlan(multicast) ","tags":["NETWORK","VXLAN","NFV","LINUX"],"title":"Vxlan on Linux","type":"post"},{"authors":null,"categories":[],"content":"git clone https://git.zx2c4.com/wireguard-go  ","date":1546469635,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546469635,"objectID":"bcec419ec00e0ea1eb44104960999c40","permalink":"https://wubigo.com/post/wireguard-dev-guide/","publishdate":"2019-01-03T06:53:55+08:00","relpermalink":"/post/wireguard-dev-guide/","section":"post","summary":"git clone https://git.zx2c4.com/wireguard-go  ","tags":["VPN","NFV","NETWORK"],"title":"Wireguard开发参考","type":"post"},{"authors":null,"categories":[],"content":" 创建函数  分配角色\nzip function.zip index.js aws lambda create-function --function-name sns-db-function \\ --zip-file fileb://function.zip --handler index.handler --runtime nodejs12.x \\ --role arn:aws:iam::465691908928:role/fn-case-role   ","date":1546238672,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546238672,"objectID":"90ceb8507e96359bd66db7f1aaf425f5","permalink":"https://wubigo.com/post/aws-lambda-dynamodb/","publishdate":"2018-12-31T14:44:32+08:00","relpermalink":"/post/aws-lambda-dynamodb/","section":"post","summary":" 创建函数  分配角色\nzip function.zip index.js aws lambda create-function --function-name sns-db-function \\ --zip-file fileb://function.zip --handler index.handler --runtime nodejs12.x \\ --role arn:aws:iam::465691908928:role/fn-case-role   ","tags":["AWS","LAMBDA","SNS","SLS","SERVERLESS","NOSQL"],"title":"Lambda订阅SNS通知(下)","type":"post"},{"authors":null,"categories":[],"content":" 发布测试消息到SNS git clone https://github.com/wubigo/node-fn/blob/master/fn-case/sns_publishtotopic.js node sns_publishtotopic.js Message MESSAGE_TEXT send sent to the topic arn:aws:sns:ap-northeast-1:465691908928:func-topic MessageID is 8b5c90f2-0c74-5985-8a34-c676c0370f73  根据MessageID查看函数执行结果 The $ in [$LATEST] needs to be escaped\u0026hellip;[\\$LATEST].\naws logs describe-log-groups --query logGroups[*].logGroupName aws logs describe-log-streams --log-group-name '/aws/lambda/my-function' --query logStreams[*].logStreamName aws logs get-log-events --log-group-name '/aws/lambda/my-function' --log-stream-name '2019/12/31/[$LATEST]7467497f9cdb4078a876ab889797793c' { \u0026quot;ingestionTime\u0026quot;: 1577764111252, \u0026quot;timestamp\u0026quot;: 1577764096184, \u0026quot;message\u0026quot;: \u0026quot;2019-12-31T03:48:16.183Z\\tc01c9f5e-6c33-40a1-a6d9-c11ab248ab48\\tINFO\\tEVENT\\n{\\n \\\u0026quot;Records\\\u0026quot;: [\\n {\\n \\\u0026quot;EventSource\\\u0026quot;: \\\u0026quot;aws:sns\\\u0026quot;,\\n \\\u0026quot;EventVersion\\\u0026quot;: \\\u0026quot;1.0\\\u0026quot;,\\n \\\u0026quot;EventSubscriptionArn\\\u0026quot;: \\\u0026quot;arn:aws:sns:ap-northeast-1:465691908928:func-topic:2e0e0d95-f1c8-47bd-90ff-40ca4129794b\\\u0026quot;,\\n \\\u0026quot;Sns\\\u0026quot;: {\\n \\\u0026quot;Type\\\u0026quot;: \\\u0026quot;Notification\\\u0026quot;,\\n \\\u0026quot;MessageId\\\u0026quot;: \\\u0026quot;5f80d26e-bdeb-579f-bc81-84ea7ad4e2ae\\\u0026quot;,\\n \\\u0026quot;TopicArn\\\u0026quot;: \\\u0026quot;arn:aws:sns:ap-northeast-1:465691908928:func-topic\\\u0026quot;,\\n \\\u0026quot;Subject\\\u0026quot;: null,\\n \\\u0026quot;Message\\\u0026quot;: \\\u0026quot;MESSAGE_TEXT\\\u0026quot;,\\n \\\u0026quot;Timestamp\\\u0026quot;: \\\u0026quot;2019-12-31T03:48:15.624Z\\\u0026quot;,\\n \\\u0026quot;SignatureVersion\\\u0026quot;: \\\u0026quot;1\\\u0026quot;,\\n \\\u0026quot;Signature\\\u0026quot;: \\\u0026quot;UYKR86o1NPa3xh8LQ6gM8eDxm6l/TY3YHU9Ez5m8Ve8eCHa5e4Xp6+PT/01mjyXGWNHjhcPVYg9esiRLzljcYm26VDODhlTe8q9h20h43azYbMM8CjtK+GhuxDPFoSG/N2FuwnKRZK9JWw+QbG2OD09vy6k5g7EX2BcEGR+A0LGQ0EXvVm9j3fvC2P2yiLCRwZPulsgqMEJeR9rZiBfUnsnhY+oCnTk7OcBhkMQ9LQctFbFXdrG7BQOkqJgN0pJa9f8kwF48lG6eCAZinxNRQ7DoR0pg608XWjbMZF6uu1ttmU1iPNjYwnH0B9HIgK9E0Rs0s819jKqCaHqXW5KjUg==\\\u0026quot;,\\n \\\u0026quot;SigningCertUrl\\\u0026quot;: \\\u0026quot;https://sns.ap-northeast-1.amazonaws.com/SimpleNotificationService-6aad65c2f9911b05cd53efda11f913f9.pem\\\u0026quot;,\\n \\\u0026quot;UnsubscribeUrl\\\u0026quot;: \\\u0026quot;https://sns.ap-northeast-1.amazonaws.com/?Action=Unsubscribe\u0026amp;SubscriptionArn=arn:aws:sns:ap-northeast-1:465691908928:func-topic:2e0e0d95-f1c8-47bd-90ff-40ca4129794b\\\u0026quot;,\\n \\\u0026quot;MessageAttributes\\\u0026quot;: {}\\n }\\n }\\n ]\\n}\u0026quot; }  ","date":1546225477,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546225477,"objectID":"d75a83ef8d5522e5304cd2638b9d605d","permalink":"https://wubigo.com/post/aws-lambda-as-a-subscriber-of-sns/","publishdate":"2018-12-31T11:04:37+08:00","relpermalink":"/post/aws-lambda-as-a-subscriber-of-sns/","section":"post","summary":"发布测试消息到SNS git clone https://github.com/wubigo/node-fn/blob/master/fn-case/sns_publishtotopic.js node sns_publishtotopic.js Message MESSAGE_TEXT send sent to the topic arn:aws:sns:ap-northeast-1:465691908928:func-topic MessageID is 8b5c90f2-0c74-5985-8a34-c676c0370f73  根据MessageID查看函数执行结果 The $ in [$LATEST] needs to be escaped\u0026hellip;[\\$LATEST].\naws logs describe-log-groups --query logGroups[*].logGroupName aws logs describe-log-streams --log-group-name '/aws/lambda/my-function' --query logStreams[*].logStreamName aws logs get-log-events --log-group-name '/aws/lambda/my-function' --log-stream-name '2019/12/31/[$LATEST]7467497f9cdb4078a876ab889797793c' { \u0026quot;ingestionTime\u0026quot;: 1577764111252, \u0026quot;timestamp\u0026quot;: 1577764096184, \u0026quot;message\u0026quot;: \u0026quot;2019-12-31T03:48:16.183Z\\tc01c9f5e-6c33-40a1-a6d9-c11ab248ab48\\tINFO\\tEVENT\\n{\\n \\\u0026quot;Records\\\u0026quot;: [\\n {\\n \\\u0026quot;EventSource\\\u0026quot;: \\\u0026quot;aws:sns\\\u0026quot;,\\n \\\u0026quot;EventVersion\\\u0026quot;: \\\u0026quot;1.0\\\u0026quot;,\\n \\\u0026quot;EventSubscriptionArn\\\u0026quot;: \\\u0026quot;arn:aws:sns:ap-northeast-1:465691908928:func-topic:2e0e0d95-f1c8-47bd-90ff-40ca4129794b\\\u0026quot;,\\n \\\u0026quot;Sns\\\u0026quot;: {\\n \\\u0026quot;Type\\\u0026quot;: \\\u0026quot;Notification\\\u0026quot;,\\n \\\u0026quot;MessageId\\\u0026quot;: \\\u0026quot;5f80d26e-bdeb-579f-bc81-84ea7ad4e2ae\\\u0026quot;,\\n \\\u0026quot;TopicArn\\\u0026quot;: \\\u0026quot;arn:aws:sns:ap-northeast-1:465691908928:func-topic\\\u0026quot;,\\n \\\u0026quot;Subject\\\u0026quot;: null,\\n \\\u0026quot;Message\\\u0026quot;: \\\u0026quot;MESSAGE_TEXT\\\u0026quot;,\\n \\\u0026quot;Timestamp\\\u0026quot;: \\\u0026quot;2019-12-31T03:48:15.","tags":["AWS","LAMBDA","SNS","SLS","SERVERLESS"],"title":"Lambda订阅SNS通知(上)","type":"post"},{"authors":null,"categories":[],"content":"https://medium.com/people-ai-engineering/building-a-data-lake-in-aws-9c1fb3876e23\nhttps://towardsdatascience.com/building-a-data-pipeline-from-scratch-on-aws-35f139420ebc\n","date":1545783637,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545783637,"objectID":"cd13d4456c2c6d7e5887404751af8f25","permalink":"https://wubigo.com/post/aws-s3-data-lake/","publishdate":"2018-12-26T08:20:37+08:00","relpermalink":"/post/aws-s3-data-lake/","section":"post","summary":"https://medium.com/people-ai-engineering/building-a-data-lake-in-aws-9c1fb3876e23\nhttps://towardsdatascience.com/building-a-data-pipeline-from-scratch-on-aws-35f139420ebc","tags":["AWS","S3","DATAlAKE"],"title":"Aws S3 Data Lake","type":"post"},{"authors":null,"categories":[],"content":" restore snapshot shell snapshot.sh\n!#/bin/bash wget https://nodejs.org/dist/v12.13.1/node-v12.13.1-linux-x64.tar.xz tar xvf node-v12.13.1-linux-x64.tar.xz export PATH=/home/ubuntu/node-v12.13.1-linux-x64/bin:$PATH wget https://manning-content.s3.amazonaws.com/download/0/ddbbd36-251d-42ef-9934-55e5a881a336/FinalSourceCode.zip sudo apt update sudo apt install unzip unzip FinalSourceCode.zip mv Final\\ Source\\ Code/ sls sudo apt install python-pip pip install awscli which aws_completer cp ~/.bashrc ~/.bashrc_orig tee -a ~/.bashrc \u0026lt;\u0026lt;-'EOF' complete -C '/home/ubuntu/.local/bin/aws_completer' aws export PATH=/home/ubuntu/node-v12.13.1-linux-x64/bin:$PATH EOF  aws configure npm install claudia -g claudia -v 5.11.0 cd chapter-03 npm install claudia create \\ --region ap-northeast-1 \\ --api-module api packaging files npm install -q --no-audit --production npm WARN pizza-api@1.0.0 No repository field. saving configuration { \u0026quot;lambda\u0026quot;: { \u0026quot;role\u0026quot;: \u0026quot;pizza-api-executor\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;pizza-api\u0026quot;, \u0026quot;region\u0026quot;: \u0026quot;ap-northeast-1\u0026quot; }, \u0026quot;api\u0026quot;: { \u0026quot;id\u0026quot;: \u0026quot;4auh8wzh16\u0026quot;, \u0026quot;module\u0026quot;: \u0026quot;api\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://4auh8wzh16.execute-api.ap-northeast-1.amazonaws.com/latest\u0026quot; } }  IAM policy to that allows Lambda function to communicate with database  aws iam put-role-policy \\ \u0026gt; --role-name pizza-api-executor \\ \u0026gt; --policy-name PizzaApiDynamoDB \\ \u0026gt; --policy-document file://./roles/dynamodb.json  package.json\n\u0026quot;dependencies\u0026quot;: { \u0026quot;claudia-api-builder\u0026quot;: \u0026quot;^4.1.2\u0026quot; },  aws apigateway get-resources --rest-api-id \u0026quot;4auh8wzh16\u0026quot; { \u0026quot;path\u0026quot;: \u0026quot;/pizzas/{id}\u0026quot;, \u0026quot;resourceMethods\u0026quot;: { \u0026quot;OPTIONS\u0026quot;: {}, \u0026quot;GET\u0026quot;: {} }, \u0026quot;id\u0026quot;: \u0026quot;i9rknj\u0026quot;, \u0026quot;pathPart\u0026quot;: \u0026quot;{id}\u0026quot;, \u0026quot;parentId\u0026quot;: \u0026quot;el67kl\u0026quot; }  claudia update updating REST API apigateway.setAcceptHeader { \u0026quot;FunctionName\u0026quot;: \u0026quot;pizza-api\u0026quot;, \u0026quot;FunctionArn\u0026quot;: \u0026quot;arn:aws:lambda:ap-northeast-1:465691908928:function:pizza-api:2\u0026quot;, \u0026quot;Runtime\u0026quot;: \u0026quot;nodejs12.x\u0026quot;, \u0026quot;Role\u0026quot;: \u0026quot;arn:aws:iam::465691908928:role/pizza-api-executor\u0026quot;, \u0026quot;Handler\u0026quot;: \u0026quot;api.proxyRouter\u0026quot;, \u0026quot;CodeSize\u0026quot;: 17910, \u0026quot;Description\u0026quot;: \u0026quot;A pizza API, an example app from \\\u0026quot;Serverless applications with Claudia.js\\\u0026quot;\u0026quot;, \u0026quot;Timeout\u0026quot;: 3, \u0026quot;MemorySize\u0026quot;: 128, \u0026quot;LastModified\u0026quot;: \u0026quot;2019-12-16T02:01:14.766+0000\u0026quot;, \u0026quot;CodeSha256\u0026quot;: \u0026quot;RDF1eXIMV2PKlTQZt9uUSayhdREMECZzTQaP92WWCqg=\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;KMSKeyArn\u0026quot;: null, \u0026quot;TracingConfig\u0026quot;: { \u0026quot;Mode\u0026quot;: \u0026quot;PassThrough\u0026quot; }, \u0026quot;MasterArn\u0026quot;: null, \u0026quot;RevisionId\u0026quot;: \u0026quot;d0e645d2-86e5-44b2-b20c-f95526be2094\u0026quot;, \u0026quot;State\u0026quot;: \u0026quot;Active\u0026quot;, \u0026quot;StateReason\u0026quot;: null, \u0026quot;StateReasonCode\u0026quot;: null, \u0026quot;LastUpdateStatus\u0026quot;: \u0026quot;Successful\u0026quot;, \u0026quot;LastUpdateStatusReason\u0026quot;: null, \u0026quot;LastUpdateStatusReasonCode\u0026quot;: null, \u0026quot;url\u0026quot;: \u0026quot;https://4auh8wzh16.execute-api.ap-northeast-1.amazonaws.com/latest\u0026quot; }  ","date":1544885679,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544885679,"objectID":"5ffc2bf1744a2fb7e68c8113e32d364b","permalink":"https://wubigo.com/post/serverless-nodejs/","publishdate":"2018-12-15T22:54:39+08:00","relpermalink":"/post/serverless-nodejs/","section":"post","summary":"restore snapshot shell snapshot.sh\n!#/bin/bash wget https://nodejs.org/dist/v12.13.1/node-v12.13.1-linux-x64.tar.xz tar xvf node-v12.13.1-linux-x64.tar.xz export PATH=/home/ubuntu/node-v12.13.1-linux-x64/bin:$PATH wget https://manning-content.s3.amazonaws.com/download/0/ddbbd36-251d-42ef-9934-55e5a881a336/FinalSourceCode.zip sudo apt update sudo apt install unzip unzip FinalSourceCode.zip mv Final\\ Source\\ Code/ sls sudo apt install python-pip pip install awscli which aws_completer cp ~/.bashrc ~/.bashrc_orig tee -a ~/.bashrc \u0026lt;\u0026lt;-'EOF' complete -C '/home/ubuntu/.local/bin/aws_completer' aws export PATH=/home/ubuntu/node-v12.13.1-linux-x64/bin:$PATH EOF  aws configure npm install claudia -g claudia -v 5.11.0 cd chapter-03 npm install claudia create \\ --region ap-northeast-1 \\ --api-module api packaging files npm install -q --no-audit --production npm WARN pizza-api@1.","tags":["AWS","SLS","CLOUD","NODE","SERVERLESS"],"title":"函数计算Nodejs实例","type":"post"},{"authors":null,"categories":[],"content":" Debugging the Build Process Gatsby’s build and develop steps run as a Node.js application\nwhich you can debug using standard tools for Node.js applications.\nDebugging with Node.js’ built-in console console.log(args)  VS Code Debugger (Auto-Config)  Preferences: Type node debug into the search bar. Make sure the Auto Attach option is set to on.\n launch.json\n  launch.json\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;node\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Gatsby develop\u0026quot;, \u0026quot;skipFiles\u0026quot;: [ \u0026quot;\u0026lt;node_internals\u0026gt;/**\u0026quot; ], \u0026quot;program\u0026quot;: \u0026quot;D:/Downloads/node-v12.13.1-win-x64/node_modules/gatsby/dist/bin/gatsby\u0026quot;, \u0026quot;args\u0026quot;: [\u0026quot;develop\u0026quot;], \u0026quot;stopOnEntry\u0026quot;: false, \u0026quot;runtimeArgs\u0026quot;: [\u0026quot;--nolazy\u0026quot;], \u0026quot;sourceMaps\u0026quot;: false } ] }  After putting a breakpoint in gatsby-node.js and\nusing the Start debugging command from VS Code you can see the final result\n","date":1544672714,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544672714,"objectID":"825511b8380d188aff010bb8d4e2fd8e","permalink":"https://wubigo.com/post/gatsby-debug/","publishdate":"2018-12-13T11:45:14+08:00","relpermalink":"/post/gatsby-debug/","section":"post","summary":"Debugging the Build Process Gatsby’s build and develop steps run as a Node.js application\nwhich you can debug using standard tools for Node.js applications.\nDebugging with Node.js’ built-in console console.log(args)  VS Code Debugger (Auto-Config)  Preferences: Type node debug into the search bar. Make sure the Auto Attach option is set to on.\n launch.json\n  launch.json\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes.","tags":["NODEJS","NODE","REACT"],"title":"Gatsby Debug","type":"post"},{"authors":null,"categories":[],"content":" Client Side Rendering(CSR) Rendering an app in a browser, generally using the DOM\nThe initial HTML rendered by the server is a placeholder and\nthe entire user interface and data rendered in the browser\nonce all your scripts load.\nPROS  Rich site interactions Fast rendering after the initial load Partial real-time updates Cheaper to host \u0026amp; scale  CONS  SEO and index issues Mostly initial bundle.js load duration Performance issues on old mobile devices/slow networks Social Media crawlers and sharing problems (SMO)  Server Side Rendering(SSR) Server rendering generates the full HTML for a page\non the server in response to navigation\nPROS  Consistent SEO Performance, initial page load Works well with Social Media crawlers and platforms (SMO)  CONS  Frequent requests Slow page rendering (TTFB — Time to first byte) Complex architecture (For universal approach)  ","date":1544663666,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544663666,"objectID":"88e883cc53d6fb37f06200f4abf2c454","permalink":"https://wubigo.com/post/server-side-rendering/","publishdate":"2018-12-13T09:14:26+08:00","relpermalink":"/post/server-side-rendering/","section":"post","summary":"Client Side Rendering(CSR) Rendering an app in a browser, generally using the DOM\nThe initial HTML rendered by the server is a placeholder and\nthe entire user interface and data rendered in the browser\nonce all your scripts load.\nPROS  Rich site interactions Fast rendering after the initial load Partial real-time updates Cheaper to host \u0026amp; scale  CONS  SEO and index issues Mostly initial bundle.js load duration Performance issues on old mobile devices/slow networks Social Media crawlers and sharing problems (SMO)  Server Side Rendering(SSR) Server rendering generates the full HTML for a page","tags":[],"title":"Server Side Rendering","type":"post"},{"authors":null,"categories":[],"content":" 安装两种方式 从最新源代码发布版安装  git设置代理\n[user] email = hi@wubigo.com name = bigo [http] proxy = http://127.0.0.1:49210 sslverify = false  系统代理\nset HTTP_PROXY=http://127.0.0.1:49210/ set HTTPS_PROXY=http://127.0.0.1:49210/  安装\ngo get github.com/minio/minio   MAKE mkdir -p $GOPATH/src/github.com/minio cd $GOPATH/src/github.com/minio git clone https://github.com/minio/minio.git cd minio/ git checkout RELEASE.2020-01-03T19-12-21Z make -n test go install -v mkdir -p /home/bigo/go/bin which golint 1\u0026gt;/dev/null || (echo \u0026quot;Installing golint\u0026quot; \u0026amp;\u0026amp; GO111MODULE=off go get -u golang.org/x/lint/golint) which staticcheck 1\u0026gt;/dev/null || (echo \u0026quot;Installing staticcheck\u0026quot; \u0026amp;\u0026amp; wget --quiet https://github.com/dominikh/go-tools/releases/download/2019.2.3/staticcheck_linux_amd64.tar.gz \u0026amp;\u0026amp; tar xf staticcheck_linux_amd64.tar.gz \u0026amp;\u0026amp; mv staticcheck/staticcheck /home/bigo/go/bin/staticcheck \u0026amp;\u0026amp; chmod +x /home/bigo/go/bin/staticcheck \u0026amp;\u0026amp; rm -f staticcheck_linux_amd64.tar.gz \u0026amp;\u0026amp; rm -rf staticcheck) which misspell 1\u0026gt;/dev/null || (echo \u0026quot;Installing misspell\u0026quot; \u0026amp;\u0026amp; GO111MODULE=off go get -u github.com/client9/misspell/cmd/misspell) echo \u0026quot;Running vet check\u0026quot; GO111MODULE=on go vet github.com/minio/minio/... echo \u0026quot;Running fmt check\u0026quot; GO111MODULE=on gofmt -d cmd/ GO111MODULE=on gofmt -d pkg/ echo \u0026quot;Running lint check\u0026quot; GO111MODULE=on /home/bigo/go/bin/golint -set_exit_status github.com/minio/minio/cmd/... GO111MODULE=on /home/bigo/go/bin/golint -set_exit_status github.com/minio/minio/pkg/... echo \u0026quot;Running staticcheck check\u0026quot; GO111MODULE=on /home/bigo/go/bin/staticcheck github.com/minio/minio/cmd/... GO111MODULE=on /home/bigo/go/bin/staticcheck github.com/minio/minio/pkg/... echo \u0026quot;Running spelling check\u0026quot; GO111MODULE=on /home/bigo/go/bin/misspell -locale US -error `find cmd/` GO111MODULE=on /home/bigo/go/bin/misspell -locale US -error `find pkg/` GO111MODULE=on /home/bigo/go/bin/misspell -locale US -error `find docs/` GO111MODULE=on /home/bigo/go/bin/misspell -locale US -error `find buildscripts/` GO111MODULE=on /home/bigo/go/bin/misspell -locale US -error `find dockerscripts/` echo \u0026quot;Checking dependencies\u0026quot; (env bash /home/bigo/go/src/github.com/minio/minio/buildscripts/checkdeps.sh) echo \u0026quot;Building minio binary to './minio'\u0026quot; GO111MODULE=on CGO_ENABLED=0 go build -tags kqueue --ldflags '-s -w -X github.com/minio/minio/cmd.Version=2020-01-11T22:29:24Z -X github.com/minio/minio/cmd.ReleaseTag=DEVELOPMENT.2020-01-11T22-29-24Z -X github.com/minio/minio/cmd.CommitID=b00cda8ad49ed0defa9df5e7230f8b536b8ccb17 -X github.com/minio/minio/cmd.ShortCommitID=b00cda8ad49e -X github.com/minio/minio/cmd.GOPATH=/home/bigo/go -X github.com/minio/minio/cmd.GOROOT=' -o /home/bigo/go/src/github.com/minio/minio/minio 1\u0026gt;/dev/null echo \u0026quot;Running unit tests\u0026quot; GO111MODULE=on CGO_ENABLED=0 go test -tags kqueue ./... 1\u0026gt;/dev/null  ","date":1544514362,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544514362,"objectID":"b1c62c899ade9d4d7aa16e2b06bfd789","permalink":"https://wubigo.com/post/minio-install-from-source/","publishdate":"2018-12-11T15:46:02+08:00","relpermalink":"/post/minio-install-from-source/","section":"post","summary":"安装两种方式 从最新源代码发布版安装  git设置代理\n[user] email = hi@wubigo.com name = bigo [http] proxy = http://127.0.0.1:49210 sslverify = false  系统代理\nset HTTP_PROXY=http://127.0.0.1:49210/ set HTTPS_PROXY=http://127.0.0.1:49210/  安装\ngo get github.com/minio/minio   MAKE mkdir -p $GOPATH/src/github.com/minio cd $GOPATH/src/github.com/minio git clone https://github.com/minio/minio.git cd minio/ git checkout RELEASE.2020-01-03T19-12-21Z make -n test go install -v mkdir -p /home/bigo/go/bin which golint 1\u0026gt;/dev/null || (echo \u0026quot;Installing golint\u0026quot; \u0026amp;\u0026amp; GO111MODULE=off go get -u golang.org/x/lint/golint) which staticcheck 1\u0026gt;/dev/null || (echo \u0026quot;Installing staticcheck\u0026quot; \u0026amp;\u0026amp; wget --quiet https://github.","tags":["MINIO","STORAGE","SDS","GO"],"title":"Minio源代码安装","type":"post"},{"authors":null,"categories":[],"content":" JavaScript Arrow Functions https://zendev.com/2018/10/01/javascript-arrow-functions-how-why-when.html\n","date":1543896709,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543896709,"objectID":"f581293e8ce222e32aebe81a5e21c3ea","permalink":"https://wubigo.com/post/javascript-arrow-functions/","publishdate":"2018-12-04T12:11:49+08:00","relpermalink":"/post/javascript-arrow-functions/","section":"post","summary":"JavaScript Arrow Functions https://zendev.com/2018/10/01/javascript-arrow-functions-how-why-when.html","tags":["FUNCTION","FP","JS"],"title":"JavaScript Arrow Functions","type":"post"},{"authors":null,"categories":[],"content":" 微服务安全要点  通信链路加密 灵活的服务访问控制，包括细粒度访问策略 访问日志审计 服务提供方可替代性(batteries included)和可集成性  基本概念  安全标识  在K8S，安全标识(service account)代表一个用户，一个服务或一组服务。\n 安全命名  安全命名定义可运行服务的安全标识\n微服务认证  传输层认证 终端用户认证  每一个终端请求通过JWT(JSON Web Token)校验, 支持Auth0, Firebase。\nhttps://medium.facilelogin.com/securing-microservices-with-oauth-2-0-jwt-and-xacml-d03770a9a838\n","date":1543622508,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622508,"objectID":"58d61e8fb20ba75d82e752e80e209eda","permalink":"https://wubigo.com/post/microservice-security-aaa/","publishdate":"2018-12-01T08:01:48+08:00","relpermalink":"/post/microservice-security-aaa/","section":"post","summary":"微服务安全要点  通信链路加密 灵活的服务访问控制，包括细粒度访问策略 访问日志审计 服务提供方可替代性(batteries included)和可集成性  基本概念  安全标识  在K8S，安全标识(service account)代表一个用户，一个服务或一组服务。\n 安全命名  安全命名定义可运行服务的安全标识\n微服务认证  传输层认证 终端用户认证  每一个终端请求通过JWT(JSON Web Token)校验, 支持Auth0, Firebase。\nhttps://medium.facilelogin.com/securing-microservices-with-oauth-2-0-jwt-and-xacml-d03770a9a838","tags":["K8S","SECURITY"],"title":"微服务安全：认证，授权和审计(AAA)","type":"post"},{"authors":null,"categories":[],"content":" AWS leverages a standard JSON Identity and Access Management (IAM)\npolicy document format across many services to control authorization\nto resources and API actions\nterraform https://www.terraform.io/docs/providers/aws/r/iam_role_policy.html\nresource \u0026quot;aws_iam_role_policy\u0026quot; \u0026quot;s3_policy\u0026quot; { name = \u0026quot;s3_policy\u0026quot; role = \u0026quot;${aws_iam_role.lambda_s3_role.id}\u0026quot; policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;ListObjectsInBucket\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [\u0026quot;s3:ListBucket\u0026quot;], \u0026quot;Resource\u0026quot;: [\u0026quot;arn:aws:s3:::bucket-name\u0026quot;] }, { \u0026quot;Sid\u0026quot;: \u0026quot;AllObjectActions\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:*Object\u0026quot;, \u0026quot;Resource\u0026quot;: [\u0026quot;arn:aws:s3:::bucket-name/*\u0026quot;] } ] } EOF } resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;lambda_s3_role\u0026quot; { name = \u0026quot;lambda_s3_role\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Sid\u0026quot;: \u0026quot;\u0026quot; } ] } EOF }  ","date":1543593247,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543593247,"objectID":"0acd972780f73573e42c0a6fbe945911","permalink":"https://wubigo.com/post/aws-iam-policy/","publishdate":"2018-11-30T23:54:07+08:00","relpermalink":"/post/aws-iam-policy/","section":"post","summary":"AWS leverages a standard JSON Identity and Access Management (IAM)\npolicy document format across many services to control authorization\nto resources and API actions\nterraform https://www.terraform.io/docs/providers/aws/r/iam_role_policy.html\nresource \u0026quot;aws_iam_role_policy\u0026quot; \u0026quot;s3_policy\u0026quot; { name = \u0026quot;s3_policy\u0026quot; role = \u0026quot;${aws_iam_role.lambda_s3_role.id}\u0026quot; policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;ListObjectsInBucket\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [\u0026quot;s3:ListBucket\u0026quot;], \u0026quot;Resource\u0026quot;: [\u0026quot;arn:aws:s3:::bucket-name\u0026quot;] }, { \u0026quot;Sid\u0026quot;: \u0026quot;AllObjectActions\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:*Object\u0026quot;, \u0026quot;Resource\u0026quot;: [\u0026quot;arn:aws:s3:::bucket-name/*\u0026quot;] } ] } EOF } resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;lambda_s3_role\u0026quot; { name = \u0026quot;lambda_s3_role\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.","tags":["SERVERLESS","TERRAFORM","AWS","IAM"],"title":"Aws IAM Policy","type":"post"},{"authors":null,"categories":[],"content":" 运行环境 terraform -v Terraform v0.12.16 + provider.aws v2.39.0  创建函数 main.js\n'use strict' exports.handler = function(event, context, callback) { var response = { statusCode: 200, headers: { 'Content-Type': 'text/html; charset=utf-8' }, body: '\u0026lt;p\u0026gt;Hello world!\u0026lt;/p\u0026gt;' } callback(null, response) }  zip ../example.zip main.js  上传 awslocal s3api create-bucket --bucket=terraform-serverless-example awslocal s3 cp example.zip s3://terraform-serverless-example/v1.0.0/example.zip  创建资源 lambda.tf\nresource \u0026quot;aws_lambda_function\u0026quot; \u0026quot;example\u0026quot; { function_name = \u0026quot;ServerlessExample\u0026quot; # The bucket name as created earlier with \u0026quot;aws s3api create-bucket\u0026quot; s3_bucket = \u0026quot;terraform-serverless-example\u0026quot; s3_key = \u0026quot;v1.0.0/example.zip\u0026quot; # \u0026quot;main\u0026quot; is the filename within the zip file (main.js) and \u0026quot;handler\u0026quot; # is the name of the property under which the handler function was # exported in that file. handler = \u0026quot;main.handler\u0026quot; runtime = \u0026quot;nodejs10.x\u0026quot; role = aws_iam_role.lambda_exec.arn } # IAM role which dictates what other AWS services the Lambda function # may access. resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;lambda_exec\u0026quot; { name = \u0026quot;serverless_example_lambda\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF {\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Sid\u0026quot;: \u0026quot;\u0026quot; } ] } EOF }  api_gateway.tf\nresource \u0026quot;aws_api_gateway_rest_api\u0026quot; \u0026quot;example\u0026quot; { name = \u0026quot;ServerlessExample\u0026quot; description = \u0026quot;Terraform Serverless Application Example\u0026quot; } resource \u0026quot;aws_api_gateway_resource\u0026quot; \u0026quot;proxy\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id parent_id = aws_api_gateway_rest_api.example.root_resource_id path_part = \u0026quot;{proxy+}\u0026quot; } resource \u0026quot;aws_api_gateway_method\u0026quot; \u0026quot;proxy\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id resource_id = aws_api_gateway_resource.proxy.id http_method = \u0026quot;ANY\u0026quot; authorization = \u0026quot;NONE\u0026quot; } resource \u0026quot;aws_api_gateway_integration\u0026quot; \u0026quot;lambda\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id resource_id = aws_api_gateway_method.proxy.resource_id http_method = aws_api_gateway_method.proxy.http_method integration_http_method = \u0026quot;POST\u0026quot; type = \u0026quot;AWS_PROXY\u0026quot; uri = aws_lambda_function.example.invoke_arn } resource \u0026quot;aws_api_gateway_method\u0026quot; \u0026quot;proxy_root\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id resource_id = aws_api_gateway_rest_api.example.root_resource_id http_method = \u0026quot;ANY\u0026quot; authorization = \u0026quot;NONE\u0026quot; } resource \u0026quot;aws_api_gateway_integration\u0026quot; \u0026quot;lambda_root\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id resource_id = aws_api_gateway_method.proxy_root.resource_id http_method = aws_api_gateway_method.proxy_root.http_method integration_http_method = \u0026quot;POST\u0026quot; type = \u0026quot;AWS_PROXY\u0026quot; uri = aws_lambda_function.example.invoke_arn } resource \u0026quot;aws_api_gateway_deployment\u0026quot; \u0026quot;example\u0026quot; { depends_on = [ aws_api_gateway_integration.lambda, aws_api_gateway_integration.lambda_root, ] rest_api_id = aws_api_gateway_rest_api.example.id stage_name = \u0026quot;test\u0026quot; }  get invoke_url  get api-id\nawslocal apigateway get-rest-apis  get stage\nawslocal apigateway get-stages --rest-api-id  http://localhost:4567/restapis/\u0026lt;api-id\u0026gt;/\u0026lt;stage\u0026gt;/_user_request_/   ","date":1543419498,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543419498,"objectID":"c815a2817487074c09101689133994b5","permalink":"https://wubigo.com/post/lambda-apigateway/","publishdate":"2018-11-28T23:38:18+08:00","relpermalink":"/post/lambda-apigateway/","section":"post","summary":"运行环境 terraform -v Terraform v0.12.16 + provider.aws v2.39.0  创建函数 main.js\n'use strict' exports.handler = function(event, context, callback) { var response = { statusCode: 200, headers: { 'Content-Type': 'text/html; charset=utf-8' }, body: '\u0026lt;p\u0026gt;Hello world!\u0026lt;/p\u0026gt;' } callback(null, response) }  zip ../example.zip main.js  上传 awslocal s3api create-bucket --bucket=terraform-serverless-example awslocal s3 cp example.zip s3://terraform-serverless-example/v1.0.0/example.zip  创建资源 lambda.tf\nresource \u0026quot;aws_lambda_function\u0026quot; \u0026quot;example\u0026quot; { function_name = \u0026quot;ServerlessExample\u0026quot; # The bucket name as created earlier with \u0026quot;aws s3api create-bucket\u0026quot; s3_bucket = \u0026quot;terraform-serverless-example\u0026quot; s3_key = \u0026quot;v1.","tags":["LOCALSTACK","SERVERLESS","TERRAFORM"],"title":"函数＋网关","type":"post"},{"authors":null,"categories":null,"content":"Normally, ${SNAP_DATA} points to /var/snap/microK8S/current. snap.microK8S.daemon-docker, is the docker daemon started using the arguments in ${SNAP_DATA}/args/dockerd\n$snap start microK8S $microK8S.docker pull registry.cn-beijing.aliyuncs.com/google_containers/pause:3.1 $microK8S.docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 K8S.gcr.io/pause:3.1  for resource under namespace kube-system all-namespaces don\u0026rsquo;t include kube-system\n$microK8S.kubectl describe po calico-node-4sq5r --namespace=kube-system  https://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf\n","date":1542931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542931200,"objectID":"80247d4c6c90ebe4d510bb399f0bb85b","permalink":"https://wubigo.com/post/2018-11-24-microk8s/","publishdate":"2018-11-23T00:00:00Z","relpermalink":"/post/2018-11-24-microk8s/","section":"post","summary":"Normally, ${SNAP_DATA} points to /var/snap/microK8S/current. snap.microK8S.daemon-docker, is the docker daemon started using the arguments in ${SNAP_DATA}/args/dockerd\n$snap start microK8S $microK8S.docker pull registry.cn-beijing.aliyuncs.com/google_containers/pause:3.1 $microK8S.docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 K8S.gcr.io/pause:3.1  for resource under namespace kube-system all-namespaces don\u0026rsquo;t include kube-system\n$microK8S.kubectl describe po calico-node-4sq5r --namespace=kube-system  https://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf","tags":["K8S"],"title":"MicroK8S","type":"post"},{"authors":null,"categories":null,"content":" generate configuration file $jupyter notebook --generate-config Writing default config to: /home/bigo/.jupyter/jupyter_notebook_config.py $ diff jupyter_notebook_config.py jupyter_notebook_config.py.bak c.NotebookApp.allow_remote_access = True c.NotebookApp.ip = '0.0.0.0' c.NotebookApp.open_browser = False  set or reset password $jupyter notebook password Enter password: Verify password: [NotebookPasswordApp] Wrote hashed password to /home/bigo/.jupyter/jupyter_notebook_config.json  then restart notebook server\nSharing notebooks When people talk of sharing their notebooks, there are generally two paradigms they may be considering. Most often, individuals share the end-result of their work which means sharing non-interactive, pre-rendered versions of their notebooks; however, it is also possible to collaborate on notebooks with the aid version control systems such as Git\nReferences https://www.dataquest.io/blog/jupyter-notebook-tutorial/\n","date":1542931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542931200,"objectID":"24686d6a0d054aaf9d5cb425834973cf","permalink":"https://wubigo.com/post/2018-11-28-jupyternotebook/","publishdate":"2018-11-23T00:00:00Z","relpermalink":"/post/2018-11-28-jupyternotebook/","section":"post","summary":"generate configuration file $jupyter notebook --generate-config Writing default config to: /home/bigo/.jupyter/jupyter_notebook_config.py $ diff jupyter_notebook_config.py jupyter_notebook_config.py.bak c.NotebookApp.allow_remote_access = True c.NotebookApp.ip = '0.0.0.0' c.NotebookApp.open_browser = False  set or reset password $jupyter notebook password Enter password: Verify password: [NotebookPasswordApp] Wrote hashed password to /home/bigo/.jupyter/jupyter_notebook_config.json  then restart notebook server\nSharing notebooks When people talk of sharing their notebooks, there are generally two paradigms they may be considering. Most often, individuals share the end-result of their work which means sharing non-interactive, pre-rendered versions of their notebooks; however, it is also possible to collaborate on notebooks with the aid version control systems such as Git","tags":["PYTHON"],"title":"entry into jupyter notebook","type":"post"},{"authors":null,"categories":[],"content":" 函数计算概论 函数计算就是事件驱动架构(EDA），目前函数计算支持的事件类型列表\n函数计算事件列表\n计费模式  请求数\n 执行时间\n 内存分配\n  优劣势分析  真正做到谁开发谁运维(who code it who run it)\n 不需要提前做计算容量规划，服务器配置，负责均衡，扩容\n  代表性产品  DB: Aurora   ","date":1542849452,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542849452,"objectID":"76abd545f9bf167624aa1f53f4138c7d","permalink":"https://wubigo.com/post/computing-in-function-way/","publishdate":"2018-11-22T09:17:32+08:00","relpermalink":"/post/computing-in-function-way/","section":"post","summary":" 函数计算概论 函数计算就是事件驱动架构(EDA），目前函数计算支持的事件类型列表\n函数计算事件列表\n计费模式  请求数\n 执行时间\n 内存分配\n  优劣势分析  真正做到谁开发谁运维(who code it who run it)\n 不需要提前做计算容量规划，服务器配置，负责均衡，扩容\n  代表性产品  DB: Aurora   ","tags":["SERVERLESS"],"title":"函数计算之道","type":"post"},{"authors":null,"categories":null,"content":" The Container Network Interface (CNI) is a library definition, and a set of tools under the umbrella of the Cloud Native Computing Foundation project. For more information visit their GitHub project. Kubernetes uses CNI as an interface between network providers and Kubernetes networking.\nWhy Use CNI Kubernetes default networking provider, kubenet, is a simple network plugin that works with various cloud providers. Kubenet is a very basic network provider, and basic is good, but does not have very many features. Moreover, kubenet has many limitations. For instance, when running kubenet in AWS Cloud, you are limited to 50 EC2 instances. Route tables are used to configure network traffic between Kubernetes nodes, and are limited to 50 entries per VPC. Moreover, a cluster cannot be set up in a Private VPC, since that network topology uses multiple route tables. Other more advanced features, such as BGP, egress control, and mesh networking, are only available with different CNI providers.\nCNI in kops At last count, kops supports seven different CNI providers besides kubenet. Choosing from seven different network providers is a daunting task.\nHere is our current list of providers that can be installed out of the box, sorted in alphabetical order.\nCalico Canal (Flannel + Calico) flannel kopeio-vxlan kube-router romana Weave Net Any of these CNI providers can be used without kops. All of the CNI providers use a daemonset installation model, where their product deploys a Kubernetes Daemonset. Just use kubectl to install the provider on the master once the K8S API server has started. Please refer to each projects specific documentation\nSupport Matrix a table of different features from each of the CNI providers mentioned:\n| Provider | Network Model| Route Distribution|Network Policies|Mesh | |External Datastore|Encryption|Ingress/Egress Policies| Commercial Support| | :\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | :\u0026mdash;\u0026mdash;\u0026mdash;-: | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;: |:\u0026mdash;\u0026mdash;\u0026ndash; | :\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-: | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-: |:\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; : | | Calico | Layer 3 | Yes |Yes | Etcd | Yes | Yes | | flannel | Layer 2 vxlan| Mo |No | None | No | No | | Weave | Layer 2 vxlan| N/A |Yes | No | Yes | Yes |\n Calico and Canal include a feature to connect directly to Kubernetes, and not use Etcd. Weave Net can operate in AWS-VPC mode without vxlan, but is limited to 50 nodes in EC2. Weave Net does not have egress rules out of the box.  Table Details\nNetwork Model The Network Model with providers is either encapsulated networking such as VXLAN, or unencapsulated layer 2 networking. Encapsulating network traffic requires compute to process, so theoretically is slower. In my opinion, most use cases will not be impacted by the overhead. More about VXLAN on wikipedia.\nRoute Distribution For layer 3 CNI providers, route distribution is necssary. Route distribution is typically via BGP. Route distribution is nice to have a feature with CNI, if you plan to build clusters split across network segments. It is an exterior gateway protocol designed to exchange routing and reachability information on the internet. BGP can assist with pod to pod networking between clusters.\nNetwork Policies A kubernetes.io blog post about network policies in 1.8 here.\nKubernetes now offers functionality to enforce rules about which pods can communicate with each other using network policies. This feature is has become stable Kubernetes 1.7 and is ready to use with supported networking plugins. The Kubernetes 1.8 release has added better capabilities to this feature.  Mesh Networking This feature allows for “Pod to Pod” networking between Kubernetes clusters. This technology is not Kubernetes federation, but is pure networking between Pods.\nEncyption Encrypting the network control plane, so all TCP and UDP traffic is encrypted.\nIngress / Egress Policies The network policies are both Kubernetes and Non-Kubernetes routing control. For instance, many providers will allow an administrator to block a pod communicating with an EC2 instance meta and data service on 169.254.169.254.\nSummary If you do not need the advanced features that a CNI provider delivers, use kubenet. It is stable, and fast. Otherwise, pick one. If you do need run more than 50 nodes on AWS, or need other advanced features, make a decision quickly (don’t spend days deciding), and test with your cluster. File bugs, and develop a relationship with your network provider. At this point in time, networking is not boring in Kubernetes. It is getting more boring every day! Monitor test and monitor more.\nhttps://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/\n","date":1542844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542844800,"objectID":"a61dafcd727687faf6d4e6ce3be57298","permalink":"https://wubigo.com/post/2018-11-22-cninetworkproviderforkubernetes/","publishdate":"2018-11-22T00:00:00Z","relpermalink":"/post/2018-11-22-cninetworkproviderforkubernetes/","section":"post","summary":"The Container Network Interface (CNI) is a library definition, and a set of tools under the umbrella of the Cloud Native Computing Foundation project. For more information visit their GitHub project. Kubernetes uses CNI as an interface between network providers and Kubernetes networking.\nWhy Use CNI Kubernetes default networking provider, kubenet, is a simple network plugin that works with various cloud providers. Kubenet is a very basic network provider, and basic is good, but does not have very many features.","tags":["K8S","CNI","DOCKER"],"title":"Choosing a CNI Network Provider for Kubernetes","type":"post"},{"authors":null,"categories":[],"content":"https://serverless.com/framework/docs/providers/aws/events/\n","date":1542686207,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542686207,"objectID":"5806eb52408bce9bb91e9968ecfa267b","permalink":"https://wubigo.com/post/serverless-supports-events-type-on-aws/","publishdate":"2018-11-20T11:56:47+08:00","relpermalink":"/post/serverless-supports-events-type-on-aws/","section":"post","summary":"https://serverless.com/framework/docs/providers/aws/events/","tags":["SERVERLESS"],"title":"Serverless Supports Events Type on Aws","type":"post"},{"authors":null,"categories":[],"content":" get s3 object creation notification  create queue\nawslocal s3 mb s3://localstack awslocal sqs create-queue --queue-name localstack  get queue arn\nawslocal sqs get-queue-attributes --queue-url http://localhost:4576/queue/localstack --attribute-names All { \u0026quot;Attributes\u0026quot;: { \u0026quot;ApproximateNumberOfMessagesNotVisible\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ApproximateNumberOfMessagesDelayed\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CreatedTimestamp\u0026quot;: \u0026quot;1574152022\u0026quot;, \u0026quot;ApproximateNumberOfMessages\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;ReceiveMessageWaitTimeSeconds\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DelaySeconds\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;VisibilityTimeout\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;LastModifiedTimestamp\u0026quot;: \u0026quot;1574152022\u0026quot;, \u0026quot;QueueArn\u0026quot;: \u0026quot;arn:aws:sqs:us-east-1:000000000000:localstack\u0026quot; } }  create s3 notification config\ncat notification.json { \u0026quot;QueueConfigurations\u0026quot;: [ { \u0026quot;QueueArn\u0026quot;: \u0026quot;arn:aws:sqs:local:000000000000:localstack\u0026quot;, \u0026quot;Events\u0026quot;: [ \u0026quot;s3:ObjectCreated:*\u0026quot; ] } ] }  make notification effect\nawslocal s3api put-bucket-notification-configuration --bucket localstack --notification-configuration file://notification.json  upload object to s3\nawslocal s3 cp notification.json s3://localstack  get notification\nawslocal sqs receive-message --queue-url http://localhost:4576/queue/localstack   ","date":1542617173,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542617173,"objectID":"2c374d9db83e62a59668e6a8550bcc20","permalink":"https://wubigo.com/post/aws-get-sqs-notification-on-s3-object-creation/","publishdate":"2018-11-19T16:46:13+08:00","relpermalink":"/post/aws-get-sqs-notification-on-s3-object-creation/","section":"post","summary":"get s3 object creation notification  create queue\nawslocal s3 mb s3://localstack awslocal sqs create-queue --queue-name localstack  get queue arn\nawslocal sqs get-queue-attributes --queue-url http://localhost:4576/queue/localstack --attribute-names All { \u0026quot;Attributes\u0026quot;: { \u0026quot;ApproximateNumberOfMessagesNotVisible\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ApproximateNumberOfMessagesDelayed\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CreatedTimestamp\u0026quot;: \u0026quot;1574152022\u0026quot;, \u0026quot;ApproximateNumberOfMessages\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;ReceiveMessageWaitTimeSeconds\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DelaySeconds\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;VisibilityTimeout\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;LastModifiedTimestamp\u0026quot;: \u0026quot;1574152022\u0026quot;, \u0026quot;QueueArn\u0026quot;: \u0026quot;arn:aws:sqs:us-east-1:000000000000:localstack\u0026quot; } }  create s3 notification config\ncat notification.json { \u0026quot;QueueConfigurations\u0026quot;: [ { \u0026quot;QueueArn\u0026quot;: \u0026quot;arn:aws:sqs:local:000000000000:localstack\u0026quot;, \u0026quot;Events\u0026quot;: [ \u0026quot;s3:ObjectCreated:*\u0026quot; ] } ] }  make notification effect","tags":["LOCALSTACK","SERVERLESS"],"title":"基于local stack的本地S3对象创建通知","type":"post"},{"authors":null,"categories":[],"content":"AWS Lambda By default, all native logs within a Lambda function are stored in the function execution result within Lambda. Additionally, if you would like to review log information immediately after executing a function, invoking the Lambda function with the LogType parameter will retrieve the last 4KB of log data generated by the function. This information is returned in the x-amz-log-results header in the HTTP response.\nWhile these methods are great ways to test and debug issues associated with individual function calls, they do not do much by way of analysis or alerting. Thankfully, the log data that is stored in the Lambda function result is also stored in CloudWatch, Amazon’s log aggregation service. To access the CloudWatch logs for a given function, you will need to know the log group and log stream names, which can be retrieved by adding them to the function call logs and retrieving them in the x-amz-log-results response as mentioned above. As an example, this context can be retrieved and logged in Node.js like so:\nconsole.log(‘logGroupName =’, context.log_group_name); console.log(‘logStreamName =’, context.log_stream_name);\n","date":1542355225,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542355225,"objectID":"6ea29161ba48f748a151f825cf70624a","permalink":"https://wubigo.com/post/aws-log/","publishdate":"2018-11-16T16:00:25+08:00","relpermalink":"/post/aws-log/","section":"post","summary":"AWS Lambda By default, all native logs within a Lambda function are stored in the function execution result within Lambda. Additionally, if you would like to review log information immediately after executing a function, invoking the Lambda function with the LogType parameter will retrieve the last 4KB of log data generated by the function. This information is returned in the x-amz-log-results header in the HTTP response.\nWhile these methods are great ways to test and debug issues associated with individual function calls, they do not do much by way of analysis or alerting.","tags":["LOCALSTACK","SERVERLESS"],"title":"Aws Log","type":"post"},{"authors":null,"categories":[],"content":"serverless install -u https://github.com/serverless/examples/tree/master/aws-node-upload-to-s3-and-postprocess -n aws-node-upload-to-s3-and-postprocess sls deploy -s local awslocal logs describe-log-groups { \u0026quot;logGroups\u0026quot;: [ { \u0026quot;arn\u0026quot;: \u0026quot;arn:aws:logs:us-east-1:1:log-group:/aws/lambda/uload-local-postprocess\u0026quot;, \u0026quot;creationTime\u0026quot;: 1573867924377.624, \u0026quot;metricFilterCount\u0026quot;: 0, \u0026quot;logGroupName\u0026quot;: \u0026quot;/aws/lambda/upload-local-postprocess\u0026quot;, \u0026quot;storedBytes\u0026quot;: 0 } ] } awslocal logs describe-log-streams --log-group-name /aws/lambda/uload-local-postprocess { \u0026quot;logStreams\u0026quot;: [] }  serverless install -u https://github.com/serverless/examples/tree/master/aws-node-s3-file-replicator -n aws-node-s3-file-replicator sls deploy -s local awslocal s3api get-bucket-notification-configuration --bucket bbbb awslocal s3api get-bucket-acl --bucket output-bucket-12345  lambda_function.py\nimport json def my_handler(event, context): print(\u0026quot;Received event: \u0026quot; + json.dumps(event, indent=2)) message = 'Hello {} {}!'.format(event['first_name'], event['last_name']) return { 'message': message }  ","date":1542324319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542324319,"objectID":"6b4f0b7b3c33dadcc0173d9750ef0976","permalink":"https://wubigo.com/post/aws-serverless-localstack-examples/","publishdate":"2018-11-16T07:25:19+08:00","relpermalink":"/post/aws-serverless-localstack-examples/","section":"post","summary":"serverless install -u https://github.com/serverless/examples/tree/master/aws-node-upload-to-s3-and-postprocess -n aws-node-upload-to-s3-and-postprocess sls deploy -s local awslocal logs describe-log-groups { \u0026quot;logGroups\u0026quot;: [ { \u0026quot;arn\u0026quot;: \u0026quot;arn:aws:logs:us-east-1:1:log-group:/aws/lambda/uload-local-postprocess\u0026quot;, \u0026quot;creationTime\u0026quot;: 1573867924377.624, \u0026quot;metricFilterCount\u0026quot;: 0, \u0026quot;logGroupName\u0026quot;: \u0026quot;/aws/lambda/upload-local-postprocess\u0026quot;, \u0026quot;storedBytes\u0026quot;: 0 } ] } awslocal logs describe-log-streams --log-group-name /aws/lambda/uload-local-postprocess { \u0026quot;logStreams\u0026quot;: [] }  serverless install -u https://github.com/serverless/examples/tree/master/aws-node-s3-file-replicator -n aws-node-s3-file-replicator sls deploy -s local awslocal s3api get-bucket-notification-configuration --bucket bbbb awslocal s3api get-bucket-acl --bucket output-bucket-12345  lambda_function.py\nimport json def my_handler(event, context): print(\u0026quot;Received event: \u0026quot; + json.","tags":["LOCALSTACK","SERVERLESS"],"title":"基于Localstack的函数计算开发应用列表","type":"post"},{"authors":null,"categories":[],"content":" install nodejs install serverless npm install -g serverless npm install serverless-localstack   check serverless version\nserverless -v Framework Core: 1.57.0 Plugin: 3.2.3 SDK: 2.2.1 Components Core: 1.1.2 Components CLI: 1.4.0   create serverless function  serverless create --template aws-nodejs --path my-service cd my-service  serverless.yml\nfunctions: hello: handler: handler.hello events: - http: path: ping method: get  plugins: - serverless-localstack custom: localstack: debug: true stages: - local - dev host: http://localhost endpoints: S3: http://localhost:4572 DynamoDB: http://localhost:4570 CloudFormation: http://localhost:4581 Elasticsearch: http://localhost:4571 ES: http://localhost:4578 SNS: http://localhost:4575 SQS: http://localhost:4576 Lambda: http://localhost:4574 Kinesis: http://localhost:4568 APIGateway: http://localhost:4567 CloudWatch: http://localhost:4582 CloudWatchLogs: http://localhost:4586 CloudWatchEvents: http://localhost:4587  deploy redeploy if all Functions, Events or Resources\nin serverless.yml changed\nserverless deploy --verbose --stage local Serverless: Using serverless-localstack Serverless: Reconfiguring service apigateway to use http://localhost:4567 Serverless: Reconfiguring service cloudformation to use http://localhost:4581 Serverless: Reconfiguring service cloudwatch to use http://localhost:4582 Serverless: Reconfiguring service lambda to use http://localhost:4574 Serverless: Reconfiguring service dynamodb to use http://localhost:4569 Serverless: Reconfiguring service kinesis to use http://localhost:4568 Serverless: Reconfiguring service route53 to use http://localhost:4580 Serverless: Reconfiguring service firehose to use http://localhost:4573 Serverless: Reconfiguring service stepfunctions to use http://localhost:4585 Serverless: Reconfiguring service es to use http://localhost:4578 Serverless: Reconfiguring service s3 to use http://localhost:4572 Serverless: Reconfiguring service ses to use http://localhost:4579 Serverless: Reconfiguring service sns to use http://localhost:4575 Serverless: Reconfiguring service sqs to use http://localhost:4576 Serverless: Reconfiguring service sts to use http://localhost:4592 Serverless: Reconfiguring service iam to use http://localhost:4593 Serverless: Reconfiguring service ssm to use http://localhost:4583 Serverless: Reconfiguring service rds to use http://localhost:4594 Serverless: Reconfiguring service ec2 to use http://localhost:4597 Serverless: Reconfiguring service elasticache to use http://localhost:4598 Serverless: Reconfiguring service kms to use http://localhost:4599 Serverless: Reconfiguring service secretsmanager to use http://localhost:4584 Serverless: Reconfiguring service logs to use http://localhost:4586 Serverless: Reconfiguring service cloudwatchlogs to use http://localhost:4586 Serverless: Reconfiguring service iot to use http://localhost:4589 Serverless: Reconfiguring service cognito-idp to use http://localhost:4590 Serverless: Reconfiguring service cognito-identity to use http://localhost:4591 Serverless: Reconfiguring service ecs to use http://localhost:4601 Serverless: Reconfiguring service eks to use http://localhost:4602 Serverless: Reconfiguring service xray to use http://localhost:4603 Serverless: Reconfiguring service appsync to use http://localhost:4605 Serverless: Reconfiguring service cloudfront to use http://localhost:4606 Serverless: Reconfiguring service athena to use http://localhost:4607 Serverless: Reconfiguring service S3 to use http://localhost:4572 Serverless: Reconfiguring service DynamoDB to use http://localhost:4570 Serverless: Reconfiguring service CloudFormation to use http://localhost:4581 Serverless: Reconfiguring service Elasticsearch to use http://localhost:4571 Serverless: Reconfiguring service ES to use http://localhost:4578 Serverless: Reconfiguring service SNS to use http://localhost:4575 Serverless: Reconfiguring service SQS to use http://localhost:4576 Serverless: Reconfiguring service Lambda to use http://localhost:4574 Serverless: Reconfiguring service Kinesis to use http://localhost:4568 Serverless: config.options_stage: local Serverless: serverless.service.custom.stage: undefined Serverless: serverless.service.provider.stage: dev Serverless: config.stage: local Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Creating Stack... Serverless: Checking Stack create progress... CloudFormation - CREATE_COMPLETE - AWS::CloudFormation::Stack - my-service-local Serverless: Stack create finished... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service my-service.zip file to S3 (389 B)... Serverless: Validating template... Serverless: Skipping template validation: Unsupported in Localstack Serverless: Updating Stack... Serverless: Checking Stack update progress... CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - my-service-local Serverless: Stack update finished... Service Information service: my-service stage: local region: us-east-1 stack: my-service-local resources: 11 api keys: None endpoints: GET - http://localhost:4567/restapis/gnz8rtc0xd/local/_user_request_/ping functions: hello: my-service-local-hello layers: None Stack Outputs ServerlessDeploymentBucketName: my-service-local-ServerlessDeploymentBucket-01EGLIMU6EYB HelloLambdaFunctionQualifiedArn: HelloLambdaVersionssJzcvFmzKtAcczGFSyyYtxtSzfXFRBYUf4ZEfoXes ServiceEndpoint: https://gnz8rtc0xd.execute-api.us-east-1.amazonaws.com/local  debug sls SLS_DEBUG=* sls deploy -s local  test function  with serverless\nserverless invoke local -f hello -l  with curl\ncurl http://localhost:4567/restapis/gnz8rtc0xd/local/_user_request_/ping   deploy function to localstack serverless deploy function -f hello -v --stage local  serverless invoke local -f hello -l  serverless-localstack issue  service name has to be less than 5 character(the deployment bucket name gets truncated at 63 characters)  By default, Serverless creates a bucket with a generated name like\n--serverlessdeploymentbuck-1x6jug5lzfnl7\nto store your service\u0026rsquo;s stack state\nhttps://github.com/MikeSouza/serverless-deployment-bucket\nhttps://github.com/localstack/serverless-localstack/issues/30\ncheck the buckets after deployment\nawslocal s3 ls 2006-02-03 08:45:09 my-service-local-ServerlessDeploymentBucket-JBV56BIWWO4T 2006-02-03 08:45:09 simple-point-local-ServerlessDeploymentBucket-NI6J3QAYAJ55  ","date":1542267577,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542267577,"objectID":"9bdac2020300ea8848f8e9b362ec8dcd","permalink":"https://wubigo.com/post/aws-localstack-serverless/","publishdate":"2018-11-15T15:39:37+08:00","relpermalink":"/post/aws-localstack-serverless/","section":"post","summary":"install nodejs install serverless npm install -g serverless npm install serverless-localstack   check serverless version\nserverless -v Framework Core: 1.57.0 Plugin: 3.2.3 SDK: 2.2.1 Components Core: 1.1.2 Components CLI: 1.4.0   create serverless function  serverless create --template aws-nodejs --path my-service cd my-service  serverless.yml\nfunctions: hello: handler: handler.hello events: - http: path: ping method: get  plugins: - serverless-localstack custom: localstack: debug: true stages: - local - dev host: http://localhost endpoints: S3: http://localhost:4572 DynamoDB: http://localhost:4570 CloudFormation: http://localhost:4581 Elasticsearch: http://localhost:4571 ES: http://localhost:4578 SNS: http://localhost:4575 SQS: http://localhost:4576 Lambda: http://localhost:4574 Kinesis: http://localhost:4568 APIGateway: http://localhost:4567 CloudWatch: http://localhost:4582 CloudWatchLogs: http://localhost:4586 CloudWatchEvents: http://localhost:4587  deploy redeploy if all Functions, Events or Resources","tags":["LOCALSTACK","SERVERLESS"],"title":"基于Localstack的Serverless框架本地集成","type":"post"},{"authors":null,"categories":null,"content":" Note:\n Starting with TensorFlow 1.6, binaries use AVX instructions which may not run on older CPUs Have to build 1.6 or higher from source to run on older CPU\n Bazel 0.19.0 doesn\u0026rsquo;t read tools/bazel.rc anymore WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files: tensorflow-1.12.0/tools/bazel.rc\n$bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --cxxopt=\u0026quot;-D_GLIBCXX_USE_CXX11_ABI=0\u0026quot; --sandbox_debug \u0026gt; build.log 2\u0026gt;\u0026amp;1 $bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg $pip install /tmp/tensorflow_pkg/tensorflow-\u0026lt;blah\u0026gt;.whl $python -c 'import tensorflow as tf; print(tf.__version__)' $pip list | grep tensorflow   network-performance-monitoring https://github.com/tensorflow/tensorflow/issues/23402\n","date":1541635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541635200,"objectID":"e37caefe6bf0fd93fa4d32a82a6041bf","permalink":"https://wubigo.com/post/2018-11-08-buildtensorflow-1-12/","publishdate":"2018-11-08T00:00:00Z","relpermalink":"/post/2018-11-08-buildtensorflow-1-12/","section":"post","summary":"Note:\n Starting with TensorFlow 1.6, binaries use AVX instructions which may not run on older CPUs Have to build 1.6 or higher from source to run on older CPU\n Bazel 0.19.0 doesn\u0026rsquo;t read tools/bazel.rc anymore WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files: tensorflow-1.12.0/tools/bazel.rc\n$bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --cxxopt=\u0026quot;-D_GLIBCXX_USE_CXX11_ABI=0\u0026quot; --sandbox_debug \u0026gt; build.","tags":["DEEPLEARNING","TENSORFLOW"],"title":"build tensorflow 1.12","type":"post"},{"authors":null,"categories":null,"content":" putting /tmp on tmpfs https://blog.ubuntu.com/2016/01/20/data-driven-analysis-tmp-on-tmpfs\nInterrupt Coalescence ubuntu 16 default\nInterrupt Coalescence (IC)\n$ethtool -c enp0s25 Coalesce parameters for enp0s25: Adaptive RX: off TX: off  Pause frames\n$ethtool -a enp0s25 Pause parameters for enp0s25: Autonegotiate:\ton RX:\ton TX:\ton  network Tuning the network adapter (NIC)\nuse Jumbo frames\nifconfig eth0 mtu 9000  ip result for a healthy system with no packet drops\nip -s link show eth0  stop irqbalance for home user\nsudo systemctl disable/stop irqbalance  network-performance-monitoring https://opensourceforu.com/2016/10/network-performance-monitoring/\nLinux Network (TCP) Performance Tuning with Sysctl https://www.slashroot.in/linux-network-tcp-performance-tuning-sysctl\n","date":1541548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541548800,"objectID":"dbee25a2eefe542b5e9c76a69361b6bc","permalink":"https://wubigo.com/post/2018-11-07-linuxperformancetuning/","publishdate":"2018-11-07T00:00:00Z","relpermalink":"/post/2018-11-07-linuxperformancetuning/","section":"post","summary":"putting /tmp on tmpfs https://blog.ubuntu.com/2016/01/20/data-driven-analysis-tmp-on-tmpfs\nInterrupt Coalescence ubuntu 16 default\nInterrupt Coalescence (IC)\n$ethtool -c enp0s25 Coalesce parameters for enp0s25: Adaptive RX: off TX: off  Pause frames\n$ethtool -a enp0s25 Pause parameters for enp0s25: Autonegotiate:\ton RX:\ton TX:\ton  network Tuning the network adapter (NIC)\nuse Jumbo frames\nifconfig eth0 mtu 9000  ip result for a healthy system with no packet drops\nip -s link show eth0  stop irqbalance for home user","tags":["LINUX"],"title":"Linux performance","type":"post"},{"authors":null,"categories":[],"content":"Improve docker container detection and resource configuration usage\nhttps://blog.softwaremill.com/docker-support-in-new-java-8-finally-fd595df0ca54\nhttps://www.oracle.com/technetwork/java/javase/8u191-relnotes-5032181.html\n","date":1541381628,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541381628,"objectID":"118d36b693ba2f8145a1444efe7899ef","permalink":"https://wubigo.com/post/dockering-java-8/","publishdate":"2018-11-05T09:33:48+08:00","relpermalink":"/post/dockering-java-8/","section":"post","summary":"Improve docker container detection and resource configuration usage\nhttps://blog.softwaremill.com/docker-support-in-new-java-8-finally-fd595df0ca54\nhttps://www.oracle.com/technetwork/java/javase/8u191-relnotes-5032181.html","tags":["DOCKER","JAVA"],"title":"Dockering Java 8","type":"post"},{"authors":null,"categories":[],"content":" awslocal lambda add-permission --function-name ServerlessExample --action lambda:InvokeFunction --statement-id sns-topic --principal apigateway.amazonaws.com --source-arn \u0026quot;arn:aws:execute-api:us-east-1:123456789012:pmte6kdjb6/*/*\u0026quot;  ","date":1540536597,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540536597,"objectID":"98eb1333bdda3a1aad957784a3d16990","permalink":"https://wubigo.com/post/aws-lambda-apigateway/","publishdate":"2018-10-26T14:49:57+08:00","relpermalink":"/post/aws-lambda-apigateway/","section":"post","summary":" awslocal lambda add-permission --function-name ServerlessExample --action lambda:InvokeFunction --statement-id sns-topic --principal apigateway.amazonaws.com --source-arn \u0026quot;arn:aws:execute-api:us-east-1:123456789012:pmte6kdjb6/*/*\u0026quot;  ","tags":[],"title":"Aws Lambda Apigateway","type":"post"},{"authors":null,"categories":[],"content":" Status-Line The first line of a Response message is the Status-Line, consisting of the protocol version followed by a numeric status code and its associated textual phrase, with each element separated by SP characters. No CR or LF is allowed except in the final CRLF sequence.\n Status-Line = HTTP-Version SP Status-Code SP Reason-Phrase CRLF  status code vs status in body https://www.codetinkerer.com/2015/12/04/choosing-an-http-status-code.html\nhttps://httpstatuses.com/\nThe main choice is do you want to treat the HTTP status code as part of your REST API or not.\nBoth ways work fine. I agree that, strictly speaking, one of the ideas of REST is that you should use the HTTP Status code as a part of your API (return 200 or 201 for a successful operation and a 4xx or 5xx depending on various error cases.) However, there are no REST police. You can do what you want. I have seen far more egregious non-REST APIs being called \u0026ldquo;RESTful.\u0026rdquo;\nAt this point (August, 2015) I do recommend that you use the HTTP Status code as part of your API. It is now much easier to see the return code when using frameworks than it was in the past. In particular, it is now easier to see the non-200 return case and the body of non-200 responses than it was in the past.\nThe HTTP Status code is part of your api\nYou will need to carefully pick 4xx codes that fit your error conditions. You can include a rest, xml, or plaintext message as the payload that includes a sub-code and a descriptive comment.\nThe clients will need to use a software framework that enables them to get at the HTTP-level status code. Usually do-able, not always straight-forward.\nThe clients will have to distinguish between HTTP status codes that indicate a communications error and your own status codes that indicate an application-level issue.\nThe HTTP Status code is NOT part of your api\nThe HTTP status code will always be 200 if your app received the request and then responded (both success and error cases)\nALL of your responses should include \u0026ldquo;envelope\u0026rdquo; or \u0026ldquo;header\u0026rdquo; information. Typically something like:\nenvelope_ver: 1.0 status: # use any codes you like. Reserve a code for success. msg: \u0026quot;ok\u0026quot; # A human string that reflects the code. Useful for debugging. data: ... # The data of the response, if any.  This method can be easier for clients since the status for the response is always in the same place (no sub-codes needed), no limits on the codes, no need to fetch the HTTP-level status-code.\nhttps://cloud.google.com/blog/products/api-management/restful-api-design-what-about-errors\n","date":1540523654,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540523654,"objectID":"861371b511efeaff945652b8e1154ce0","permalink":"https://wubigo.com/post/http-status-code-for-reset-api/","publishdate":"2018-10-26T11:14:14+08:00","relpermalink":"/post/http-status-code-for-reset-api/","section":"post","summary":"Status-Line The first line of a Response message is the Status-Line, consisting of the protocol version followed by a numeric status code and its associated textual phrase, with each element separated by SP characters. No CR or LF is allowed except in the final CRLF sequence.\n Status-Line = HTTP-Version SP Status-Code SP Reason-Phrase CRLF  status code vs status in body https://www.codetinkerer.com/2015/12/04/choosing-an-http-status-code.html\nhttps://httpstatuses.com/\nThe main choice is do you want to treat the HTTP status code as part of your REST API or not.","tags":["HTTP","REST"],"title":"REST API中如何使用Http状态码","type":"post"},{"authors":null,"categories":[],"content":" 事件 事件源发送的事件是JSON格式，LAMBDA运行时把原始JSON事件转换为\n对象并发送给函数代码。\n事件的结构和内容由事件源决定\n支持事件源的服务\n Kinesis DynamoDB Simple Queue Service  权限 通过权限策略(permissions policy)来管理IAM用户，组或者角色对lambda\nAPI和资源(函数或函数层）访问权限。\n权限策略也可以授权给资源本身，让资源或服务访问lambda。\n每一个lamdba函数都有一个执行角色(execution role), 该角色授权lamdba函数\n本身对其他资源和服务的访问。执行角色至少包含对CLOUDWATCH日志的访问权限。\nlambda也通过执行角色请求对事件源的读取权限。\n资源  函数 版本 别名 层级  举例：授权SNS 调用 my-function\naws lambda add-permission --function-name my-function --action lambda:InvokeFunction --statement-id sns \\ \u0026gt; --principal sns.amazonaws.com --output text {\u0026quot;Sid\u0026quot;:\u0026quot;sns\u0026quot;,\u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;,\u0026quot;Principal\u0026quot;:{\u0026quot;Service\u0026quot;:\u0026quot;sns.amazonaws.com\u0026quot;},\u0026quot;Action\u0026quot;:\u0026quot;lambda:InvokeFunction\u0026quot;,\u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:lambda:ap-northeast-1:465691908928:function:my-function\u0026quot;}  serveless backend Lambda allows to trigger execution of code\nin response to events in AWS, enabling\nserverless backend solutions.\nThe Lambda Function itself includes source code\nand runtime configuration.\n","date":1540345584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540345584,"objectID":"6c4bb9f0f612ca3a46a8982937de682b","permalink":"https://wubigo.com/post/aws-lambda-notes/","publishdate":"2018-10-24T09:46:24+08:00","relpermalink":"/post/aws-lambda-notes/","section":"post","summary":"事件 事件源发送的事件是JSON格式，LAMBDA运行时把原始JSON事件转换为\n对象并发送给函数代码。\n事件的结构和内容由事件源决定\n支持事件源的服务\n Kinesis DynamoDB Simple Queue Service  权限 通过权限策略(permissions policy)来管理IAM用户，组或者角色对lambda\nAPI和资源(函数或函数层）访问权限。\n权限策略也可以授权给资源本身，让资源或服务访问lambda。\n每一个lamdba函数都有一个执行角色(execution role), 该角色授权lamdba函数\n本身对其他资源和服务的访问。执行角色至少包含对CLOUDWATCH日志的访问权限。\nlambda也通过执行角色请求对事件源的读取权限。\n资源  函数 版本 别名 层级  举例：授权SNS 调用 my-function\naws lambda add-permission --function-name my-function --action lambda:InvokeFunction --statement-id sns \\ \u0026gt; --principal sns.amazonaws.com --output text {\u0026quot;Sid\u0026quot;:\u0026quot;sns\u0026quot;,\u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;,\u0026quot;Principal\u0026quot;:{\u0026quot;Service\u0026quot;:\u0026quot;sns.amazonaws.com\u0026quot;},\u0026quot;Action\u0026quot;:\u0026quot;lambda:InvokeFunction\u0026quot;,\u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:lambda:ap-northeast-1:465691908928:function:my-function\u0026quot;}  serveless backend Lambda allows to trigger execution of code\nin response to events in AWS, enabling\nserverless backend solutions.\nThe Lambda Function itself includes source code","tags":["SERVERLESS","LOCALSTACK","LAMBDA"],"title":"Aws Lambda Notes","type":"post"},{"authors":null,"categories":[],"content":"根据多年在IAAS/PAAS平台的建议经验，并帮助多个行业例如医疗，电信， 建筑行业客户向互联网IAAS/PAAS平台的迁移，在这里做一些直观的分享， 希望能为中小企业上云提供一些有价值的建议。\n本帖中部分建议只是针对没有自己数据中心的中小企业，如果已经拥有自己的数据中心， 需要根据目前的计算能力选择相应的云服务厂商\n云服务模型\n从云服务模型可以看出，企业上云有以下路径可选。 但导向是一致的，专注业务，IT外包。\n 从自建数据中心向IaaS平台迁移 从IaaS平台向PaaS迁移 在公有云厂家间迁移 从IaaS平台向FaaS迁移 混合云  上云的初级阶段: 购买云主机， 购买云主机的性价比指标和物理主机的指标完全不同， 如何选择最高性价比的云主机，参考\n云主机主要性价比指标\n硬盘转速的指标对云主机不再适用，云主机的性价比指标主要有以下几个：\n 云主机内网/外网平均带宽(最核心指标) 存储(网络硬盘)IO吞吐量 初始化完成后首次资源利用率  其中内网平均带宽是最核心的指标，不仅关乎到MONEY的问题，而且对你的应用架构 起决定性的影响，原来的应用架构在云主机模式将会长期处于网络IO阻塞状态。造成 这种问题的根本原因不是你原来的架构不好，而是公有云厂商的网络性能太差，在2016 年的时候，阿里的平均带宽还不到100M每秒。AWS大概在200M每秒。\n网络带宽是很多企业战略转型决定上云遇到的第一个大坑\n 内网平均带宽也是衡量私有云厂商服务能力的核心指标\n 所以公有云厂商又开始向你兜售混合云概念。混合云不符合公有云厂商的核心利益\n国内外云厂商主要产品基准测试对比图\n上云注意事项\n上云的中级阶段：PaaS\n如何选择PAAS供应商\n上云的终级阶段: 不需要服务器。\n如何选择FAAS供应商\n","date":1540136089,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540136089,"objectID":"8e7811d692fc4ea6c448dfb65f9b571d","permalink":"https://wubigo.com/post/pass-over-iaas/","publishdate":"2018-10-21T23:34:49+08:00","relpermalink":"/post/pass-over-iaas/","section":"post","summary":"根据多年在IAAS/PAAS平台的建议经验，并帮助多个行业例如医疗，电信， 建筑行业客户向互联网IAAS/PAAS平台的迁移，在这里做一些直观的分享， 希望能为中小企业上云提供一些有价值的建议。\n本帖中部分建议只是针对没有自己数据中心的中小企业，如果已经拥有自己的数据中心， 需要根据目前的计算能力选择相应的云服务厂商\n云服务模型\n从云服务模型可以看出，企业上云有以下路径可选。 但导向是一致的，专注业务，IT外包。\n 从自建数据中心向IaaS平台迁移 从IaaS平台向PaaS迁移 在公有云厂家间迁移 从IaaS平台向FaaS迁移 混合云  上云的初级阶段: 购买云主机， 购买云主机的性价比指标和物理主机的指标完全不同， 如何选择最高性价比的云主机，参考\n云主机主要性价比指标\n硬盘转速的指标对云主机不再适用，云主机的性价比指标主要有以下几个：\n 云主机内网/外网平均带宽(最核心指标) 存储(网络硬盘)IO吞吐量 初始化完成后首次资源利用率  其中内网平均带宽是最核心的指标，不仅关乎到MONEY的问题，而且对你的应用架构 起决定性的影响，原来的应用架构在云主机模式将会长期处于网络IO阻塞状态。造成 这种问题的根本原因不是你原来的架构不好，而是公有云厂商的网络性能太差，在2016 年的时候，阿里的平均带宽还不到100M每秒。AWS大概在200M每秒。\n网络带宽是很多企业战略转型决定上云遇到的第一个大坑\n 内网平均带宽也是衡量私有云厂商服务能力的核心指标\n 所以公有云厂商又开始向你兜售混合云概念。混合云不符合公有云厂商的核心利益\n国内外云厂商主要产品基准测试对比图\n上云注意事项\n上云的中级阶段：PaaS\n如何选择PAAS供应商\n上云的终级阶段: 不需要服务器。\n如何选择FAAS供应商","tags":["PAAS"],"title":"面向应用的无服务器架构","type":"post"},{"authors":null,"categories":[],"content":"https://github.com/wubigo/localstack-examples\n","date":1540012926,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540012926,"objectID":"d85e8d88869409eda04f55a44953a3e2","permalink":"https://wubigo.com/post/aws-s3-upload-with-localstack/","publishdate":"2018-10-20T13:22:06+08:00","relpermalink":"/post/aws-s3-upload-with-localstack/","section":"post","summary":"https://github.com/wubigo/localstack-examples","tags":["LOCALSTACK","SERVERLESS"],"title":"Aws S3 Upload With Localstack","type":"post"},{"authors":null,"categories":[],"content":"Listen Notes(以下简称LN)是一个iPod试听资料搜索引擎和数据库，\n它使用的技术是非常过时的，没有AI，没有深度学习，\n没有区块链。“如果有谁一定要说我在使用AI，那他一定没有\n使用真正的AI\u0026rdquo;。\n通过阅读本帖，你能完全复制一个Listen Notes和其他相似的网站。\n你不需要雇佣很多工程师。是否还记得，当脸书以5700万美元收购\nInstagram的时候，Instagram总共只有13名员工，包括非工程技术\n人员。Instagram是在2012初成立的，现在是2019，云计算技术更加\n成熟，站在巨人的肩膀上，一个规模较小的工程团队更有可能做出一\n些有意义的产品，即使是像我这样的OPC(一个人独立成立的公司)。\n我发现本帖已经在HN和reddit上被广泛分享，对此，我在这里做些\n澄清：\n 本帖并不是最新的。LN使用的技术栈在不停的演进，经过过去两年的\n全职的开发，技术开始变得有点复杂了。LN在2017年初启动的时候\n只用了3台VPS.这里所说的“过时”是指我只使用了我熟悉的技术来快速\n的开发产品，聚焦到业务侧\n 现在我每天只花20%的时间在工程技术上，其他的时间都在进行业务沟通，\n回复邮件，每天不断总结思考\n 如果因为没有使用你推荐的技术，或者没有回复你提出的问题，而冒犯了\n你的话，请你原谅。我不能做到让每个人都开心满意\n 本帖只是告诉你做互联网产品的一种方法，但并不是唯一方法。而且它可能\n不是最好的方法。它通过使用数据手段帮你了解技术世界。\n  https://broadcast.listennotes.com/the-boring-technology-behind-listen-notes-56697c2e347b\n","date":1539861014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539861014,"objectID":"29526ac5ecbf924b51abdb7e45458dc7","permalink":"https://wubigo.com/post/the-boring-technology-behind-a-one-person-internet-company/","publishdate":"2018-10-18T19:10:14+08:00","relpermalink":"/post/the-boring-technology-behind-a-one-person-internet-company/","section":"post","summary":"Listen Notes(以下简称LN)是一个iPod试听资料搜索引擎和数据库，\n它使用的技术是非常过时的，没有AI，没有深度学习，\n没有区块链。“如果有谁一定要说我在使用AI，那他一定没有\n使用真正的AI\u0026rdquo;。\n通过阅读本帖，你能完全复制一个Listen Notes和其他相似的网站。\n你不需要雇佣很多工程师。是否还记得，当脸书以5700万美元收购\nInstagram的时候，Instagram总共只有13名员工，包括非工程技术\n人员。Instagram是在2012初成立的，现在是2019，云计算技术更加\n成熟，站在巨人的肩膀上，一个规模较小的工程团队更有可能做出一\n些有意义的产品，即使是像我这样的OPC(一个人独立成立的公司)。\n我发现本帖已经在HN和reddit上被广泛分享，对此，我在这里做些\n澄清：\n 本帖并不是最新的。LN使用的技术栈在不停的演进，经过过去两年的\n全职的开发，技术开始变得有点复杂了。LN在2017年初启动的时候\n只用了3台VPS.这里所说的“过时”是指我只使用了我熟悉的技术来快速\n的开发产品，聚焦到业务侧\n 现在我每天只花20%的时间在工程技术上，其他的时间都在进行业务沟通，\n回复邮件，每天不断总结思考\n 如果因为没有使用你推荐的技术，或者没有回复你提出的问题，而冒犯了\n你的话，请你原谅。我不能做到让每个人都开心满意\n 本帖只是告诉你做互联网产品的一种方法，但并不是唯一方法。而且它可能\n不是最好的方法。它通过使用数据手段帮你了解技术世界。\n  https://broadcast.listennotes.com/the-boring-technology-behind-listen-notes-56697c2e347b","tags":["STARTUP"],"title":"一个OPC互联网公司使用的老技术","type":"post"},{"authors":null,"categories":[],"content":" Create function index.js\nexports.handler = async function(event, context) { console.log(\u0026quot;ENVIRONMENT VARIABLES\\n\u0026quot; + JSON.stringify(process.env, null, 2)) console.log(\u0026quot;EVENT\\n\u0026quot; + JSON.stringify(event, null, 2)) return context.logStreamName }   打包\nzip function.zip index.js  aws lambda create-function --function-name my-function --zip-file fileb://function.zip --handler index.handler --runtime nodejs10.x --role arn:aws:iam::123456789012:role/lambda-cli-role --endpoint-url=http://localhost:4574  aws lambda get-function --function-name my-function --endpoint-url=http://localhost:4574 { \u0026quot;Code\u0026quot;: { \u0026quot;Location\u0026quot;: \u0026quot;http://localhost:4574/2015-03-31/functions/my-function/code\u0026quot; }, \u0026quot;Configuration\u0026quot;: { \u0026quot;TracingConfig\u0026quot;: { \u0026quot;Mode\u0026quot;: \u0026quot;PassThrough\u0026quot; }, \u0026quot;Version\u0026quot;: \u0026quot;$LATEST\u0026quot;, \u0026quot;CodeSha256\u0026quot;: \u0026quot;3d149vplmMjIEgZuPhQgnFJ+tndL4I9D11GL1qdgT6M=\u0026quot;, \u0026quot;FunctionName\u0026quot;: \u0026quot;my-function\u0026quot;, \u0026quot;LastModified\u0026quot;: \u0026quot;2019-09-29T01:16:43.752+0000\u0026quot;, \u0026quot;RevisionId\u0026quot;: \u0026quot;c79398c9-556b-4ed1-ad72-91332dd1f6e0\u0026quot;, \u0026quot;CodeSize\u0026quot;: 322, \u0026quot;FunctionArn\u0026quot;: \u0026quot;arn:aws:lambda:us-east-1:000000000000:function:my-function\u0026quot;, \u0026quot;Handler\u0026quot;: \u0026quot;index.handler\u0026quot;, \u0026quot;Role\u0026quot;: \u0026quot;arn:aws:iam::123456789012:role/lambda-cli-role\u0026quot;, \u0026quot;Timeout\u0026quot;: 3, \u0026quot;Runtime\u0026quot;: \u0026quot;nodejs10.x\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;\u0026quot; } }  验证调用\n(venv) $aws lambda list-functions --endpoint-url=http://localhost:4574 (venv) $aws lambda invoke --function-name my-function out --log-type Tail --endpoint-url=http://localhost:4574  清理\naws lambda delete-function --function-name my-function --endpoint-url=http://localhost:4574   ","date":1538185168,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538185168,"objectID":"d45ba8b26ff8f465bbcd411242ef7ba6","permalink":"https://wubigo.com/post/aws-lambda-with-localstack/","publishdate":"2018-09-29T09:39:28+08:00","relpermalink":"/post/aws-lambda-with-localstack/","section":"post","summary":"Create function index.js\nexports.handler = async function(event, context) { console.log(\u0026quot;ENVIRONMENT VARIABLES\\n\u0026quot; + JSON.stringify(process.env, null, 2)) console.log(\u0026quot;EVENT\\n\u0026quot; + JSON.stringify(event, null, 2)) return context.logStreamName }   打包\nzip function.zip index.js  aws lambda create-function --function-name my-function --zip-file fileb://function.zip --handler index.handler --runtime nodejs10.x --role arn:aws:iam::123456789012:role/lambda-cli-role --endpoint-url=http://localhost:4574  aws lambda get-function --function-name my-function --endpoint-url=http://localhost:4574 { \u0026quot;Code\u0026quot;: { \u0026quot;Location\u0026quot;: \u0026quot;http://localhost:4574/2015-03-31/functions/my-function/code\u0026quot; }, \u0026quot;Configuration\u0026quot;: { \u0026quot;TracingConfig\u0026quot;: { \u0026quot;Mode\u0026quot;: \u0026quot;PassThrough\u0026quot; }, \u0026quot;Version\u0026quot;: \u0026quot;$LATEST\u0026quot;, \u0026quot;CodeSha256\u0026quot;: \u0026quot;3d149vplmMjIEgZuPhQgnFJ+tndL4I9D11GL1qdgT6M=\u0026quot;, \u0026quot;FunctionName\u0026quot;: \u0026quot;my-function\u0026quot;, \u0026quot;LastModified\u0026quot;: \u0026quot;2019-09-29T01:16:43.","tags":["LOCALSTACK","SERVERLESS"],"title":"基于Localstack的本地Lambda开发","type":"post"},{"authors":null,"categories":[],"content":" 在windows，启动卷必须线启用共享驱动\n启用共享驱动 1: Open \u0026quot;Settings\u0026quot; in Docker Desktop -\u0026gt; \u0026quot;Shared Drives\u0026quot; -\u0026gt; \u0026quot;Reset Credentials\u0026quot; -\u0026gt; select drive \u0026quot;D\u0026quot; -\u0026gt; \u0026quot;Apply\u0026quot;  检查测试卷 docker run --rm -v d:/tmp:/data alpine ls /data  ","date":1537953252,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537953252,"objectID":"9baf22530856a63af95966a655adb969","permalink":"https://wubigo.com/post/docker-win10-volume/","publishdate":"2018-09-26T17:14:12+08:00","relpermalink":"/post/docker-win10-volume/","section":"post","summary":" 在windows，启动卷必须线启用共享驱动\n启用共享驱动 1: Open \u0026quot;Settings\u0026quot; in Docker Desktop -\u0026gt; \u0026quot;Shared Drives\u0026quot; -\u0026gt; \u0026quot;Reset Credentials\u0026quot; -\u0026gt; select drive \u0026quot;D\u0026quot; -\u0026gt; \u0026quot;Apply\u0026quot;  检查测试卷 docker run --rm -v d:/tmp:/data alpine ls /data  ","tags":["DOCKER","WINDOWS"],"title":"WIN用户使用Docker卷","type":"post"},{"authors":null,"categories":[],"content":" 安装AWS CLI\n(venv) d:\\code\\venv\u0026gt;pip install awscli pip install awscli-local   awslocal = aws \u0026ndash;endpoint-url=http://localhost:\n可以安装到系统环境\n 配置AWS CLI\n(venv) d:\\code\\venv\u0026gt;aws configure AWS Access Key ID [None]: any-id-is-ok AWS Secret Access Key [None]: fake-key Default region name [local]: local Default output format [None]:   命令行自动完成\n$which aws_completer ~/code/venv/bin/aws_completer   tee ~/.bashrc \u0026lt;\u0026lt;-'EOF' complete -C '~/code/venv/bin/aws_completer' aws EOF   安装AWS SAM CLI\n(venv) d:\\code\u0026gt;pip install aws-sam-cli (venv) d:\\code\u0026gt;sam --version SAM CLI, version 0.22.0  启动S3\n(venv) d:\\code\u0026gt;localstack\\docker-compose up  创建bucket\n(venv) d:\\code\u0026gt;aws configure get region local (venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 s3 mb s3://demo-bucket   upload a file to bucket\n(venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 s3 cp java0.log s3://demo-bucket (venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 s3 ls s3://demo-bucket  Attach an ACL to the bucket so it is readable:\naws --endpoint-url=http://localhost:4572 s3api put-bucket-acl --bucket demo-bucket --acl public-read   list object acl\naws --endpoint-url=http://localhost:4572 s3api get-object-acl --bucket demo-bucket --key java0.log { \u0026quot;Owner\u0026quot;: { \u0026quot;DisplayName\u0026quot;: \u0026quot;webfile\u0026quot;, \u0026quot;ID\u0026quot;: \u0026quot;75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\u0026quot; }, \u0026quot;Grants\u0026quot;: [ { \u0026quot;Grantee\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;CanonicalUser\u0026quot;, \u0026quot;ID\u0026quot;: \u0026quot;75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\u0026quot; }, \u0026quot;Permission\u0026quot;: \u0026quot;FULL_CONTROL\u0026quot; } ] }  set object url and can be downloaded by public\naws --endpoint-url=http://localhost:4572 s3api put-object-acl --bucket demo-bucket --key java0.log --acl public-read aws --endpoint-url=http://localhost:4572 s3 presign s3://demo-bucket/java0.log http://localhost:4572/demo-bucket/java0.log   display the names of all S3 buckets (across all regions)\n(venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 s3api list-buckets --query \u0026quot;Buckets[].Name\u0026quot; [ \u0026quot;demo-bucket\u0026quot; ] aws --endpoint-url=http://localhost:4572 s3api list-objects --bucket demo-bucket { \u0026quot;Contents\u0026quot;: [ { \u0026quot;LastModified\u0026quot;: \u0026quot;2019-09-29T10:17:02.386Z\u0026quot;, \u0026quot;ETag\u0026quot;: \u0026quot;\\\u0026quot;d41d8cd98f00b204e9800998ecf8427e\\\u0026quot;\u0026quot;, \u0026quot;StorageClass\u0026quot;: \u0026quot;STANDARD\u0026quot;, \u0026quot;Key\u0026quot;: \u0026quot;java0.log\u0026quot;, \u0026quot;Owner\u0026quot;: { \u0026quot;DisplayName\u0026quot;: \u0026quot;webfile\u0026quot;, \u0026quot;ID\u0026quot;: \u0026quot;75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\u0026quot; }, \u0026quot;Size\u0026quot;: 0 } ] }  or specified region\n(venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 --region local s3api list-buckets --query \u0026quot;Buckets[].Name\u0026quot; [ \u0026quot;demo-bucket\u0026quot; ]   下载样例程序\n(venv) [code]$sam init --runtime python2.7 [+] Initializing project structure... Project generated: ./sam-app Steps you can take next within the project folder =================================================== [*] Invoke Function: sam local invoke HelloWorldFunction --event event.json [*] Start API Gateway locally: sam local start-api Read sam-app/README.md for further instructions  本地调用\necho '{\u0026quot;message\u0026quot;: \u0026quot;Hey, are you there?\u0026quot; }' | sam local invoke \u0026quot;HelloWorldFunction\u0026quot;  编译\n(venv) [sam-app]$ cd sam-app \u0026amp;\u0026amp; sam build Build Succeeded Built Artifacts : .aws-sam/build Built Template : .aws-sam/build/template.yaml Commands you can use next ========================= [*] Invoke Function: sam local invoke [*] Package: sam package --s3-bucket \u0026lt;yourbucket\u0026gt;  启动本地API网关\nvenv) [sam-app]$ sam local start-api 2019-09-27 10:18:10 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)  $curl http://127.0.0.1:3000/hello {\u0026quot;message\u0026quot;: \u0026quot;hello world\u0026quot;}  启动lambda服务\n(venv) [sam-app]$ sam local start-lambda  运行函数计算服务\naws --endpoint-url=http://localhost:4585 stepfunctions list-state-machines --region local { \u0026quot;activities\u0026quot;: [] }  aws stepfunctions --endpoint http://localhost:4585 create-state-machine --definition \u0026quot;{\\ \\\u0026quot;Comment\\\u0026quot;: \\\u0026quot;A Hello World example of the Amazon States Language using an AWS Lambda Local function\\\u0026quot;,\\ \\\u0026quot;StartAt\\\u0026quot;: \\\u0026quot;HelloWorld\\\u0026quot;,\\ \\\u0026quot;States\\\u0026quot;: {\\ \\\u0026quot;HelloWorld\\\u0026quot;: {\\ \\\u0026quot;Type\\\u0026quot;: \\\u0026quot;Task\\\u0026quot;,\\ \\\u0026quot;Resource\\\u0026quot;: \\\u0026quot;arn:aws:lambda:us-east-1:123456789012:function:HelloWorldFunction\\\u0026quot;,\\ \\\u0026quot;End\\\u0026quot;: true\\ }\\ }\\ }\\ }}\u0026quot; --name \u0026quot;HelloWorld\u0026quot; --role-arn \u0026quot;arn:aws:iam::012345678901:role/DummyRole\u0026quot; --region local   aws \u0026ndash;endpoint-url=http://localhost:4585 \u0026ndash;lambda-endpoint http://localhost:3001\n","date":1537944234,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537944234,"objectID":"5349d6e9ec9f6d5233ae068c6375bb5f","permalink":"https://wubigo.com/post/aws-step-function-with-local-lambda/","publishdate":"2018-09-26T14:43:54+08:00","relpermalink":"/post/aws-step-function-with-local-lambda/","section":"post","summary":"安装AWS CLI\n(venv) d:\\code\\venv\u0026gt;pip install awscli pip install awscli-local   awslocal = aws \u0026ndash;endpoint-url=http://localhost:\n可以安装到系统环境\n 配置AWS CLI\n(venv) d:\\code\\venv\u0026gt;aws configure AWS Access Key ID [None]: any-id-is-ok AWS Secret Access Key [None]: fake-key Default region name [local]: local Default output format [None]:   命令行自动完成\n$which aws_completer ~/code/venv/bin/aws_completer   tee ~/.bashrc \u0026lt;\u0026lt;-'EOF' complete -C '~/code/venv/bin/aws_completer' aws EOF   安装AWS SAM CLI\n(venv) d:\\code\u0026gt;pip install aws-sam-cli (venv) d:\\code\u0026gt;sam --version SAM CLI, version 0.","tags":["SERVERLESS","LOCALSTACK"],"title":"基于local stack的Step Function本地化开发","type":"post"},{"authors":null,"categories":[],"content":" bind eip gatsby develop -- --host=0.0.0.0  Prettier VS Code plugin JSX The hybrid “HTML-in-JS” is actually a syntax extension\nof JavaScript, for React, called JSX\nIn pure JavaScript, it looks more like this:\nsrc/pages/index.js\nimport React from \u0026quot;react\u0026quot; export default () =\u0026gt; React.createElement(\u0026quot;div\u0026quot;, null, \u0026quot;Hello world!\u0026quot;)  Now you can spot the use of the \u0026lsquo;react\u0026rsquo; import! But wait. You’re writing JSX, not pure HTML and\nJavaScript. How does the browser read that? The short answer: It doesn’t. Gatsby sites come with\ntooling already set up to convert your source code into something that browsers can interpret.\ndefault plugins query MyQuery { allSitePlugin { totalCount edges { node { name browserAPIs pluginFilepath version resolve pluginOptions { name path } } } } }  implement an API API\nTo implement an API, export a function with the name of the API\ngatsby-node.js\nexports.onCreateNode = ({ node }) =\u0026gt; { console.log(node.internal.type) }  This onCreateNode function will be called by Gatsby whenever a new node is created (or updated).\nGraphQL Playground GATSBY_GRAPHQL_IDE=playground gatsby develop View the GraphQL Playground, an in-browser IDE, to explore your site's data and schema ⠀ http://localhost:8000/___graphql  The GATSBY_GRAPHQL_IDE=playground part of this command is optional.\nAdding it enables the GraphQL Playground instead of GraphiQL,\nwhich is an older interface for exploring GraphQL.\nGraphQL query template All context values are made available to a template’s GraphQL queries\nas arguments prefaced with $\n exports.createPages = async function({ actions, graphql }) { const { data } = await graphql(` query { allMarkdownRemark { edges { node { fields { slug } } } } } `) data.allMarkdownRemark.edges.forEach(edge =\u0026gt; { const slug = edge.node.fields.slug actions.createPage({ path: slug, component: require.resolve(`./src/templates/blog-post.js`), context: { slug: slug }, }) }) }  Server-side environment variables gatsby-config.js or gatsby-node.js:\n require(\u0026quot;dotenv\u0026quot;).config({ path: `.env.${process.env.NODE_ENV}`, })  .env.development\nGATSBY_GRAPHQL_IDE=playground  MDX After installing gatsby-plugin-mdx, MDX files located in src/pages will turn into pages.\nPages are rendered at a URL that is constructed from the filesystem path inside src/pages.\nAn MDX file at src/pages/awesome.mdx will result in a page being rendered at mysite.com/awesome\nFAQ  There are multiple modules with names that only differ in casing(WIN)  Potential solutions:\ncd C:\\gatsby\\dir before starting gatsby, specifying uppercase Use powershell instead of cmd (powershell will redirect to the correct dir and set the env var correctly) Use UNIX file paths and let Windows figure it out (cd /gatsby/dir instead of cd c:\\gatsby\\dir), but note this will only help if you always use UNIX paths for shell navigation; if you're already in the bogus dir, cmd will not handle it properly. Gatsby could always enforce/assume uppercase drive letters when checking paths  d:\\code\\hello-world\u0026gt;gatsby -v Gatsby CLI version: 2.8.18 Gatsby version: 2.18.8  ","date":1535936617,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535936617,"objectID":"f5aa97ebbcbdd3ccc2d90cf4956da28e","permalink":"https://wubigo.com/post/gatsby-notes/","publishdate":"2018-09-03T09:03:37+08:00","relpermalink":"/post/gatsby-notes/","section":"post","summary":"bind eip gatsby develop -- --host=0.0.0.0  Prettier VS Code plugin JSX The hybrid “HTML-in-JS” is actually a syntax extension\nof JavaScript, for React, called JSX\nIn pure JavaScript, it looks more like this:\nsrc/pages/index.js\nimport React from \u0026quot;react\u0026quot; export default () =\u0026gt; React.createElement(\u0026quot;div\u0026quot;, null, \u0026quot;Hello world!\u0026quot;)  Now you can spot the use of the \u0026lsquo;react\u0026rsquo; import! But wait. You’re writing JSX, not pure HTML and\nJavaScript. How does the browser read that?","tags":["NODE","JS"],"title":"Gatsby Notes","type":"post"},{"authors":null,"categories":[],"content":" glide To upgrade dependencies, please make the necessary modifications in glide.yaml and run glide update.\n","date":1535495211,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535495211,"objectID":"cce73b1fae9184578dda0b661ffb918e","permalink":"https://wubigo.com/post/lang-go-dep-manage/","publishdate":"2018-08-29T06:26:51+08:00","relpermalink":"/post/lang-go-dep-manage/","section":"post","summary":"glide To upgrade dependencies, please make the necessary modifications in glide.yaml and run glide update.","tags":["LANG","GO"],"title":"Lang Go Dep Manage","type":"post"},{"authors":null,"categories":[],"content":" Add notification configuration to SNS Topic resource \u0026quot;aws_sns_topic\u0026quot; \u0026quot;topic\u0026quot; { name = \u0026quot;s3-event-notification-topic\u0026quot; policy = \u0026lt;\u0026lt;POLICY { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[{ \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: {\u0026quot;AWS\u0026quot;:\u0026quot;*\u0026quot;}, \u0026quot;Action\u0026quot;: \u0026quot;SNS:Publish\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:sns:*:*:s3-event-notification-topic\u0026quot;, \u0026quot;Condition\u0026quot;:{ \u0026quot;ArnLike\u0026quot;:{\u0026quot;aws:SourceArn\u0026quot;:\u0026quot;${aws_s3_bucket.bucket.arn}\u0026quot;} } }] } POLICY } resource \u0026quot;aws_s3_bucket\u0026quot; \u0026quot;bucket\u0026quot; { bucket = \u0026quot;your_bucket_name\u0026quot; } resource \u0026quot;aws_s3_bucket_notification\u0026quot; \u0026quot;bucket_notification\u0026quot; { bucket = \u0026quot;${aws_s3_bucket.bucket.id}\u0026quot; topic { topic_arn = \u0026quot;${aws_sns_topic.topic.arn}\u0026quot; events = [\u0026quot;s3:ObjectCreated:*\u0026quot;] filter_suffix = \u0026quot;.log\u0026quot; } }  Add notification configuration to Lambda Function resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;iam_for_lambda\u0026quot; { name = \u0026quot;iam_for_lambda\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] } EOF } resource \u0026quot;aws_lambda_permission\u0026quot; \u0026quot;allow_bucket\u0026quot; { statement_id = \u0026quot;AllowExecutionFromS3Bucket\u0026quot; action = \u0026quot;lambda:InvokeFunction\u0026quot; function_name = \u0026quot;${aws_lambda_function.func.arn}\u0026quot; principal = \u0026quot;s3.amazonaws.com\u0026quot; source_arn = \u0026quot;${aws_s3_bucket.bucket.arn}\u0026quot; } resource \u0026quot;aws_lambda_function\u0026quot; \u0026quot;func\u0026quot; { filename = \u0026quot;your-function.zip\u0026quot; function_name = \u0026quot;example_lambda_name\u0026quot; role = \u0026quot;${aws_iam_role.iam_for_lambda.arn}\u0026quot; handler = \u0026quot;exports.example\u0026quot; runtime = \u0026quot;go1.x\u0026quot; } resource \u0026quot;aws_s3_bucket\u0026quot; \u0026quot;bucket\u0026quot; { bucket = \u0026quot;your_bucket_name\u0026quot; } resource \u0026quot;aws_s3_bucket_notification\u0026quot; \u0026quot;bucket_notification\u0026quot; { bucket = \u0026quot;${aws_s3_bucket.bucket.id}\u0026quot; lambda_function { lambda_function_arn = \u0026quot;${aws_lambda_function.func.arn}\u0026quot; events = [\u0026quot;s3:ObjectCreated:*\u0026quot;] filter_prefix = \u0026quot;AWSLogs/\u0026quot; filter_suffix = \u0026quot;.log\u0026quot; } }  import S3 bucket notification can be imported using the bucket\nterraform import aws_s3_bucket_notification.bucket_notification bucket-name  https://www.terraform.io/docs/providers/aws/r/s3_bucket_notification.html\nlocalstack endpoints https://www.terraform.io/docs/providers/aws/guides/custom-service-endpoints.html#localstack\n","date":1532337464,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532337464,"objectID":"601b17556296914a246acdbd34f4e949","permalink":"https://wubigo.com/post/terraform-aws-s3-resource-example/","publishdate":"2018-07-23T17:17:44+08:00","relpermalink":"/post/terraform-aws-s3-resource-example/","section":"post","summary":"Add notification configuration to SNS Topic resource \u0026quot;aws_sns_topic\u0026quot; \u0026quot;topic\u0026quot; { name = \u0026quot;s3-event-notification-topic\u0026quot; policy = \u0026lt;\u0026lt;POLICY { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[{ \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: {\u0026quot;AWS\u0026quot;:\u0026quot;*\u0026quot;}, \u0026quot;Action\u0026quot;: \u0026quot;SNS:Publish\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:sns:*:*:s3-event-notification-topic\u0026quot;, \u0026quot;Condition\u0026quot;:{ \u0026quot;ArnLike\u0026quot;:{\u0026quot;aws:SourceArn\u0026quot;:\u0026quot;${aws_s3_bucket.bucket.arn}\u0026quot;} } }] } POLICY } resource \u0026quot;aws_s3_bucket\u0026quot; \u0026quot;bucket\u0026quot; { bucket = \u0026quot;your_bucket_name\u0026quot; } resource \u0026quot;aws_s3_bucket_notification\u0026quot; \u0026quot;bucket_notification\u0026quot; { bucket = \u0026quot;${aws_s3_bucket.bucket.id}\u0026quot; topic { topic_arn = \u0026quot;${aws_sns_topic.topic.arn}\u0026quot; events = [\u0026quot;s3:ObjectCreated:*\u0026quot;] filter_suffix = \u0026quot;.log\u0026quot; } }  Add notification configuration to Lambda Function resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;iam_for_lambda\u0026quot; { name = \u0026quot;iam_for_lambda\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.","tags":[],"title":"Terraform Aws S3 Resource Example","type":"post"},{"authors":null,"categories":[],"content":" 准备  搭建测试环境  可以参考从源代码构件K8S开发环境\n","date":1529982647,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529982647,"objectID":"8586d462dfbc6d894d5fa6a3818c6442","permalink":"https://wubigo.com/post/k8s_cni_calico/","publishdate":"2018-06-26T11:10:47+08:00","relpermalink":"/post/k8s_cni_calico/","section":"post","summary":"准备  搭建测试环境  可以参考从源代码构件K8S开发环境","tags":["K8S","CNI","NETWORK"],"title":"K8s CNI之Calico实现","type":"post"},{"authors":null,"categories":[],"content":" 删除 systemctl list-unit-files --all | grep yunion systemctl disable yunion-executor systemctl disable yunion-registry systemctl disable kubelet rm -rf /opt/yunion systemctl disable libvirtd.service ip link set virbr0 down brctl delbr virbr0  operator kubectl logs -n onecloud default-region- -c init  kubectl edit deployments. -n onecloud onecloud-operator containers: - command: - /bin/onecloud-controller-manager - -sync-user  onecloud-operator，加上‘-sync-user\u0026rsquo; 会自动修改用户密码， 然后再 kubectl delete deployments -n onecloud default-region 等待重建再试试\nWEB 前端代码是很多 git 仓库组成的，需要用 \u0026lsquo;yarn sync release/3.1\u0026rsquo; 统一切分支，\n依赖 bash 执行脚本\nyarn sync release/3.1  C:\\code\u0026gt;node -v v10.21.0  kubectl logs -f -n onecloud default-apigateway-  kubectl get svc -n onecloud  git clone https://github.com/yunionio/dashboard.git yarn setup:dev yarn install 修改 vue.config.js 里面api 后端 server 为: target: 'https://x.x.x.x:30300' 然后切换分支使用：yarn sync release/3.1 yarn run serve  climc export KUBECONFIG=/etc/kubernetes/admin.conf source \u0026lt;(kubectl completion bash) source \u0026lt;(ocadm cluster rcadmin) ocadm cluster rcadmin export OS_AUTH_URL=https://10.8.3.231:30500/v3 export OS_USERNAME=sysadmin export OS_PASSWORD=kHJ8RUv9ZnXM8dB3 export OS_PROJECT_NAME=system export YUNION_INSECURE=true export OS_REGION_NAME=region0 export OS_ENDPOINT_TYPE=publicURL climc service-list --limit 30 +----------------------------------+------------------+------------------+ | ID | Name | Type | +----------------------------------+------------------+------------------+ | e3542d1d411342128a27d768f0ee2355 | monitor | monitor | | 78d193ba157c4f5e894c6ba562a4bf46 | notify | notify | | 6d0f8ba57a2b4f95834aabf35b2b60ee | log | log | | 7b81c471a8174a7181e966e9c240031a | cloudevent | cloudevent | | 45a984fa8cde426a8aff471a5235c6fa | devtool | devtool | | 3b8a28d49abb43238d5d70743cc82073 | k8s | k8s | | aa92817f666b468f858e12e24543a50a | autoupdate | autoupdate | | 69c977c7a1d348738ebb4afa4004c1a0 | yunionconf | yunionconf | | 5aa6bef960c84fcd8b0befd9433eac13 | cloudnet | cloudnet | | 44540d5bee304eca8df2afa6db245c07 | baremetal | baremetal | | 757fc06c274d4898882c8454d21e9f38 | webconsole | webconsole | | 6a489082b899415a8ebb7ff36549372a | s3gateway | s3gateway | | 0433e9da686940428e270da2726d1999 | influxdb | influxdb | | 2ba92f93d97c423c898181ee01553bbf | host | host | | d1ac196db62c4dfb8ae89386fa9adbe4 | meter | meter | | 5f032e88f8c143178396de326e132880 | websocket | websocket | | 18f7844be9f343d48dccf09c0ccf2d22 | yunionapi | yunionapi | | 8286d967b6a543368a003af964ea7d8f | ansible | ansible | | 2598d51c504241cf87816946471918e5 | yunionagent | yunionagent | | a8bdb942b5d94ec7820908fca06024ed | glance | image | | 80f40414f0bb43d88f4d7d3c7c0c0102 | scheduler | scheduler | | e5ec699df47046cc8cfc886f6f1d43ef | region2 | compute_v2 | | c889bded7d1240088d4a6319a0ffc59d | keystone | identity | | 62be59b2ff3f433e8eef225558759dd9 | offlinecloudmeta | offlinecloudmeta | | 5e6c21fe140045538b6defbc51c4ac76 | torrent-tracker | torrent-tracker | | 59c52837183d4e6488baa87a17136c64 | cloudmeta | cloudmeta | | 57238806433146da81540364d4699e78 | common | common | | 0107fd19803c41c58eadfb5920908ab3 | external-service | external-service | +----------------------------------+------------------+------------------+ *** Total: 28 Pages: 1 Limit: 30 Offset: 0 Page: 1 *** climc user-list ----system --limit 50 +----------------------------------+-----------------+-----------+---------+-------------------+-------------------+------------+ | ID | Name | Domain_Id | Enabled | is_system_account | allow_web_console | enable_mfa | +----------------------------------+-----------------+-----------+---------+------------------- | 785bed264dd743ae8195a6b04251c091 | autoupdate | default | true | true | true | true | | 4d2f62971c484785813a646d7c359ce3 | webconsole | default | true | true | true | true | | 2bdab8e377e043fd83ba0e6609a4e171 | devtooladmin | default | true | true | true | true | | 782c97e3f957473884911e5de6133f68 | kubeserver | default | true | true | true | true | | da0cd74578ef4a2d8977fb2b2d42fffa | cloudeventadmin | default | true | true | true | true | | 5cdd64e1a5d0406784f0b9cd3c8e8810 | monitoradmin | default | true | true | true | true | | 2f7b52ef757c4e91867c8142ea929e69 | cloudnetadmin | default | true | true | true | true | | a052a6ef82864cbf8ed3b93b41fb5f61 | loggeradmin | default | true | true | true | true | | b60f1f9e647549f288f88cabc46d22e9 | notify | default | true | true | true | true | | e26576d1d6b944eb8b49f0f5fa51d8d1 | yunionconf | default | true | true | true | true | | 07e79ecc86de401f8d044852ffeb78ea | s3gatewayadm | default | true | true | true | true | | 05d3dce5930a423d89c0d7949d7f47d1 | hostadmin | default | true | true | true | true | | f43807f7d2d14f4386a54969882da1b9 | baremetal | default | true | true | true | true | | 78baf2c80c6e43d78de3154b807036f8 | vpcagentadmin | default | true | true | true | true | | 31d80c3b895d42f78742992c5a979bc4 | esxiagent | default | true | true | true | true | | 92f048c9992d49c68c1fa74c464bf392 | meterdocker | default | true | true | true | true | | 74fbc304ab9b4b8a8ac61f68f5d3207d | yunionapi | default | true | true | true | true | | 942ad02a2bd64fae82f6c60a7b436bf8 | ansibleadmin | default | true | true | true | true | | 6f35945d01614e2381909b9f1f106966 | yunionagent | default | true | true | true | true | | f11f9af9a9e5429c83becc7e8f18a174 | glance | default | true | true | true | true | | 6e112d6d6453411f850c5dc6bcd0ab9f | regionadmin | default | true | true | true | true | | 7d3ede40910a4ab9812b7f8d5a9ddd6d | sysadmin | default | true | true | false | false | +----------------------------------+-----------------+-----------+---------+-------------------+-------------------+------------+ climc user-update --password demo123 demo climc session-show  keystone git clone https://github.com/yunionio/onecloud.git --branch=v3.1.8  \\etc\\yunion\\keystone.conf\n 初始化\nauto_sync_table: true   自动创建表，创建完后关闭\nkubectl describe cm -n onecloud default-keystone  climc { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;Launch\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;go\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;program\u0026quot;: \u0026quot;${fileDirname}/../cmd/climc/main.go\u0026quot;, \u0026quot;env\u0026quot;: {\u0026quot;OS_AUTH_URL\u0026quot;: \u0026quot;https://192.168.137.176:30500/v3\u0026quot;, \u0026quot;OS_USERNAME\u0026quot;: \u0026quot;sysadmin\u0026quot;, \u0026quot;OS_PASSWORD\u0026quot;: \u0026quot;tNZKXrk3SggGBtm9\u0026quot;, \u0026quot;OS_PROJECT_NAME\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;YUNION_CERT_FILE\u0026quot;: \u0026quot;/etc/yunion/pki/service.crt\u0026quot;, \u0026quot;YUNION_KEY_FILE\u0026quot;: \u0026quot;/etc/yunion/pki/service.key\u0026quot;, \u0026quot;YUNION_INSECURE\u0026quot;: \u0026quot;true\u0026quot;, }, \u0026quot;args\u0026quot;: [] } ] }  本地开发测试 https://docs.yunion.io/docs/contribute/contrib/#本地开发调试\nonecloud-operator pkg/apis/onecloud/v1alpha1/register.go\nfunc init() { localSchemeBuilder.Register(addKownTypes, addDefaultingFuncs) }  完成服务组件用户创建工作\n 测试代码  onecloud-operator\\pkg\\util\\k8s\\marshal_test.go\nTestMarshalToYamlForCodecs  服务暂停 kubectl scale --replicas=0 deployment -n onecloud onecloud-operator kubectl get deployments. -n onecloud | grep default | awk '{print $1}' | xargs kubectl delete deployments. -n onecloud   恢复\nkubectl scale --replicas=1 deployment -n onecloud onecloud-operator   ","date":1528600389,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528600389,"objectID":"43e7bd2b852321df2dc1608588fac15d","permalink":"https://wubigo.com/post/cmp-onecloud-source-code-dev-note/","publishdate":"2018-06-10T11:13:09+08:00","relpermalink":"/post/cmp-onecloud-source-code-dev-note/","section":"post","summary":"删除 systemctl list-unit-files --all | grep yunion systemctl disable yunion-executor systemctl disable yunion-registry systemctl disable kubelet rm -rf /opt/yunion systemctl disable libvirtd.service ip link set virbr0 down brctl delbr virbr0  operator kubectl logs -n onecloud default-region- -c init  kubectl edit deployments. -n onecloud onecloud-operator containers: - command: - /bin/onecloud-controller-manager - -sync-user  onecloud-operator，加上‘-sync-user\u0026rsquo; 会自动修改用户密码， 然后再 kubectl delete deployments -n onecloud default-region 等待重建再试试\nWEB 前端代码是很多 git 仓库组成的，需要用 \u0026lsquo;yarn sync release/3.","tags":["CMP"],"title":"Onecloud Source Code Dev Note","type":"post"},{"authors":null,"categories":[],"content":" 避开Tiller使用Helm部署K8S应用\nTiller存在的问题  破坏RBAC访问机制  全局的Tiller拥有cluster-admin角色，所以在安装过程中，服务以cluster-admin 角色可以越权访问资源\n 部署名字不能重复且唯一  部署名字唯一且很多chart中部署名字也添加到服务名中，导致服务名字混乱。\n独立使用helm  获取模板 使用配置修改模板 生产yaml文件\ngit clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6 helm template install/kubernetes/helm/istio --name istio --namespace istio-system \\ --set security.enabled=false \\ --set ingress.enabled=false \\ --set gateways.istio-ingressgateway.enabled=false \\ --set gateways.istio-egressgateway.enabled=false \\ --set galley.enabled=false \\ --set sidecarInjectorWebhook.enabled=false \\ --set mixer.enabled=false \\ --set prometheus.enabled=false \\ --set global.proxy.envoyStatsd.enabled=false \\ --set pilot.sidecar=false \u0026gt; $HOME/istio-minimal.yaml kubectl create namespace istio-system kubectl apply -f $HOME/istio-minimal.yaml   ","date":1528117376,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528117376,"objectID":"6c4c0ce07df34850631b924a37f82dac","permalink":"https://wubigo.com/post/helm-without-tiller/","publishdate":"2018-06-04T21:02:56+08:00","relpermalink":"/post/helm-without-tiller/","section":"post","summary":"避开Tiller使用Helm部署K8S应用\nTiller存在的问题  破坏RBAC访问机制  全局的Tiller拥有cluster-admin角色，所以在安装过程中，服务以cluster-admin 角色可以越权访问资源\n 部署名字不能重复且唯一  部署名字唯一且很多chart中部署名字也添加到服务名中，导致服务名字混乱。\n独立使用helm  获取模板 使用配置修改模板 生产yaml文件\ngit clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6 helm template install/kubernetes/helm/istio --name istio --namespace istio-system \\ --set security.enabled=false \\ --set ingress.enabled=false \\ --set gateways.istio-ingressgateway.enabled=false \\ --set gateways.istio-egressgateway.enabled=false \\ --set galley.enabled=false \\ --set sidecarInjectorWebhook.enabled=false \\ --set mixer.enabled=false \\ --set prometheus.enabled=false \\ --set global.proxy.envoyStatsd.enabled=false \\ --set pilot.sidecar=false \u0026gt; $HOME/istio-minimal.yaml kubectl create namespace istio-system kubectl apply -f $HOME/istio-minimal.","tags":["K8S","DEVOPS"],"title":"避开Tiller使用Helm部署K8S应用","type":"post"},{"authors":null,"categories":[],"content":" Container When working with cloud native solutions such as Kubernetes, resources are volatile. Services come and go by design, and that’s fine—as long as the whole system operates in a regular way. Classical monitoring solutions aren’t always able to handle this transience gracefully\nGraphite Graphite has no direct data collection support. Carbon listens passively for data, but in order to enable data collection, you should include solutions like fluentd, statd, collectd, or others in your time series data pipeline. Once collected, Graphite has a built-in UI with which to visualize data.\nPrometheus, on the other hand, is a complete monitoring solution, which includes built-in collection, along with storage, visualization, and exporting.\nIf you want a clustered solution that can hold historical data of any sort long term, Graphite may be a better choice due to its simplicity and long history of doing exactly that. Graphite also has rollup of data built in. Similarly, Graphite may be preferred if your existing infrastructure already uses collection tools like fluentd, collectd, or statd, because Graphite supports them.\nmetric vs log subsystems in your applications, making it easier to determine what exactly is causing a slowdown. Logs could not record that many fields, but once you know which sub‐ system is to blame, logs can help you figure out which exact user requests are involved. This is where the tradeoff between logs and metrics becomes most apparent. Metrics allow you to collect information about events from all over your process, but with generally no more than one or two fields of context with bounded cardinality. Logs allow you to collect information about all of one type of event, but can only track a hundred fields of context with unbounded cardinality. This notion of cardinality and the limits it places on metrics is important to understand, and I will come back to it in later chapters.\n","date":1526808181,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526808181,"objectID":"c7c8a77164dc9049104db9053cfbbf20","permalink":"https://wubigo.com/post/monitoring/","publishdate":"2018-05-20T17:23:01+08:00","relpermalink":"/post/monitoring/","section":"post","summary":"Container When working with cloud native solutions such as Kubernetes, resources are volatile. Services come and go by design, and that’s fine—as long as the whole system operates in a regular way. Classical monitoring solutions aren’t always able to handle this transience gracefully\nGraphite Graphite has no direct data collection support. Carbon listens passively for data, but in order to enable data collection, you should include solutions like fluentd, statd, collectd, or others in your time series data pipeline.","tags":["MONITORING","DEVOPS"],"title":"Monitoring notes","type":"post"},{"authors":null,"categories":[],"content":" Headless services Without POD selectors This creates a service, but it doesn’t know where to send the traffic. This allows you to manually create an Endpoints object that will receive traffic from this service.\nkind: Endpoints apiVersion: v1 metadata: name: mongo subsets: - addresses: - ip: 10.240.0.4 ports: - port: 2701  CNAME records for ExternalName This service does a simple CNAME redirection at the kernel level, so there is very minimal impact on performance.\nkind: Service apiVersion: v1 metadata: name: mongo spec: type: ExternalName externalName: ds149763.mlab.com  ","date":1525948342,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525948342,"objectID":"1d5ac9fc01ac8850430cc92daee5ba32","permalink":"https://wubigo.com/post/k8s-external-services/","publishdate":"2018-05-10T18:32:22+08:00","relpermalink":"/post/k8s-external-services/","section":"post","summary":"Headless services Without POD selectors This creates a service, but it doesn’t know where to send the traffic. This allows you to manually create an Endpoints object that will receive traffic from this service.\nkind: Endpoints apiVersion: v1 metadata: name: mongo subsets: - addresses: - ip: 10.240.0.4 ports: - port: 2701  CNAME records for ExternalName This service does a simple CNAME redirection at the kernel level, so there is very minimal impact on performance.","tags":["K8S"],"title":"K8s External Services","type":"post"},{"authors":null,"categories":[],"content":" INSTALL  docker\ndocker run -d --name=netdata \\ -p 19999:19999 \\ -v /etc/passwd:/host/etc/passwd:ro \\ -v /etc/group:/host/etc/group:ro \\ -v /proc:/host/proc:ro \\ -v /sys:/host/sys:ro \\ -v /etc/os-release:/host/etc/os-release:ro \\ --cap-add SYS_PTRACE \\ --security-opt apparmor=unconfined \\ netdata/netdata  script\nbash \u0026lt;(curl -Ss https://my-netdata.io/kickstart.sh) --stable-channel --disable-telemetry Attempting another netdata start using command 'systemctl start netdata' [/tmp/netdata-kickstart-uytL3g/netdata-v1.21.1]# systemctl start netdata OK OK netdata started! Downloading default configuration from netdata... [/tmp/netdata-kickstart-uytL3g/netdata-v1.21.1]# curl -sSL --connect-timeout 10 --retry 3 http://localhost:19999/netdata.conf OK [/tmp/netdata-kickstart-uytL3g/netdata-v1.21.1]# mv /etc/netdata/netdata.conf.new /etc/netdata/netdata.conf OK OK New configuration saved for you to edit at /etc/netdata/netdata.conf [/tmp/netdata-kickstart-uytL3g/netdata-v1.21.1]# chmod 0644 /etc/netdata/netdata.conf OK --- Check KSM (kernel memory deduper) --- Memory de-duplication instructions You have kernel memory de-duper (called Kernel Same-page Merging, or KSM) available, but it is not currently enabled. To enable it run: echo 1 \u0026gt;/sys/kernel/mm/ksm/run echo 1000 \u0026gt;/sys/kernel/mm/ksm/sleep_millisecs If you enable it, you will save 40-60% of netdata memory. --- Check version.txt --- --- Check apps.plugin --- --- Copy uninstaller --- --- Basic netdata instructions --- netdata by default listens on all IPs on port 19999, so you can access it with: http://this.machine.ip:19999/ To stop netdata run: To start netdata run: systemctl start netdata Uninstall script copied to: /usr/libexec/netdata/netdata-uninstaller.sh --- Installing (but not enabling) the netdata updater tool --- Update script is located at /usr/libexec/netdata/netdata-updater.sh --- Check if we must enable/disable the netdata updater tool --- Adding to cron Auto-updating has been enabled. Updater script linked to: /etc/cron.daily/netdata-updater netdata-updater.sh works from cron. It will trigger an email from cron only if it fails (it should not print anything when it can update netdata). --- Wrap up environment set up --- Preparing .environment file [/tmp/netdata-kickstart-uytL3g/netdata-v1.21.1]# chmod 0644 /etc/netdata/.environment OK Setting netdata.tarball.checksum to 'new_installation'   uninstall sudo /usr/libexec/netdata/netdata-uninstaller.sh --yes  ","date":1525474001,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525474001,"objectID":"340c9ae325e1e0329285dac64b999bff","permalink":"https://wubigo.com/post/netdata-notes/","publishdate":"2018-05-05T06:46:41+08:00","relpermalink":"/post/netdata-notes/","section":"post","summary":"INSTALL  docker\ndocker run -d --name=netdata \\ -p 19999:19999 \\ -v /etc/passwd:/host/etc/passwd:ro \\ -v /etc/group:/host/etc/group:ro \\ -v /proc:/host/proc:ro \\ -v /sys:/host/sys:ro \\ -v /etc/os-release:/host/etc/os-release:ro \\ --cap-add SYS_PTRACE \\ --security-opt apparmor=unconfined \\ netdata/netdata  script\nbash \u0026lt;(curl -Ss https://my-netdata.io/kickstart.sh) --stable-channel --disable-telemetry Attempting another netdata start using command 'systemctl start netdata' [/tmp/netdata-kickstart-uytL3g/netdata-v1.21.1]# systemctl start netdata OK OK netdata started! Downloading default configuration from netdata... [/tmp/netdata-kickstart-uytL3g/netdata-v1.21.1]# curl -sSL --connect-timeout 10 --retry 3 http://localhost:19999/netdata.","tags":["MONITOR"],"title":"Netdata notes","type":"post"},{"authors":null,"categories":[],"content":" 物联网架构 An IoT Architecture consists of the following: - Peripherals, which we call “things”. - Sensors attached to these things to gauge and transmit their data and information. - Network connection through which data is transmitted (wireless or wired). - Remote Cloud to which data is transmitted by the system.\n物联网核心构件 Sensors Microcontrollers Gateways Applications 物联网分层架构 ","date":1525257818,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525257818,"objectID":"9ef5dc01eea26e69cce9ab82fbd551f1","permalink":"https://wubigo.com/post/iot-building-blocks/","publishdate":"2018-05-02T18:43:38+08:00","relpermalink":"/post/iot-building-blocks/","section":"post","summary":" 物联网架构 An IoT Architecture consists of the following: - Peripherals, which we call “things”. - Sensors attached to these things to gauge and transmit their data and information. - Network connection through which data is transmitted (wireless or wired). - Remote Cloud to which data is transmitted by the system.\n物联网核心构件 Sensors Microcontrollers Gateways Applications 物联网分层架构 ","tags":["IOT"],"title":"物联网核心构件","type":"post"},{"authors":null,"categories":null,"content":"The most important conversation you ever have is the one with yourself\n","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"fec0f9fdbade4d5491c25a75127e566b","permalink":"https://wubigo.com/post/2019-01-01-sevenlevelcommunication/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/post/2019-01-01-sevenlevelcommunication/","section":"post","summary":"The most important conversation you ever have is the one with yourself","tags":["communication"],"title":"7 level communication","type":"post"},{"authors":null,"categories":[],"content":" 典型无服务器架构应用场景  应用后台\n 数据处理\n 实时分析\n 遗留应用API代理\n 调度服务\n RPA\n  最新实现参考  基于S3和SES邮件服务器  ","date":1525099387,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525099387,"objectID":"92819a379c93d5b8a649074d26652d1a","permalink":"https://wubigo.com/post/serverless-user-case/","publishdate":"2018-04-30T22:43:07+08:00","relpermalink":"/post/serverless-user-case/","section":"post","summary":" 典型无服务器架构应用场景  应用后台\n 数据处理\n 实时分析\n 遗留应用API代理\n 调度服务\n RPA\n  最新实现参考  基于S3和SES邮件服务器  ","tags":["SERVERLESS"],"title":"无服务器架构应用场景","type":"post"},{"authors":null,"categories":[],"content":"https://www.freecodecamp.org/news/rest-is-the-new-soap-97ff6c09896d/\n","date":1525008173,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525008173,"objectID":"3eddac81ef9ff2a8440b5c11b4e8b3c8","permalink":"https://wubigo.com/post/grpc-vs-rest/","publishdate":"2018-04-29T21:22:53+08:00","relpermalink":"/post/grpc-vs-rest/","section":"post","summary":"https://www.freecodecamp.org/news/rest-is-the-new-soap-97ff6c09896d/","tags":["REST","OPENAPI","MICROSERVICE"],"title":"GRPC vs REST","type":"post"},{"authors":null,"categories":[],"content":" https://stackoverflow.com/questions/44547574/create-api-gateway-in-localstack/48682628\nhttps://github.com/localstack/localstack/issues/632\nAWS SAM is an extension for the AWS CloudFormation template language that lets you define serverless applications at a higher level\nlocalstack default regrion us-east-1\ncreate stack file path has to be in file URL format(file:///home/user/\u0026hellip;)\nfunc.yaml\nAWSTemplateFormatVersion: '2010-09-09' Description: Simple CloudFormation Test Template Resources: S3Bucket: Type: AWS::S3::Bucket Properties: AccessControl: PublicRead BucketName: test-bucket-1  aws cloudformation create-stack --stack-name funstack --template-body file:///data/func.yaml --endpoint-url=http://localhost:4581 --region us-east-1  aws cloudformation describe-stacks --endpoint-url=http://localhost:4581 --region us-east-1  ","date":1524982842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524982842,"objectID":"7030e28d328f13e5d86ff6c13c5258f2","permalink":"https://wubigo.com/post/aws-cloudformation-with-localstack/","publishdate":"2018-04-29T14:20:42+08:00","relpermalink":"/post/aws-cloudformation-with-localstack/","section":"post","summary":" https://stackoverflow.com/questions/44547574/create-api-gateway-in-localstack/48682628\nhttps://github.com/localstack/localstack/issues/632\nAWS SAM is an extension for the AWS CloudFormation template language that lets you define serverless applications at a higher level\nlocalstack default regrion us-east-1\ncreate stack file path has to be in file URL format(file:///home/user/\u0026hellip;)\nfunc.yaml\nAWSTemplateFormatVersion: '2010-09-09' Description: Simple CloudFormation Test Template Resources: S3Bucket: Type: AWS::S3::Bucket Properties: AccessControl: PublicRead BucketName: test-bucket-1  aws cloudformation create-stack --stack-name funstack --template-body file:///data/func.yaml --endpoint-url=http://localhost:4581 --region us-east-1  aws cloudformation describe-stacks --endpoint-url=http://localhost:4581 --region us-east-1  ","tags":["LOCALSTACK","SERVERLESS"],"title":"基于Localstack的本地云服务编排","type":"post"},{"authors":null,"categories":[],"content":" 设计目标  存取（入库和分析）高效\n 节省存储空间\n  评估单台设备基于采集评率的每年存储成本\nhttp://mysql.rjweb.org/doc.php/datawarehouse\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"e8283ab0add8e9a2c33e46784ad3a915","permalink":"https://wubigo.com/post/data-lake/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/data-lake/","section":"post","summary":"设计目标  存取（入库和分析）高效\n 节省存储空间\n  评估单台设备基于采集评率的每年存储成本\nhttp://mysql.rjweb.org/doc.php/datawarehouse","tags":["DATALAKE","MYSQL"],"title":"Data Lake","type":"post"},{"authors":null,"categories":[],"content":" docker proxy run cmd as administrator cmd\u0026gt;cd $GIT_HOME cmd\u0026gt;echo \u0026gt; .bash_profile export HTTP_PROXY=http://127.0.0.1:1080 export HTTPS_PROXY=http://127.0.0.1:1080 export no_proxy=localhost,127.0.0.1,192.168.99.100  ","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"03f312aefa2bb8217d69618ba303dc99","permalink":"https://wubigo.com/post/docker-windows7-docker-toolbox/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/docker-windows7-docker-toolbox/","section":"post","summary":" docker proxy run cmd as administrator cmd\u0026gt;cd $GIT_HOME cmd\u0026gt;echo \u0026gt; .bash_profile export HTTP_PROXY=http://127.0.0.1:1080 export HTTPS_PROXY=http://127.0.0.1:1080 export no_proxy=localhost,127.0.0.1,192.168.99.100  ","tags":["WINDOWS","DOCKER"],"title":"Docker Windows7 Docker Toolbox","type":"post"},{"authors":null,"categories":[],"content":" execution environment Creates an execution environment that represents the context in which the program is currently executed. If the program is invoked standalone, this method returns a local execution environment. If the program is invoked from within the command line client to be submitted to a cluster, this method returns the execution environment of this cluster.  REST instead of akka in 1.5 changing the client to communicate via REST instead of akka. Thus, the port you usually want to specify is 8081, the same that the WebUI runs on.\nThe exception is caused by the client sending http messages to the port that akka listens on, which it of course cannot properly read.\nflink list -m localhost:8081  StreamExecutionEnvironment.createRemoteEnvironment(\u0026quot;localhost\u0026quot;, 8081)  Flink Kafka Consumer The Flink Kafka Consumer integrates with Flink’s checkpointing mechanism to provide exactly-once processing semantics. To achieve that, Flink does not purely rely on Kafka’s consumer group offset tracking, but tracks and checkpoints these offsets internally as well.\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"f9ff1f7dbc73349140a820d81042ca2a","permalink":"https://wubigo.com/post/flink-cluster/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/flink-cluster/","section":"post","summary":"execution environment Creates an execution environment that represents the context in which the program is currently executed. If the program is invoked standalone, this method returns a local execution environment. If the program is invoked from within the command line client to be submitted to a cluster, this method returns the execution environment of this cluster.  REST instead of akka in 1.5 changing the client to communicate via REST instead of akka.","tags":["STREAM","BIGDATA"],"title":"Flink Cluster","type":"post"},{"authors":null,"categories":[],"content":"https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"30093daa5af1ae89d2c62209092a0a56","permalink":"https://wubigo.com/post/grpc-load-balancing-on-kubernetes-without-tears/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/grpc-load-balancing-on-kubernetes-without-tears/","section":"post","summary":"https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/","tags":["GRPC","K8S"],"title":"GRPC Load Balancing on Kubernetes Without Tears","type":"post"},{"authors":null,"categories":[],"content":"  Allow more ways of creating objects using literals\n Introduce new datatypes together with their operators and expressions.\n  Closure simple abbreviated syntax of closures: after a method call, put code in braces with parameters delimited from the closure body by an arrow.\nlog = '' (1..10).each{ log += it } assert log == '12345678910' log = '' (1..10).each{ counter -\u0026gt; log += counter } assert log == '12345678910'  A second way of declaring a closure is to directly assign it to a variable:\ndef printer = { line -\u0026gt; println line }   a single parameter default name(it)  Similarly, if the closure needs to take only a single parameter to work on, Groovy provides a default name—it—so that you don’t need to declare it specifically\n Closures are Groovy’s way of providing transparent callback targets as first-class citizens.  ","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"0dece19b024b6a1abe91a5ae15f864c2","permalink":"https://wubigo.com/post/groovy-notes/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/groovy-notes/","section":"post","summary":"Allow more ways of creating objects using literals\n Introduce new datatypes together with their operators and expressions.\n  Closure simple abbreviated syntax of closures: after a method call, put code in braces with parameters delimited from the closure body by an arrow.\nlog = '' (1..10).each{ log += it } assert log == '12345678910' log = '' (1..10).each{ counter -\u0026gt; log += counter } assert log == '12345678910'  A second way of declaring a closure is to directly assign it to a variable:","tags":["JAVA","GROOVY"],"title":"Groovy Notes","type":"post"},{"authors":null,"categories":[],"content":" install client pip install shadowsocks  client.json\n{ \u0026quot;server\u0026quot;:\u0026quot;server-ip\u0026quot;, \u0026quot;server_port\u0026quot;:8000, \u0026quot;local_port\u0026quot;:3050, \u0026quot;password\u0026quot;:\u0026quot;your-password\u0026quot;, \u0026quot;timeout\u0026quot;:600, \u0026quot;method\u0026quot;:\u0026quot;aes-256-cfb\u0026quot; }  { \u0026quot;server\u0026quot;:\u0026quot;your_server_ip\u0026quot;, #ss服务器IP \u0026quot;server_port\u0026quot;:your_server_port, #端口 \u0026quot;local_address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, #本地ip \u0026quot;local_port\u0026quot;:1080, #本地端口 \u0026quot;password\u0026quot;:\u0026quot;your_server_passwd\u0026quot;,#连接ss密码 \u0026quot;timeout\u0026quot;:300, #等待超时 \u0026quot;method\u0026quot;:\u0026quot;rc4-md5\u0026quot;, #加密方式 \u0026quot;fast_open\u0026quot;: false, # true 或 false。如果你的服务器 Linux 内核在3.7+，可以开启 fast_open 以降低延迟。开启方法： echo 3 \u0026gt; /proc/sys/net/ipv4/tcp_fastopen 开启之后，将 fast_open 的配置设置为 true 即可 \u0026quot;workers\u0026quot;: 1 # 工作线程数 }  sudo apt-get install privoxy  /etc/privoxy/config\nlisten-address 127.0.0.1:8118 forward-socks5 / 127.0.0.1:1080 .  systemctl restart privoxy.service  curl -x 127.0.0.1:1080 www.google.com  ","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"b96cb476e93157ba1009f63631e5e30b","permalink":"https://wubigo.com/post/shadowsocks-ubuntu-client/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/shadowsocks-ubuntu-client/","section":"post","summary":"install client pip install shadowsocks  client.json\n{ \u0026quot;server\u0026quot;:\u0026quot;server-ip\u0026quot;, \u0026quot;server_port\u0026quot;:8000, \u0026quot;local_port\u0026quot;:3050, \u0026quot;password\u0026quot;:\u0026quot;your-password\u0026quot;, \u0026quot;timeout\u0026quot;:600, \u0026quot;method\u0026quot;:\u0026quot;aes-256-cfb\u0026quot; }  { \u0026quot;server\u0026quot;:\u0026quot;your_server_ip\u0026quot;, #ss服务器IP \u0026quot;server_port\u0026quot;:your_server_port, #端口 \u0026quot;local_address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, #本地ip \u0026quot;local_port\u0026quot;:1080, #本地端口 \u0026quot;password\u0026quot;:\u0026quot;your_server_passwd\u0026quot;,#连接ss密码 \u0026quot;timeout\u0026quot;:300, #等待超时 \u0026quot;method\u0026quot;:\u0026quot;rc4-md5\u0026quot;, #加密方式 \u0026quot;fast_open\u0026quot;: false, # true 或 false。如果你的服务器 Linux 内核在3.7+，可以开启 fast_open 以降低延迟。开启方法： echo 3 \u0026gt; /proc/sys/net/ipv4/tcp_fastopen 开启之后，将 fast_open 的配置设置为 true 即可 \u0026quot;workers\u0026quot;: 1 # 工作线程数 }  sudo apt-get install privoxy  /etc/privoxy/config\nlisten-address 127.0.0.1:8118 forward-socks5 / 127.0.0.1:1080 .  systemctl restart privoxy.","tags":["SHELL","VPN"],"title":"Shadowsocks Ubuntu Client","type":"post"},{"authors":null,"categories":[],"content":" setup  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  enable web client all endpoints are exposed to JMX and WEB clents\nBy default, all endpoints except for shutdown are enabled.\n enable all endpoings\n enable all endpoints accessed by web\nmanagement: endpoints: enabled-by-default: true web: exposure: include: \u0026quot;*\u0026quot;   https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html\nWebApplicationType  spring: main: web-application-type: reactive   NONE  The application should not run as a web application and should not start an embedded web server.\n REACTIVE  The application should run as a reactive web application and should start an embedded reactive web server.\n SERVLET  The application should run as a servlet-based web application and should start an embedded servlet web server.\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"fc2e98991beb4d616dfd04cffc8782fd","permalink":"https://wubigo.com/post/spring-actuator/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/spring-actuator/","section":"post","summary":"setup  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  enable web client all endpoints are exposed to JMX and WEB clents\nBy default, all endpoints except for shutdown are enabled.\n enable all endpoings\n enable all endpoints accessed by web\nmanagement: endpoints: enabled-by-default: true web: exposure: include: \u0026quot;*\u0026quot;   https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html\nWebApplicationType  spring: main: web-application-type: reactive   NONE  The application should not run as a web application and should not start an embedded web server.","tags":["SPRING","MICROSERVICE"],"title":"Spring Actuator","type":"post"},{"authors":null,"categories":[],"content":" docker run -it -p 9090:9090 -p 1883:1883 -p 5683:5683/udp -v ~/.mytb-data:/data -v ~/.mytb-logs:/var/log/thingsboard --name mytb --restart always thingsboard/tb:2.3.1  default credentials: Systen Administrator: sysadmin@thingsboard.org / sysadmin Tenant Administrator: tenant@thingsboard.org / tenant Customer User: customer@thingsboard.org / customer  Manage device credentials login as tenant\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"be988aba65180299d0691749c2d5a1b9","permalink":"https://wubigo.com/post/thingboard/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/thingboard/","section":"post","summary":"docker run -it -p 9090:9090 -p 1883:1883 -p 5683:5683/udp -v ~/.mytb-data:/data -v ~/.mytb-logs:/var/log/thingsboard --name mytb --restart always thingsboard/tb:2.3.1  default credentials: Systen Administrator: sysadmin@thingsboard.org / sysadmin Tenant Administrator: tenant@thingsboard.org / tenant Customer User: customer@thingsboard.org / customer  Manage device credentials login as tenant","tags":["WEB","MONITOR"],"title":"Thingboard","type":"post"},{"authors":null,"categories":[],"content":" Single File Components single-file components with a .vue extension is build by tools\nsuch as Webpack or Browserify\nA single-file component consists of three parts:\n- \u0026lt;template\u0026gt; which contains the component’s markup in plain HTML - \u0026lt;script\u0026gt; which exports the component object constructor that consists of all the JS logic within that component - \u0026lt;style\u0026gt; which contains all the component styles  CORE  Virtual DOM Component-based UI Focus on the view library—separate concerns for routing, state management Official component library for building mobile apps\nD:\\code\\API\\web\u0026gt;npm config list ; cli configs metrics-registry = \u0026quot;https://registry.npmjs.org/\u0026quot; scope = \u0026quot;\u0026quot; user-agent = \u0026quot;npm/6.4.1 node/v10.15.3 win32 x64\u0026quot;  npm config set registry \u0026lt;registry url\u0026gt;  install with another registry\nnpm install --registry=https://registry.npmjs.org/   install vue-cli npm install -g @vue/cli  vue create my-project  OR\nvue ui  Vue props Props are how the variables and other information pass around between different components.\n Props are passed down the component tree to descendents (not up) Props are read-only and cannot be modified  Vue uses one way data flow, meaning that data can only flow from a parent into a child component.\nAnd because that parent component \u0026ldquo;owns\u0026rdquo; that value it passed down, the child can\u0026rsquo;t modify it.\nFor a child component to use the props provided to it, it needs to explictly declare the props it\nreceives with the props option\nThe v-bind directive is used to bind dynamic values (or objects) as props in a parent instance\nThe v-bind directive can be shortened with the : symbol:\n // the full syntax \u0026lt;img v-bind:src=\u0026quot;prop1\u0026quot; /\u0026gt; // the shorthand syntax \u0026lt;img :src=\u0026quot;prop1\u0026quot; /\u0026gt; // the full syntax \u0026lt;span v-on:click=\u0026quot;func1(prop1)\u0026quot;\u0026gt;\u0026lt;/span\u0026gt; // the shorthand syntax \u0026lt;span @click=\u0026quot;func1(prop1)\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;  ","date":1524628961,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524628961,"objectID":"0746a17773f3ef685eb2e5d504a96581","permalink":"https://wubigo.com/post/js-vue/","publishdate":"2018-04-25T12:02:41+08:00","relpermalink":"/post/js-vue/","section":"post","summary":"Single File Components single-file components with a .vue extension is build by tools\nsuch as Webpack or Browserify\nA single-file component consists of three parts:\n- \u0026lt;template\u0026gt; which contains the component’s markup in plain HTML - \u0026lt;script\u0026gt; which exports the component object constructor that consists of all the JS logic within that component - \u0026lt;style\u0026gt; which contains all the component styles  CORE  Virtual DOM Component-based UI Focus on the view library—separate concerns for routing, state management Official component library for building mobile apps","tags":["WEB","JS","VUE"],"title":"Vue notes","type":"post"},{"authors":null,"categories":[],"content":" install winpcap and windump https://www.winpcap.org\nlist all interfaces windump -D  dump on interface windump -i 1 -n dst host 172.17.17.6  See entire packet payload using tcpdump windump -nnvvXSs 1514 -i 1 -n dst host 172.17.17.6  [1] [The Secret To 10 Million Concurrent Connections]http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html\n[2] [A User-Level TCP Stack for Processing 40 Million Concurrent TCP Connections]https://ieeexplore.ieee.org/abstract/document/8422993\n","date":1524569089,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524569089,"objectID":"1222025a6861e453bda47d3b930e1ee3","permalink":"https://wubigo.com/post/tcpdump-windows/","publishdate":"2018-04-24T19:24:49+08:00","relpermalink":"/post/tcpdump-windows/","section":"post","summary":"install winpcap and windump https://www.winpcap.org\nlist all interfaces windump -D  dump on interface windump -i 1 -n dst host 172.17.17.6  See entire packet payload using tcpdump windump -nnvvXSs 1514 -i 1 -n dst host 172.17.17.6  [1] [The Secret To 10 Million Concurrent Connections]http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html\n[2] [A User-Level TCP Stack for Processing 40 Million Concurrent TCP Connections]https://ieeexplore.ieee.org/abstract/document/8422993","tags":["TCP","NETWORK","WEB"],"title":"Tcpdump Windows","type":"post"},{"authors":null,"categories":[],"content":" spring-cloud-greenwich-release To get started with Maven with a BOM (dependency management only):\n\u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;Greenwich.RELEASE\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;  ","date":1523485284,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523485284,"objectID":"a70c3d69fe15d53d310406a5ab1682fd","permalink":"https://wubigo.com/post/lang-java-spring-cloud/","publishdate":"2018-04-12T06:21:24+08:00","relpermalink":"/post/lang-java-spring-cloud/","section":"post","summary":" spring-cloud-greenwich-release To get started with Maven with a BOM (dependency management only):\n\u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;Greenwich.RELEASE\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;  ","tags":["JAVA","LANG"],"title":"Lang Java Spring Cloud","type":"post"},{"authors":null,"categories":[],"content":" JVM bind with IPv4 Disable IPv6 address lookups when -Djava.net.preferIPv4Stack=true\n-Djava.net.preferIPv4Stack=true  Spring Boot Actuator \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  Actuator comes with most endpoints disabled. Thus, the only two available by default are /health and /info.\nmanagement.endpoints.web.exposure.include=*  by default, all Actuator endpoints are now placed under the /actuator path\nmvn dependency:tree [INFO] +- org.springframework.boot:spring-boot-starter-data-jpa:jar:2.1.4.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter-aop:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework:spring-aop:jar:5.1.6.RELEASE:compile [INFO] | | \\- org.aspectj:aspectjweaver:jar:1.9.2:compile [INFO] | +- org.springframework.boot:spring-boot-starter-jdbc:jar:2.1.4.RELEASE:compile [INFO] | | +- com.zaxxer:HikariCP:jar:3.2.0:compile [INFO] | | \\- org.springframework:spring-jdbc:jar:5.1.6.RELEASE:compile [INFO] | +- javax.transaction:javax.transaction-api:jar:1.3:compile [INFO] | +- javax.xml.bind:jaxb-api:jar:2.3.1:compile [INFO] | | \\- javax.activation:javax.activation-api:jar:1.2.0:compile [INFO] | +- org.hibernate:hibernate-core:jar:5.3.9.Final:compile [INFO] | | +- org.jboss.logging:jboss-logging:jar:3.3.2.Final:compile [INFO] | | +- javax.persistence:javax.persistence-api:jar:2.2:compile [INFO] | | +- org.javassist:javassist:jar:3.23.1-GA:compile [INFO] | | +- net.bytebuddy:byte-buddy:jar:1.9.12:compile [INFO] | | +- antlr:antlr:jar:2.7.7:compile [INFO] | | +- org.jboss:jandex:jar:2.0.5.Final:compile [INFO] | | +- com.fasterxml:classmate:jar:1.4.0:compile [INFO] | | +- org.dom4j:dom4j:jar:2.1.1:compile [INFO] | | \\- org.hibernate.common:hibernate-commons-annotations:jar:5.0.4.Final:compile [INFO] | +- org.springframework.data:spring-data-jpa:jar:2.1.6.RELEASE:compile [INFO] | | +- org.springframework.data:spring-data-commons:jar:2.1.6.RELEASE:compile [INFO] | | +- org.springframework:spring-orm:jar:5.1.6.RELEASE:compile [INFO] | | +- org.springframework:spring-context:jar:5.1.6.RELEASE:compile [INFO] | | +- org.springframework:spring-tx:jar:5.1.6.RELEASE:compile [INFO] | | +- org.springframework:spring-beans:jar:5.1.6.RELEASE:compile [INFO] | | \\- org.slf4j:slf4j-api:jar:1.7.26:compile [INFO] | \\- org.springframework:spring-aspects:jar:5.1.6.RELEASE:compile [INFO] +- org.springframework.boot:spring-boot-starter-thymeleaf:jar:2.1.4.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework.boot:spring-boot:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework.boot:spring-boot-autoconfigure:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework.boot:spring-boot-starter-logging:jar:2.1.4.RELEASE:compile [INFO] | | | +- ch.qos.logback:logback-classic:jar:1.2.3:compile [INFO] | | | | \\- ch.qos.logback:logback-core:jar:1.2.3:compile [INFO] | | | +- org.apache.logging.log4j:log4j-to-slf4j:jar:2.11.2:compile [INFO] | | | | \\- org.apache.logging.log4j:log4j-api:jar:2.11.2:compile [INFO] | | | \\- org.slf4j:jul-to-slf4j:jar:1.7.26:compile [INFO] | | +- javax.annotation:javax.annotation-api:jar:1.3.2:compile [INFO] | | \\- org.yaml:snakeyaml:jar:1.23:runtime [INFO] | +- org.thymeleaf:thymeleaf-spring5:jar:3.0.11.RELEASE:compile [INFO] | | \\- org.thymeleaf:thymeleaf:jar:3.0.11.RELEASE:compile [INFO] | | +- org.attoparser:attoparser:jar:2.0.5.RELEASE:compile [INFO] | | \\- org.unbescape:unbescape:jar:1.1.6.RELEASE:compile [INFO] | \\- org.thymeleaf.extras:thymeleaf-extras-java8time:jar:3.0.4.RELEASE:compile [INFO] +- org.springframework.boot:spring-boot-starter-web:jar:2.1.4.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter-json:jar:2.1.4.RELEASE:compile [INFO] | | +- com.fasterxml.jackson.core:jackson-databind:jar:2.9.8:compile [INFO] | | | +- com.fasterxml.jackson.core:jackson-annotations:jar:2.9.0:compile [INFO] | | | \\- com.fasterxml.jackson.core:jackson-core:jar:2.9.8:compile [INFO] | | +- com.fasterxml.jackson.datatype:jackson-datatype-jdk8:jar:2.9.8:compile [INFO] | | +- com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.9.8:compile [INFO] | | \\- com.fasterxml.jackson.module:jackson-module-parameter-names:jar:2.9.8:compile [INFO] | +- org.springframework.boot:spring-boot-starter-tomcat:jar:2.1.4.RELEASE:compile [INFO] | | +- org.apache.tomcat.embed:tomcat-embed-core:jar:9.0.17:compile [INFO] | | +- org.apache.tomcat.embed:tomcat-embed-el:jar:9.0.17:compile [INFO] | | \\- org.apache.tomcat.embed:tomcat-embed-websocket:jar:9.0.17:compile [INFO] | +- org.hibernate.validator:hibernate-validator:jar:6.0.16.Final:compile [INFO] | | \\- javax.validation:validation-api:jar:2.0.1.Final:compile [INFO] | +- org.springframework:spring-web:jar:5.1.6.RELEASE:compile [INFO] | \\- org.springframework:spring-webmvc:jar:5.1.6.RELEASE:compile [INFO] | \\- org.springframework:spring-expression:jar:5.1.6.RELEASE:compile [INFO] +- com.h2database:h2:jar:1.4.199:runtime [INFO] \\- org.springframework.boot:spring-boot-starter-test:jar:2.1.4.RELEASE:test [INFO] +- org.springframework.boot:spring-boot-test:jar:2.1.4.RELEASE:test [INFO] +- org.springframework.boot:spring-boot-test-autoconfigure:jar:2.1.4.RELEASE:test [INFO] +- com.jayway.jsonpath:json-path:jar:2.4.0:test [INFO] | \\- net.minidev:json-smart:jar:2.3:test [INFO] | \\- net.minidev:accessors-smart:jar:1.2:test [INFO] | \\- org.ow2.asm:asm:jar:5.0.4:test [INFO] +- junit:junit:jar:4.12:test [INFO] +- org.assertj:assertj-core:jar:3.11.1:test [INFO] +- org.mockito:mockito-core:jar:2.23.4:test [INFO] | +- net.bytebuddy:byte-buddy-agent:jar:1.9.12:test [INFO] | \\- org.objenesis:objenesis:jar:2.6:test [INFO] +- org.hamcrest:hamcrest-core:jar:1.3:test [INFO] +- org.hamcrest:hamcrest-library:jar:1.3:test [INFO] +- org.skyscreamer:jsonassert:jar:1.5.0:test [INFO] | \\- com.vaadin.external.google:android-json:jar:0.0.20131108.vaadin1:test [INFO] +- org.springframework:spring-core:jar:5.1.6.RELEASE:compile [INFO] | \\- org.springframework:spring-jcl:jar:5.1.6.RELEASE:compile [INFO] +- org.springframework:spring-test:jar:5.1.6.RELEASE:test [INFO] \\- org.xmlunit:xmlunit-core:jar:2.6.2:test  ","date":1523407698,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523407698,"objectID":"ea0d844a632be5bb714a76ded1de6b40","permalink":"https://wubigo.com/post/lang-java-spring-boot-v2/","publishdate":"2018-04-11T08:48:18+08:00","relpermalink":"/post/lang-java-spring-boot-v2/","section":"post","summary":"JVM bind with IPv4 Disable IPv6 address lookups when -Djava.net.preferIPv4Stack=true\n-Djava.net.preferIPv4Stack=true  Spring Boot Actuator \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  Actuator comes with most endpoints disabled. Thus, the only two available by default are /health and /info.\nmanagement.endpoints.web.exposure.include=*  by default, all Actuator endpoints are now placed under the /actuator path\nmvn dependency:tree [INFO] +- org.springframework.boot:spring-boot-starter-data-jpa:jar:2.1.4.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter-aop:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework:spring-aop:jar:5.1.6.RELEASE:compile [INFO] | | \\- org.","tags":["JAVA","LANG"],"title":"Lang Java Spring Boot V2","type":"post"},{"authors":null,"categories":null,"content":" Start Dgraph cluster dgraph zero  start Dgraph server  dgraph server --memory_mb 2048 --zero localhost:5080 --port_offset 2000 Note:port_offsetValue added to all listening port numbers. [Internal=7080, HTTP=8080, Grpc=9080]  How do I configure Go to use a proxy https://stackoverflow.com/questions/10383299/how-do-i-configure-go-to-use-a-proxy\nWeb based graph visualization with D3 and KeyLines https://cambridge-intelligence.com/web-graph-visualization-d3-keylines/\nSETUP CLIENT set http_proxy=192.168.0.119:3128 git config --global http.proxy http://192.168.0.119:3128 go get -u github.com/derekparker/delve/cmd/dlv go get -u -v github.com/dgraph-io/dgo  ","date":1523059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523059200,"objectID":"db686c5b514d2e320e7018c5b058cc03","permalink":"https://wubigo.com/post/2018-04-07-dgraphnotes/","publishdate":"2018-04-07T00:00:00Z","relpermalink":"/post/2018-04-07-dgraphnotes/","section":"post","summary":" Start Dgraph cluster dgraph zero  start Dgraph server  dgraph server --memory_mb 2048 --zero localhost:5080 --port_offset 2000 Note:port_offsetValue added to all listening port numbers. [Internal=7080, HTTP=8080, Grpc=9080]  How do I configure Go to use a proxy https://stackoverflow.com/questions/10383299/how-do-i-configure-go-to-use-a-proxy\nWeb based graph visualization with D3 and KeyLines https://cambridge-intelligence.com/web-graph-visualization-d3-keylines/\nSETUP CLIENT set http_proxy=192.168.0.119:3128 git config --global http.proxy http://192.168.0.119:3128 go get -u github.com/derekparker/delve/cmd/dlv go get -u -v github.com/dgraph-io/dgo  ","tags":["NOSQL","GRAPH"],"title":"Dgraph note","type":"post"},{"authors":null,"categories":[],"content":" SNAPSHOT // Snapshot is an internally consistent snapshot of xDS resources. // Consistentcy is important for the convergence as different resource types // from the snapshot may be delivered to the proxy in arbitrary order. type Snapshot struct { // Endpoints are items in the EDS response payload. Endpoints Resources // Clusters are items in the CDS response payload. Clusters Resources // Routes are items in the RDS response payload. Routes Resources // Listeners are items in the LDS response payload. Listeners Resources // Secrets are items in the SDS response payload. Secrets Resources }  graceful shutdown mechanism Get shutdown notice /healthcheck/fail Wait X time Shutdown  BUILD go get -d github.com/envoyproxy/go-control-plane cd $GOPATH/src/github.com/envoyproxy/go-control-plane docker build -t bigo/envoy-test:v1 .  TEST XDS in docker docker run -it --rm -e \u0026quot;XDS=ads\u0026quot; bigo/envoy-test:v1 -debug docker run -it --rm -e \u0026quot;XDS=xds\u0026quot; bigo/envoy-test:v1 -debug docker run -it --rm -e \u0026quot;XDS=rest\u0026quot; bigo/envoy-test:v1 -debug\t TEST XDS with tls in docker docker run -it --rm -e \u0026quot;XDS=ads\u0026quot; bigo/envoy-test:v1 -debug -tls docker run -it --rm -e \u0026quot;XDS=xds\u0026quot; bigo/envoy-test:v1 -debug -tls docker run -it --rm -e \u0026quot;XDS=rest\u0026quot; bigo/envoy-test:v1 -debug -tls  Test it in code 在code里面启动ADS API服务器\npkg/test/main\nlaunch.json\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;Launch\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;go\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;program\u0026quot;: \u0026quot;${fileDirname}\u0026quot;, \u0026quot;env\u0026quot;: {}, \u0026quot;args\u0026quot;: [\u0026quot;-debug\u0026quot;, \u0026quot;--xds=ads\u0026quot;] } ] }  在主机上运行\ndocker run -it --name envoy --entrypoint sh bigo/envoy-test:v1 docker cp envoy:/usr/local/bin/envoy ~/go/bin/ envoy -c /sample/bootstrap-ads.yaml --drain-time-s 1 -l debug [source/server/server.cc:270] admin address: 127.0.0.1:19000  在容器内运行\ndocker run -it --rm --name envoy --entrypoint sh bigo/envoy-test:v1  修改配置 /sample/bootstrap-ads.yaml\n允许远程访问代理服务器管理端口\nadmin: address: socket_address: address: 0.0.0.0  docker cp sample/bootstrap-ads.yaml envoy:/sample/bootstrap-ads.yaml #envoy -c /sample/bootstrap-ads.yaml --drain-time-s 1  查询容器IP http://172.17.0.2:19000/clusters xds_cluster::192.168.1.5:28000::cx_connect_fail::6  如果管理服务器没有启动，会出现连接失败计数。\n通过manager server下发配置\ndocker run -it --rm -e \u0026quot;XDS=ads\u0026quot; bigo/envoy-test:v1 -debug  再次检查ENVOY的配置\nhttp://localhost:19000/config_dump\nBootstrap configuration To use the v2 API, it’s necessary to supply a bootstrap configuration file. This provides static server configuration and configures Envoy to access dynamic configuration if needed. This is supplied on the command-line via the -c flag\n./envoy -c \u0026lt;path to config\u0026gt;.{json,yaml,pb,pb_text}  The Bootstrap message is the root of the configuration. Resources such as a Listener or Cluster may be supplied either statically in static_resources or have an xDS service such as LDS or CDS configured in dynamic_resources.\nEnvoy Initialization  During startup, the cluster manager goes through a multi-phase initialization where it first initializes static/DNS clusters, then predefined EDS clusters. Then it initializes CDS if applicable, waits for one response (or failure), and does the same primary/secondary initialization of CDS provided clusters.\n If clusters use active health checking, Envoy also does a single active health check round.\n Once cluster manager initialization is done, RDS and LDS initialize (if applicable). The server doesn’t start accepting connections until there has been at least one response (or failure) for LDS/RDS requests.\n If LDS itself returns a listener that needs an RDS response, Envoy further waits until an RDS response (or failure) is received. Note that this process takes place on every future listener addition via LDS and is known as listener warming.\n After all of the previous steps have taken place, the listeners start accepting new connections. This flow ensures that during hot restart the new process is fully capable of accepting and processing new connections before the draining of the old process begins.\n  ","date":1522795843,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522795843,"objectID":"a3a9a0e201239a79a4abb589caf60042","permalink":"https://wubigo.com/post/envoy-control-plane-api-server-testing/","publishdate":"2018-04-04T06:50:43+08:00","relpermalink":"/post/envoy-control-plane-api-server-testing/","section":"post","summary":"SNAPSHOT // Snapshot is an internally consistent snapshot of xDS resources. // Consistentcy is important for the convergence as different resource types // from the snapshot may be delivered to the proxy in arbitrary order. type Snapshot struct { // Endpoints are items in the EDS response payload. Endpoints Resources // Clusters are items in the CDS response payload. Clusters Resources // Routes are items in the RDS response payload.","tags":["SERVICEMESH","MICROSERVICE"],"title":"Envoy Control Plane API Server Testing","type":"post"},{"authors":null,"categories":[],"content":"envoy.yaml.tmpl\nadmin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 80 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: local_service domains: [\u0026quot;*\u0026quot;] routes: - match: { prefix: \u0026quot;/\u0026quot; } route: { host_rewrite: nginx, cluster: nginx_cluster, timeout: 60s } http_filters: - name: envoy.router clusters: - name: nginx_cluster connect_timeout: 0.25s type: STRICT_DNS dns_lookup_family: V4_ONLY lb_policy: ${ENVOY_LB_ALG} hosts: [{ socket_address: { address: ${SERVICE_NAME}, port_value: 80 }}]  docker-entrypoint.sh\n#!/bin/sh set -e echo \u0026quot;Generating envoy.yaml config file...\u0026quot; cat /tmpl/envoy.yaml.tmpl | envsubst \\$ENVOY_LB_ALG,\\$SERVICE_NAME \u0026gt; /etc/envoy/envoy.yaml echo \u0026quot;Starting Envoy...\u0026quot; /usr/local/bin/envoy -c /etc/envoy/envoy.yaml -l debug  Dockerfile\nFROM envoyproxy/envoy:latest COPY /tmpl/envoy.yaml.tmpl /tmpl/envoy.yaml.tmpl COPY docker-entrypoint.sh / RUN chmod 500 /docker-entrypoint.sh RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install gettext -y ENTRYPOINT [\u0026quot;/docker-entrypoint.sh\u0026quot;]  docker build -t bigo-envoy:v1 . docker tag bigo-envoy:v1 egistry.cn-beijing.aliyuncs.com/k4s/bigo-envoy:v1 docker push registry.cn-beijing.aliyuncs.com/k4s/bigo-envoy:v1 docker network create test docker run -dit --name nginx --network test nginx:alpine docker run -it --rm --name envoy --network test -e ENVOY_LB_ALG=LEAST_REQUEST -e SERVICE_NAME=nginx bigo-envoy:v1 ENVOY_IP=$(docker inspect envoy --format='{{.NetworkSettings.Networks.test.IPAddress}}') curl $ENVOY_IP docker logs nginx | grep $ENVOY_IP 172.18.0.3 - [02/Apr/2019:05:59:12 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.47.0\u0026quot; \u0026quot;-\u0026quot;  ","date":1522638965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522638965,"objectID":"4f767043c14d8e90eaf185356e8c0958","permalink":"https://wubigo.com/post/k8s-envoy-example/","publishdate":"2018-04-02T11:16:05+08:00","relpermalink":"/post/k8s-envoy-example/","section":"post","summary":"envoy.yaml.tmpl\nadmin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 80 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: local_service domains: [\u0026quot;*\u0026quot;] routes: - match: { prefix: \u0026quot;/\u0026quot; } route: { host_rewrite: nginx, cluster: nginx_cluster, timeout: 60s } http_filters: - name: envoy.router clusters: - name: nginx_cluster connect_timeout: 0.25s type: STRICT_DNS dns_lookup_family: V4_ONLY lb_policy: ${ENVOY_LB_ALG} hosts: [{ socket_address: { address: ${SERVICE_NAME}, port_value: 80 }}]  docker-entrypoint.","tags":["K8S"],"title":"K8s Envoy Example","type":"post"},{"authors":null,"categories":[],"content":" To ensure stable network ID , need to define a headless service for stateful applications\nStatefulSets are valuable for applications that require one or more of the following.\n Stable, unique network identifiers. Stable, persistent storage. Ordered, graceful deployment and scaling. Ordered, automated rolling updates  `headless-nginx.yaml\u0026rsquo;\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: \u0026quot;nginx\u0026quot; replicas: 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: nginx:1.14-alpine ports: - containerPort: 80 name: web  the .spec.selector field of a StatefulSet must match the labels of its .spec.template.metadata.labels\nkubectl apply -f headless-nginx.yaml kubectl exec -it web-0 -- nslookup nginx Name: nginx Address 1: 10.2.12.86 web-0.nginx.default.svc.cluster.local  kubectl run -it --image busybox test --restart=Never --rm nslookup web-0.nginx  enable KUBE-DNS LOG kubectl -n kube-system edit configmap coredns  Then add log in the Corefile section\n Corefile: | .:53 { log errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 loop reload loadbalance }  kubectl exec busybox cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5  kubectl get svc -n kube-system -o wide|grep dns kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 41d k8s-app=kube-dns  ","date":1522574432,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522574432,"objectID":"679217484bc5bb2099f6ff38a8ebe581","permalink":"https://wubigo.com/post/k8s-service-headless/","publishdate":"2018-04-01T17:20:32+08:00","relpermalink":"/post/k8s-service-headless/","section":"post","summary":"To ensure stable network ID , need to define a headless service for stateful applications\nStatefulSets are valuable for applications that require one or more of the following.\n Stable, unique network identifiers. Stable, persistent storage. Ordered, graceful deployment and scaling. Ordered, automated rolling updates  `headless-nginx.yaml\u0026rsquo;\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .","tags":["K8S"],"title":"K8s Service Headless","type":"post"},{"authors":null,"categories":[],"content":" 术语  端点 Envoy discovers the cluster members via EDS Management server: A logical server implementing the v2 Envoy APIs Upstream: An upstream host receives connections and requests from Envoy and returns responses xDS: CDS/EDS/HDS/LDS/RLS/RDS/SDS APIs. Configuration Cache: cache Envoy configurations in memory in an attempt to provide fast response to consumer Envoys  The simplest way to use Envoy without providing the control plane in the form of a dynamic API is to add the hardcoded configuration to a static yaml file.\n参数化定制Envoy镜像 clusters: - name: myapp_cluster connect_timeout: 0.25s type: STRICT_DNS dns_lookup_family: V4_ONLY lb_policy: ${ENVOY_LB_ALG} hosts: [{ socket_address: { address: ${SERVICE_NAME}, port_value: 80 }}]  docker-entrypoint.shin 做环境变量替换\n#!/bin/sh set -e echo \u0026quot;Generating envoy.yaml config file...\u0026quot; cat /tmpl/envoy.yaml.tmpl | envsubst \\$ENVOY_LB_ALG,\\$SERVICE_NAME \u0026gt; /etc/envoy.yaml echo \u0026quot;Starting Envoy...\u0026quot; /usr/local/bin/envoy -c /etc/envoy.yaml  Dockerfile\nFROM envoyproxy/envoy:latest COPY envoy.yaml /tmpl/envoy.yaml.tmpl COPY docker-entrypoint.sh / RUN chmod 500 /docker-entrypoint.sh RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install gettext -y ENTRYPOINT [\u0026quot;/docker-entrypoint.sh\u0026quot;]  设置时间 docker history --no-trunc envoyproxy/envoy-dev:48082bcd22fe9165eb73bed6d27857f578df63b5  Dockerfile\nFROM envoyproxy/envoy-dev:48082bcd22fe9165eb73bed6d27857f578df63b5 COPY envoy.yaml /etc/envoy/envoy.yaml RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl ethtool tzdata \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV TZ Asia/Shanghai # CMD [\u0026quot;envoy\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;/etc/envoy/envoy.yaml\u0026quot;, \u0026quot;-l\u0026quot;, \u0026quot;debug\u0026quot;]  docker build -t envoy:v1 .  docker run -d --rm --name envoy -p 9901:9901 -p 10000:10000 envoy:v1 envoy -c /etc/envoy/envoy.yaml -l debug docker exec -it envoy bash #ps fax 1 ? Ssl 0:00 envoy -c /etc/envoy/envoy.yaml -l debug  ENVOY配置 Envoy supports multiple configurations:\n static configuration API-based configuration service-discovery-based configuration     资源类别      listeners 暴露给外部客户的端点   cluster 后台服务集群     集群  Clusters are composed of endpoints – a set of network locations that can serve requests for the cluster. Endpoints can also be defined directly as socket addresses, or read dynamically via the Endpoint Discovery Service\n监听器  监听过滤器（内置）\n envoy.client_ssl_auth envoy.echo envoy.http_connection_manager(代理HTTP请求)  http_connection_manager.v2.HttpFilter  envoy.buffer envoy.cors envoy.fault envoy.gzip envoy.http_dynamo_filter envoy.grpc_http1_bridge envoy.grpc_json_transcoder envoy.grpc_web envoy.health_check envoy.header_to_metadata envoy.ip_tagging envoy.lua envoy.rate_limit envoy.router envoy.squash   envoy.mongo_proxy envoy.ratelimit envoy.redis_proxy envoy.tcp_proxy\nroute_config: virtual_hosts: domains: -\u0026gt; matched against the http requests Host header    config envoy by following its api api document is automatically generated from protocol buffers\nhttps://www.envoyproxy.io/docs/envoy/v1.8.0/api-v2/api\n以上都是静态资源配置，但是在K8S环境，容器是动态分配的，手动配置无法 保证配置信息同步。于是就需要服务发现功能。ENVOY所需的发现服务包括:\n routes (“what cluster should requests with this HTTP header go to”)[RDS] clusters (“what backends does this service have?”)[CDS] listener (the filters for a port)[LDS] endpoints[EDS]\n v1\nXDS = [ RDS, CDS, LDS, and EDS]  v2\n  Health Discovery Service (HDS)\nAggregated Discovery Service (ADS)\nSecret Discovery Service (SDS)\n CDS type\nCluster.DiscoveryType\n STATIC STRICT_DNS LOGICAL_DNS\n EDS ⁣ ORIGINAL_DST\nclusters: - name: service_backend type: []    istio-pilot是ENVOY发现服务提供者之一，istio-pilot根据K8S API为envoy提供配置routes和clusters服务\n/envoy/examples/front-proxy$ git diff --word-diff diff --git a/examples/front-proxy/Dockerfile-frontenvoy b/examples/front-proxy/Dockerfile-frontenvoy index 83b5ba806..2e203a204 100644 --- a/examples/front-proxy/Dockerfile-frontenvoy +++ b/examples/front-proxy/Dockerfile-frontenvoy @@ -1,5 +1,5 @@ FROM envoyproxy/envoy-dev:latest RUN apt-get update \u0026amp;\u0026amp; apt-get -q install -y \\ curl {+tzdata+} CMD /usr/local/bin/envoy -c /etc/front-envoy.yaml {+-l debug+} --service-cluster front-proxy diff --git a/examples/front-proxy/Dockerfile-service b/examples/front-proxy/Dockerfile-service index c3f5bafef..987b21814 100644 --- a/examples/front-proxy/Dockerfile-service +++ b/examples/front-proxy/Dockerfile-service @@ -1,6 +1,6 @@ FROM envoyproxy/envoy-alpine-dev:latest RUN apk update \u0026amp;\u0026amp; apk add python3 bash curl {+tzdata+} RUN pip3 install -q Flask==0.11.1 requests==2.18.4 RUN mkdir /code ADD ./service.py /code diff --git a/examples/front-proxy/docker-compose.yml b/examples/front-proxy/docker-compose.yml index 2c121d598..05d7eb844 100644 --- a/examples/front-proxy/docker-compose.yml +++ b/examples/front-proxy/docker-compose.yml @@ -15,6 +15,8 @@ services: ports: - \u0026quot;8000:80\u0026quot; - \u0026quot;8001:8001\u0026quot; {+environment:+} {+ - TZ=Asia/Shanghai+} service1: build: @@ -28,8 +30,10 @@ services: - service1 environment: - SERVICE_NAME=1 {+- TZ=Asia/Shanghai+} expose: - \u0026quot;80\u0026quot; service2: build: @@ -43,6 +47,7 @@ services: - service2 environment: - SERVICE_NAME=2 {+- TZ=Asia/Shanghai+} expose: - \u0026quot;80\u0026quot; diff --git a/examples/front-proxy/start_service.sh b/examples/front-proxy/start_service.sh index cc529bcf2..57176eff3 100644 --- a/examples/front-proxy/start_service.sh +++ b/examples/front-proxy/start_service.sh @@ -1,3 +1,3 @@ #!/bin/sh python3 /code/service.py \u0026amp; envoy -c /etc/service-envoy.yaml {+-l debug+} --service-cluster service${SERVICE_NAME}  https://jvns.ca/blog/2018/10/27/envoy-basics/\nhttps://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a\n","date":1522466210,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522466210,"objectID":"b6ffd8cd9ba29e52ef56662c94e592d1","permalink":"https://wubigo.com/post/envoy-notes/","publishdate":"2018-03-31T11:16:50+08:00","relpermalink":"/post/envoy-notes/","section":"post","summary":"术语  端点 Envoy discovers the cluster members via EDS Management server: A logical server implementing the v2 Envoy APIs Upstream: An upstream host receives connections and requests from Envoy and returns responses xDS: CDS/EDS/HDS/LDS/RLS/RDS/SDS APIs. Configuration Cache: cache Envoy configurations in memory in an attempt to provide fast response to consumer Envoys  The simplest way to use Envoy without providing the control plane in the form of a dynamic API is to add the hardcoded configuration to a static yaml file.","tags":["K8S"],"title":"Envoy NOTES","type":"post"},{"authors":null,"categories":[],"content":" 常用命令 常用命令\nbandwidth utilization tool wget -qO- https://github.com/imsnif/bandwhich/releases/download/${WHAT_VERSION}/bandwhich-v${WHAT_VERSION}-x86_64-unknown-linux-musl.tar.gz | tar xvz -C ~/bin/ bandwhich  TCP tcpdump-windows\nTCP DUMP\n","date":1522448952,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522448952,"objectID":"e0a9ff88c4dda8c9539a8d352c70beda","permalink":"https://wubigo.com/post/network-tools/","publishdate":"2018-03-31T06:29:12+08:00","relpermalink":"/post/network-tools/","section":"post","summary":"常用命令 常用命令\nbandwidth utilization tool wget -qO- https://github.com/imsnif/bandwhich/releases/download/${WHAT_VERSION}/bandwhich-v${WHAT_VERSION}-x86_64-unknown-linux-musl.tar.gz | tar xvz -C ~/bin/ bandwhich  TCP tcpdump-windows\nTCP DUMP","tags":["NETWORK","LINUX"],"title":"常用的网络工具","type":"post"},{"authors":null,"categories":[],"content":" 禁止启动时候恢复页面提示 Version 73.0.3683.86 (Official Build) (64-bit)\n 方法1（可靠）  首先关闭chrome，然后修改下面的设置，修改完后重启\n.config/google-chrome/Default/Preferences\nsed -i '/exit_type:Crashed/exit_type:Normal/`  windows用户请参考下面的视频\nChrome Didn\u0026rsquo;t Shut Down Correctly Error Solved Windows 7\n 方法2（不可靠）  Type in address bar (Crtl+L).\nchrome://flags/#infinite-session-restore  Click on the right drop-down menu and change the \u0026lsquo;Default\u0026rsquo; value to \u0026lsquo;Disable\u0026rsquo;. Then restart Chrome to apply that setting\nenter password to unlock your keyring  方法1（可靠）\n set password-store to basic\ndpkg -L google-chrome-stable |grep desktop | xargs cp {1} ~/.local/share/applications   修改.local/share/applications/google-chrome.desktop\nExec=/usr/bin/google-chrome-stable --password-store=basic %U   方法2（不可靠）\n seahorse\nseahorse   选择login，右键删除\nHow to allow unsafe ports in Chrome http://douglastarr.com/how-to-allow-unsafe-ports-in-chrome\n","date":1522448437,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522448437,"objectID":"baacc8852cddb889f5f8b0f25941eca6","permalink":"https://wubigo.com/post/linux-chrome/","publishdate":"2018-03-31T06:20:37+08:00","relpermalink":"/post/linux-chrome/","section":"post","summary":"禁止启动时候恢复页面提示 Version 73.0.3683.86 (Official Build) (64-bit)\n 方法1（可靠）  首先关闭chrome，然后修改下面的设置，修改完后重启\n.config/google-chrome/Default/Preferences\nsed -i '/exit_type:Crashed/exit_type:Normal/`  windows用户请参考下面的视频\nChrome Didn\u0026rsquo;t Shut Down Correctly Error Solved Windows 7\n 方法2（不可靠）  Type in address bar (Crtl+L).\nchrome://flags/#infinite-session-restore  Click on the right drop-down menu and change the \u0026lsquo;Default\u0026rsquo; value to \u0026lsquo;Disable\u0026rsquo;. Then restart Chrome to apply that setting\nenter password to unlock your keyring  方法1（可靠）\n set password-store to basic\ndpkg -L google-chrome-stable |grep desktop | xargs cp {1} ~/.","tags":["WEB","LINUX"],"title":"Linux Chrome","type":"post"},{"authors":null,"categories":[],"content":"helm install --debug install/kubernetes/helm/istio --name istio --namespace istio-system --set security.enabled=false --set ingress.enabled=false --set gateways.istio-ingressgateway.enabled=false --set gateways.istio-egressgateway.enabled=false --set galley.enabled=false --set mixer.enabled=false --set prometheus.enabled=false --set global.proxy.envoyStatsd.enabled=false --set pilot.sidecar=false --set sidecarInjectorWebhook.enabled=false [debug] Created tunnel using local port: '44471' [debug] SERVER: \u0026quot;127.0.0.1:44471\u0026quot; [debug] Original chart version: \u0026quot;\u0026quot; [debug] CHART PATH: /home/bigo/istio/install/kubernetes/helm/istio NAME: istio REVISION: 1 RELEASED: Sat Mar 30 06:30:03 2019 CHART: istio-1.0.6 USER-SUPPLIED VALUES: galley: enabled: false gateways: istio-egressgateway: enabled: false istio-ingressgateway: enabled: false global: proxy: envoyStatsd: enabled: false ingress: enabled: false mixer: enabled: false pilot: sidecar: false prometheus: enabled: false security: enabled: false sidecarInjectorWebhook: enabled: false COMPUTED VALUES: certmanager: enabled: false hub: quay.io/jetstack resources: {} tag: v0.3.1 galley: enabled: false image: galley replicaCount: 1 gateways: enabled: true global: arch: amd64: 2 ppc64le: 2 s390x: 2 configValidation: true controlPlaneSecurityEnabled: false crds: true defaultResources: requests: cpu: 10m disablePolicyChecks: false enableTracing: true hub: gcr.io/istio-release hyperkube: hub: quay.io/coreos tag: v1.7.6_coreos.0 imagePullPolicy: IfNotPresent imagePullSecrets: null k8sIngressHttps: false k8sIngressSelector: ingress meshExpansion: false meshExpansionILB: false mtls: enabled: false oneNamespace: false policyCheckFailOpen: false priorityClassName: \u0026quot;\u0026quot; proxy: accessLogFile: /dev/stdout autoInject: enabled concurrency: 0 discoveryDomain: \u0026quot;\u0026quot; enableCoreDump: false envoyStatsd: enabled: false host: null port: null excludeIPRanges: \u0026quot;\u0026quot; excludeInboundPorts: \u0026quot;\u0026quot; image: proxyv2 includeIPRanges: '*' includeInboundPorts: '*' privileged: false proxyDomain: \u0026quot;\u0026quot; readinessFailureThreshold: 30 readinessInitialDelaySeconds: 1 readinessPeriodSeconds: 2 resources: requests: cpu: 10m stats: prometheusPort: 15090 statusPort: 0 proxy_init: image: proxy_init tag: release-1.0-latest-daily istio-egressgateway: autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 enabled: false labels: app: istio-egressgateway istio: egressgateway ports: - name: http2 port: 80 - name: https port: 443 replicaCount: 1 secretVolumes: - mountPath: /etc/istio/egressgateway-certs name: egressgateway-certs secretName: istio-egressgateway-certs - mountPath: /etc/istio/egressgateway-ca-certs name: egressgateway-ca-certs secretName: istio-egressgateway-ca-certs serviceAnnotations: {} type: ClusterIP istio-ilbgateway: autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 enabled: false labels: app: istio-ilbgateway istio: ilbgateway loadBalancerIP: \u0026quot;\u0026quot; ports: - name: grpc-pilot-mtls port: 15011 - name: grpc-pilot port: 15010 - name: tcp-citadel-grpc-tls port: 8060 targetPort: 8060 - name: tcp-dns port: 853 replicaCount: 1 resources: requests: cpu: 800m memory: 512Mi secretVolumes: - mountPath: /etc/istio/ilbgateway-certs name: ilbgateway-certs secretName: istio-ilbgateway-certs - mountPath: /etc/istio/ilbgateway-ca-certs name: ilbgateway-ca-certs secretName: istio-ilbgateway-ca-certs serviceAnnotations: cloud.google.com/load-balancer-type: internal type: LoadBalancer istio-ingressgateway: autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 enabled: false labels: app: istio-ingressgateway istio: ingressgateway loadBalancerIP: \u0026quot;\u0026quot; ports: - name: http2 nodePort: 31380 port: 80 targetPort: 80 - name: https nodePort: 31390 port: 443 - name: tcp nodePort: 31400 port: 31400 - name: tcp-pilot-grpc-tls port: 15011 targetPort: 15011 - name: tcp-citadel-grpc-tls port: 8060 targetPort: 8060 - name: tcp-dns-tls port: 853 targetPort: 853 - name: http2-prometheus port: 15030 targetPort: 15030 - name: http2-grafana port: 15031 targetPort: 15031 replicaCount: 1 resources: {} secretVolumes: - mountPath: /etc/istio/ingressgateway-certs name: ingressgateway-certs secretName: istio-ingressgateway-certs - mountPath: /etc/istio/ingressgateway-ca-certs name: ingressgateway-ca-certs secretName: istio-ingressgateway-ca-certs serviceAnnotations: {} type: LoadBalancer global: arch: amd64: 2 ppc64le: 2 s390x: 2 configValidation: true controlPlaneSecurityEnabled: false crds: true defaultResources: requests: cpu: 10m disablePolicyChecks: false enableTracing: true hub: gcr.io/istio-release hyperkube: hub: quay.io/coreos tag: v1.7.6_coreos.0 imagePullPolicy: IfNotPresent imagePullSecrets: null k8sIngressHttps: false k8sIngressSelector: ingress meshExpansion: false meshExpansionILB: false mtls: enabled: false oneNamespace: false policyCheckFailOpen: false priorityClassName: \u0026quot;\u0026quot; proxy: accessLogFile: /dev/stdout autoInject: enabled concurrency: 0 discoveryDomain: \u0026quot;\u0026quot; enableCoreDump: false envoyStatsd: enabled: false host: null port: null excludeIPRanges: \u0026quot;\u0026quot; excludeInboundPorts: \u0026quot;\u0026quot; image: proxyv2 includeIPRanges: '*' includeInboundPorts: '*' privileged: false proxyDomain: \u0026quot;\u0026quot; readinessFailureThreshold: 30 readinessInitialDelaySeconds: 1 readinessPeriodSeconds: 2 resources: requests: cpu: 10m stats: prometheusPort: 15090 statusPort: 0 proxy_init: image: proxy_init tag: release-1.0-latest-daily grafana: accessMode: ReadWriteMany enabled: false image: repository: grafana/grafana tag: 5.2.3 persist: false replicaCount: 1 security: enabled: false passphraseKey: passphrase secretName: grafana usernameKey: username service: annotations: {} externalPort: 3000 internalPort: 3000 name: http type: ClusterIP storageClassName: \u0026quot;\u0026quot; ingress: autoscaleMax: 5 autoscaleMin: 1 enabled: false replicaCount: 1 service: annotations: {} loadBalancerIP: \u0026quot;\u0026quot; ports: - name: http nodePort: 32000 port: 80 - name: https port: 443 selector: istio: ingress type: LoadBalancer kiali: dashboard: passphraseKey: passphrase secretName: kiali usernameKey: username enabled: false hub: docker.io/kiali ingress: annotations: null enabled: false tls: null replicaCount: 1 tag: v0.12 mixer: autoscaleMax: 5 autoscaleMin: 1 enabled: false env: GODEBUG: gctrace=2 image: mixer istio-policy: autoscaleEnabled: true autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 istio-telemetry: autoscaleEnabled: true autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 prometheusStatsdExporter: hub: docker.io/prom tag: v0.6.0 replicaCount: 1 pilot: autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 enabled: true env: GODEBUG: gctrace=2 PILOT_PUSH_THROTTLE_COUNT: 100 global: arch: amd64: 2 ppc64le: 2 s390x: 2 configValidation: true controlPlaneSecurityEnabled: false crds: true defaultResources: requests: cpu: 10m disablePolicyChecks: false enableTracing: true hub: gcr.io/istio-release hyperkube: hub: quay.io/coreos tag: v1.7.6_coreos.0 imagePullPolicy: IfNotPresent imagePullSecrets: null k8sIngressHttps: false k8sIngressSelector: ingress meshExpansion: false meshExpansionILB: false mtls: enabled: false oneNamespace: false policyCheckFailOpen: false priorityClassName: \u0026quot;\u0026quot; proxy: accessLogFile: /dev/stdout autoInject: enabled concurrency: 0 discoveryDomain: \u0026quot;\u0026quot; enableCoreDump: false envoyStatsd: enabled: false host: null port: null excludeIPRanges: \u0026quot;\u0026quot; excludeInboundPorts: \u0026quot;\u0026quot; image: proxyv2 includeIPRanges: '*' includeInboundPorts: '*' privileged: false proxyDomain: \u0026quot;\u0026quot; readinessFailureThreshold: 30 readinessInitialDelaySeconds: 1 readinessPeriodSeconds: 2 resources: requests: cpu: 10m stats: prometheusPort: 15090 statusPort: 0 proxy_init: image: proxy_init tag: release-1.0-latest-daily image: pilot replicaCount: 1 resources: requests: cpu: 500m memory: 2048Mi sidecar: false traceSampling: 1 prometheus: enabled: false hub: docker.io/prom replicaCount: 1 service: annotations: {} nodePort: enabled: false port: 32090 tag: v2.3.1 security: enabled: false image: citadel replicaCount: 1 selfSigned: true servicegraph: enabled: false image: servicegraph ingress: annotations: null enabled: false hosts: - servicegraph.local tls: null prometheusAddr: http://prometheus:9090 replicaCount: 1 service: annotations: {} externalPort: 8088 internalPort: 8088 name: http type: ClusterIP sidecarInjectorWebhook: enableNamespacesByDefault: false enabled: false image: sidecar_injector replicaCount: 1 telemetry-gateway: gatewayName: ingressgateway global: arch: amd64: 2 ppc64le: 2 s390x: 2 configValidation: true controlPlaneSecurityEnabled: false crds: true defaultResources: requests: cpu: 10m disablePolicyChecks: false enableTracing: true hub: gcr.io/istio-release hyperkube: hub: quay.io/coreos tag: v1.7.6_coreos.0 imagePullPolicy: IfNotPresent imagePullSecrets: null k8sIngressHttps: false k8sIngressSelector: ingress meshExpansion: false meshExpansionILB: false mtls: enabled: false oneNamespace: false policyCheckFailOpen: false priorityClassName: \u0026quot;\u0026quot; proxy: accessLogFile: /dev/stdout autoInject: enabled concurrency: 0 discoveryDomain: \u0026quot;\u0026quot; enableCoreDump: false envoyStatsd: enabled: false host: null port: null excludeIPRanges: \u0026quot;\u0026quot; excludeInboundPorts: \u0026quot;\u0026quot; image: proxyv2 includeIPRanges: '*' includeInboundPorts: '*' privileged: false proxyDomain: \u0026quot;\u0026quot; readinessFailureThreshold: 30 readinessInitialDelaySeconds: 1 readinessPeriodSeconds: 2 resources: requests: cpu: 10m stats: prometheusPort: 15090 statusPort: 0 proxy_init: image: proxy_init tag: release-1.0-latest-daily grafanaEnabled: false prometheusEnabled: false tracing: enabled: false ingress: annotations: null enabled: false hosts: - tracing.local tls: null jaeger: hub: docker.io/jaegertracing ingress: annotations: null enabled: false hosts: - jaeger.local tls: null memory: max_traces: 50000 tag: 1.5 ui: port: 16686 provider: jaeger replicaCount: 1 service: annotations: {} externalPort: 9411 internalPort: 9411 name: http type: ClusterIP HOOKS: --- # virtualservices.networking.istio.io # # these CRDs only make sense when pilot is enabled # apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: virtualservices.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install labels: app: istio-pilot spec: group: networking.istio.io names: kind: VirtualService listKind: VirtualServiceList plural: virtualservices singular: virtualservice categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 --- # destinationrules.networking.istio.io apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: destinationrules.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install labels: app: istio-pilot spec: group: networking.istio.io names: kind: DestinationRule listKind: DestinationRuleList plural: destinationrules singular: destinationrule categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 --- # serviceentries.networking.istio.io apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: serviceentries.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install labels: app: istio-pilot spec: group: networking.istio.io names: kind: ServiceEntry listKind: ServiceEntryList plural: serviceentries singular: serviceentry categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 --- # gateways.networking.istio.io apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: gateways.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install \u0026quot;helm.sh/hook-weight\u0026quot;: \u0026quot;-5\u0026quot; labels: app: istio-pilot spec: group: networking.istio.io names: kind: Gateway plural: gateways singular: gateway categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 --- # envoyfilters.networking.istio.io apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: envoyfilters.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install labels: app: istio-pilot spec: group: networking.istio.io names: kind: EnvoyFilter plural: envoyfilters singular: envoyfilter categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 MANIFEST: --- # Source: istio/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: istio namespace: istio-system labels: app: istio chart: istio-1.0.6 release: istio heritage: Tiller data: mesh: |- # Set the following variable to true to disable policy checks by the Mixer. # Note that metrics will still be reported to the Mixer. disablePolicyChecks: false # Set enableTracing to false to disable request tracing. enableTracing: true # Set accessLogFile to empty string to disable access log. accessLogFile: \u0026quot;/dev/stdout\u0026quot; # # Deprecated: mixer is using EDS # Unix Domain Socket through which envoy communicates with NodeAgent SDS to get # key/cert for mTLS. Use secret-mount files instead of SDS if set to empty. sdsUdsPath: \u0026quot;\u0026quot; # How frequently should Envoy fetch key/cert from NodeAgent. sdsRefreshDelay: 15s # defaultConfig: # # TCP connection timeout between Envoy \u0026amp; the application, and between Envoys. connectTimeout: 10s # ### ADVANCED SETTINGS ############# # Where should envoy's configuration be stored in the istio-proxy container configPath: \u0026quot;/etc/istio/proxy\u0026quot; binaryPath: \u0026quot;/usr/local/bin/envoy\u0026quot; # The pseudo service name used for Envoy. serviceCluster: istio-proxy # These settings that determine how long an old Envoy # process should be kept alive after an occasional reload. drainDuration: 45s parentShutdownDuration: 1m0s # # The mode used to redirect inbound connections to Envoy. This setting # has no effect on outbound traffic: iptables REDIRECT is always used for # outbound connections. # If \u0026quot;REDIRECT\u0026quot;, use iptables REDIRECT to NAT and redirect to Envoy. # The \u0026quot;REDIRECT\u0026quot; mode loses source addresses during redirection. # If \u0026quot;TPROXY\u0026quot;, use iptables TPROXY to redirect to Envoy. # The \u0026quot;TPROXY\u0026quot; mode preserves both the source and destination IP # addresses and ports, so that they can be used for advanced filtering # and manipulation. # The \u0026quot;TPROXY\u0026quot; mode also configures the sidecar to run with the # CAP_NET_ADMIN capability, which is required to use TPROXY. #interceptionMode: REDIRECT # # Port where Envoy listens (on local host) for admin commands # You can exec into the istio-proxy container in a pod and # curl the admin port (curl http://localhost:15000/) to obtain # diagnostic information from Envoy. See # https://lyft.github.io/envoy/docs/operations/admin.html # for more details proxyAdminPort: 15000 # # Set concurrency to a specific number to control the number of Proxy worker threads. # If set to 0 (default), then start worker thread for each CPU thread/core. concurrency: 0 # # Zipkin trace collector zipkinAddress: zipkin.istio-system:9411 # # Mutual TLS authentication between sidecars and istio control plane. controlPlaneAuthPolicy: NONE # # Address where istio Pilot service is running discoveryAddress: istio-pilot.istio-system:15007 --- # Source: istio/templates/sidecar-injector-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: istio-sidecar-injector namespace: istio-system labels: app: istio chart: istio-1.0.6 release: istio heritage: Tiller istio: sidecar-injector data: config: |- policy: enabled template: |- initContainers: - name: istio-init image: \u0026quot;gcr.io/istio-release/proxy_init:release-1.0-latest-daily\u0026quot; args: - \u0026quot;-p\u0026quot; - [[ .MeshConfig.ProxyListenPort ]] - \u0026quot;-u\u0026quot; - 1337 - \u0026quot;-m\u0026quot; - [[ annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode ]] - \u0026quot;-i\u0026quot; - \u0026quot;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeOutboundIPRanges` \u0026quot;*\u0026quot; ]]\u0026quot; - \u0026quot;-x\u0026quot; - \u0026quot;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundIPRanges` \u0026quot;\u0026quot; ]]\u0026quot; - \u0026quot;-b\u0026quot; - \u0026quot;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeInboundPorts` (includeInboundPorts .Spec.Containers) ]]\u0026quot; - \u0026quot;-d\u0026quot; - \u0026quot;[[ excludeInboundPort (annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ) (annotation .ObjectMeta `traffic.sidecar.istio.io/excludeInboundPorts` \u0026quot;\u0026quot; ) ]]\u0026quot; imagePullPolicy: IfNotPresent securityContext: capabilities: add: - NET_ADMIN privileged: true restartPolicy: Always containers: - name: istio-proxy image: [[ annotation .ObjectMeta `sidecar.istio.io/proxyImage` \u0026quot;gcr.io/istio-release/proxyv2:release-1.0-latest-daily\u0026quot; ]] ports: - containerPort: 15090 protocol: TCP name: http-envoy-prom args: - proxy - sidecar - --configPath - [[ .ProxyConfig.ConfigPath ]] - --binaryPath - [[ .ProxyConfig.BinaryPath ]] - --serviceCluster [[ if ne \u0026quot;\u0026quot; (index .ObjectMeta.Labels \u0026quot;app\u0026quot;) -]] - [[ index .ObjectMeta.Labels \u0026quot;app\u0026quot; ]] [[ else -]] - \u0026quot;istio-proxy\u0026quot; [[ end -]] - --drainDuration - [[ formatDuration .ProxyConfig.DrainDuration ]] - --parentShutdownDuration - [[ formatDuration .ProxyConfig.ParentShutdownDuration ]] - --discoveryAddress - [[ annotation .ObjectMeta `sidecar.istio.io/discoveryAddress` .ProxyConfig.DiscoveryAddress ]] - --discoveryRefreshDelay - [[ formatDuration .ProxyConfig.DiscoveryRefreshDelay ]] - --zipkinAddress - [[ .ProxyConfig.ZipkinAddress ]] - --connectTimeout - [[ formatDuration .ProxyConfig.ConnectTimeout ]] - --proxyAdminPort - [[ .ProxyConfig.ProxyAdminPort ]] [[ if gt .ProxyConfig.Concurrency 0 -]] - --concurrency - [[ .ProxyConfig.Concurrency ]] [[ end -]] - --controlPlaneAuthPolicy - [[ annotation .ObjectMeta `sidecar.istio.io/controlPlaneAuthPolicy` .ProxyConfig.ControlPlaneAuthPolicy ]] [[- if (ne (annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ) \u0026quot;0\u0026quot;) ]] - --statusPort - [[ annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ]] - --applicationPorts - \u0026quot;[[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/applicationPorts` (applicationPorts .Spec.Containers) ]]\u0026quot; [[- end ]] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: ISTIO_META_INTERCEPTION_MODE value: [[ or (index .ObjectMeta.Annotations \u0026quot;sidecar.istio.io/interceptionMode\u0026quot;) .ProxyConfig.InterceptionMode.String ]] [[ if .ObjectMeta.Annotations ]] - name: ISTIO_METAJSON_ANNOTATIONS value: | [[ toJson .ObjectMeta.Annotations ]] [[ end ]] [[ if .ObjectMeta.Labels ]] - name: ISTIO_METAJSON_LABELS value: | [[ toJson .ObjectMeta.Labels ]] [[ end ]] imagePullPolicy: IfNotPresent [[ if (ne (annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ) \u0026quot;0\u0026quot;) ]] readinessProbe: httpGet: path: /healthz/ready port: [[ annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ]] initialDelaySeconds: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/initialDelaySeconds` 1 ]] periodSeconds: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/periodSeconds` 2 ]] failureThreshold: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/failureThreshold` 30 ]] [[ end -]]securityContext: readOnlyRootFilesystem: true [[ if eq (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) \u0026quot;TPROXY\u0026quot; -]] capabilities: add: - NET_ADMIN runAsGroup: 1337 [[ else -]] runAsUser: 1337 [[ end -]] restartPolicy: Always resources: [[ if (isset .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU`) -]] requests: cpu: \u0026quot;[[ index .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU` ]]\u0026quot; memory: \u0026quot;[[ index .ObjectMeta.Annotations `sidecar.istio.io/proxyMemory` ]]\u0026quot; [[ else -]] requests: cpu: 10m [[ end -]] volumeMounts: - mountPath: /etc/istio/proxy name: istio-envoy - mountPath: /etc/certs/ name: istio-certs readOnly: true volumes: - emptyDir: medium: Memory name: istio-envoy - name: istio-certs secret: optional: true [[ if eq .Spec.ServiceAccountName \u0026quot;\u0026quot; -]] secretName: istio.default [[ else -]] secretName: [[ printf \u0026quot;istio.%s\u0026quot; .Spec.ServiceAccountName ]] [[ end -]] --- # Source: istio/charts/pilot/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: istio-pilot-service-account namespace: istio-system labels: app: istio-pilot chart: pilot-1.0.6 heritage: Tiller release: istio --- # Source: istio/charts/pilot/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: istio-pilot-istio-system labels: app: istio-pilot chart: pilot-1.0.6 heritage: Tiller release: istio rules: - apiGroups: [\u0026quot;config.istio.io\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;rbac.istio.io\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;] - apiGroups: [\u0026quot;networking.istio.io\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;authentication.istio.io\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;apiextensions.k8s.io\u0026quot;] resources: [\u0026quot;customresourcedefinitions\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;extensions\u0026quot;] resources: [\u0026quot;thirdpartyresources\u0026quot;, \u0026quot;thirdpartyresources.extensions\u0026quot;, \u0026quot;ingresses\u0026quot;, \u0026quot;ingresses/status\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;configmaps\u0026quot;] verbs: [\u0026quot;create\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;, \u0026quot;pods\u0026quot;, \u0026quot;services\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;namespaces\u0026quot;, \u0026quot;nodes\u0026quot;, \u0026quot;secrets\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] --- # Source: istio/charts/pilot/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: istio-pilot-istio-system labels: app: istio-pilot chart: pilot-1.0.6 heritage: Tiller release: istio roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: istio-pilot-istio-system subjects: - kind: ServiceAccount name: istio-pilot-service-account namespace: istio-system --- # Source: istio/charts/pilot/templates/service.yaml apiVersion: v1 kind: Service metadata: name: istio-pilot namespace: istio-system labels: app: istio-pilot chart: pilot-1.0.6 release: istio heritage: Tiller spec: ports: - port: 15010 name: grpc-xds # direct - port: 15011 name: https-xds # mTLS - port: 8080 name: http-legacy-discovery # direct - port: 9093 name: http-monitoring selector: istio: pilot --- # Source: istio/charts/pilot/templates/deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: istio-pilot namespace: istio-system # TODO: default template doesn't have this, which one is right ? labels: app: istio-pilot chart: pilot-1.0.6 release: istio heritage: Tiller istio: pilot annotations: checksum/config-volume: f8da08b6b8c170dde721efd680270b2901e750d4aa186ebb6c22bef5b78a43f9 spec: replicas: 1 template: metadata: labels: istio: pilot app: pilot annotations: sidecar.istio.io/inject: \u0026quot;false\u0026quot; scheduler.alpha.kubernetes.io/critical-pod: \u0026quot;\u0026quot; spec: serviceAccountName: istio-pilot-service-account containers: - name: discovery image: \u0026quot;gcr.io/istio-release/pilot:release-1.0-latest-daily\u0026quot; imagePullPolicy: IfNotPresent args: - \u0026quot;discovery\u0026quot; - --secureGrpcAddr - \u0026quot;:15011\u0026quot; ports: - containerPort: 8080 - containerPort: 15010 - containerPort: 15011 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 30 timeoutSeconds: 5 env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: PILOT_CACHE_SQUASH value: \u0026quot;5\u0026quot; - name: GODEBUG value: \u0026quot;gctrace=2\u0026quot; - name: PILOT_PUSH_THROTTLE_COUNT value: \u0026quot;100\u0026quot; - name: PILOT_TRACE_SAMPLING value: \u0026quot;1\u0026quot; resources: requests: cpu: 500m memory: 2048Mi volumeMounts: - name: config-volume mountPath: /etc/istio/config - name: istio-certs mountPath: /etc/certs readOnly: true volumes: - name: config-volume configMap: name: istio - name: istio-certs secret: secretName: istio.istio-pilot-service-account optional: true affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 - ppc64le - s390x preferredDuringSchedulingIgnoredDuringExecution: - weight: 2 preference: matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 - weight: 2 preference: matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - ppc64le - weight: 2 preference: matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - s390x --- # Source: istio/templates/crds.yaml # # these CRDs only make sense when security is enabled # # # --- # Source: istio/charts/pilot/templates/gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: istio-autogenerated-k8s-ingress namespace: istio-system spec: selector: istio: ingress servers: - port: number: 80 protocol: HTTP2 name: http hosts: - \u0026quot;*\u0026quot; --- # Source: istio/charts/pilot/templates/autoscale.yaml apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: istio-pilot namespace: istio-system spec: maxReplicas: 5 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1beta1 kind: Deployment name: istio-pilot metrics: - type: Resource resource: name: cpu targetAverageUtilization: 80 LAST DEPLOYED: Sat Mar 30 06:30:03 2019 NAMESPACE: istio-system STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/ConfigMap NAME DATA AGE istio 1 4s istio-sidecar-injector 1 4s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-pilot ClusterIP 10.106.159.203 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 3s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE istio-pilot 1 1 1 0 3s ==\u0026gt; v1alpha3/Gateway NAME AGE istio-autogenerated-k8s-ingress 2s ==\u0026gt; v2beta1/HorizontalPodAutoscaler NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE istio-pilot Deployment/istio-pilot \u0026lt;unknown\u0026gt;/80% 1 5 0 2s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE istio-pilot-754ccc994f-t7wg2 0/1 ContainerCreating 0 3s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE istio-pilot-service-account 1 5s ==\u0026gt; v1beta1/ClusterRole NAME AGE istio-pilot-istio-system 4s ==\u0026gt; v1beta1/ClusterRoleBinding NAME AGE istio-pilot-istio-system 4s  ","date":1522362783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522362783,"objectID":"83441f2c09ede7e7eb3f86ac3332b0a3","permalink":"https://wubigo.com/post/helm-chart-istio/","publishdate":"2018-03-30T06:33:03+08:00","relpermalink":"/post/helm-chart-istio/","section":"post","summary":"helm install --debug install/kubernetes/helm/istio --name istio --namespace istio-system --set security.enabled=false --set ingress.enabled=false --set gateways.istio-ingressgateway.enabled=false --set gateways.istio-egressgateway.enabled=false --set galley.enabled=false --set mixer.enabled=false --set prometheus.enabled=false --set global.proxy.envoyStatsd.enabled=false --set pilot.sidecar=false --set sidecarInjectorWebhook.enabled=false [debug] Created tunnel using local port: '44471' [debug] SERVER: \u0026quot;127.0.0.1:44471\u0026quot; [debug] Original chart version: \u0026quot;\u0026quot; [debug] CHART PATH: /home/bigo/istio/install/kubernetes/helm/istio NAME: istio REVISION: 1 RELEASED: Sat Mar 30 06:30:03 2019 CHART: istio-1.0.6 USER-SUPPLIED VALUES: galley: enabled: false gateways: istio-egressgateway: enabled: false istio-ingressgateway: enabled: false global: proxy: envoyStatsd: enabled: false ingress: enabled: false mixer: enabled: false pilot: sidecar: false prometheus: enabled: false security: enabled: false sidecarInjectorWebhook: enabled: false COMPUTED VALUES: certmanager: enabled: false hub: quay.","tags":[],"title":"Helm Chart Istio","type":"post"},{"authors":null,"categories":[],"content":" for developers building applications to run in Kubernetes clusters, and for DevOps staff troubleshooting Kubernetes applications. Features include:\n View your clusters in an explorer tree view, and drill into workloads, services, pods and nodes. Browse Helm repos and install charts into your Kubernetes cluster. Intellisense for Kubernetes resources and Helm charts and templates. Edit Kubernetes resource manifests and apply them to your cluster. Build and run containers in your cluster from Dockerfiles in your project. View diffs of a resource\u0026rsquo;s current state against the resource manifest in your Git repo Easily check out the Git commit corresponding to a deployed application. Run commands or start a shell within your application\u0026rsquo;s pods. Get or follow logs and events from your clusters. Forward local ports to your application\u0026rsquo;s pods. Create Helm charts using scaffolding and snippets. Bootstrap applications using Draft, and rapidly deploy and debug them to speed up the development loop.  Dependencies docker if you plan to use the extension to build applications rather than only browse.\n kubectl docker helm draft  ","date":1521846156,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521846156,"objectID":"358c4a7d223f3c18c026e34b138e2225","permalink":"https://wubigo.com/post/k8s-ide-tool/","publishdate":"2018-03-24T07:02:36+08:00","relpermalink":"/post/k8s-ide-tool/","section":"post","summary":"for developers building applications to run in Kubernetes clusters, and for DevOps staff troubleshooting Kubernetes applications. Features include:\n View your clusters in an explorer tree view, and drill into workloads, services, pods and nodes. Browse Helm repos and install charts into your Kubernetes cluster. Intellisense for Kubernetes resources and Helm charts and templates. Edit Kubernetes resource manifests and apply them to your cluster. Build and run containers in your cluster from Dockerfiles in your project.","tags":["K8S","IDE","APP"],"title":"K8s IDE Tool: code extension","type":"post"},{"authors":null,"categories":[],"content":" go模块代理 https://github.com/goproxy/goproxy.cn\n$go version go version go1.13.12 linux/amd64  go env -w GO111MODULE=on go env -w GOPROXY=https://goproxy.cn,direct   重置goproxy\ngo env -w GOPROXY   go doc https://golang.google.cn\nproxy  从 Github 的代码库 clone\ngo get -u github.com/golang/text mv $GOPATH/src/github.com/golang/text $GOPATH/src/golang.org/x/text go get -u github.com/golang/crypto mv $GOPATH/src/github.com/golang/crypto $GOPATH/src/golang.org/x/crypto  设置 GOPROXY 环境变量配置代理\n  例如：GOPROXY=https://goproxy.io\nhttps://github.com/northbright/Notes/blob/master/Golang/china/get-golang-packages-on-golang-org-in-china.md\nhttps://gocn.vip/article/1678\n配置代理  系统代理 GIT代理  ","date":1521700441,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521700441,"objectID":"73020612286c6fd740fe3aad2a0d3ed4","permalink":"https://wubigo.com/post/lang-go-proxy/","publishdate":"2018-03-22T14:34:01+08:00","relpermalink":"/post/lang-go-proxy/","section":"post","summary":" go模块代理 https://github.com/goproxy/goproxy.cn\n$go version go version go1.13.12 linux/amd64  go env -w GO111MODULE=on go env -w GOPROXY=https://goproxy.cn,direct   重置goproxy\ngo env -w GOPROXY   go doc https://golang.google.cn\nproxy  从 Github 的代码库 clone\ngo get -u github.com/golang/text mv $GOPATH/src/github.com/golang/text $GOPATH/src/golang.org/x/text go get -u github.com/golang/crypto mv $GOPATH/src/github.com/golang/crypto $GOPATH/src/golang.org/x/crypto  设置 GOPROXY 环境变量配置代理\n  例如：GOPROXY=https://goproxy.io\nhttps://github.com/northbright/Notes/blob/master/Golang/china/get-golang-packages-on-golang-org-in-china.md\nhttps://gocn.vip/article/1678\n配置代理  系统代理 GIT代理  ","tags":["LANG","GO"],"title":"Go穿越Firewall","type":"post"},{"authors":null,"categories":[],"content":" v3.11.0-\u0026gt;k8s 1.11\nopenshift all-in-one curl https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz tar zxf openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz cd openshift export PATH=\u0026quot;$(pwd)\u0026quot;:$PATH sudo ./openshift start master  oc setup export KUBECONFIG=\u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/admin.kubeconfig export CURL_CA_BUNDLE=\u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/ca.crt sudo chmod +r \u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/admin.kubeconfig openshift complition bash \u0026gt; /usr/share/bash-completion/completions/openshift.complition.sh  master and node configuration after installation /etc/origin/master/master-config.yaml\nidentityProviders: - name: my_allow_provider challenge: true login: true provider: apiVersion: v1 kind: AllowAllPasswordIdentityProvider corsAllowedOrigins  Identity Providers The OpenShift master includes a built-in OAuth server the Deny All identity provider is used by default, which denies access for all user names and passwords. To allow access, you must choose a different identity provider and configure the master configuration file appropriately (located at /etc/openshift/master/master-config.yaml by default).\noc login -u system:admin  ","date":1521542857,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521542857,"objectID":"f826095e87e398e9c2bf254e188605a6","permalink":"https://wubigo.com/post/k8s-openshift/","publishdate":"2018-03-20T18:47:37+08:00","relpermalink":"/post/k8s-openshift/","section":"post","summary":"v3.11.0-\u0026gt;k8s 1.11\nopenshift all-in-one curl https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz tar zxf openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz cd openshift export PATH=\u0026quot;$(pwd)\u0026quot;:$PATH sudo ./openshift start master  oc setup export KUBECONFIG=\u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/admin.kubeconfig export CURL_CA_BUNDLE=\u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/ca.crt sudo chmod +r \u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/admin.kubeconfig openshift complition bash \u0026gt; /usr/share/bash-completion/completions/openshift.complition.sh  master and node configuration after installation /etc/origin/master/master-config.yaml\nidentityProviders: - name: my_allow_provider challenge: true login: true provider: apiVersion: v1 kind: AllowAllPasswordIdentityProvider corsAllowedOrigins  Identity Providers The OpenShift master includes a built-in OAuth server the Deny All identity provider is used by default, which denies access for all user names and passwords.","tags":["K8S","PAAS"],"title":"K8s Openshift","type":"post"},{"authors":null,"categories":null,"content":" JDK Version java -version openjdk version \u0026quot;1.8.0_151\u0026quot; OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12) OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)  Verify that the target server is configured to serve SSL https://www.ssllabs.com/ssltest/\nConnecting to SSL services https://confluence.atlassian.com/kb/unable-to-connect-to-ssl-services-due-to-pkix-path-building-failed-779355358.html\nIf you are getting an exception due to \u0026ldquo;Illegal key size\u0026rdquo; and you are using Sun’s JDK, you need to install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files. See the following links for more information:\n Java 6 JCE\n Java 7 JCE\n Java 8 JCE\n  Connecting to SSL Server from eclipse Append the following to use keystore in eclipse tomcat server\n-Djavax.net.ssl.trustStore=\u0026quot;C:\\Program Files\\Java\\jdk1.8.0_121\\jre\\lib\\ security\u0026quot;  check certificate name by alias then remove from keystore files $keytool -list -v -keystore cacerts | grep 'Alias name:' $sudo keytool -delete -alias wubigo -keystore cacerts  ","date":1520380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520380800,"objectID":"148bab8c32d2df718a63a191be71e3f6","permalink":"https://wubigo.com/post/2018-03-07-connectingtosslservices/","publishdate":"2018-03-07T00:00:00Z","relpermalink":"/post/2018-03-07-connectingtosslservices/","section":"post","summary":"JDK Version java -version openjdk version \u0026quot;1.8.0_151\u0026quot; OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12) OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)  Verify that the target server is configured to serve SSL https://www.ssllabs.com/ssltest/\nConnecting to SSL services https://confluence.atlassian.com/kb/unable-to-connect-to-ssl-services-due-to-pkix-path-building-failed-779355358.html\nIf you are getting an exception due to \u0026ldquo;Illegal key size\u0026rdquo; and you are using Sun’s JDK, you need to install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files. See the following links for more information:","tags":null,"title":"Connecting to SSL services","type":"post"},{"authors":null,"categories":["LIFE"],"content":" 导语：本文最早刊登于《哈佛商业评论》，是其历史最佳文章之一。 作者彼得·德鲁克是现代管理学之父，其著作影响了数代追求创新以及 最佳管理实践的学者和企业家们，各类商业管理课程也都深受彼得·德鲁克思想的影响。 本文后被收录在德鲁克的著作《21 世纪的管理挑战》中。非常值得一读再读。 作者 | 彼得·德鲁克 来源 | 《21世纪的管理挑战》第 6 章 自我管理（节选） 字数 | 7150（阅读约 10 分钟）  我们生活的时代充满着前所未有的机会：如果你有雄心，又不乏智慧，那么不管你从何处起步，你都可以沿着自己所选择的道路登上事业的顶峰。\n不过，有了机会，也就有了责任。今天的公司并不怎么管员工的职业发展；实际上，知识工作者必须成为自己的首席执行官。你应该在公司中开辟自己的天地，知道何时改变发展道路，并在可能长达 50 年的职业生涯中不断努力、干出实绩。\n要做好这些事情，你首先要对自己有深刻的认识——不仅清楚自己的优点和缺点，也知道自己是怎样学习新知识和与别人共事的，并且还明白自己的价值观是什么、自己又能在哪些方面做出最大贡献。\n因为只有当所有工作都从自己的长处着眼，你才能真正做到卓尔不群。\n历史上的伟人——拿破仑、达芬奇、莫扎特——都很善于自我管理。这在很大程度上也是他们成为伟人的原因。不过，他们属于不可多得的奇才，不但有着不同于常人的天资，而且天生就会管理自己，因而才取得了不同于常人的成就。\n而我们当中的大多数人，甚至包括那些还算有点天赋的人，都不得不通过学习来掌握自我管理的技巧。\n我们必须学会自我发展，必须知道把自己放在什么样的位置上，才能做出最大的贡献，而且还必须在长达 50 年的职业生涯中保持着高度的警觉和投入。\n我们的长处是什么 多数人都以为他们知道自己擅长什么。其实不然，更多的情况是，人们只知道自己不擅长什么——即便是在这一点上，人们也往往认识不清。\n然而，一个人要有所作为，只能靠发挥自己的长处，而如果从事自己不太擅长的工作是无法取得成就的，更不用说那些自己根本干不了的事情了。\n我们需要知己所长，才能知己所属。\n要发现自己的长处，唯一途径就是回馈分析法（feedback analysis）。每当做出重要决定或采取重要行动时，你都可以事先记录下自己对结果的预期。9 到 12 个月后，再将实际结果与自己的预期比较。\n我本人采用这种方法已有 15 到 20 年了，而每次使用都有意外的收获。\n我们只要持之以恒地运用这个简单的方法，就能在较短的时间内，发现自己的长处。在采用这种方法之后，你就能知道，自己正在做（或没有做）的哪些事情会让你的长处无法发挥出来。\n同时，你也将看到自己在哪些方面能力不是特别强。最后，你还将了解到自己在哪些方面完全不擅长，做不出成绩来。\n根据回馈分析的启示，你需要在几方面采取行动。\n首先最重要的是，专注于你的长处，把自己放到那些能发挥长处的地方。\n其次，加强你的长处。回馈分析会迅速地显示，你在哪些方面需要改善自己的技能或学习新技能。它还将显示你在知识上的差距——这些差距通常都可以弥补。\n第三，发现任何由于恃才傲物而造成的偏见和无知，并且加以克服。有太多的人，尤其是那些术业有专攻的人，往往对其他领域的知识不屑一顾，或者认为聪明的头脑就可取代知识。\n比如，很多一流的工程师遇上与人相关的事就束手无策，他们还以此为荣——因为他们觉得，对条理清晰的工程师头脑来说，人太混乱无序了。与此形成鲜明对照的是，人力资源方面的专业人员常常以他们连基本的会计知识或数量分析都一无所知而自傲。\n不过，人们要是对这样的无知还沾沾自喜的话，那无异于自取灭亡。其实，要让自己的长处得到充分发挥，你就应该努力学习新技能、汲取新知识。\n另外一点也同样重要——纠正你的不良习惯。所谓不良习惯，是指那些会影响你的工作成效和工作表现的事情。这样的习惯能很快地在回馈中反映出来。\n与此同时，回馈还会反映出哪些问题是由缺乏礼貌造成的。礼貌是一个组织的润滑剂。\n礼貌，其实也很简单。无非是说声「请」和「谢谢」，记住别人的名字，或问候对方家人这样的小事。许多聪明人，尤其是聪明的年轻人，没有意识到这一点。\n如果回馈分析表明某个人只要一遇到需要别人合作的事就屡屡失败，那么很可能就意味着这个人的举止不大得体——也就是缺乏礼貌。\n把预期和实际结果进行比较，也会发现自己不能做什么。我们每个人都有许多一窍不通、毫无天分的领域，在这些领域我们甚至连平庸的水平都达不到。人们，尤其是知识工作者，就不应该试图去完成这些领域的工作和任务。他们应该尽量少把精力浪费在那些不能胜任的领域上，因为从无能到平庸要比从一流到卓越需要人们付出多得多的努力。\n我们的工作方式是怎样的 很少有人知道自己平时是怎样把事情给做成的。\n实际上，我们当中的大多数人甚至不知道，不同人有着不同的工作方式和表现。\n许多人不是以他们习惯的方式工作，这当然就容易造成无所作为。\n对于知识工作者来说，「我的工作方式是怎样的？」可能比「我的长处是什么？」这个问题更加重要。\n同一个人的长处一样，一个人的工作方式也是独一无二的，这由人的个性决定。\n通常，几个常见的个性特征就决定了一个人的工作方式。\n我属于读者型，还是听者型？\n首先，你要搞清楚的是，你是读者型（习惯阅读信息）还是听者型（习惯听取信息）的人。绝大多数人甚至都不知道还有读者型和听者型之说，而且很少有人既是读者型又是听者型。知道自己属于哪种类型的人更少。\n没有几个听者型的人可以通过努力变成合格的读者型——不管是主动还是被动的努力，反之亦然。不了解你的工作方式不可能发挥才干或取得成就。\n我们如何学习 要了解一个人的工作方式，需要弄清的第二点是，他是如何学习的。\n许多一流的笔杆子都不是好学生——温斯顿·邱吉尔就是一例。\n有关这个问题的解释是，笔头好的人一般不靠听和读来学习，而靠写来学习，这已成了一种规律。学校不让他们以这种方式学习，所以他们的成绩总是很糟糕。\n实际上，学习大概有六七种不同的方式。\n像邱吉尔这样的人靠写来学习，还有些人以详尽的笔记来学习。有些人在实干中学习，另一些人通过听自己讲话学习。\n我属于读者型还是听者型？我如何学习？这是你首先要问自己的问题。\n但光这些问题显然不够。要想做好自我管理，你还需要问这样的问题：我能与别人合作得好吗？还是喜欢单枪匹马？如果你确实有与别人进行合作的能力，你还得问问这个问题：我在怎样的关系下与他人共事？\n有些人最适合当部属。\n 二战时期美国的大英雄乔治·巴顿将军是一个很好的例子。 巴顿是美军的一名高级将领。然而，当有人提议他担任独立指挥官时， 美国陆军参谋长、可能也是美国历史上最成功的伯乐， 乔治·马歇尔将军说： 「巴顿是美国陆军造就的最优秀的部下，但是，他会成为最差劲的司令官。」  一些人作为团队成员工作最出色。另一些人单独工作最出色。一些人当教练和导师特别有天赋，另一些人却没能力做导师。\n另一个关键的问题是，我如何才能取得成果——是作为决策者还是作为顾问？许多人做顾问时的表现会很出色，但是不能够承担决策的负担和压力。与此相反，也有许多人需要顾问来迫使他们思考，随后他们才能做出决定，接着迅速、自信和大胆地执行决定。\n顺便说一下，一个组织的二号人物在提升到一号职位时常常失败，也正是因为这个原因。最高职位需要一个决策者，而一个强势的决策者常常把其信赖的人放在二号位置，当他的顾问。\n其他有助于认识自我的重要问题包括：\n 我是在压力下表现出色，还是适应一种按部就班、可预测的工作环境？ 我是在一个大公司还是在一个小公司中工作表现最佳？  我不止一次地看到有些人在大公司中十分成功，换到小公司中则很不顺利。\n反过来也是如此。\n下面这个结论值得我们反复强调：不要试图改变自我，因为这样你不大可能成功。但是，你应该努力改进你的工作方式。另外，不要从事你干不了或干不好的工作。\n我们的价值观是什么 要能够自我管理，你最后不得不问的问题是：我的价值观是什么？这不是一个有关伦理道德的问题。道德准则对每一个人都一样。要对一个人的道德进行测试，方法很简单，我把它称为「镜子测试」。\n 20 世纪初，德国驻英国大使是当时在伦敦所有大国中最受尊重的一位外交官。 显然，他命中注定会承担重任，即使不当本国的总理，至少也要当外交部长。 然而，在 1906 年，他突然辞职，不愿主持外交使团为英国国王爱德华七世举行的晚宴。 这位国王是一个臭名昭著的色鬼，并且明确表示他想出席什么样的晚宴。 据有关报道，这位德国大使曾说： 「我不想早晨刮脸时在镜子里看到一个皮条客。」  这就是镜子测试。\n我们所尊从的伦理道德要求你问自己：我每天早晨在镜子里想看到一个什么样的人？在一个组织或一种情形下合乎道德的行为，在另一个组织或另一种情形下也是合乎道德的。但是，道德只是价值体系的一部分——尤其对于一个组织的价值体系来说。\n如果一个组织的价值体系不为自己所接受或者与自己的价值观不相容，人们就会备感沮丧，工作效力低下。\n一个人的工作方式和他的长处很少发生冲突，相反，两者能产生互补。但是，一个人的价值观有时会与他的长处发生冲突。\n我们属于何处 少数人很早就知道他们属于何处。\n比如，数学家、音乐家和厨师，通常在四五岁的时候就知道自己会成为数学家、音乐家和厨师了。物理学家通常在十几岁甚至更早的时候就决定了自己的工作生涯。\n但是，大多数人，尤其是很有天赋的人，至少要过了二十五六岁才知道他们将身属何处。\n然而，到这个时候，他们应该知道上面所谈的三个问题的答案：\n- 我的长处是什么？ - 我的工作方式是怎样的？ - 我的价值观是什么？  随后，他们就能够并且应该决定自己该向何处投入精力。或者，他们应该能够决定自己不属于何处。\n已经知道自己在大公司里干不好的人，应该学会拒绝在一个大公司中任职。已经知道自己不适合担任决策者的人，应该学会拒绝做决策工作。\n成功的事业不是预先规划的，而是在人们知道了自己的长处、工作方式和价值观后，准备把握机遇时水到渠成的。知道自己属于何处，可使一个勤奋、有能力但原本表现平平的普通人，变成出类拔萃的工作者。\n我该做什么贡献 综观人类的发展史，绝大多数人永远都不需要提出这样一个问题：我该做出什么贡献？因为他们该做出什么贡献是由别人告知的，他们的任务或是由工作本身决定的（例如农民或工匠的任务），或是由主人决定的（例如佣人的任务）。\n对于知识工作者来说，他们不得不提出一个以前从来没有提出过的问题：我的贡献应该是什么？\n要回答这个问题，他们必须考虑三个不同的因素：\n- 当前形势的要求是什么？ - 鉴于我的长处、我的工作方式以及我的价值观，我怎样才能对需要完成的任务做出最大贡献？ - 最后，必须取得什么结果才能产生重要影响？  一般来说，一项计划的时间跨度如果超过了 18 个月，就很难做到明确和具体。\n因此，在多数情况下我们应该提出的问题是：\n- 我在哪些方面能取得将在今后一年半内见效的结果？ - 如何取得这样的结果？  回答这个问题时必须对几个方面进行权衡。\n首先，这些结果应该是比较难实现的，要有「张力」 （stretching）。但这些结果也应该是能力所及的。\n其次，这些结果应该富有意义，要能够产生一定影响。\n最后，结果应该明显可见，如果可能的话，还应当能够衡量。确定了要实现的结果之后，接着就可以制订行动方针：做什么，从何处着手，如何开始，目标是什么，在多长时间内完成。\n对人际关系负责 除了少数伟大的艺术家、科学家和运动员，很少有人是靠自己单枪匹马而取得成果的。不管是组织成员还是个体职业者，大多数人都要与别人进行合作，并且是有效的合作。要实现自我管理，你需要对自己的人际关系负起责任。这包括两部分内容。\n首先要接受别人是和你一样的个体这个事实。\n他们有自己的长处，自己的做事方式和价值观。因此，要想卓有成效，你就必须知道共事者的长处、工作方式和价值观。\n这个道理听起来让人很容易明白，但是没有几个人真正会去注意。\n一个习惯于写报告的人就是个典型的例子——他在第一份工作时就培养起写报告的习惯，因为他的老板是一个读者型的人，而即使下一个老板是个听者型，此人也会继续写着那肯定没有任何结果的报告。这位老板因此肯定会认为这个员工愚蠢、无能、懒惰，肯定干不好工作。但是，如果这个员工事先研究过新老板的情况，并分析过这位老板的工作方式，这种情况本来可以避免。\n老板既不是组织结构图上的一个头衔，也不是一个「职能」。他们是有个性的人，他们有权以自己最得心应手的方式来工作。与他们共事的人有责任观察他们，了解他们的工作方式，并做出相应的自我调整，去适应老板最有效的工作方式。\n事实上，这就是「管理」上司的秘诀\n这种方法适用于所有与你共事的人。至于工作方式，人各有别。提高效力的第一个秘诀是了解跟你合作和你要依赖的人，以利用他们的长处、工作方式和价值观。工作关系应当既以工作为基础，也以人为基础。\n人际关系责任的第二部分内容是沟通责任。\n在我或是其他人开始给一个组织做咨询时，我们听到的第一件事都与个性冲突有关。其中大部分冲突都是因为：人们不知道别人在做什么，他们又是采取怎样的工作方式，专注于做出什么样的贡献以及期望得到怎样的结果。而这些人不了解情况的原因是，他们没有去问，结果也就不得而知。\n即使一些人懂得负起人际关系责任的重要性，他们和同事的交流也往往不够。他们总是有所顾虑，怕别人把自己看成是一个冒昧、愚蠢、爱打听的人。他们错了。\n因为我们看到，每当有人找到他的同事说「这是我所擅长的工作。这是我的做事方式。这是我的价值观。这是我计划做出的贡献和应当取得的成果」，这个人总会得到如此回答：「这太有帮助了，可你为什么不早点告诉我？」\n如果一个人继续问道：「那么，关于你的长处、你的工作方式、你的价值观以及你计划做出的贡献，我需要知道什么？」他也会得到类似的答复——据我的经验，无一例外。\n事实上，知识工作者应该向与他们共事的每一个人，不管是下属、上司、同事还是团队成员，都发出这样的疑问。\n组织已不再建立在强权的基础上，而是建立在信任的基础上。人与人之间相互信任，不一定意味着他们彼此喜欢对方，而是意味着彼此了解。因此，人们绝对有必要对自己的人际关系负责。\n这是一种义务。不管一个人是公司的一名成员，还是公司的顾问、供应商或经销商，他都需要对他的所有共事者负起这种责任。所谓共事者，是指在工作上他所依赖的同事以及依赖他的同事。\n管理后半生 我们听到了许多有关经理人中年危机的谈论，「厌倦」这个词在其中频频出现。\n45 岁时，多数经理人的职业生涯达到了顶峰。但是他们学不到新东西，也没有什么新贡献，从工作中得不到挑战，因而也谈不上满足感。在他们面前，还有 20 到 25 年的职业道路要走。这就是为什么经理人在进行自我管理后，越来越多地开始发展第二职业的原因。\n发展第二职业有三种方式：\n 第一种是完全投身于新工作。  这常常只需要从一种组织转到另一种组织。\n例如，一家大公司某事业部的会计师成为一家中型医院的财务总监。\n但是也有越来越多的人转入完全不同的职业。还有许多人在第一份职业中取得的成功有限，于是改行从事第二职业。这样的人有很多技能，他们也知道该如何工作。\n为后半生做准备的第二种方式是， - 发展一个平行的职业。\n许多人的第一职业十分成功，他们还会继续从事原有工作。除此之外，他们会开创一项平行的工作，通常是在非营利机构。\n 最后一种方法是社会创业。  社会创业者通常是在第一职业中非常成功的人士。他们都热爱自己的工作，但是这种工作对他们已经不再有挑战性。\n他们虽然继续做着原来的工作，但在这份工作上花的时间越来越少。他们同时开创了另一项事业，通常是非营利性活动。\n管理好自己后半生的人可能总是少数。多数人可能数着年头一年一年过去，直至退休。但正是这些少数人，这些把漫长的工作寿命看做是自己和社会之机会的人，才会成为领袖和模范。\n管理好后半生有一个先决条件：你必须早在你进入后半生之前就开始行动。当 30 年前人们首次认识到工作寿命正在迅速延长时，许多观察家（包括我自己）认为，退休人员会越来越多地成为非营利机构的志愿者。可是，这种情况并没有发生。一个人如果不在 40 岁之前就开始做志愿者，那他 60 岁之后也不会去做志愿者。\n同样，我认识的所有社会创业者，都是早在他们原有的事业达到顶峰之前就开始从事他们的第二事业。\n发展第二兴趣还有一个原因：任何人都不能指望在生活或工作中很长时间都不遭遇严重挫折。在这样的时刻，第二兴趣——不仅仅是业余爱好——可能发挥重要作用。\n在一个崇尚成功的社会里，拥有各种选择变得越来越重要。在知识社会里，我们期望每一个人都能取得成功。这显然是不可能的。\n对许多人来说，能避免失败就行。可是有成功的地方，就会有失败。因此，有一个能够让人们做出贡献、发挥影响力或成为「大人物」的领域，这不仅对个人十分重要，对个人的家庭也同样重要。\n这意味着人们需要找到一个能够有机会成为领袖、受到尊重、取得成功的第二领域——可能是第二份职业。\n自我管理中面临的挑战看上去比较明显。但自我管理需要我们做出以前从未做过的事情。自我管理需要每一个知识工作者在思想和行动上都要成为自己的首席执行官。\n更进一步来看，这样的转变——从一切听从别人吩咐的体力劳动者到不得不自我管理的知识工作者——也使得社会结构发生了深刻变化。\n历史上每一个社会，甚至是个人主义倾向最强的社会，都认为两件事情理所当然（即使只是下意识的）：\n- 第一，组织比员工更长寿； - 第二，大多数人从不挪地方。  如今，情况恰恰相反。知识工作者的寿命超过了组织寿命，而且他们来去自如。\n于是，人们对自我管理的需要在人类事务中掀起了一场革命。\n","date":1520380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520380800,"objectID":"83ad231316907510eb063abd4a52482f","permalink":"https://wubigo.com/post/2018-03-17-%E8%87%AA%E6%88%91%E7%AE%A1%E7%90%86/","publishdate":"2018-03-07T00:00:00Z","relpermalink":"/post/2018-03-17-%E8%87%AA%E6%88%91%E7%AE%A1%E7%90%86/","section":"post","summary":"导语：本文最早刊登于《哈佛商业评论》，是其历史最佳文章之一。 作者彼得·德鲁克是现代管理学之父，其著作影响了数代追求创新以及 最佳管理实践的学者和企业家们，各类商业管理课程也都深受彼得·德鲁克思想的影响。 本文后被收录在德鲁克的著作《21 世纪的管理挑战》中。非常值得一读再读。 作者 | 彼得·德鲁克 来源 | 《21世纪的管理挑战》第 6 章 自我管理（节选） 字数 | 7150（阅读约 10 分钟）  我们生活的时代充满着前所未有的机会：如果你有雄心，又不乏智慧，那么不管你从何处起步，你都可以沿着自己所选择的道路登上事业的顶峰。\n不过，有了机会，也就有了责任。今天的公司并不怎么管员工的职业发展；实际上，知识工作者必须成为自己的首席执行官。你应该在公司中开辟自己的天地，知道何时改变发展道路，并在可能长达 50 年的职业生涯中不断努力、干出实绩。\n要做好这些事情，你首先要对自己有深刻的认识——不仅清楚自己的优点和缺点，也知道自己是怎样学习新知识和与别人共事的，并且还明白自己的价值观是什么、自己又能在哪些方面做出最大贡献。\n因为只有当所有工作都从自己的长处着眼，你才能真正做到卓尔不群。\n历史上的伟人——拿破仑、达芬奇、莫扎特——都很善于自我管理。这在很大程度上也是他们成为伟人的原因。不过，他们属于不可多得的奇才，不但有着不同于常人的天资，而且天生就会管理自己，因而才取得了不同于常人的成就。\n而我们当中的大多数人，甚至包括那些还算有点天赋的人，都不得不通过学习来掌握自我管理的技巧。\n我们必须学会自我发展，必须知道把自己放在什么样的位置上，才能做出最大的贡献，而且还必须在长达 50 年的职业生涯中保持着高度的警觉和投入。\n我们的长处是什么 多数人都以为他们知道自己擅长什么。其实不然，更多的情况是，人们只知道自己不擅长什么——即便是在这一点上，人们也往往认识不清。\n然而，一个人要有所作为，只能靠发挥自己的长处，而如果从事自己不太擅长的工作是无法取得成就的，更不用说那些自己根本干不了的事情了。\n我们需要知己所长，才能知己所属。\n要发现自己的长处，唯一途径就是回馈分析法（feedback analysis）。每当做出重要决定或采取重要行动时，你都可以事先记录下自己对结果的预期。9 到 12 个月后，再将实际结果与自己的预期比较。\n我本人采用这种方法已有 15 到 20 年了，而每次使用都有意外的收获。\n我们只要持之以恒地运用这个简单的方法，就能在较短的时间内，发现自己的长处。在采用这种方法之后，你就能知道，自己正在做（或没有做）的哪些事情会让你的长处无法发挥出来。\n同时，你也将看到自己在哪些方面能力不是特别强。最后，你还将了解到自己在哪些方面完全不擅长，做不出成绩来。\n根据回馈分析的启示，你需要在几方面采取行动。\n首先最重要的是，专注于你的长处，把自己放到那些能发挥长处的地方。\n其次，加强你的长处。回馈分析会迅速地显示，你在哪些方面需要改善自己的技能或学习新技能。它还将显示你在知识上的差距——这些差距通常都可以弥补。\n第三，发现任何由于恃才傲物而造成的偏见和无知，并且加以克服。有太多的人，尤其是那些术业有专攻的人，往往对其他领域的知识不屑一顾，或者认为聪明的头脑就可取代知识。\n比如，很多一流的工程师遇上与人相关的事就束手无策，他们还以此为荣——因为他们觉得，对条理清晰的工程师头脑来说，人太混乱无序了。与此形成鲜明对照的是，人力资源方面的专业人员常常以他们连基本的会计知识或数量分析都一无所知而自傲。\n不过，人们要是对这样的无知还沾沾自喜的话，那无异于自取灭亡。其实，要让自己的长处得到充分发挥，你就应该努力学习新技能、汲取新知识。\n另外一点也同样重要——纠正你的不良习惯。所谓不良习惯，是指那些会影响你的工作成效和工作表现的事情。这样的习惯能很快地在回馈中反映出来。\n与此同时，回馈还会反映出哪些问题是由缺乏礼貌造成的。礼貌是一个组织的润滑剂。\n礼貌，其实也很简单。无非是说声「请」和「谢谢」，记住别人的名字，或问候对方家人这样的小事。许多聪明人，尤其是聪明的年轻人，没有意识到这一点。\n如果回馈分析表明某个人只要一遇到需要别人合作的事就屡屡失败，那么很可能就意味着这个人的举止不大得体——也就是缺乏礼貌。\n把预期和实际结果进行比较，也会发现自己不能做什么。我们每个人都有许多一窍不通、毫无天分的领域，在这些领域我们甚至连平庸的水平都达不到。人们，尤其是知识工作者，就不应该试图去完成这些领域的工作和任务。他们应该尽量少把精力浪费在那些不能胜任的领域上，因为从无能到平庸要比从一流到卓越需要人们付出多得多的努力。\n我们的工作方式是怎样的 很少有人知道自己平时是怎样把事情给做成的。\n实际上，我们当中的大多数人甚至不知道，不同人有着不同的工作方式和表现。\n许多人不是以他们习惯的方式工作，这当然就容易造成无所作为。\n对于知识工作者来说，「我的工作方式是怎样的？」可能比「我的长处是什么？」这个问题更加重要。\n同一个人的长处一样，一个人的工作方式也是独一无二的，这由人的个性决定。\n通常，几个常见的个性特征就决定了一个人的工作方式。\n我属于读者型，还是听者型？\n首先，你要搞清楚的是，你是读者型（习惯阅读信息）还是听者型（习惯听取信息）的人。绝大多数人甚至都不知道还有读者型和听者型之说，而且很少有人既是读者型又是听者型。知道自己属于哪种类型的人更少。\n没有几个听者型的人可以通过努力变成合格的读者型——不管是主动还是被动的努力，反之亦然。不了解你的工作方式不可能发挥才干或取得成就。\n我们如何学习 要了解一个人的工作方式，需要弄清的第二点是，他是如何学习的。","tags":null,"title":"自我管理","type":"post"},{"authors":null,"categories":[],"content":" 安装Golang Dep go get -v github.com/tools/godep  安装client-go go get k8s.io/client-go/kubernetes cd $GOPATH/src/k8s.io/client-go git checkout v10.0.0 godep restore ./...  集群外开发 集群内开发 ","date":1520081150,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520081150,"objectID":"1c706e685ada4b7edf079cff0d994081","permalink":"https://wubigo.com/post/k8s-sdk-setup/","publishdate":"2018-03-03T20:45:50+08:00","relpermalink":"/post/k8s-sdk-setup/","section":"post","summary":" 安装Golang Dep go get -v github.com/tools/godep  安装client-go go get k8s.io/client-go/kubernetes cd $GOPATH/src/k8s.io/client-go git checkout v10.0.0 godep restore ./...  集群外开发 集群内开发 ","tags":["K8S","SDK"],"title":"K8S SDK Setup","type":"post"},{"authors":null,"categories":null,"content":"Organizations are looking for ways to reduce their physical data center footprints, particularly for secondary workloads such as backups, files, or on-demand workloads. However, bridging data between private data centers and the public cloud comes with a unique set of challenges. Traditional data center services rely on low-latency network attached storage (NAS) and storage area network (SAN) protocols to access storage locally. Cloud-native applications are generally optimized for API access to data in scalable and durable cloud object storage, such as Amazon Simple Storage Service (Amazon S3).\nA file gateway provides a simple solution for presenting one or more Amazon S3 buckets and their objects as a mountable NFS to one or more clients on-premises.\n","date":1519948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519948800,"objectID":"916e22deac7af087ed6cf28ed5f5184c","permalink":"https://wubigo.com/post/2018-03-02-filegatewayforhybridarchitectures/","publishdate":"2018-03-02T00:00:00Z","relpermalink":"/post/2018-03-02-filegatewayforhybridarchitectures/","section":"post","summary":"Organizations are looking for ways to reduce their physical data center footprints, particularly for secondary workloads such as backups, files, or on-demand workloads. However, bridging data between private data centers and the public cloud comes with a unique set of challenges. Traditional data center services rely on low-latency network attached storage (NAS) and storage area network (SAN) protocols to access storage locally. Cloud-native applications are generally optimized for API access to data in scalable and durable cloud object storage, such as Amazon Simple Storage Service (Amazon S3).","tags":null,"title":"File Gateway for Hybrid Architectures; Overview and Best Practices","type":"post"},{"authors":null,"categories":[],"content":"Amazon EC2 networking doesn\u0026rsquo;t allow to use private ips in the containers\nthrough bridges or macvlan. Dedicating a network interface to a\ncontainer makes it directly unreachable from the host.\ndocker network create -d macvlan --subnet 172.30.80.0/20 --gateway 172.30.80.1 -o parent=eth0 pub_net docker run -d --network pub_net --ip 172.30.80.10 busybox  ","date":1519808176,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519808176,"objectID":"eb0e881c68daf1c90bbe5862b12b14d7","permalink":"https://wubigo.com/post/aws-ec2-macvaln/","publishdate":"2018-02-28T16:56:16+08:00","relpermalink":"/post/aws-ec2-macvaln/","section":"post","summary":"Amazon EC2 networking doesn\u0026rsquo;t allow to use private ips in the containers\nthrough bridges or macvlan. Dedicating a network interface to a\ncontainer makes it directly unreachable from the host.\ndocker network create -d macvlan --subnet 172.30.80.0/20 --gateway 172.30.80.1 -o parent=eth0 pub_net docker run -d --network pub_net --ip 172.30.80.10 busybox  ","tags":["AWS","SDN"],"title":"Aws EC2 MACVLAN","type":"post"},{"authors":null,"categories":[],"content":"Understanding Real-World Concurrency Bugs in Go\nhttps://songlh.github.io/paper/go-study.pdf\n","date":1519802547,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519802547,"objectID":"3f8e5055c2c166beb90b140c5da6f08b","permalink":"https://wubigo.com/post/effective-coding-go/","publishdate":"2018-02-28T15:22:27+08:00","relpermalink":"/post/effective-coding-go/","section":"post","summary":"Understanding Real-World Concurrency Bugs in Go\nhttps://songlh.github.io/paper/go-study.pdf","tags":[],"title":"Effective Coding Go","type":"post"},{"authors":null,"categories":[],"content":"高效编程\n JAVA PYTHON GO  ","date":1519802289,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519802289,"objectID":"8e036e1d3ca18101de945a11ae33f294","permalink":"https://wubigo.com/post/effective-coding/","publishdate":"2018-02-28T15:18:09+08:00","relpermalink":"/post/effective-coding/","section":"post","summary":"高效编程\n JAVA PYTHON GO  ","tags":["LANG"],"title":"Effective Coding","type":"post"},{"authors":null,"categories":[],"content":" 准备  搭建测试环境  可以参考从源代码构件K8S开发环境\n","date":1519614668,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519614668,"objectID":"5eb1c4f6bb98d786e4e2e3fa3b56c449","permalink":"https://wubigo.com/post/k8s_cni_kube-router/","publishdate":"2018-02-26T11:11:08+08:00","relpermalink":"/post/k8s_cni_kube-router/","section":"post","summary":"准备  搭建测试环境  可以参考从源代码构件K8S开发环境","tags":["K8S","CNI","NETWORK"],"title":"K8s CNI之Kube Router实现","type":"post"},{"authors":null,"categories":[],"content":" 准备  初始化\ndraft init ... Installing default plugins... Preparing to install into /home/bigo/.draft/plugins/draft-pack-repo draft-pack-repo installed into /home/bigo/.draft/plugins/draft-pack-repo/draft-pack-repo Installed plugin: pack-repo Installation of default plugins complete Installing default pack repositories... Installing pack repo from https://github.com/Azure/draft Installed pack repository github.com/Azure/draft Installation of default pack repositories complete $DRAFT_HOME has been configured at /home/bigo/.draft. ...  设置docker镜像寄存器\ndraft config set registry registry.cn-beijing.aliyuncs.com/k4s   or\n skip the push process entirely using the \u0026ndash;skip-image-push flag\n 应用设置 cd code/go/ ls app.go draft create ls app.go charts Dockerfile draft.toml  发布应用 draft up Draft Up Started: 'go-web': 01D6QETAPPM7ZYAM7G733ZVMY7 go-web: Building Docker Image: SUCCESS ⚓ (0.9999s) go-web: Pushing Docker Image: SUCCESS ⚓ (139.6931s) go-web: Releasing Application: SUCCESS ⚓ (4.3545s) Inspect the logs with `draft logs 01D6QETAPPM7ZYAM7G733ZVMY7`  检查  检查日志\ndraft logs 01D6QETAPPM7ZYAM7G733ZVMY7  检查软件列表\nhelm ls | grep go-web ... NAME REVISION\tUPDATED STATUS CHART APP VERSION\tNAMESPACE go-web Sun Mar 24 17:01:54 2018\tDEPLOYED\tgo-web-v0.1.0\tdefault ...  检查PODS\nkubectl get pods | grep go-web NAME READY STATUS RESTARTS AGE go-web-f94bd78d5-qcmq9 1/1 Running 0 5m38s   访问应用 draft connect ... Connect to go-web:8080 on **localhost:34261** [go-web]: * Environment: production [go-web]: * Debug mode: off [go-web]: * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit) ...  curl localhost:34261  应用迭代  修改go-web/web.go\n 发布\n draft up  测试\ndraft connect   删除应用 draft delete helm ls |grep go-web  ","date":1519428653,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519428653,"objectID":"2a6ebeb404c63a77ea5474c0032ee69d","permalink":"https://wubigo.com/post/k8s-development-streamline/","publishdate":"2018-02-24T07:30:53+08:00","relpermalink":"/post/k8s-development-streamline/","section":"post","summary":"准备  初始化\ndraft init ... Installing default plugins... Preparing to install into /home/bigo/.draft/plugins/draft-pack-repo draft-pack-repo installed into /home/bigo/.draft/plugins/draft-pack-repo/draft-pack-repo Installed plugin: pack-repo Installation of default plugins complete Installing default pack repositories... Installing pack repo from https://github.com/Azure/draft Installed pack repository github.com/Azure/draft Installation of default pack repositories complete $DRAFT_HOME has been configured at /home/bigo/.draft. ...  设置docker镜像寄存器\ndraft config set registry registry.cn-beijing.aliyuncs.com/k4s   or\n skip the push process entirely using the \u0026ndash;skip-image-push flag","tags":["K8S","APP"],"title":"K8s Development Streamline with draft","type":"post"},{"authors":null,"categories":[],"content":" PersistentVolume A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass. Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster’s default StorageClass is used instead\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet\nIn local clusters, the default StorageClass uses the hostPath provisioner. hostPath volumes are only suitable for development and testing. With hostPath volumes, the data lives in /tmp on the node the Pod is scheduled onto and does not move between nodes\nProvisioning There are two ways PVs may be provisioned: statically or dynamically.\n Static  A cluster administrator creates a number of PVs. They carry the details of the real storage which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.\n Dynamic  When none of the static PVs the administrator created matches a user’s PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class in order for dynamic provisioning to occur. Claims that request the class \u0026ldquo;\u0026rdquo; effectively disable dynamic provisioning for themselves.\nTo enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass admission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the comma-delimited, ordered list of values for the \u0026ndash;enable-admission-plugins flag of the API server component. For more information on API server command line flags, please check kube-apiserver documentation.\n","date":1519426553,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519426553,"objectID":"ef1314c59426d86855db66979c79537e","permalink":"https://wubigo.com/post/k8s-csi/","publishdate":"2018-02-24T06:55:53+08:00","relpermalink":"/post/k8s-csi/","section":"post","summary":"PersistentVolume A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass. Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster’s default StorageClass is used instead\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet","tags":["STORAGE","K8S","CSI"],"title":"K8S CSI","type":"post"},{"authors":null,"categories":[],"content":" 如果xml文件带有名字空间，XPATH支持\n还不够完善。下面介绍两种可以工作的方式\nnamespace for XML documents http.get(\u0026quot;https://wubigo.com/en/sitemap.xml\u0026quot;, function(res) {  useNamespaces const select = xpath.useNamespaces({\u0026quot;ns0\u0026quot;: \u0026quot;http://www.sitemaps.org/schemas/sitemap/0.9\u0026quot;}); const nodes = select(\u0026quot;//ns0:loc\u0026quot;, doc); nodes.forEach((value) =\u0026gt; console.log(\u0026quot;ns0:\u0026quot;+value));  Implementing a Default Namespace Resolver const nsResolver = function nsResolver(prefix) { const ns = { 'ns0' : 'http://www.sitemaps.org/schemas/sitemap/0.9', 'mathml': 'http://www.w3.org/1998/Math/MathML' }; return ns[prefix] || null; }; nsResolver.lookupNamespaceURI = nsResolver; var result = xpath.evaluate( \u0026quot;//ns0:loc\u0026quot;, // xpathExpression doc, // contextNode nsResolver, // namespaceResolver xpath.XPathResult.ANY_TYPE, // resultType null // result ) node = result.iterateNext(); while (node) { console.log(\u0026quot;url=\u0026quot;+node.toString()); node = result.iterateNext(); }  ","date":1519199105,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519199105,"objectID":"17aa1acfdcce8cd2eda632d0b1ac97b2","permalink":"https://wubigo.com/post/nodejs-xpath/","publishdate":"2018-02-21T15:45:05+08:00","relpermalink":"/post/nodejs-xpath/","section":"post","summary":"如果xml文件带有名字空间，XPATH支持\n还不够完善。下面介绍两种可以工作的方式\nnamespace for XML documents http.get(\u0026quot;https://wubigo.com/en/sitemap.xml\u0026quot;, function(res) {  useNamespaces const select = xpath.useNamespaces({\u0026quot;ns0\u0026quot;: \u0026quot;http://www.sitemaps.org/schemas/sitemap/0.9\u0026quot;}); const nodes = select(\u0026quot;//ns0:loc\u0026quot;, doc); nodes.forEach((value) =\u0026gt; console.log(\u0026quot;ns0:\u0026quot;+value));  Implementing a Default Namespace Resolver const nsResolver = function nsResolver(prefix) { const ns = { 'ns0' : 'http://www.sitemaps.org/schemas/sitemap/0.9', 'mathml': 'http://www.w3.org/1998/Math/MathML' }; return ns[prefix] || null; }; nsResolver.lookupNamespaceURI = nsResolver; var result = xpath.evaluate( \u0026quot;//ns0:loc\u0026quot;, // xpathExpression doc, // contextNode nsResolver, // namespaceResolver xpath.XPathResult.ANY_TYPE, // resultType null // result ) node = result.","tags":["NODE","XML"],"title":"Nodejs Xpath名字空间","type":"post"},{"authors":null,"categories":[],"content":" 在windows上搭建Leveldb的开发环境\nLeveldb使用了checkstyle\nhttps://github.com/wubigo/leveldb/blob/master/src/checkstyle/checks.xml\n\u0026lt;module name=\u0026quot;RegexpMultiline\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;format\u0026quot; value=\u0026quot;\\r\u0026quot;/\u0026gt; \u0026lt;property name=\u0026quot;message\u0026quot; value=\u0026quot;Line contains carriage return\u0026quot;/\u0026gt; \u0026lt;/module\u0026gt;  使用git默认设置拣出代码的时候，换行设置编译的时候会报错如下\n[ERROR] src\\main\\java\\org\\iq80\\leveldb\\WriteOptions.java:[6] (regexp) RegexpMultiline: Line contains carriage return [ERROR] src\\main\\java\\org\\iq80\\leveldb\\WriteOptions.java:[7] (regexp) RegexpMultiline: Line contains carriage return  解决办法之一是调整git换行设置\ngit换行设置(line endings) 换行设置的3个选项:\n Checkout Windows-style, commit Unix-style  Git will convert LF to CRLF when checking out text files. When committing text files, CRLF will be converted to LF. For cross-platform projects, this is the recommended setting on Windows (\u0026ldquo;core.autocrlf\u0026rdquo; is set to \u0026ldquo;true\u0026rdquo;)\n Checkout as-is, commit Unix-style  Git will not perform any conversion when checking out text files. When committing text files, CRLF will be converted to LF. For cross-platform projects this is the recommended setting on Unix (\u0026ldquo;core.autocrlf\u0026rdquo; is set to \u0026ldquo;input\u0026rdquo;).\n Checkout as-is, commit as-is  Git will not perform any conversions when checking out or committing text files. Choosing this option is not recommended for cross-platform projects (\u0026ldquo;core.autocrlf\u0026rdquo; is set to \u0026ldquo;false\u0026rdquo;)\ngit config --global core.autocrlf false  git clone git@github.com:wubigo/leveldb.git mvn package  ","date":1519036357,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519036357,"objectID":"ea5857741aa150c34726117a33f952d0","permalink":"https://wubigo.com/post/leveldb-port-in-java-dev/","publishdate":"2018-02-19T18:32:37+08:00","relpermalink":"/post/leveldb-port-in-java-dev/","section":"post","summary":"在windows上搭建Leveldb的开发环境\nLeveldb使用了checkstyle\nhttps://github.com/wubigo/leveldb/blob/master/src/checkstyle/checks.xml\n\u0026lt;module name=\u0026quot;RegexpMultiline\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;format\u0026quot; value=\u0026quot;\\r\u0026quot;/\u0026gt; \u0026lt;property name=\u0026quot;message\u0026quot; value=\u0026quot;Line contains carriage return\u0026quot;/\u0026gt; \u0026lt;/module\u0026gt;  使用git默认设置拣出代码的时候，换行设置编译的时候会报错如下\n[ERROR] src\\main\\java\\org\\iq80\\leveldb\\WriteOptions.java:[6] (regexp) RegexpMultiline: Line contains carriage return [ERROR] src\\main\\java\\org\\iq80\\leveldb\\WriteOptions.java:[7] (regexp) RegexpMultiline: Line contains carriage return  解决办法之一是调整git换行设置\ngit换行设置(line endings) 换行设置的3个选项:\n Checkout Windows-style, commit Unix-style  Git will convert LF to CRLF when checking out text files. When committing text files, CRLF will be converted to LF. For cross-platform projects, this is the recommended setting on Windows (\u0026ldquo;core.","tags":["KVS","CACHE"],"title":"在windows上搭建Leveldb的开发环境","type":"post"},{"authors":null,"categories":[],"content":" 表分区 主键=((分区键) + [簇键])\ncreate table kvstor ( k_part_one text, k_part_two int, k_clust_one text, k_clust_two int, k_clust_three uuid, data text, PRIMARY KEY((k_part_one, k_part_two), k_clust_one, k_clust_two, k_clust_three) );  分区键 分区键决定数据在集群内的分布在哪个分区\n簇键 簇建决定数据的在分区内的排列顺序\nCassandra DB 主要缺点  Cassandra has big issue with Data Read Latency Hard to tune-up for both latency and throughput Highly depended on Work load and type Max 20 % P99 latency drop Most memory consumed by storage engine To store huge amount of data, JVM is required to manage the memory to clean up garbage collection that is not done by the application but by a language in Cassandra  Cassandra is not recommended if you have following use cases :\n you are not storing volumes of data across racks of clusters. if you have a strong requirement for ACID properties. if you want to use aggregate function. if you are not partitioning your servers. if you are application has more read requests than writes. if you require strong Consistency.  RocksDB SST 文件在不同 Level 的特性  L0 层：SST 文件资身是按 Key 排序，但 L0 层的 SST 文件之间是无序的，每个 L0 层的 SST 文件之间会发生 Key Range 的重合，也就是说相同 Key 的数据可能存在于在 L0 层的每一个 SST 文件中。 L1 ~ Ln 层：多个 L0 层的 SST 文件达到 Compaction 条件后，与若干个 L1 层文件进行 Compaction 后形成新的 L1 层 SST 文件，L1 层 SST 文件之间不会出现 Key Range 的重合，也就是说相同 Key 的数据最多只会存在于 L1 层的一个 SST 文件中（L2 ~ Ln 层同理）。读取数据时，数据可能存在于 Memtable、Block Cache、SST 文件中。  读取操作：  Point Lookup（点查）：先从 Memtable 和 Block Cache 中尝试获取结果，如果没有找到则会按照层级查找 SST 文件。对于 L0 层 SST 文件，先通过 KeyRange 过滤出可能包含此 Key 的 SST 文件再进行查找；再对于 L1~Ln 层的文件进行二分查找定位对应的 SST 文件并进行读取。 Range Scan：多路归并的思想，返回给用户的 Iterator 由多个 Iterator 组成：每个 Memtable、Immutable Memtable、L0 层 SST 文件、以及多个 L1 ~ Ln 层 SST 文件中构建 Iterator，并以多路归并的方式返回给用户具体的值。  Compaction策略 Compaction 是将多个文件合并成一个文件的过程，在合并过程中会进行相同 Key 的去重，过期 Key 的删除等操作。一次 Compaction 可以简单看作将 N 个文件数据读取后，经过整理再重新写一遍的过程。在这里举两个极端的例子：\n 完全不发生 Compaction：SST 文件只存在于 L0 层，由于 L0 层不保证 SST 之间的 Key Range 不发生重合，所以数据读取需要访问很多 L0 层 SST 文件，在读取性能上会非常差。 持续发生 Compaction：假如每生成一个 SST 文件，我们就将它和其他 SST 文件进行 Compaction，那么数据写入的开销则会非常大。  可以看出，Compaction 策略的不同决定了读写放大，也决定了读写的性能，所以一个合理的 Compaction 策略其实是对读写性能的平衡，针对不同场景的需求，我们应该认真考虑其场景所适合的 Compaction 策略。RocksDB 默认提供三种 Compaction 策略，每个策略的触发条件都比较复杂，原理可看对应链接，这里仅描述一下它们的特点：\n Leveled Compaction（默认策略）：Compaction 触发频率相对高，读放大低，写放大高 Universal Compaction：Compaction 触发频率相对低，读放大高，写放大低 FIFO Compaction：几乎不发生 Compaction，读放大高，写放大几乎没有  ","date":1518764734,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518764734,"objectID":"222982ae67a5ac1c44f5404a37b39f4c","permalink":"https://wubigo.com/post/rocksdb-notes/","publishdate":"2018-02-16T15:05:34+08:00","relpermalink":"/post/rocksdb-notes/","section":"post","summary":"表分区 主键=((分区键) + [簇键])\ncreate table kvstor ( k_part_one text, k_part_two int, k_clust_one text, k_clust_two int, k_clust_three uuid, data text, PRIMARY KEY((k_part_one, k_part_two), k_clust_one, k_clust_two, k_clust_three) );  分区键 分区键决定数据在集群内的分布在哪个分区\n簇键 簇建决定数据的在分区内的排列顺序\nCassandra DB 主要缺点  Cassandra has big issue with Data Read Latency Hard to tune-up for both latency and throughput Highly depended on Work load and type Max 20 % P99 latency drop Most memory consumed by storage engine To store huge amount of data, JVM is required to manage the memory to clean up garbage collection that is not done by the application but by a language in Cassandra  Cassandra is not recommended if you have following use cases :","tags":["KVS","KVSTOR"],"title":"Rocksdb Notes","type":"post"},{"authors":null,"categories":[],"content":" 问题 长链接\n通过连接池和数据库保持长链接\n Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application\u0026rsquo;s needs. It enables you to run your database in the cloud without managing any database instances. It\u0026rsquo;s a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.\n ","date":1518545819,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518545819,"objectID":"1315c078a311ead68e79c4086d6cf665","permalink":"https://wubigo.com/post/serverless-database/","publishdate":"2018-02-14T02:16:59+08:00","relpermalink":"/post/serverless-database/","section":"post","summary":" 问题 长链接\n通过连接池和数据库保持长链接\n Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application\u0026rsquo;s needs. It enables you to run your database in the cloud without managing any database instances. It\u0026rsquo;s a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.\n ","tags":["SERVERLESS","SQL","EDA"],"title":"数据库无服务器架构","type":"post"},{"authors":null,"categories":[],"content":" webpack-simple Webpack is a module bundler for Javascript applications,\nit starts at the entrypoint and then build a dependency\ngraph of the whole application, pulling those dependencies\ninto one or more bundles that can be included in application.\nIt supports multiple different file types through loaders,\nloaders will take files that have no concept of modules (e.g. css)\nand process them in a way that allows them to participate\nin the overall dependency graph that webpack is building.\nnpm install -g @vue/cli-init vue init webpack-simple webpack-simple-app  Inspect Webpack Config Without Ejecting vue inspect \u0026gt; webpack.config.js vue inspect { mode: 'development', context: 'D:\\\\code\\\\vue-hello-world', node: { setImmediate: false, process: 'mock', dgram: 'empty', fs: 'empty', net: 'empty', tls: 'empty', child_process: 'empty' }, output: { path: 'D:\\\\code\\\\vue-hello-world\\\\dist', filename: 'js/[name].js', publicPath: '/', chunkFilename: 'js/[name].js' }, resolve: { alias: { '@': 'D:\\\\code\\\\vue-hello-world\\\\src', vue$: 'vue/dist/vue.runtime.esm.js' }, extensions: [ '.mjs', '.js', '.jsx', '.vue', '.json', '.wasm' ], modules: [ 'node_modules', 'D:\\\\code\\\\vue-hello-world\\\\node_modules', 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\@vue\\\\cli-service\\\\node_modules' ] }, resolveLoader: { modules: [ 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\@vue\\\\cli-plugin-babel\\\\node_modules', 'node_modules', 'D:\\\\code\\\\vue-hello-world\\\\node_modules', 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\@vue\\\\cli-service\\\\node_modules' ] }, module: { noParse: /^(vue|vue-router|vuex|vuex-router-sync)$/, rules: [ /* config.module.rule('vue') */ { test: /\\.vue$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\cache-loader\\\\dist\\\\cjs.js', options: { cacheDirectory: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\.cache\\\\vue-loader', cacheIdentifier: 'a54ba46e' } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-loader\\\\lib\\\\index.js', options: { compilerOptions: { whitespace: 'condense' }, cacheDirectory: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\.cache\\\\vue-loader', cacheIdentifier: 'a54ba46e' } } ] }, /* config.module.rule('images') */ { test: /\\.(png|jpe?g|gif|webp)(\\?.*)?$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\url-loader\\\\dist\\\\cjs.js', options: { limit: 4096, fallback: { loader: 'file-loader', options: { name: 'img/[name].[hash:8].[ext]' } } } } ] }, /* config.module.rule('svg') */ { test: /\\.(svg)(\\?.*)?$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\file-loader\\\\dist\\\\cjs.js', options: { name: 'img/[name].[hash:8].[ext]' } } ] }, /* config.module.rule('media') */ { test: /\\.(mp4|webm|ogg|mp3|wav|flac|aac)(\\?.*)?$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\url-loader\\\\dist\\\\cjs.js', options: { limit: 4096, fallback: { loader: 'file-loader', options: { name: 'media/[name].[hash:8].[ext]' } } } } ] }, /* config.module.rule('fonts') */ { test: /\\.(woff2?|eot|ttf|otf)(\\?.*)?$/i, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\url-loader\\\\dist\\\\cjs.js', options: { limit: 4096, fallback: { loader: 'file-loader', options: { name: 'fonts/[name].[hash:8].[ext]' } } } } ] }, /* config.module.rule('pug') */ { test: /\\.pug$/, oneOf: [ /* config.module.rule('pug').rule('pug-vue') */ { resourceQuery: /vue/, use: [ { loader: 'pug-plain-loader' } ] }, /* config.module.rule('pug').rule('pug-template') */ { use: [ { loader: 'raw-loader' }, { loader: 'pug-plain-loader' } ] } ] }, /* config.module.rule('css') */ { test: /\\.css$/, oneOf: [ /* config.module.rule('css').rule('vue-modules') */ { resourceQuery: /module/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } } ] }, /* config.module.rule('css').rule('vue') */ { resourceQuery: /\\?vue/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } } ] }, /* config.module.rule('css').rule('normal-modules') */ { test: /\\.module\\.\\w+$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } } ] }, /* config.module.rule('css').rule('normal') */ { use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } } ] } ] }, /* config.module.rule('postcss') */ { test: /\\.p(ost)?css$/, oneOf: [ /* config.module.rule('postcss').rule('vue-modules') */ { resourceQuery: /module/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } } ] }, /* config.module.rule('postcss').rule('vue') */ { resourceQuery: /\\?vue/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } } ] }, /* config.module.rule('postcss').rule('normal-modules') */ { test: /\\.module\\.\\w+$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } } ] }, /* config.module.rule('postcss').rule('normal') */ { use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } } ] } ] }, /* config.module.rule('scss') */ { test: /\\.scss$/, oneOf: [ /* config.module.rule('scss').rule('vue-modules') */ { resourceQuery: /module/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'sass-loader', options: { sourceMap: false } } ] }, /* config.module.rule('scss').rule('vue') */ { resourceQuery: /\\?vue/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'sass-loader', options: { sourceMap: false } } ] }, /* config.module.rule('scss').rule('normal-modules') */ { test: /\\.module\\.\\w+$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'sass-loader', options: { sourceMap: false } } ] }, /* config.module.rule('scss').rule('normal') */ { use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'sass-loader', options: { sourceMap: false } } ] } ] }, /* config.module.rule('sass') */ { test: /\\.sass$/, oneOf: [ /* config.module.rule('sass').rule('vue-modules') */ { resourceQuery: /module/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'sass-loader', options: { sourceMap: false, sassOptions: { indentedSyntax: true } } } ] }, /* config.module.rule('sass').rule('vue') */ { resourceQuery: /\\?vue/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'sass-loader', options: { sourceMap: false, sassOptions: { indentedSyntax: true } } } ] }, /* config.module.rule('sass').rule('normal-modules') */ { test: /\\.module\\.\\w+$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'sass-loader', options: { sourceMap: false, sassOptions: { indentedSyntax: true } } } ] }, /* config.module.rule('sass').rule('normal') */ { use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'sass-loader', options: { sourceMap: false, sassOptions: { indentedSyntax: true } } } ] } ] }, /* config.module.rule('less') */ { test: /\\.less$/, oneOf: [ /* config.module.rule('less').rule('vue-modules') */ { resourceQuery: /module/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'less-loader', options: { sourceMap: false } } ] }, /* config.module.rule('less').rule('vue') */ { resourceQuery: /\\?vue/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'less-loader', options: { sourceMap: false } } ] }, /* config.module.rule('less').rule('normal-modules') */ { test: /\\.module\\.\\w+$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'less-loader', options: { sourceMap: false } } ] }, /* config.module.rule('less').rule('normal') */ { use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'less-loader', options: { sourceMap: false } } ] } ] }, /* config.module.rule('stylus') */ { test: /\\.styl(us)?$/, oneOf: [ /* config.module.rule('stylus').rule('vue-modules') */ { resourceQuery: /module/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'stylus-loader', options: { sourceMap: false, preferPathResolver: 'webpack' } } ] }, /* config.module.rule('stylus').rule('vue') */ { resourceQuery: /\\?vue/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'stylus-loader', options: { sourceMap: false, preferPathResolver: 'webpack' } } ] }, /* config.module.rule('stylus').rule('normal-modules') */ { test: /\\.module\\.\\w+$/, use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2, modules: { localIdentName: '[name]_[local]_[hash:base64:5]' } } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'stylus-loader', options: { sourceMap: false, preferPathResolver: 'webpack' } } ] }, /* config.module.rule('stylus').rule('normal') */ { use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\vue-style-loader\\\\index.js', options: { sourceMap: false, shadowMode: false } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\css-loader\\\\dist\\\\cjs.js', options: { sourceMap: false, importLoaders: 2 } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\postcss-loader\\\\src\\\\index.js', options: { sourceMap: false, plugins: [ function () { /* omitted long function */ } ] } }, { loader: 'stylus-loader', options: { sourceMap: false, preferPathResolver: 'webpack' } } ] } ] }, /* config.module.rule('js') */ { test: /\\.m?jsx?$/, exclude: [ function () { /* omitted long function */ } ], use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\cache-loader\\\\dist\\\\cjs.js', options: { cacheDirectory: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\.cache\\\\babel-loader', cacheIdentifier: '6c0768d1' } }, { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\babel-loader\\\\lib\\\\index.js' } ] }, /* config.module.rule('eslint') */ { enforce: 'pre', test: /\\.(vue|(j|t)sx?)$/, exclude: [ /node_modules/, 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\@vue\\\\cli-service\\\\lib' ], use: [ { loader: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\eslint-loader\\\\index.js', options: { extensions: [ '.js', '.jsx', '.vue' ], cache: true, cacheIdentifier: 'a0403876', emitWarning: false, emitError: false, eslintPath: 'D:\\\\code\\\\vue-hello-world\\\\node_modules\\\\eslint', formatter: function () { /* omitted long function */ } } } ] } ] }, optimization: { splitChunks: { cacheGroups: { vendors: { name: 'chunk-vendors', test: /[\\\\/]node_modules[\\\\/]/, priority: -10, chunks: 'initial' }, common: { name: 'chunk-common', minChunks: 2, priority: -20, chunks: 'initial', reuseExistingChunk: true } } }, minimizer: [ { options: { test: /\\.m?js(\\?.*)?$/i, chunkFilter: () =\u0026gt; true, warningsFilter: () =\u0026gt; true, extractComments: false, sourceMap: true, cache: true, cacheKeys: defaultCacheKeys =\u0026gt; defaultCacheKeys, parallel: true, include: undefined, exclude: undefined, minify: undefined, terserOptions: { compress: { arrows: false, collapse_vars: false, comparisons: false, computed_props: false, hoist_funs: false, hoist_props: false, hoist_vars: false, inline: false, loops: false, negate_iife: false, properties: false, reduce_funcs: false, reduce_vars: false, switches: false, toplevel: false, typeofs: false, booleans: true, if_return: true, sequences: true, unused: true, conditionals: true, dead_code: true, evaluate: true }, mangle: { safari10: true } } } } ] }, plugins: [ /* config.plugin('vue-loader') */ new VueLoaderPlugin(), /* config.plugin('define') */ new DefinePlugin( { 'process.env': { NODE_ENV: '\u0026quot;development\u0026quot;', VUE_APP_DEBUG: '\u0026quot;true\u0026quot;', BASE_URL: '\u0026quot;/\u0026quot;' } } ), /* config.plugin('case-sensitive-paths') */ new CaseSensitivePathsPlugin(), /* config.plugin('friendly-errors') */ new FriendlyErrorsWebpackPlugin( { additionalTransformers: [ function () { /* omitted long function */ } ], additionalFormatters: [ function () { /* omitted long function */ } ] } ), /* config.plugin('html') */ new HtmlWebpackPlugin( { templateParameters: function () { /* omitted long function */ }, template: 'D:\\\\code\\\\vue-hello-world\\\\public\\\\index.html' } ), /* config.plugin('preload') */ new PreloadPlugin( { rel: 'preload', include: 'initial', fileBlacklist: [ /\\.map$/, /hot-update\\.js$/ ] } ), /* config.plugin('prefetch') */ new PreloadPlugin( { rel: 'prefetch', include: 'asyncChunks' } ), /* config.plugin('copy') */ new CopyPlugin( [ { from: 'D:\\\\code\\\\vue-hello-world\\\\public', to: 'D:\\\\code\\\\vue-hello-world\\\\dist', toType: 'dir', ignore: [ '.DS_Store', { glob: 'index.html', matchBase: false } ] } ] ) ], entry: { app: [ './src/main.js' ] }, devtool: 'source-map' }  ","date":1518272498,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518272498,"objectID":"25147b91a99d7323eaac84e2ffb8bcad","permalink":"https://wubigo.com/post/js-vue-cli-note/","publishdate":"2018-02-10T22:21:38+08:00","relpermalink":"/post/js-vue-cli-note/","section":"post","summary":"webpack-simple Webpack is a module bundler for Javascript applications,\nit starts at the entrypoint and then build a dependency\ngraph of the whole application, pulling those dependencies\ninto one or more bundles that can be included in application.\nIt supports multiple different file types through loaders,\nloaders will take files that have no concept of modules (e.g. css)\nand process them in a way that allows them to participate","tags":["WEB","JS","VUE"],"title":"Web vue-cli Note","type":"post"},{"authors":null,"categories":null,"content":" Understanding Dynamic Routing between Capsules https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/\n","date":1517529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517529600,"objectID":"b2e1d541771bcdee60a291eb52a62110","permalink":"https://wubigo.com/post/2018-02-02-dynamicroutingbetweencapsules/","publishdate":"2018-02-02T00:00:00Z","relpermalink":"/post/2018-02-02-dynamicroutingbetweencapsules/","section":"post","summary":"Understanding Dynamic Routing between Capsules https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/","tags":null,"title":"Understanding Dynamic Routing between Capsules","type":"post"},{"authors":null,"categories":[],"content":"Serverless Architectures\n\nISBN-10: 1617293822  ","date":1517283791,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517283791,"objectID":"4dc6cf4733ff196921ee3cd9a3b32eeb","permalink":"https://wubigo.com/post/serverless-architectures/","publishdate":"2018-01-30T11:43:11+08:00","relpermalink":"/post/serverless-architectures/","section":"post","summary":"Serverless Architectures\n\nISBN-10: 1617293822  ","tags":["SERVERLESS"],"title":"书籍推荐:无服务器架构及应用案例详解","type":"post"},{"authors":null,"categories":[],"content":"Linux Bridge supported GRE Tunnels, but not the newer and more scalable VXLAN model https://vincent.bernat.ch/en/blog/2017-vxlan-linux\nThis post will talk\nabout the various building blocks available to speed up packet processing\nboth hardware based e.g.SR-IOV, RDT, QAT, VMDq, VTD\nand software based e.g. DPDK, Fd.io/VPP, OVS etc and give\nhands on lab experience\nhttps://www.telcocloudbridge.com/blog/dpdk-vs-sr-iov-for-nfv-why-a-wrong-decision-can-impact-performance/\n","date":1517275931,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517275931,"objectID":"22e47f371326a16b157fbaf2ef1bfac7","permalink":"https://wubigo.com/post/sr-iov-vs-dpdk/","publishdate":"2018-01-30T09:32:11+08:00","relpermalink":"/post/sr-iov-vs-dpdk/","section":"post","summary":"Linux Bridge supported GRE Tunnels, but not the newer and more scalable VXLAN model https://vincent.bernat.ch/en/blog/2017-vxlan-linux\nThis post will talk\nabout the various building blocks available to speed up packet processing\nboth hardware based e.g.SR-IOV, RDT, QAT, VMDq, VTD\nand software based e.g. DPDK, Fd.io/VPP, OVS etc and give\nhands on lab experience\nhttps://www.telcocloudbridge.com/blog/dpdk-vs-sr-iov-for-nfv-why-a-wrong-decision-can-impact-performance/","tags":["NFV","SDN"],"title":"SR-IOV vs DPDK/VPP for NFV","type":"post"},{"authors":null,"categories":[],"content":"The dispersed nature of the Internet of Things (IoT) presents a major operational challenge that is uncommon in the traditional Internet or enterprise networks[5]. Devices that are managed together \u0026mdash; say a fleet of railcars \u0026mdash; are not physically colocated. Instead, they are widely geographically distributed. The operational approaches for management and security used in enterprise networks, where most hosts are densely contained in buildings or campuses, do not translate to the IoT. IoT devices operate outside of the enterprise network security and operational perimeter and the corporate LAN firewall can’t protect them. Dispatching technicians is expensive, so manual provisioning and configuration doesn’t scale. Devices connect to the Internet via a variety of last-mile ISPs, so many devices won’t share share a common IP prefix and addresses will change at arbitrary times. Any configuration based on these IPs will require continued upkeep and will often be out-of-date, exposing hosts and devices to external threats.\n物联网的协议分为两种，即接入协议与通讯协议。接入协议大多不属于TCP/IP协议族，只能用于设备子网（设备与网关组成的局域网）内的通讯；而通讯协议属于TCP/IP协议族，能够在互联网中进行数据传输 第一句话\n物联网的协议分为两种，即接入协议与通讯协议。接入协议大多不属于TCP/IP协议族，只能用于设备子网（设备与网关组成的局域网）内的通讯；而通讯协议属于TCP/IP协议族，能够在互联网中进行数据传输。\n第二句话\n采用接入协议的物联网设备，需要通过网关进行协议转换，转换成通讯协议才能接入互联网。而采用通讯协议的物联网设备，则可以直接接入互联网。\n第三句话\n常用的接入协议包括蓝牙、ZigBee、LoRa、NB-IoT、Wifi、RS485、RS232、NFC、RFID等；常用的通讯协议包括HTTP、CoAP、MQTT、XMPP、AMQP、JMS等。\n","date":1517221221,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517221221,"objectID":"b49750ddae1ab601ce4880204c8af367","permalink":"https://wubigo.com/post/iot-notes/","publishdate":"2018-01-29T18:20:21+08:00","relpermalink":"/post/iot-notes/","section":"post","summary":"The dispersed nature of the Internet of Things (IoT) presents a major operational challenge that is uncommon in the traditional Internet or enterprise networks[5]. Devices that are managed together \u0026mdash; say a fleet of railcars \u0026mdash; are not physically colocated. Instead, they are widely geographically distributed. The operational approaches for management and security used in enterprise networks, where most hosts are densely contained in buildings or campuses, do not translate to the IoT.","tags":["IOT"],"title":"IOT Notes","type":"post"},{"authors":null,"categories":[],"content":" VPC之间的连接方式  专线\n VPN托管服务\n VPN Peering\n 自建或第三方VPN\n PrivateLink\n  VPC与私有网络之间的连接方式  VPN托管服务\n 专线\n 专线+VPN\n 自建或第三方VPN\n Transit VPC\n VPN Hub\n  ","date":1517214214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517214214,"objectID":"eb1de2d505f278546e45185000a3423c","permalink":"https://wubigo.com/post/aws-vpc-connection/","publishdate":"2018-01-29T16:23:34+08:00","relpermalink":"/post/aws-vpc-connection/","section":"post","summary":" VPC之间的连接方式  专线\n VPN托管服务\n VPN Peering\n 自建或第三方VPN\n PrivateLink\n  VPC与私有网络之间的连接方式  VPN托管服务\n 专线\n 专线+VPN\n 自建或第三方VPN\n Transit VPC\n VPN Hub\n  ","tags":["VPC","VPN","AWS"],"title":"Aws VPC之间或VPC与私有网络之间的连接方式汇总","type":"post"},{"authors":null,"categories":[],"content":" 邮枪Concept plan(免费用户并增加一张信用卡)额度介绍  每月免费发送1万封邮件，超过1万封的邮件开始收费，$0.5//1000封 接收邮件免费 支持一千个定制域名 可以向任何邮箱地址发送而不用授权  创建发送账号 接收邮件 ","date":1516196933,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516196933,"objectID":"d60c407c31c0fb7afd799d32c7352776","permalink":"https://wubigo.com/post/startup-it-cloud-hosting/","publishdate":"2018-01-17T21:48:53+08:00","relpermalink":"/post/startup-it-cloud-hosting/","section":"post","summary":" 邮枪Concept plan(免费用户并增加一张信用卡)额度介绍  每月免费发送1万封邮件，超过1万封的邮件开始收费，$0.5//1000封 接收邮件免费 支持一千个定制域名 可以向任何邮箱地址发送而不用授权  创建发送账号 接收邮件 ","tags":["HOSTING","WEB","IT云服务"],"title":"创业公司IT云服务系列之内容托管服务","type":"post"},{"authors":null,"categories":[],"content":" 邮枪Concept plan(免费用户并增加一张信用卡)额度介绍  每月免费发送1万封邮件，超过1万封的邮件开始收费，$0.5//1000封 接收邮件免费 支持一千个定制域名 可以向任何邮箱地址发送而不用授权  创建发送账号 接收邮件 ","date":1516196933,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516196933,"objectID":"4a33fa6dd25df2c632c089ef29f58b6f","permalink":"https://wubigo.com/post/startup-it-cloud-storage/","publishdate":"2018-01-17T21:48:53+08:00","relpermalink":"/post/startup-it-cloud-storage/","section":"post","summary":" 邮枪Concept plan(免费用户并增加一张信用卡)额度介绍  每月免费发送1万封邮件，超过1万封的邮件开始收费，$0.5//1000封 接收邮件免费 支持一千个定制域名 可以向任何邮箱地址发送而不用授权  创建发送账号 接收邮件 ","tags":["SDS","STORAGE","S3","IT云服务"],"title":"创业公司IT云服务系列之数据存储","type":"post"},{"authors":null,"categories":[],"content":" 邮枪Concept plan(免费用户并增加一张信用卡)额度介绍  每月免费发送1万封邮件，超过1万封的邮件开始收费，$0.5//1000封 接收邮件免费 支持一千个定制域名 可以向任何邮箱地址发送而不用授权  创建发送账号 接收邮件 ","date":1516196933,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516196933,"objectID":"78bd321468edf09dc7797ebf1e472b16","permalink":"https://wubigo.com/post/startup-it-cloud-compute/","publishdate":"2018-01-17T21:48:53+08:00","relpermalink":"/post/startup-it-cloud-compute/","section":"post","summary":" 邮枪Concept plan(免费用户并增加一张信用卡)额度介绍  每月免费发送1万封邮件，超过1万封的邮件开始收费，$0.5//1000封 接收邮件免费 支持一千个定制域名 可以向任何邮箱地址发送而不用授权  创建发送账号 接收邮件 ","tags":["EC2","SERVERLESS","IT云服务"],"title":"创业公司IT云服务系列之计算服务","type":"post"},{"authors":null,"categories":[],"content":" 邮枪Concept plan(免费用户并增加一张信用卡)额度介绍  每月免费发送1万封邮件，超过1万封的邮件开始收费，$0.5//1000封 接收邮件免费 支持一千个定制域名 可以向任何邮箱地址发送而不用授权  创建发送账号 接收邮件 ","date":1516196933,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516196933,"objectID":"9acab3e197dae9c1df084b54769e0453","permalink":"https://wubigo.com/post/startup-it-cloud-email/","publishdate":"2018-01-17T21:48:53+08:00","relpermalink":"/post/startup-it-cloud-email/","section":"post","summary":" 邮枪Concept plan(免费用户并增加一张信用卡)额度介绍  每月免费发送1万封邮件，超过1万封的邮件开始收费，$0.5//1000封 接收邮件免费 支持一千个定制域名 可以向任何邮箱地址发送而不用授权  创建发送账号 接收邮件 ","tags":["SMTP","POP3","STARTUP","IT云服务"],"title":"创业公司IT云服务系列之邮件收发服务","type":"post"},{"authors":null,"categories":[],"content":" 依赖 go list -m all go list -m -versions github.com/minio/cli  ","date":1516057820,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516057820,"objectID":"31736a38ed00cb02dca6e3c36fb22692","permalink":"https://wubigo.com/post/minio-contribution-note/","publishdate":"2018-01-16T07:10:20+08:00","relpermalink":"/post/minio-contribution-note/","section":"post","summary":" 依赖 go list -m all go list -m -versions github.com/minio/cli  ","tags":["MINIO","STORAGE","GO"],"title":"Minio开发笔记","type":"post"},{"authors":null,"categories":[],"content":" 模块初始化 mkdir -p $GOPATH/src/github.com cd $GOPATH/src/github.com mkdir -p wubigo/API/go/hello cd wubigo/API/go/hello go mod init github.com/wubigo/API/go/hello   检查go.mod\nll go.mod cat go.mod module github.com/wubigo/API/go/hello go 1.13   创建程序 hello.go\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/google/go-cmp/cmp\u0026quot; ) func main() { fmt.Println(cmp.Diff(\u0026quot;Hello World\u0026quot;, \u0026quot;Hello Go\u0026quot;)) }  package main声明该模块是一个可执行程序而不是共享库\n编译测试 go install github.com/wubigo/API/go/hello  或者\ngo install .  或者\ngo install -n   检查go.mod  go.mod\nmodule github.com/wubigo/API/go/hello go 1.13 require github.com/google/go-cmp v0.4.0   检查程序\nll $$GOPATH/bin hello  测试\n  清理 go clean -i -n  提交代码 提交代码应包含go.mod\ngit add . git commit -m \u0026quot;application managed by module\u0026quot;  ","date":1515714161,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515714161,"objectID":"aa3966f33dd2e9d0103243baa8252513","permalink":"https://wubigo.com/post/golang-first-app-as-module/","publishdate":"2018-01-12T07:42:41+08:00","relpermalink":"/post/golang-first-app-as-module/","section":"post","summary":"模块初始化 mkdir -p $GOPATH/src/github.com cd $GOPATH/src/github.com mkdir -p wubigo/API/go/hello cd wubigo/API/go/hello go mod init github.com/wubigo/API/go/hello   检查go.mod\nll go.mod cat go.mod module github.com/wubigo/API/go/hello go 1.13   创建程序 hello.go\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/google/go-cmp/cmp\u0026quot; ) func main() { fmt.Println(cmp.Diff(\u0026quot;Hello World\u0026quot;, \u0026quot;Hello Go\u0026quot;)) }  package main声明该模块是一个可执行程序而不是共享库\n编译测试 go install github.com/wubigo/API/go/hello  或者\ngo install .  或者\ngo install -n   检查go.mod  go.mod\nmodule github.com/wubigo/API/go/hello go 1.","tags":["GO","GOMOD"],"title":"Go语言第一个基于模块应用","type":"post"},{"authors":null,"categories":null,"content":" Set namespace preference kubectl config set-context $(kubectl config current-context) --namespace=\u0026lt;bigo\u0026gt;  watch pod kubectl get pods pod1 --watch  Check Performance kubectl top node kubectl top pod  copy file between pod and local kubectl cp ~/f1 \u0026lt;namespace\u0026gt;/\u0026lt;pod-name\u0026gt;:/tmp/ kubectl cp \u0026lt;namespace\u0026gt;/\u0026lt;pod-name\u0026gt;:/tmp/ ~/  enable RBAC  kube-apiserver - --authorization-mode=RBAC  User CRUD openssl genrsa -out bigo.key 2048 openssl req -new -key bigo.key -out bigo.csr -subj \u0026quot;/CN=wubigo/O=bigo LLC\u0026quot; sudo openssl x509 -req -in bigo.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out bigo.crt -days 500 kubectl config set-credentials bigo --client-certificate=./bigo.crt --client-key=./bigo.key kubectl config set-context bigo-context --cluster=kubernetes --namespace=bigo-NS --user=bigo kubectl config get-contexts ... CURRENT NAME CLUSTER AUTHINFO NAMESPACE bigo-context kubernetes bigo bigo * kubernetes-admin@kubernetes kubernetes kubernetes-admin ...  binding role to user cat rolebinding-bigo-access.yaml kind: RoleBinding apiVersion: rbac.authorization.K8S.io/v1beta1 metadata: name: access-manager-binding namespace: bigo-NS subjects: - kind: User name: bigo apiGroup: \u0026quot;\u0026quot; roleRef: kind: Role name: access-role apiGroup: \u0026quot;\u0026quot; kubectl create -f rolebinding-bigo-access.yaml  USER, GROUP, ROLE , ROLEBIND, RBAC  list all users\nkubectl config view ... users: - name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED ...   Enable Helm in cluster  Create a Service Account tiller for the Tiller server (in the kube-system namespace). Service Accounts are meant for intra-cluster processes running in Pods.\n Bind the cluster-admin ClusterRole to this Service Account. ClusterRoleBindings to be applicable in all namespaces. Tiller to manage resources in all namespaces.\n Update the existing Tiller deployment (tiller-deploy) to associate its pod with the Service Account tiller.\nkubectl create serviceaccount tiller --namespace kube-system kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}'   or\ncat tiller-clusterrolebinding.yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.K8S.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026quot;\u0026quot; docker pull registry.cn-beijing.aliyuncs.com/k4s/tiller:v2.12.3 kubectl create -f tiller-clusterrolebinding.yaml # Update the existing tiller-deploy deployment with the Service Account helm init --service-account tiller --upgrade  helm install --name prometheus stable/prometheus helm install --name prometheus1 stable/prometheus --set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false,  PVC using local PV  create PVC\ncat storage-class-hdd.yaml apiVersion: storage.K8S.io/v1 kind: StorageClass metadata: name: local-hdd provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer   v1.18.3\napiVersion: storage.k8s.io/v1   kubectl apply -f storage-class-hdd.yaml\n  create local PV\ncat local_volume.yaml apiVersion: v1 kind: PersistentVolume metadata: name: local-hdd spec: capacity: storage: 8Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-hdd local: path: /mnt/pv/ nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - bigo-vm4    kubectl apply -f local_volume.yaml\n PersistentVolume nodeAffinity is required when using local volumes. It enables the Kubernetes scheduler to correctly schedule Pods using local volumes to the correct node.\nPersistentVolume volumeMode can now be set to “Block” (instead of the default value “Filesystem”) to expose the local volume as a raw block device. The volumeMode field requires BlockVolume Alpha feature gate to be enabled.\nWhen using local volumes, it is recommended to create a StorageClass with volumeBindingMode set to WaitForFirstConsumer. See the example. Delaying volume binding ensures that the PersistentVolumeClaim binding decision will also be evaluated with any other node constraints the Pod may have, such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity\n   https://www.nebulaworks.com/blog/2019/08/27/leveraging-aws-ebs-for-kubernetes-persistent-volumes/\nPort Forwarding a local port to a port on K8S kubectl port-forward \u0026lt;podname\u0026gt; 9090:9090 or kubectl port-forward pods/\u0026lt;podname\u0026gt; 9090:9090 or kubectl port-forward deployment/prometheus 9090:9090 or kubectl port-forward svc/prometheus 9090:9090 or kubectl port-forward rs/prometheus 9090:9090  ","date":1515628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515628800,"objectID":"0d9d01a1ee982b0db9c49cd2b2fd4f52","permalink":"https://wubigo.com/post/k8s-kubectl-cheatsheet/","publishdate":"2018-01-11T00:00:00Z","relpermalink":"/post/k8s-kubectl-cheatsheet/","section":"post","summary":"Set namespace preference kubectl config set-context $(kubectl config current-context) --namespace=\u0026lt;bigo\u0026gt;  watch pod kubectl get pods pod1 --watch  Check Performance kubectl top node kubectl top pod  copy file between pod and local kubectl cp ~/f1 \u0026lt;namespace\u0026gt;/\u0026lt;pod-name\u0026gt;:/tmp/ kubectl cp \u0026lt;namespace\u0026gt;/\u0026lt;pod-name\u0026gt;:/tmp/ ~/  enable RBAC  kube-apiserver - --authorization-mode=RBAC  User CRUD openssl genrsa -out bigo.key 2048 openssl req -new -key bigo.key -out bigo.csr -subj \u0026quot;/CN=wubigo/O=bigo LLC\u0026quot; sudo openssl x509 -req -in bigo.","tags":["K8S"],"title":"kubectl cheat sheet","type":"post"},{"authors":null,"categories":[],"content":" InnoDB MySQL 5.1 shipping the older version of InnoDB, If you’re using MySQL 5.1, please ensure that you’re using the InnoDB plugin. It’s much better than the older version of InnoDB.\nIt now scales well to 24 CPU cores, and arguably up to 32 or even more cores depending on the scenario\n","date":1515392448,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515392448,"objectID":"89f018e127206a2985f4b28ec8c94994","permalink":"https://wubigo.com/post/mysql-high-performance/","publishdate":"2018-01-08T14:20:48+08:00","relpermalink":"/post/mysql-high-performance/","section":"post","summary":"InnoDB MySQL 5.1 shipping the older version of InnoDB, If you’re using MySQL 5.1, please ensure that you’re using the InnoDB plugin. It’s much better than the older version of InnoDB.\nIt now scales well to 24 CPU cores, and arguably up to 32 or even more cores depending on the scenario","tags":["MYSQL","SQL"],"title":"Mysql High Performance","type":"post"},{"authors":null,"categories":null,"content":" Puppeteer vs Selenium/WebDriver  Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers. Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.  That said, you can use Puppeteer to run tests against Chromium, e.g. using the community-driven jest-puppeteer. While this probably shouldn’t be your only testing solution, it does have a few good points compared to WebDriver: * Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it very easy to start with. At the end of the day, it’s better to have a few tests running chromium-only, than no tests at all. * Puppeteer has event-driven architecture, which removes a lot of potential flakiness. There’s no need for evil “sleep(1000)” calls in puppeteer scripts. * Puppeteer runs headless by default, which makes it fast to run. Puppeteer v1.5.0 also exposes browser contexts, making it possible to efficiently parallelize test execution. * Puppeteer shines when it comes to debugging: flip the “headless” bit to false, add “slowMo”, and you’ll see what the browser is doing. You can even open Chrome DevTools to inspect the test environment.\n爬虫 为了爬取js渲染的html页面，我们需要用浏览器来解析js后生成html。 在scrapy中可以利用pyppeteer来实现对应功能\n","date":1515283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515283200,"objectID":"10b6c1db1cfd215584023df7ed5469c7","permalink":"https://wubigo.com/post/2018-01-07-webtestingautomation/","publishdate":"2018-01-07T00:00:00Z","relpermalink":"/post/2018-01-07-webtestingautomation/","section":"post","summary":"Puppeteer vs Selenium/WebDriver  Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers. Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.  That said, you can use Puppeteer to run tests against Chromium, e.g. using the community-driven jest-puppeteer. While this probably shouldn’t be your only testing solution, it does have a few good points compared to WebDriver: * Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it very easy to start with.","tags":null,"title":"web testing automation","type":"post"},{"authors":null,"categories":[],"content":" 安装 sudo add-apt-repository ppa:wireguard/wireguard sudo apt-get update sudo apt-get install wireguard -y   打开安全组  配置  创建key\nwg genkey | tee privatekey | wg pubkey \u0026gt; publickey private_key=$(wg genkey) public_key=$(echo $private_key | wg pubkey)  配置\n ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000 link/ether 0a:81:39:72:97:90 brd ff:ff:ff:ff:ff:ff inet 10.12.0.154/24 brd 10.12.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::881:39ff:fe72:9790/64 scope link valid_lft forever preferred_lft forever   /etc/wireguard/wg0.conf\n[Interface] PrivateKey = AKUINjvxFqVLMiJc7qX95bEyiRlqnAWFHpy3hLeCI1s= Address = 10.12.4.1/24 ListenPort = 51820 PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE [Peer] # My laptop (this is just a comment, change it to identify the device) PublicKey = k2XMyPcLbPcNhJ5ThKrwbzNPMw6h1JKkDOcw1rqstF4= AllowedIPs = 10.12.4.0/24   启动\nsudo wg-quick up wg0 sudo wg show wg0 help sudo netstat -anp|grep 51820 udp 0 0 0.0.0.0:51820 0.0.0.0:* - udp6 0 0 :::51820 :::*   查看正在使用的密钥对\nwg show wg0 dump   显示公钥和vpn服务运行状态\nsudo wg  启动主机路由\nsudo sysctl -p net.ipv4.ip_forward = 1   stop sudo wg-quick down wg0  客户端 Vpn客户端设置参考\nPS: Wireguard在前几天被合并到LINUX的内核代码\nhttps://www.linuxbabe.com/ubuntu/wireguard-vpn-server-ubuntu\n","date":1514867672,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514867672,"objectID":"668b1a6b6427a58e0742a81118b0c833","permalink":"https://wubigo.com/post/vpn-with-wireguard/","publishdate":"2018-01-02T12:34:32+08:00","relpermalink":"/post/vpn-with-wireguard/","section":"post","summary":"安装 sudo add-apt-repository ppa:wireguard/wireguard sudo apt-get update sudo apt-get install wireguard -y   打开安全组  配置  创建key\nwg genkey | tee privatekey | wg pubkey \u0026gt; publickey private_key=$(wg genkey) public_key=$(echo $private_key | wg pubkey)  配置\n ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000 link/ether 0a:81:39:72:97:90 brd ff:ff:ff:ff:ff:ff inet 10.","tags":["VPN","SDN","NFV"],"title":"Vpn With Wireguard","type":"post"},{"authors":null,"categories":null,"content":" Death of Microservice Madness http://www.dwmkerr.com/the-death-of-microservice-madness-in-2018/\n","date":1514851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514851200,"objectID":"70d65632982e837a935dc764b934fd16","permalink":"https://wubigo.com/post/2018-01-02-deathofmicroservicemadness/","publishdate":"2018-01-02T00:00:00Z","relpermalink":"/post/2018-01-02-deathofmicroservicemadness/","section":"post","summary":"Death of Microservice Madness http://www.dwmkerr.com/the-death-of-microservice-madness-in-2018/","tags":null,"title":"Death of Microservice Madness","type":"post"},{"authors":null,"categories":[],"content":" how-to-allow-local-network-when-using-wireguard-vpn-tunnel  允许非隧道流量\nOpen the WireGaurd Windows client. In the left pane, select the tunnel that you want local network routing to work, if you have more than one tunnel. Hit the Edit button. Uncheck Block untunneled traffic (kill-switch) option  增加本地的网络\nAllowedIPs = 192.168.0.0/16, 0.0.0.0/1, 128.0.0.0/1, ::/1, 8000::/1   安装 https://download.wireguard.com/windows-client/wireguard-amd64-0.0.38.msi\n配置  更改公钥\n Endpoint所在的vpn服务器地址\n  https://github.com/Nyr/openvpn-install\nhttps://github.com/hwdsl2/setup-ipsec-vpn\nhttps://wireguard.isystem.io/\nhttps://github.com/meshbird/meshbird\nhttps://www.tinc-vpn.org/\nhttps://github.com/isystem-io/wireguard-aws\nDownload and install the TunSafe, which is a Wireguard client for Windows.\n wget https://git.io/vpnsetup -O vpnsetup.sh \u0026amp;\u0026amp; sudo sh vpnsetup.sh --2020-01-01 22:26:54-- https://git.io/vpnsetup Resolving git.io (git.io)... 54.165.216.26, 54.224.175.112, 34.227.147.55, ... Connecting to git.io (git.io)|54.165.216.26|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/hwdsl2/setup-ipsec-vpn/master/vpnsetup.sh [following] --2020-01-01 22:26:55-- https://raw.githubusercontent.com/hwdsl2/setup-ipsec-vpn/master/vpnsetup.sh Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.188.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.188.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 15318 (15K) [text/plain] Saving to: ‘vpnsetup.sh’ vpnsetup.sh 100%[=============================================================================\u0026gt;] 14.96K --.-KB/s in 0.001s 2020-01-01 22:26:55 (13.1 MB/s) - ‘vpnsetup.sh’ saved [15318/15318] ## VPN credentials not set by user. Generating random PSK and password... ## VPN setup in progress... Please be patient. ## Populating apt-get cache... Hit:1 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial InRelease Hit:2 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates InRelease Hit:3 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-backports InRelease Hit:4 http://security.ubuntu.com/ubuntu xenial-security InRelease Reading package lists... ## Installing packages required for setup... Reading package lists... Building dependency tree... Reading state information... gawk is already the newest version (1:4.1.3+dfsg-0.1). iptables is already the newest version (1.6.0-2ubuntu3). net-tools is already the newest version (1.60-26ubuntu1). sed is already the newest version (4.2.2-7). dnsutils is already the newest version (1:9.10.3.dfsg.P4-8ubuntu1.15). grep is already the newest version (2.25-1~16.04.1). iproute2 is already the newest version (4.3.0-1ubuntu3.16.04.5). openssl is already the newest version (1.0.2g-1ubuntu4.15). wget is already the newest version (1.17.1-1ubuntu1.5). 0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded. ## Trying to auto discover IP of this server... In case the script hangs here for more than a few minutes, press Ctrl-C to abort. Then edit it and manually enter IP. ## Installing packages required for the VPN... Reading package lists... Building dependency tree... Reading state information... gcc is already the newest version (4:5.3.1-1ubuntu1). gcc set to manually installed. make is already the newest version (4.1-6). make set to manually installed. The following additional packages will be installed: libbison-dev libcurl3-nss libevent-core-2.0-5 libevent-extra-2.0-5 libevent-openssl-2.0-5 libevent-pthreads-2.0-5 libfl-dev libnspr4 libnss3 libnss3-nssdb libpcre16-3 libpcre3-dev libpcre32-3 libpcrecpp0v5 libsepol1-dev m4 Suggested packages: bison-doc libcurl4-doc libcurl3-dbg libidn11-dev libkrb5-dev libldap2-dev librtmp-dev zlib1g-dev The following NEW packages will be installed: bison flex libbison-dev libcap-ng-dev libcap-ng-utils libcurl3-nss libcurl4-nss-dev libevent-core-2.0-5 libevent-dev libevent-extra-2.0-5 libevent-openssl-2.0-5 libevent-pthreads-2.0-5 libfl-dev libnspr4 libnspr4-dev libnss3 libnss3-dev libnss3-nssdb libnss3-tools libpam0g-dev libpcre16-3 libpcre3-dev libpcre32-3 libpcrecpp0v5 libselinux1-dev libsepol1-dev m4 pkg-config ppp xl2tpd 0 upgraded, 30 newly installed, 0 to remove and 59 not upgraded. Need to get 6,229 kB of archives. After this operation, 25.3 MB of additional disk space will be used. Get:1 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 m4 amd64 1.4.17-5 [195 kB] Get:2 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 libfl-dev amd64 2.6.0-11 [12.5 kB] Get:3 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 flex amd64 2.6.0-11 [290 kB] Get:4 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 libpcrecpp0v5 amd64 2:8.38-3.1 [15.2 kB] Get:5 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 libbison-dev amd64 2:3.0.4.dfsg-1 [338 kB] Get:6 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 bison amd64 2:3.0.4.dfsg-1 [259 kB] Get:7 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 libcap-ng-dev amd64 0.7.7-1 [21.6 kB] Get:8 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/universe amd64 libcap-ng-utils amd64 0.7.7-1 [14.9 kB] Get:9 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libnspr4 amd64 2:4.13.1-0ubuntu0.16.04.1 [112 kB] Get:10 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libnss3-nssdb all 2:3.28.4-0ubuntu0.16.04.9 [10.6 kB] Get:11 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libnss3 amd64 2:3.28.4-0ubuntu0.16.04.9 [1,146 kB] Get:12 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libcurl3-nss amd64 7.47.0-1ubuntu2.14 [190 kB] Get:13 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libcurl4-nss-dev amd64 7.47.0-1ubuntu2.14 [267 kB] Get:14 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libevent-core-2.0-5 amd64 2.0.21-stable-2ubuntu0.16.04.1 [70.6 kB] Get:15 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libevent-extra-2.0-5 amd64 2.0.21-stable-2ubuntu0.16.04.1 [51.1 kB] Get:16 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libevent-pthreads-2.0-5 amd64 2.0.21-stable-2ubuntu0.16.04.1 [5,020 B] Get:17 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libevent-openssl-2.0-5 amd64 2.0.21-stable-2ubuntu0.16.04.1 [10.6 kB] Get:18 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libevent-dev amd64 2.0.21-stable-2ubuntu0.16.04.1 [211 kB] Get:19 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libnspr4-dev amd64 2:4.13.1-0ubuntu0.16.04.1 [213 kB] Get:20 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libnss3-dev amd64 2:3.28.4-0ubuntu0.16.04.9 [230 kB] Get:21 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libnss3-tools amd64 2:3.28.4-0ubuntu0.16.04.9 [840 kB] Get:22 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpam0g-dev amd64 1.1.8-3.2ubuntu2.1 [109 kB] Get:23 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 libpcre16-3 amd64 2:8.38-3.1 [144 kB] Get:24 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 libpcre32-3 amd64 2:8.38-3.1 [136 kB] Get:25 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 libpcre3-dev amd64 2:8.38-3.1 [525 kB] Get:26 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 libsepol1-dev amd64 2.4-2 [249 kB] Get:27 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 libselinux1-dev amd64 2.4-3build2 [122 kB] Get:28 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 pkg-config amd64 0.29.1-0ubuntu1 [45.0 kB] Get:29 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 ppp amd64 2.4.7-1+2ubuntu1.16.04.1 [330 kB] Get:30 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 xl2tpd amd64 1.3.6+dfsg-4ubuntu0.16.04.2 [65.6 kB] Fetched 6,229 kB in 2s (2,631 kB/s) Selecting previously unselected package m4. (Reading database ... 57764 files and directories currently installed.) Preparing to unpack .../archives/m4_1.4.17-5_amd64.deb ... Unpacking m4 (1.4.17-5) ... Selecting previously unselected package libfl-dev:amd64. Preparing to unpack .../libfl-dev_2.6.0-11_amd64.deb ... Unpacking libfl-dev:amd64 (2.6.0-11) ... Selecting previously unselected package flex. Preparing to unpack .../flex_2.6.0-11_amd64.deb ... Unpacking flex (2.6.0-11) ... Selecting previously unselected package libpcrecpp0v5:amd64. Preparing to unpack .../libpcrecpp0v5_2%3a8.38-3.1_amd64.deb ... Unpacking libpcrecpp0v5:amd64 (2:8.38-3.1) ... Selecting previously unselected package libbison-dev:amd64. Preparing to unpack .../libbison-dev_2%3a3.0.4.dfsg-1_amd64.deb ... Unpacking libbison-dev:amd64 (2:3.0.4.dfsg-1) ... Selecting previously unselected package bison. Preparing to unpack .../bison_2%3a3.0.4.dfsg-1_amd64.deb ... Unpacking bison (2:3.0.4.dfsg-1) ... Selecting previously unselected package libcap-ng-dev. Preparing to unpack .../libcap-ng-dev_0.7.7-1_amd64.deb ... Unpacking libcap-ng-dev (0.7.7-1) ... Selecting previously unselected package libcap-ng-utils. Preparing to unpack .../libcap-ng-utils_0.7.7-1_amd64.deb ... Unpacking libcap-ng-utils (0.7.7-1) ... Selecting previously unselected package libnspr4:amd64. Preparing to unpack .../libnspr4_2%3a4.13.1-0ubuntu0.16.04.1_amd64.deb ... Unpacking libnspr4:amd64 (2:4.13.1-0ubuntu0.16.04.1) ... Selecting previously unselected package libnss3-nssdb. Preparing to unpack .../libnss3-nssdb_2%3a3.28.4-0ubuntu0.16.04.9_all.deb ... Unpacking libnss3-nssdb (2:3.28.4-0ubuntu0.16.04.9) ... Selecting previously unselected package libnss3:amd64. Preparing to unpack .../libnss3_2%3a3.28.4-0ubuntu0.16.04.9_amd64.deb ... Unpacking libnss3:amd64 (2:3.28.4-0ubuntu0.16.04.9) ... Selecting previously unselected package libcurl3-nss:amd64. Preparing to unpack .../libcurl3-nss_7.47.0-1ubuntu2.14_amd64.deb ... Unpacking libcurl3-nss:amd64 (7.47.0-1ubuntu2.14) ... Selecting previously unselected package libcurl4-nss-dev:amd64. Preparing to unpack .../libcurl4-nss-dev_7.47.0-1ubuntu2.14_amd64.deb ... Unpacking libcurl4-nss-dev:amd64 (7.47.0-1ubuntu2.14) ... Selecting previously unselected package libevent-core-2.0-5:amd64. Preparing to unpack .../libevent-core-2.0-5_2.0.21-stable-2ubuntu0.16.04.1_amd64.deb ... Unpacking libevent-core-2.0-5:amd64 (2.0.21-stable-2ubuntu0.16.04.1) ... Selecting previously unselected package libevent-extra-2.0-5:amd64. Preparing to unpack .../libevent-extra-2.0-5_2.0.21-stable-2ubuntu0.16.04.1_amd64.deb ... Unpacking libevent-extra-2.0-5:amd64 (2.0.21-stable-2ubuntu0.16.04.1) ... Selecting previously unselected package libevent-pthreads-2.0-5:amd64. Preparing to unpack .../libevent-pthreads-2.0-5_2.0.21-stable-2ubuntu0.16.04.1_amd64.deb ... Unpacking libevent-pthreads-2.0-5:amd64 (2.0.21-stable-2ubuntu0.16.04.1) ... Selecting previously unselected package libevent-openssl-2.0-5:amd64. Preparing to unpack .../libevent-openssl-2.0-5_2.0.21-stable-2ubuntu0.16.04.1_amd64.deb ... Unpacking libevent-openssl-2.0-5:amd64 (2.0.21-stable-2ubuntu0.16.04.1) ... Selecting previously unselected package libevent-dev. Preparing to unpack .../libevent-dev_2.0.21-stable-2ubuntu0.16.04.1_amd64.deb ... Unpacking libevent-dev (2.0.21-stable-2ubuntu0.16.04.1) ... Selecting previously unselected package libnspr4-dev. Preparing to unpack .../libnspr4-dev_2%3a4.13.1-0ubuntu0.16.04.1_amd64.deb ... Unpacking libnspr4-dev (2:4.13.1-0ubuntu0.16.04.1) ... Selecting previously unselected package libnss3-dev:amd64. Preparing to unpack .../libnss3-dev_2%3a3.28.4-0ubuntu0.16.04.9_amd64.deb ... Unpacking libnss3-dev:amd64 (2:3.28.4-0ubuntu0.16.04.9) ... Selecting previously unselected package libnss3-tools. Preparing to unpack .../libnss3-tools_2%3a3.28.4-0ubuntu0.16.04.9_amd64.deb ... Unpacking libnss3-tools (2:3.28.4-0ubuntu0.16.04.9) ... Selecting previously unselected package libpam0g-dev:amd64. Preparing to unpack .../libpam0g-dev_1.1.8-3.2ubuntu2.1_amd64.deb ... Unpacking libpam0g-dev:amd64 (1.1.8-3.2ubuntu2.1) ... Selecting previously unselected package libpcre16-3:amd64. Preparing to unpack .../libpcre16-3_2%3a8.38-3.1_amd64.deb ... Unpacking libpcre16-3:amd64 (2:8.38-3.1) ... Selecting previously unselected package libpcre32-3:amd64. Preparing to unpack .../libpcre32-3_2%3a8.38-3.1_amd64.deb ... Unpacking libpcre32-3:amd64 (2:8.38-3.1) ... Selecting previously unselected package libpcre3-dev:amd64. Preparing to unpack .../libpcre3-dev_2%3a8.38-3.1_amd64.deb ... Unpacking libpcre3-dev:amd64 (2:8.38-3.1) ... Selecting previously unselected package libsepol1-dev:amd64. Preparing to unpack .../libsepol1-dev_2.4-2_amd64.deb ... Unpacking libsepol1-dev:amd64 (2.4-2) ... Selecting previously unselected package libselinux1-dev:amd64. Preparing to unpack .../libselinux1-dev_2.4-3build2_amd64.deb ... Unpacking libselinux1-dev:amd64 (2.4-3build2) ... Selecting previously unselected package pkg-config. Preparing to unpack .../pkg-config_0.29.1-0ubuntu1_amd64.deb ... Unpacking pkg-config (0.29.1-0ubuntu1) ... Selecting previously unselected package ppp. Preparing to unpack .../ppp_2.4.7-1+2ubuntu1.16.04.1_amd64.deb ... Unpacking ppp (2.4.7-1+2ubuntu1.16.04.1) ... Selecting previously unselected package xl2tpd. Preparing to unpack .../xl2tpd_1.3.6+dfsg-4ubuntu0.16.04.2_amd64.deb ... Unpacking xl2tpd (1.3.6+dfsg-4ubuntu0.16.04.2) ... Processing triggers for install-info (6.1.0.dfsg.1-5) ... Processing triggers for man-db (2.7.5-1) ... Processing triggers for libc-bin (2.23-0ubuntu11) ... Processing triggers for ureadahead (0.100.0-19.1) ... Processing triggers for systemd (229-4ubuntu21.22) ... Setting up m4 (1.4.17-5) ... Setting up libfl-dev:amd64 (2.6.0-11) ... Setting up flex (2.6.0-11) ... Setting up libpcrecpp0v5:amd64 (2:8.38-3.1) ... Setting up libbison-dev:amd64 (2:3.0.4.dfsg-1) ... Setting up bison (2:3.0.4.dfsg-1) ... update-alternatives: using /usr/bin/bison.yacc to provide /usr/bin/yacc (yacc) in auto mode Setting up libcap-ng-dev (0.7.7-1) ... Setting up libcap-ng-utils (0.7.7-1) ... Setting up libnspr4:amd64 (2:4.13.1-0ubuntu0.16.04.1) ... Setting up libevent-core-2.0-5:amd64 (2.0.21-stable-2ubuntu0.16.04.1) ... Setting up libevent-extra-2.0-5:amd64 (2.0.21-stable-2ubuntu0.16.04.1) ... Setting up libevent-pthreads-2.0-5:amd64 (2.0.21-stable-2ubuntu0.16.04.1) ... Setting up libevent-openssl-2.0-5:amd64 (2.0.21-stable-2ubuntu0.16.04.1) ... Setting up libevent-dev (2.0.21-stable-2ubuntu0.16.04.1) ... Setting up libnspr4-dev (2:4.13.1-0ubuntu0.16.04.1) ... Setting up libpam0g-dev:amd64 (1.1.8-3.2ubuntu2.1) ... Setting up libpcre16-3:amd64 (2:8.38-3.1) ... Setting up libpcre32-3:amd64 (2:8.38-3.1) ... Setting up libpcre3-dev:amd64 (2:8.38-3.1) ... Setting up libsepol1-dev:amd64 (2.4-2) ... Setting up libselinux1-dev:amd64 (2.4-3build2) ... Setting up pkg-config (0.29.1-0ubuntu1) ... Setting up ppp (2.4.7-1+2ubuntu1.16.04.1) ... Setting up xl2tpd (1.3.6+dfsg-4ubuntu0.16.04.2) ... Setting up libnss3-nssdb (2:3.28.4-0ubuntu0.16.04.9) ... Setting up libnss3:amd64 (2:3.28.4-0ubuntu0.16.04.9) ... Setting up libcurl3-nss:amd64 (7.47.0-1ubuntu2.14) ... Setting up libcurl4-nss-dev:amd64 (7.47.0-1ubuntu2.14) ... Setting up libnss3-dev:amd64 (2:3.28.4-0ubuntu0.16.04.9) ... Setting up libnss3-tools (2:3.28.4-0ubuntu0.16.04.9) ... Processing triggers for libc-bin (2.23-0ubuntu11) ... Processing triggers for ureadahead (0.100.0-19.1) ... Processing triggers for systemd (229-4ubuntu21.22) ... ## Installing Fail2Ban to protect SSH... Reading package lists... Building dependency tree... Reading state information... The following additional packages will be installed: python3-pyinotify whois Suggested packages: mailx monit python-pyinotify-doc The following NEW packages will be installed: fail2ban python3-pyinotify whois 0 upgraded, 3 newly installed, 0 to remove and 59 not upgraded. Need to get 286 kB of archives. After this operation, 1,474 kB of additional disk space will be used. Get:1 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/universe amd64 fail2ban all 0.9.3-1 [227 kB] Get:2 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 python3-pyinotify all 0.9.6-0fakesync1 [24.7 kB] Get:3 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial/main amd64 whois amd64 5.2.11 [34.0 kB] Fetched 286 kB in 0s (605 kB/s) Selecting previously unselected package fail2ban. (Reading database ... 59042 files and directories currently installed.) Preparing to unpack .../fail2ban_0.9.3-1_all.deb ... Unpacking fail2ban (0.9.3-1) ... Selecting previously unselected package python3-pyinotify. Preparing to unpack .../python3-pyinotify_0.9.6-0fakesync1_all.deb ... Unpacking python3-pyinotify (0.9.6-0fakesync1) ... Selecting previously unselected package whois. Preparing to unpack .../whois_5.2.11_amd64.deb ... Unpacking whois (5.2.11) ... Processing triggers for man-db (2.7.5-1) ... Processing triggers for ureadahead (0.100.0-19.1) ... Processing triggers for systemd (229-4ubuntu21.22) ... Setting up fail2ban (0.9.3-1) ... Setting up python3-pyinotify (0.9.6-0fakesync1) ... Setting up whois (5.2.11) ... Processing triggers for ureadahead (0.100.0-19.1) ... Processing triggers for systemd (229-4ubuntu21.22) ... ## Compiling and installing Libreswan... 2020-01-01 22:27:11 URL:https://codeload.github.com/libreswan/libreswan/tar.gz/v3.29 [3848730] -\u0026gt; \u0026quot;libreswan-3.29.tar.gz\u0026quot; [1] Reading package lists... Building dependency tree... Reading state information... The following additional packages will be installed: libpam-systemd libsystemd0 systemd Suggested packages: systemd-ui systemd-container The following NEW packages will be installed: libsystemd-dev The following packages will be upgraded: libpam-systemd libsystemd0 systemd 3 upgraded, 1 newly installed, 0 to remove and 56 not upgraded. Need to get 4,255 kB of archives. After this operation, 396 kB of additional disk space will be used. Get:1 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpam-systemd amd64 229-4ubuntu21.23 [115 kB] Get:2 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libsystemd0 amd64 229-4ubuntu21.23 [204 kB] Get:3 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 systemd amd64 229-4ubuntu21.23 [3,777 kB] Get:4 http://us-west-1.ec2.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libsystemd-dev amd64 229-4ubuntu21.23 [159 kB] Fetched 4,255 kB in 0s (18.4 MB/s) (Reading database ... 59432 files and directories currently installed.) Preparing to unpack .../libpam-systemd_229-4ubuntu21.23_amd64.deb ... Unpacking libpam-systemd:amd64 (229-4ubuntu21.23) over (229-4ubuntu21.22) ... Preparing to unpack .../libsystemd0_229-4ubuntu21.23_amd64.deb ... Unpacking libsystemd0:amd64 (229-4ubuntu21.23) over (229-4ubuntu21.22) ... Processing triggers for man-db (2.7.5-1) ... Processing triggers for libc-bin (2.23-0ubuntu11) ... Setting up libsystemd0:amd64 (229-4ubuntu21.23) ... Processing triggers for libc-bin (2.23-0ubuntu11) ... (Reading database ... 59432 files and directories currently installed.) Preparing to unpack .../systemd_229-4ubuntu21.23_amd64.deb ... Unpacking systemd (229-4ubuntu21.23) over (229-4ubuntu21.22) ... Processing triggers for ureadahead (0.100.0-19.1) ... Processing triggers for dbus (1.10.6-1ubuntu3.4) ... Processing triggers for man-db (2.7.5-1) ... Setting up systemd (229-4ubuntu21.23) ... addgroup: The group `systemd-journal' already exists as a system group. Exiting. [/usr/lib/tmpfiles.d/var.conf:14] Duplicate line for path \u0026quot;/var/log\u0026quot;, ignoring. Selecting previously unselected package libsystemd-dev:amd64. (Reading database ... 59432 files and directories currently installed.) Preparing to unpack .../libsystemd-dev_229-4ubuntu21.23_amd64.deb ... Unpacking libsystemd-dev:amd64 (229-4ubuntu21.23) ... Processing triggers for man-db (2.7.5-1) ... Setting up libpam-systemd:amd64 (229-4ubuntu21.23) ... Setting up libsystemd-dev:amd64 (229-4ubuntu21.23) ... a - x509dn.o a - asn1.o a - oid.o a - constants.o a - alloc.o a - diag.o a - id.o a - initaddr.o a - initsaid.o a - initsubnet.o a - keyblobtoid.o a - lex.o a - lswconf.o a - lswfips.o a - rangetosubnet.o a - sameaddr.o a - secrets.o a - subnettot.o a - subnettypeof.o a - ttoaddr.o a - ttodata.o a - ttoprotoport.o a - ttosa.o a - ttosubnet.o a - ttoul.o a - secitem_chunk.o a - base64_pubkey.o a - lswnss.o a - lsw_passert_fail.o a - alg_byname.o a - certs.o a - addr_lookup.o a - log_ip.o a - af_info.o a - fd.o a - kernel_alg.o a - kernel_sadb.o a - role.o a - addrtot.o a - addrtypeof.o a - anyaddr.o a - datatot.o a - goodmask.o a - satot.o a - ultot.o a - proposals.o a - v1_proposals.o a - v2_proposals.o a - esp_info.o a - ah_info.o a - ike_info.o a - ckaid.o a - chunk.o a - shunk.o a - ip_address.o a - ip_endpoint.o a - ip_range.o a - ip_subnet.o a - lmod.o a - lset.o a - deltatime.o a - realtime.o a - monotime.o a - debug.o a - impair.o a - keywords.o a - dbg.o a - DBG_dump.o a - DBG_log.o a - log_to_log.o a - libreswan_exit_log_errno.o a - libreswan_log_errno.o a - libreswan_pexpect.o a - libreswan_pexpect_log.o a - libreswan_bad_case.o a - rate_log.o a - libreswan_log.o a - libreswan_log_rc.o a - fmtbuf.o a - lswlog.o a - lswlog_dbg.o a - lswlog_nss_error.o a - lswlog_nss_ckm.o a - lswlog_nss_ckf.o a - lswlog_nss_cka.o a - lswlog_nss_secitem.o a - lswlog_source_line.o a - lswlog_sanitized.o a - lswlog_errno.o a - lswlog_bytes.o a - lswlog_enum_lset_short.o a - lswlog_realtime.o a - lswlog_monotime.o a - lswlog_to_file_stream.o a - lswlog_pexpect.o a - lswlog_passert.o a - ike_alg.o a - ike_alg_test.o a - ike_alg_encrypt_chacha20_poly1305.o a - ike_alg_encrypt_nss_aead_ops.o a - ike_alg_encrypt_nss_cbc_ops.o a - ike_alg_encrypt_nss_ctr_ops.o a - ike_alg_encrypt_nss_gcm_ops.o a - ike_alg_desc.o a - ike_alg_3des.o a - ike_alg_aes.o a - ike_alg_camellia.o a - ike_alg_dh.o a - ike_alg_md5.o a - ike_alg_none.o a - ike_alg_serpent.o a - ike_alg_sha1.o a - ike_alg_sha2.o a - ike_alg_twofish.o a - nss_copies.o a - sanitizestring.o a - pfkey_sock.o a - pfkey_error.o a - pfkey_v2_build.o a - pfkey_v2_ext_bits.o a - pfkey_v2_parse.o a - pfkey_v2_debug.o a - /opt/src/libreswan-3.29/OBJ.linux.x86_64/lib/libswan/version.o a - serpent.o a - serpent_cbc.o a - twofish.o a - twofish_cbc.o a - whacklib.o a - aliascomp.o a - confread.o a - confwrite.o a - starterwhack.o a - starterlog.o a - parser.tab.o a - lex.yy.o a - keywords.o a - interfaces.o a - lswlog.o a - libreswan_exit.o IN ipsec.conf.in -\u0026gt; ../../OBJ.linux.x86_64/programs/configs/ipsec.conf IN ipsec.secrets.in -\u0026gt; ../../OBJ.linux.x86_64/programs/configs/ipsec.secrets IN clear.in -\u0026gt; ../../OBJ.linux.x86_64/programs/configs/clear IN clear-or-private.in -\u0026gt; ../../OBJ.linux.x86_64/programs/configs/clear-or-private IN private-or-clear.in -\u0026gt; ../../OBJ.linux.x86_64/programs/configs/private-or-clear IN private.in -\u0026gt; ../../OBJ.linux.x86_64/programs/configs/private IN block.in -\u0026gt; ../../OBJ.linux.x86_64/programs/configs/block IN portexcludes.conf.in -\u0026gt; ../../OBJ.linux.x86_64/programs/configs/portexcludes.conf IN _plutorun.in -\u0026gt; ../../OBJ.linux.x86_64/programs/_plutorun/_plutorun IN _stackmanager.in -\u0026gt; ../../OBJ.linux.x86_64/programs/_stackmanager/_stackmanager IN _secretcensor.in -\u0026gt; ../../OBJ.linux.x86_64/programs/_secretcensor/_secretcensor IN _updown.in -\u0026gt; ../../OBJ.linux.x86_64/programs/_updown/_updown IN _unbound-hook.in -\u0026gt; ../../OBJ.linux.x86_64/programs/_unbound-hook/_unbound-hook IN auto.in -\u0026gt; ../../OBJ.linux.x86_64/programs/auto/auto IN barf.in -\u0026gt; ../../OBJ.linux.x86_64/programs/barf/barf IN verify.in -\u0026gt; ../../OBJ.linux.x86_64/programs/verify/verify IN show.in -\u0026gt; ../../OBJ.linux.x86_64/programs/show/show IN ipsec.in -\u0026gt; ../../OBJ.linux.x86_64/programs/ipsec/ipsec IN look.in -\u0026gt; ../../OBJ.linux.x86_64/programs/look/look IN newhostkey.in -\u0026gt; ../../OBJ.linux.x86_64/programs/newhostkey/newhostkey IN setup.in -\u0026gt; ../../OBJ.linux.x86_64/programs/setup/setup IN _updown.netkey.in -\u0026gt; ../../OBJ.linux.x86_64/programs/_updown.netkey/_updown.netkey IN ipsec.service.in -\u0026gt; ../../OBJ.linux.x86_64/initsystems/systemd/ipsec.service IN libreswan.conf.in -\u0026gt; ../../OBJ.linux.x86_64/initsystems/systemd/libreswan.conf ../../OBJ.linux.x86_64/programs/pluto/pluto -\u0026gt; /usr/local/libexec/ipsec/pluto ../../OBJ.linux.x86_64/programs/whack/whack -\u0026gt; /usr/local/libexec/ipsec/whack ../../OBJ.linux.x86_64/programs/addconn/addconn -\u0026gt; /usr/local/libexec/ipsec/addconn ../../OBJ.linux.x86_64/programs/configs/ipsec.conf -\u0026gt; /etc/ipsec.conf ../../OBJ.linux.x86_64/programs/configs/ipsec.secrets -\u0026gt; /etc/ipsec.secrets ../../OBJ.linux.x86_64/programs/configs/ipsec.conf -\u0026gt; /usr/local/share/doc/libreswan/ipsec.conf-sample ../../OBJ.linux.x86_64/programs/configs/ipsec.secrets -\u0026gt; /usr/local/share/doc/libreswan/ipsec.secrets-sample ../../OBJ.linux.x86_64/programs/configs/clear -\u0026gt; /etc/ipsec.d/policies/clear ../../OBJ.linux.x86_64/programs/configs/clear-or-private -\u0026gt; /etc/ipsec.d/policies/clear-or-private ../../OBJ.linux.x86_64/programs/configs/private-or-clear -\u0026gt; /etc/ipsec.d/policies/private-or-clear ../../OBJ.linux.x86_64/programs/configs/private -\u0026gt; /etc/ipsec.d/policies/private ../../OBJ.linux.x86_64/programs/configs/block -\u0026gt; /etc/ipsec.d/policies/block ../../OBJ.linux.x86_64/programs/configs/portexcludes.conf -\u0026gt; /etc/ipsec.d/policies/portexcludes.conf ../../OBJ.linux.x86_64/programs/_plutorun/_plutorun -\u0026gt; /usr/local/libexec/ipsec/_plutorun ../../OBJ.linux.x86_64/programs/_stackmanager/_stackmanager -\u0026gt; /usr/local/libexec/ipsec/_stackmanager ../../OBJ.linux.x86_64/programs/_secretcensor/_secretcensor -\u0026gt; /usr/local/libexec/ipsec/_secretcensor ../../OBJ.linux.x86_64/programs/_updown/_updown -\u0026gt; /usr/local/libexec/ipsec/_updown ../../OBJ.linux.x86_64/programs/_unbound-hook/_unbound-hook -\u0026gt; /usr/local/libexec/ipsec/_unbound-hook ../../OBJ.linux.x86_64/programs/auto/auto -\u0026gt; /usr/local/libexec/ipsec/auto ../../OBJ.linux.x86_64/programs/barf/barf -\u0026gt; /usr/local/libexec/ipsec/barf ../../OBJ.linux.x86_64/programs/verify/verify -\u0026gt; /usr/local/libexec/ipsec/verify ../../OBJ.linux.x86_64/programs/show/show -\u0026gt; /usr/local/libexec/ipsec/show ../../OBJ.linux.x86_64/programs/ipsec/ipsec -\u0026gt; /usr/local/sbin/ipsec ../../OBJ.linux.x86_64/programs/look/look -\u0026gt; /usr/local/libexec/ipsec/look ../../OBJ.linux.x86_64/programs/newhostkey/newhostkey -\u0026gt; /usr/local/libexec/ipsec/newhostkey ../../OBJ.linux.x86_64/programs/rsasigkey/rsasigkey -\u0026gt; /usr/local/libexec/ipsec/rsasigkey ../../OBJ.linux.x86_64/programs/setup/setup -\u0026gt; /usr/local/libexec/ipsec/setup ../../OBJ.linux.x86_64/programs/showhostkey/showhostkey -\u0026gt; /usr/local/libexec/ipsec/showhostkey ../../OBJ.linux.x86_64/programs/readwriteconf/readwriteconf -\u0026gt; /usr/local/libexec/ipsec/readwriteconf ../../OBJ.linux.x86_64/programs/_import_crl/_import_crl -\u0026gt; /usr/local/libexec/ipsec/_import_crl ../../OBJ.linux.x86_64/programs/algparse/algparse -\u0026gt; /usr/local/libexec/ipsec/algparse ../../OBJ.linux.x86_64/programs/cavp/cavp -\u0026gt; /usr/local/libexec/ipsec/cavp ../../OBJ.linux.x86_64/programs/_updown.netkey/_updown.netkey -\u0026gt; /usr/local/libexec/ipsec/_updown.netkey running: systemctl --system daemon-reload running: systemd-tmpfiles --create /usr/lib/tmpfiles.d/libreswan.conf DESTDIR='' ************************** WARNING *********************************** The ipsec service is currently disabled. To enable this service issue: systemctl enable ipsec.service ********************************************************************** ../../OBJ.linux.x86_64/testing/enumcheck/enumcheck -\u0026gt; /usr/local/libexec/ipsec/enumcheck ../../OBJ.linux.x86_64/testing/ipcheck/ipcheck -\u0026gt; /usr/local/libexec/ipsec/ipcheck ../../OBJ.linux.x86_64/testing/fmtcheck/fmtcheck -\u0026gt; /usr/local/libexec/ipsec/fmtcheck ../../OBJ.linux.x86_64/testing/timecheck/timecheck -\u0026gt; /usr/local/libexec/ipsec/timecheck ## Creating VPN configuration... ## Updating sysctl settings... ## Updating IPTables rules... ## Enabling services on boot... ## Starting services...  ","date":1514846060,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514846060,"objectID":"bb0b479be176f9483d614b7183216494","permalink":"https://wubigo.com/post/vpn-win-setup/","publishdate":"2018-01-02T06:34:20+08:00","relpermalink":"/post/vpn-win-setup/","section":"post","summary":"how-to-allow-local-network-when-using-wireguard-vpn-tunnel  允许非隧道流量\nOpen the WireGaurd Windows client. In the left pane, select the tunnel that you want local network routing to work, if you have more than one tunnel. Hit the Edit button. Uncheck Block untunneled traffic (kill-switch) option  增加本地的网络\nAllowedIPs = 192.168.0.0/16, 0.0.0.0/1, 128.0.0.0/1, ::/1, 8000::/1   安装 https://download.wireguard.com/windows-client/wireguard-amd64-0.0.38.msi\n配置  更改公钥\n Endpoint所在的vpn服务器地址\n  https://github.com/Nyr/openvpn-install\nhttps://github.com/hwdsl2/setup-ipsec-vpn\nhttps://wireguard.isystem.io/\nhttps://github.com/meshbird/meshbird\nhttps://www.tinc-vpn.org/\nhttps://github.com/isystem-io/wireguard-aws\nDownload and install the TunSafe, which is a Wireguard client for Windows.","tags":["VPN","SDN","NFV"],"title":"Vpn客户端设置参考","type":"post"},{"authors":null,"categories":null,"content":" Hijack of Amazon’s domain service used to reroute web traffic for two hours https://doublepulsar.com/hijack-of-amazons-internet-domain-service-used-to-reroute-web-traffic-for-two-hours-unnoticed-3a6f0dda6a6f\nSocioeconomic group classification based on user features http://pimg-faiw.uspto.gov/fdd/83/2018/28/003/0.pdf\nThe long, tortuous and fascinating process of creating a Chinese font (qz.com) https://qz.com/522079/the-long-incredibly-tortuous-and-fascinating-process-of-creating-a-chinese-font/\nAnnouncing 1.1.1.1: the fastest, privacy-first consumer DNS service https://blog.cloudflare.com/announcing-1111/\nIntroducing Cloud Text-to-Speech powered by DeepMind WaveNet technology https://cloudplatform.googleblog.com/2018/03/introducing-Cloud-Text-to-Speech-powered-by-Deepmind-WaveNet-technology.html\nAnnouncing gRPC Support in NGINX https://www.nginx.com/blog/nginx-1-13-10-grpc/\nACME v2 and Wildcard Certificate Support is Live https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live\nPIXAR’S 22 RULES OF STORYTELLING https://www.aerogrammestudio.com/2013/03/07/pixars-22-rules-of-storytelling/\nKeybase is now supported by the Stellar Development Foundation https://keybase.io/blog/keybase-stellar\nMachine Learning Crash Course https://developers.google.com/machine-learning/crash-course/\nThe Makefile I use with JavaScript projects http://www.olioapps.com/blog/the-lost-art-of-the-makefile/\nHow GDPR Will Change The Way You Develop https://www.smashingmagazine.com/2018/02/gdpr-for-web-developers/\nUber and Waymo Reach Settlement https://www.uber.com/newsroom/uber-waymo-settlement/\nPostmortem of Service Outage at 3.4M Concurrent Users https://www.epicgames.com/fortnite/en-US/news/postmortem-of-service-outage-at-3-4m-ccu\nPerspective: Streaming pivot visualization via WebAssembly https://github.com/jpmorganchase/perspective\nTinc VPN: Secure Private Network Between Hosts https://www.tinc-vpn.org/\nA reimplementation of Winamp 2.9 in HTML5 and Javascript https://github.com/captbaritone/winamp2-js\nWhat I Learned Burning $14k on YouTube Ads for Candy Japan https://www.candyjapan.com/behind-the-scenes/what-i-learned-advertising-on-youtube\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d136c19d4861d8c757ecf23594c7153f","permalink":"https://wubigo.com/post/2018-01-01-hacknewsfavorites2018/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/post/2018-01-01-hacknewsfavorites2018/","section":"post","summary":"Hijack of Amazon’s domain service used to reroute web traffic for two hours https://doublepulsar.com/hijack-of-amazons-internet-domain-service-used-to-reroute-web-traffic-for-two-hours-unnoticed-3a6f0dda6a6f\nSocioeconomic group classification based on user features http://pimg-faiw.uspto.gov/fdd/83/2018/28/003/0.pdf\nThe long, tortuous and fascinating process of creating a Chinese font (qz.com) https://qz.com/522079/the-long-incredibly-tortuous-and-fascinating-process-of-creating-a-chinese-font/\nAnnouncing 1.1.1.1: the fastest, privacy-first consumer DNS service https://blog.cloudflare.com/announcing-1111/\nIntroducing Cloud Text-to-Speech powered by DeepMind WaveNet technology https://cloudplatform.googleblog.com/2018/03/introducing-Cloud-Text-to-Speech-powered-by-Deepmind-WaveNet-technology.html\nAnnouncing gRPC Support in NGINX https://www.nginx.com/blog/nginx-1-13-10-grpc/\nACME v2 and Wildcard Certificate Support is Live https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live\nPIXAR’S 22 RULES OF STORYTELLING https://www.","tags":null,"title":"Hacknews favorites 2018","type":"post"},{"authors":null,"categories":[],"content":"FROM centos:7 RUN echo \u0026quot;ip_resolve=4\u0026quot; \u0026gt;\u0026gt; /etc/yum.conf RUN yum update -y \u0026amp;\u0026amp; yum install -y java-1.8.0-openjdk # 设置时区(日志、调用链) RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo \u0026quot;Asia/Shanghai\u0026quot; \u0026gt; /etc/timezone ENV workdir /app/ ADD VERSION . WORKDIR ${workdir} # JAVA_OPTS环境变量JVM 启动参数，在运行时 bash 替换 # 使用 exec 以使 Java 程序可以接收 SIGTERM 信号。 CMD [\u0026quot;sh\u0026quot;, \u0026quot;-ec\u0026quot;, \u0026quot;exec java ${JAVA_OPTS} -jar ${jar}\u0026quot;]  ","date":1514619496,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514619496,"objectID":"da2f166b4a197f10e57245e0bbc90741","permalink":"https://wubigo.com/post/docker-image-timezone/","publishdate":"2017-12-30T15:38:16+08:00","relpermalink":"/post/docker-image-timezone/","section":"post","summary":"FROM centos:7 RUN echo \u0026quot;ip_resolve=4\u0026quot; \u0026gt;\u0026gt; /etc/yum.conf RUN yum update -y \u0026amp;\u0026amp; yum install -y java-1.8.0-openjdk # 设置时区(日志、调用链) RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo \u0026quot;Asia/Shanghai\u0026quot; \u0026gt; /etc/timezone ENV workdir /app/ ADD VERSION . WORKDIR ${workdir} # JAVA_OPTS环境变量JVM 启动参数，在运行时 bash 替换 # 使用 exec 以使 Java 程序可以接收 SIGTERM 信号。 CMD [\u0026quot;sh\u0026quot;, \u0026quot;-ec\u0026quot;, \u0026quot;exec java ${JAVA_OPTS} -jar ${jar}\u0026quot;]  ","tags":["DOCKER"],"title":"Docker Image Timezone","type":"post"},{"authors":null,"categories":[],"content":" DDoS 基础防护 腾讯云 DDoS 基础防护本身免费，当用户购买了腾讯云 CVM、CLB 等服务时，会自动开启 DDoS 基础防护。\n普通用户提供2Gbps的防护能力，最高可达10Gbps\n高防IP 高防包不需要更改客户业务 IP，高防IP需要客户将腾讯的高防 IP 作为业务 IP 发布；\n高防包只能防护一台云主机或一台负载均衡（ 1 个公网 IP），高防 IP 可防护多台云主机；\n高防包只能防护腾讯云内设备，高防 IP 可防护非腾讯云设备\n","date":1514616469,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514616469,"objectID":"cbc5bd9541b2aad5110989b9c1d0ec2f","permalink":"https://wubigo.com/post/saas-notes/","publishdate":"2017-12-30T14:47:49+08:00","relpermalink":"/post/saas-notes/","section":"post","summary":"DDoS 基础防护 腾讯云 DDoS 基础防护本身免费，当用户购买了腾讯云 CVM、CLB 等服务时，会自动开启 DDoS 基础防护。\n普通用户提供2Gbps的防护能力，最高可达10Gbps\n高防IP 高防包不需要更改客户业务 IP，高防IP需要客户将腾讯的高防 IP 作为业务 IP 发布；\n高防包只能防护一台云主机或一台负载均衡（ 1 个公网 IP），高防 IP 可防护多台云主机；\n高防包只能防护腾讯云内设备，高防 IP 可防护非腾讯云设备","tags":["SAAS"],"title":"SaaS Notes","type":"post"},{"authors":null,"categories":null,"content":" Gone with the Wind https://www.amazon.com/Gone-Wind-Margaret-Mitchell\nDeep Learning with Python https://www.manning.com/books/deep-learning-with-python\nHow to Win Friends and Influence People https://www.amazon.com/How-Friends-Influence-People-Chinese\nBooks I read this year https://www.gatesnotes.com/About-Bill-Gates/Best-Books-2017\n","date":1514592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514592000,"objectID":"d3ca72d17b15012cf75563e378e0a9b8","permalink":"https://wubigo.com/post/2017-12-30-booksireadthisyear/","publishdate":"2017-12-30T00:00:00Z","relpermalink":"/post/2017-12-30-booksireadthisyear/","section":"post","summary":"Gone with the Wind https://www.amazon.com/Gone-Wind-Margaret-Mitchell\nDeep Learning with Python https://www.manning.com/books/deep-learning-with-python\nHow to Win Friends and Influence People https://www.amazon.com/How-Friends-Influence-People-Chinese\nBooks I read this year https://www.gatesnotes.com/About-Bill-Gates/Best-Books-2017","tags":null,"title":"Books I read this year","type":"post"},{"authors":null,"categories":[],"content":" Enabling Enhanced Networking on Ubuntu sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y linux-aws  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide//sriov-networking.html#enhanced-networking-ubuntu\n","date":1514554169,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514554169,"objectID":"54fd2926e3da909419d2161d8071519f","permalink":"https://wubigo.com/post/aws-sr-iov/","publishdate":"2017-12-29T21:29:29+08:00","relpermalink":"/post/aws-sr-iov/","section":"post","summary":"Enabling Enhanced Networking on Ubuntu sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y linux-aws  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide//sriov-networking.html#enhanced-networking-ubuntu","tags":["AWS","NFV"],"title":"Aws SR IOV","type":"post"},{"authors":null,"categories":[],"content":" VLAN VLAN（802.1Q）是一个局域网技术，能够将一个局域网的广播域隔离为多个广播域，常被用来实现一个站点内不同的部门间的隔离\n数据中心网络虚拟化——NVo3技术端到端隧道 NVo3（Network Virtualization over Layer 3），是IETF 2014年十月份提出的数据中心虚拟化技术框架。\nNVo3基于IP/MPLS作为传输网，在其上通过隧道连接的方式，构建大规模的二层租户网络。NVo3的技术模型如下所示，\nPE设备称为NVE（Network Virtualization Element），VN Context作为Tag标识租户网络，P设备即为普通的IP/MPLS路由器。\nNVo3在设计之初，VxLAN与SDN的联合部署已经成为了数据中心的大趋势，因此NVo3的模型中专门画出了\nNVA(Network Virtualization Authority）作为NVE设备的控制器负责隧道建立、地址学习等控制逻辑\nVxLAN（Virtual eXtensible LAN，RFC 7348） Vmware和Cisco联合提出的一种二层技术，突破了VLAN ID只有4k的限制，允许通过现有的IP网络进行隧道的传输。\n别看VxLAN名字听起来和VLAN挺像，但是两者技术上可没什么必然联系。VxLAN是一种MACinUDP的隧道.\nNvGRE NvGRE（Network virtualization GRE，RFC draft）是微软搞出来的数据中心虚拟化技术，是一种MACinGRE隧道。它对传统的GRE报头进行了改造，增加了24位的VSID字段标识租户，而FlowID可用来做ECMP。由于去掉了GRE报头中的Checksum字段，因此NvGRE不支持校验和检验。NvGRE封装以太网帧，外层的报头可以为IPv4也可以为IPv6\nhttps://www.sdnlab.com/nv-subject/\n","date":1514521868,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514521868,"objectID":"63a943ffb96ff1b643bd83c49deafac7","permalink":"https://wubigo.com/post/nfv-notes/","publishdate":"2017-12-29T12:31:08+08:00","relpermalink":"/post/nfv-notes/","section":"post","summary":"VLAN VLAN（802.1Q）是一个局域网技术，能够将一个局域网的广播域隔离为多个广播域，常被用来实现一个站点内不同的部门间的隔离\n数据中心网络虚拟化——NVo3技术端到端隧道 NVo3（Network Virtualization over Layer 3），是IETF 2014年十月份提出的数据中心虚拟化技术框架。\nNVo3基于IP/MPLS作为传输网，在其上通过隧道连接的方式，构建大规模的二层租户网络。NVo3的技术模型如下所示，\nPE设备称为NVE（Network Virtualization Element），VN Context作为Tag标识租户网络，P设备即为普通的IP/MPLS路由器。\nNVo3在设计之初，VxLAN与SDN的联合部署已经成为了数据中心的大趋势，因此NVo3的模型中专门画出了\nNVA(Network Virtualization Authority）作为NVE设备的控制器负责隧道建立、地址学习等控制逻辑\nVxLAN（Virtual eXtensible LAN，RFC 7348） Vmware和Cisco联合提出的一种二层技术，突破了VLAN ID只有4k的限制，允许通过现有的IP网络进行隧道的传输。\n别看VxLAN名字听起来和VLAN挺像，但是两者技术上可没什么必然联系。VxLAN是一种MACinUDP的隧道.\nNvGRE NvGRE（Network virtualization GRE，RFC draft）是微软搞出来的数据中心虚拟化技术，是一种MACinGRE隧道。它对传统的GRE报头进行了改造，增加了24位的VSID字段标识租户，而FlowID可用来做ECMP。由于去掉了GRE报头中的Checksum字段，因此NvGRE不支持校验和检验。NvGRE封装以太网帧，外层的报头可以为IPv4也可以为IPv6\nhttps://www.sdnlab.com/nv-subject/","tags":["NFV","SDN"],"title":"NFV Notes","type":"post"},{"authors":null,"categories":[],"content":" 创建BUCKET 使用两种方式之一创建BUCKET\n terraform\ngit clone https://github.com/wubigo/iaas cd s3 terraform apply  awscli\naws s3 website s3://s.wubigo.com/ --index-document index.html --error-document 404.html aws s3api put-bucket-policy --bucket s.wubigo.com --policy file://policy.json   确认配置 aws s3api get-bucket-website --bucket s.wubigo.com { \u0026quot;IndexDocument\u0026quot;: { \u0026quot;Suffix\u0026quot;: \u0026quot;index.html\u0026quot; }, \u0026quot;ErrorDocument\u0026quot;: { \u0026quot;Key\u0026quot;: \u0026quot;404.html\u0026quot; } }  配置DNS C记录 查看S3 Website Endpoints: s.wubigo.com.s3-website-ap-northeast-1.amazonaws.com\nCNAME Record\ts s.wubigo.com.s3-website-ap-northeast-1.amazonaws.com  上传站点内容 aws s3 cp wubigo.github.io s3://s.wubigo.com/ --recursive  ","date":1514279608,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514279608,"objectID":"69945c5ad2b5a745dae7c4fac91e6de0","permalink":"https://wubigo.com/post/aws-s3-web-hosting/","publishdate":"2017-12-26T17:13:28+08:00","relpermalink":"/post/aws-s3-web-hosting/","section":"post","summary":" 创建BUCKET 使用两种方式之一创建BUCKET\n terraform\ngit clone https://github.com/wubigo/iaas cd s3 terraform apply  awscli\naws s3 website s3://s.wubigo.com/ --index-document index.html --error-document 404.html aws s3api put-bucket-policy --bucket s.wubigo.com --policy file://policy.json   确认配置 aws s3api get-bucket-website --bucket s.wubigo.com { \u0026quot;IndexDocument\u0026quot;: { \u0026quot;Suffix\u0026quot;: \u0026quot;index.html\u0026quot; }, \u0026quot;ErrorDocument\u0026quot;: { \u0026quot;Key\u0026quot;: \u0026quot;404.html\u0026quot; } }  配置DNS C记录 查看S3 Website Endpoints: s.wubigo.com.s3-website-ap-northeast-1.amazonaws.com\nCNAME Record\ts s.wubigo.com.s3-website-ap-northeast-1.amazonaws.com  上传站点内容 aws s3 cp wubigo.github.io s3://s.wubigo.com/ --recursive  ","tags":["AWS","S3"],"title":"Aws S3 Web Hosting","type":"post"},{"authors":null,"categories":[],"content":" go version go version go version go1.13.5 windows/amd64  vs proxy 根据code提示自动安装插件\n手工安装插件 go代理配置 set http_proxy=http://127.0.0.1:4910  git代理配置 git config --global http.proxy https://127.0.0.1:4910 git config --global http.sslverify \u0026quot;false\u0026quot;  手工安装插件 go get -u -v github.com/go-delve/delve/cmd/dlv go get -u -v github.com/ramya-rao-a/go-outline go get -u -v github.com/ramya-rao-a/go-outline go get -u -v github.com/acroca/go-symbols go get -u -v github.com/mdempsky/gocode go get -u -v github.com/rogpeppe/godef go get -u -v golang.org/x/tools/cmd/godoc go get -u -v github.com/zmb3/gogetdoc go get -u -v golang.org/x/lint/golint go get -u -v github.com/fatih/gomodifytags go get -u -v golang.org/x/tools/cmd/gorename go get -u -v sourcegraph.com/sqs/goreturns go get -u -v golang.org/x/tools/cmd/goimports go get -u -v github.com/cweill/gotests/... go get -u -v golang.org/x/tools/cmd/guru go get -u -v github.com/josharian/impl go get -u -v github.com/haya14busa/goplay/cmd/goplay go get -u -v github.com/uudashr/gopkgs/cmd/gopkgs go get -u -v github.com/davidrjenni/reftools/cmd/fillstruct  FAQ  vscode go build __debug_bin: Access is denied\ncreate go\\.vscode\\launch.json\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;Launch\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;go\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;program\u0026quot;: \u0026quot;${fileDirname}\u0026quot;, \u0026quot;env\u0026quot;: {}, \u0026quot;args\u0026quot;: [] } ] }  fork/exec d:\\cache\\go-build618440214\\b001\\exe\\sitemap.exe: Access is denied.\n简单的程序运行没有问题\nD:\\code\\\u0026gt;set go GOBIN=D:\\code\\go\\bin GOPATH=d:\\code\\go GOTMPDIR=d:\\cache  VSCODE没有权限访问GOTMPDIR\ngo run sitemap.go   ","date":1513927771,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513927771,"objectID":"ef29b4be90018c5707a2df3e0673e5e3","permalink":"https://wubigo.com/post/go-vscode/","publishdate":"2017-12-22T15:29:31+08:00","relpermalink":"/post/go-vscode/","section":"post","summary":"go version go version go version go1.13.5 windows/amd64  vs proxy 根据code提示自动安装插件\n手工安装插件 go代理配置 set http_proxy=http://127.0.0.1:4910  git代理配置 git config --global http.proxy https://127.0.0.1:4910 git config --global http.sslverify \u0026quot;false\u0026quot;  手工安装插件 go get -u -v github.com/go-delve/delve/cmd/dlv go get -u -v github.com/ramya-rao-a/go-outline go get -u -v github.com/ramya-rao-a/go-outline go get -u -v github.com/acroca/go-symbols go get -u -v github.com/mdempsky/gocode go get -u -v github.com/rogpeppe/godef go get -u -v golang.org/x/tools/cmd/godoc go get -u -v github.","tags":["GO"],"title":"Go Vscode环境配置","type":"post"},{"authors":null,"categories":[],"content":" 与回调函数的区别  不用写错误条件if (err) return callback(err) Promise能被作为对象返回并被后期调用\n 回调\nfunction successCallback(result) { console.log(\u0026quot;Audio file ready at URL: \u0026quot; + result); } function failureCallback(error) { console.error(\u0026quot;Error generating audio file: \u0026quot; + error); } createAudioFileAsync(audioSettings, successCallback, failureCallback);  promise\nconst promise = createAudioFileAsync(audioSettings); promise.then(successCallback, failureCallback);   or\ncreateAudioFileAsync(audioSettings).then(successCallback, failureCallback);  状态 Promise有三种状态\n pending: Initial Case where promise instantiated. fulfilled: Success Case which means promise resolved. rejected: Failure Case which means promise rejected.  方法 Promise有六个方法：\n Promise.all([promise1, promise2, …]); Promise.race([promise1, promise2, …]); Promise.reject(value); Promise.resolve(value); Promise.catch(onRejection); Promise.then(onFulFillment, onRejection);\nAPI参考\n  执行函数  Executor Functions are Parameter for Promise Constructor which holds Resolve and Reject Callbacks. It is executed immediately by the Promise implementation which provides the resolve and reject functions. It’s Triggered before the Promise constructor even returns the created object. The Resolve and Reject functions are bound to the promise to fulfill or reject. It’s expected to initiate some asynchronous work and then, once that completes, call either the resolve or reject.  可以被覆盖的方法  Promise.prototype.catch(); Promise.prototype.then();  错误处理 Promise的两种错误处理方式：\n then方法的第二个参数onRejection回调 catch\n'use strict'; // First Approach yourPromise.catch(function (error) { // Your Error Callback }); // Second Approach yourPromise.then(undefined, function (error) { // Your Error Callback });   错误检测：根据catch或onRejection定义的顺序被触发\n'use strict'; Promise.catch(onRejected); Promise.then(onFulfilled, onRejected);  ","date":1513812931,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513812931,"objectID":"7ed2dfd38139da06e83cbaa38a217551","permalink":"https://wubigo.com/post/nodejs-promise/","publishdate":"2017-12-21T07:35:31+08:00","relpermalink":"/post/nodejs-promise/","section":"post","summary":"与回调函数的区别  不用写错误条件if (err) return callback(err) Promise能被作为对象返回并被后期调用\n 回调\nfunction successCallback(result) { console.log(\u0026quot;Audio file ready at URL: \u0026quot; + result); } function failureCallback(error) { console.error(\u0026quot;Error generating audio file: \u0026quot; + error); } createAudioFileAsync(audioSettings, successCallback, failureCallback);  promise\nconst promise = createAudioFileAsync(audioSettings); promise.then(successCallback, failureCallback);   or\ncreateAudioFileAsync(audioSettings).then(successCallback, failureCallback);  状态 Promise有三种状态\n pending: Initial Case where promise instantiated. fulfilled: Success Case which means promise resolved. rejected: Failure Case which means promise rejected.","tags":["NODE"],"title":"Nodejs异步通信之Promise","type":"post"},{"authors":null,"categories":[],"content":" 对象创建有如下几种方式\n使用{} let animal = {} animal.name = 'Leo' animal.energy = 10 animal.eat = function (amount) { console.log(`${this.name} is eating.`) this.energy += amount } animal.sleep = function (length) { console.log(`${this.name} is sleeping.`) this.energy += length } animal.play = function (length) { console.log(`${this.name} is playing.`) this.energy -= length }  构造函数 function Animal (name, energy) { let animal = {} animal.name = name animal.energy = energy animal.eat = function (amount) { console.log(`${this.name} is eating.`) this.energy += amount } animal.sleep = function (length) { console.log(`${this.name} is sleeping.`) this.energy += length } animal.play = function (length) { console.log(`${this.name} is playing.`) this.energy -= length } return animal } const leo = Animal('Leo', 7) const snoop = Animal('Snoop', 10)  缺点  对象方法重复占用内存空间  方法共享 const animalMethods = { eat(amount) { console.log(`${this.name} is eating.`) this.energy += amount }, sleep(length) { console.log(`${this.name} is sleeping.`) this.energy += length }, play(length) { console.log(`${this.name} is playing.`) this.energy -= length } } function Animal (name, energy) { let animal = {} animal.name = name animal.energy = energy animal.eat = animalMethods.eat animal.sleep = animalMethods.sleep animal.play = animalMethods.play return animal } const leo = Animal('Leo', 7) const snoop = Animal('Snoop', 10)  Object.create Object.create allows you to create an object which will delegate to another\nobject on failed lookups. Put differently, Object.create allows you to create\nan object and whenever there’s a failed property lookup on that object, it can\nconsult another object to see if that other object has the property\nconst parent = { name: 'Stacey', age: 35, heritage: 'Irish' } const child = Object.create(parent) child.name = 'Ryan' child.age = 7 console.log(child.name) // Ryan console.log(child.age) // 7 console.log(child.heritage) // Irish  Object.create并方法共享 const animalMethods = { eat(amount) { console.log(`${this.name} is eating.`) this.energy += amount }, sleep(length) { console.log(`${this.name} is sleeping.`) this.energy += length }, play(length) { console.log(`${this.name} is playing.`) this.energy -= length } } function Animal (name, energy) { let animal = Object.create(animalMethods) animal.name = name animal.energy = energy return animal } const leo = Animal('Leo', 7) const snoop = Animal('Snoop', 10) leo.eat(10) snoop.play(5)  缺点  要单独管理方法对象  prototype every function in JavaScript has a prototype property that references an object\nfunction doThing () {} console.log(doThing.prototype) // {}  function Animal (name, energy) { let animal = Object.create(Animal.prototype) animal.name = name animal.energy = energy return animal } Animal.prototype.eat = function (amount) { console.log(`${this.name} is eating.`) this.energy += amount } Animal.prototype.sleep = function (length) { console.log(`${this.name} is sleeping.`) this.energy += length } Animal.prototype.play = function (length) { console.log(`${this.name} is playing.`) this.energy -= length } const leo = Animal('Leo', 7) const snoop = Animal('Snoop', 10) leo.eat(10) snoop.play(5)  ","date":1513811654,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513811654,"objectID":"360929cb29fcc235c4bfabb4cec26ed5","permalink":"https://wubigo.com/post/nodejs-prototype/","publishdate":"2017-12-21T07:14:14+08:00","relpermalink":"/post/nodejs-prototype/","section":"post","summary":"对象创建有如下几种方式\n使用{} let animal = {} animal.name = 'Leo' animal.energy = 10 animal.eat = function (amount) { console.log(`${this.name} is eating.`) this.energy += amount } animal.sleep = function (length) { console.log(`${this.name} is sleeping.`) this.energy += length } animal.play = function (length) { console.log(`${this.name} is playing.`) this.energy -= length }  构造函数 function Animal (name, energy) { let animal = {} animal.name = name animal.energy = energy animal.eat = function (amount) { console.","tags":["NODE"],"title":"Nodejs对象创建方式","type":"post"},{"authors":null,"categories":[],"content":" Architecture domain Since Stephen Spewak\u0026rsquo;s Enterprise Architecture Planning (EAP) in 1993, and perhaps before then, it has been normal to divide enterprises architecture into four architecture domains.\n Business architecture, Data architecture, Applications architecture, Technology architecture.  Layers of the enterprise architecture\n","date":1512948310,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512948310,"objectID":"e7390efc988b5d7ef3ac2ea70e2fdcc9","permalink":"https://wubigo.com/post/enterprise-architecture-framework/","publishdate":"2017-12-11T07:25:10+08:00","relpermalink":"/post/enterprise-architecture-framework/","section":"post","summary":"Architecture domain Since Stephen Spewak\u0026rsquo;s Enterprise Architecture Planning (EAP) in 1993, and perhaps before then, it has been normal to divide enterprises architecture into four architecture domains.\n Business architecture, Data architecture, Applications architecture, Technology architecture.  Layers of the enterprise architecture","tags":["EA","ToGAF"],"title":"Enterprise Architecture Framework","type":"post"},{"authors":null,"categories":[],"content":" set registry npm config set registry=http://registry.npm.taobao.org npm config ls -l userconfig = \u0026quot;C:\\\\Users\\\\Administrator\\\\.npmrc\u0026quot;  declare variables ES6 comes with two more options to declare your variables: const and let. In JavaScript ES6, you will\nrarely find var anymore.\nA variable declared with const cannot be re-assigned or re-declared. It cannot get mutated (changed,\nmodified)\nImmutability is embraced in React and its ecosystem. That’s why const should be your default\nchoice when you define a variable.\nES6 Arrow Functions // function expression function () { ... } // arrow function expression () =\u0026gt; { ... }  You can remove the parentheses when the function\ngets only one argument, but have to keep them when\nit gets multiple arguments.\n// allowed item =\u0026gt; { ... } // allowed (item) =\u0026gt; { ... } // not allowed item, key =\u0026gt; { ... } // allowed (item, key) =\u0026gt; { ... }  Additionally, you can remove the block body, meaning the curly braces, of the ES6 arrow function. In a concise body an implicit return is attached. Thus you can remove the return statement. That will happen more often in the book, so be sure to understand the difference between a block body and a concise body when using arrow functions\nES6 Object Initializer you are allowed to use computed property names in JavaScript ES6\n// ES6 const key = 'name'; const user = { \\[key]: 'Robin', };  Bindings class methods don’t\nautomatically bind this to the class instance\nThat’s a main source of bugs when using React, because if you want to access\nthis.state in your class method, it cannot be retrieved because this is undefined. So in order to\nmake this accessible in your class methods, you have to bind the class methods to this.\nclass methods can be autobound automatically without\nbinding them explicitly by using JavaScript ES6 arrow functions\nclass A { constructor() { this.foo = this.foo.bind(this) } foo() { console.log('foo from A') } } class A { foo = () =\u0026gt; { console.log('foo from A') } }  The official React documentation sticks to the class method bindings in the constructor\nEXPORT default statement  to export and import a single functionality to highlight the main functionality of the exported API of a module to have a fallback import functionality\nconst robin = { firstname: 'robin', lastname: 'wieruch', }; export default robin;   Furthermore, the import name can differ from the exported default name\nAsynchronous code execution Because most of the JavaScript runtimes are single-threaded, many longer operations, such as network requests, are executed asynchronously. Asynchronous code execution is handled by two known concepts: callbacks and promises.\n promises\nA promise represents an eventual result of an asynchronous operation Promises are just pretty wrappers around callbacks. In real-world situations, you wrap a promise around a certain action or operation. A promise can have two possible outcomes: it can be resolved (fulfilled) or rejected (unfulfilled).   enable \u0026ldquo;TypeScript and JavaScript Language Features\u0026rdquo; extension in VS Code Go to extensions and search @builtin typescript to find the extension\nuuid The fastest possible way to create random 32-char string in Node\nis by using native crypto module(no external dependency is needed):\nconst randomBytes = require('crypto').randomBytes; const uuid = randomBytes(16).toString(\u0026quot;hex\u0026quot;);  ","date":1512261592,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512261592,"objectID":"11eb496d211ce7e363a7a703a1eedd92","permalink":"https://wubigo.com/post/nodejs-notes/","publishdate":"2017-12-03T08:39:52+08:00","relpermalink":"/post/nodejs-notes/","section":"post","summary":"set registry npm config set registry=http://registry.npm.taobao.org npm config ls -l userconfig = \u0026quot;C:\\\\Users\\\\Administrator\\\\.npmrc\u0026quot;  declare variables ES6 comes with two more options to declare your variables: const and let. In JavaScript ES6, you will\nrarely find var anymore.\nA variable declared with const cannot be re-assigned or re-declared. It cannot get mutated (changed,\nmodified)\nImmutability is embraced in React and its ecosystem. That’s why const should be your default","tags":["NODE","JS"],"title":"Nodejs Notes","type":"post"},{"authors":null,"categories":null,"content":" Machine Learning 101 slidedeck https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e\u0026amp;cmp=em-data-na-na-newsltr_20171213\u0026amp;slide=id.g183f28bdc3_0_90\n","date":1512172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512172800,"objectID":"d6c0dd10ab9b50518ec7bde144b723e2","permalink":"https://wubigo.com/post/2017-12-02-machinelearning101slidedeck/","publishdate":"2017-12-02T00:00:00Z","relpermalink":"/post/2017-12-02-machinelearning101slidedeck/","section":"post","summary":"Machine Learning 101 slidedeck https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e\u0026amp;cmp=em-data-na-na-newsltr_20171213\u0026amp;slide=id.g183f28bdc3_0_90","tags":null,"title":"Machine Learning 101 slidedeck","type":"post"},{"authors":null,"categories":[],"content":" AWS免费类型  首次注册后的12个月免费\n 永久免费\n 试用\n  aws永久免费的服务 计算服务  Lambda\n1百万请求/月\n400,000 GB-seconds of compute time per month\n Step\n4000/月\n  存储  DynamoDB\n25GB\n S3\n  The S3 free tier allows users to store 5 GB of data with standard storage, issue\n20,000 GET requests and 2,000 PUT requests, and transfer 15 GB of data out each\nmonth\n Glacier\n10GB\n  Cognito 50,000 MAUs for users who sign in directly to Cognito User Pools\n12月内免费服务项目 SQS 每月15 GB of data transfer out\nAll customers can make 1 million Amazon SQS requests for free each month\nSNS first million push notifications (publishes and deliveries) are free every month\nnew AWS customers will receive free 15 GB of data transfer out each month\n","date":1512035489,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512035489,"objectID":"4dd04abc60d46efd1ea9ae12107e3b49","permalink":"https://wubigo.com/post/aws-free-tier-quota/","publishdate":"2017-11-30T17:51:29+08:00","relpermalink":"/post/aws-free-tier-quota/","section":"post","summary":"AWS免费类型  首次注册后的12个月免费\n 永久免费\n 试用\n  aws永久免费的服务 计算服务  Lambda\n1百万请求/月\n400,000 GB-seconds of compute time per month\n Step\n4000/月\n  存储  DynamoDB\n25GB\n S3\n  The S3 free tier allows users to store 5 GB of data with standard storage, issue\n20,000 GET requests and 2,000 PUT requests, and transfer 15 GB of data out each\nmonth\n Glacier\n10GB","tags":["AWS","IAAS","CLOUD","LOCALSTACK"],"title":"AWS云服务免费额度列表","type":"post"},{"authors":null,"categories":[],"content":" create rest api resource awslocal apigateway create-rest-api --name 'My First API' --description 'This is my first API'  awslocal apigateway get-rest-apis { \u0026quot;items\u0026quot;: [ { \u0026quot;createdDate\u0026quot;: 1574513755, \u0026quot;id\u0026quot;: \u0026quot;tjc336382o\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;hello_api2\u0026quot; }, { \u0026quot;description\u0026quot;: \u0026quot;This is my first API\u0026quot;, \u0026quot;createdDate\u0026quot;: 1574513755, \u0026quot;id\u0026quot;: \u0026quot;foyylqv018\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;My First API\u0026quot; } ] }  ","date":1511441566,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511441566,"objectID":"8cdde9ad919ef6347f9ce66918bc4d52","permalink":"https://wubigo.com/post/aws-apigateway-notes/","publishdate":"2017-11-23T20:52:46+08:00","relpermalink":"/post/aws-apigateway-notes/","section":"post","summary":" create rest api resource awslocal apigateway create-rest-api --name 'My First API' --description 'This is my first API'  awslocal apigateway get-rest-apis { \u0026quot;items\u0026quot;: [ { \u0026quot;createdDate\u0026quot;: 1574513755, \u0026quot;id\u0026quot;: \u0026quot;tjc336382o\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;hello_api2\u0026quot; }, { \u0026quot;description\u0026quot;: \u0026quot;This is my first API\u0026quot;, \u0026quot;createdDate\u0026quot;: 1574513755, \u0026quot;id\u0026quot;: \u0026quot;foyylqv018\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;My First API\u0026quot; } ] }  ","tags":[],"title":"Aws Apigateway Notes","type":"post"},{"authors":null,"categories":[],"content":" 查看本月费用  The Cost Explorer API enables you to programmatically query your cost and usage data. You can query for aggregated data such as total monthly costs or total daily usage. You can also query for granular data, such as the number of daily write operations for Amazon DynamoDB database tables in your production environment   Linux\naws ce get-cost-and-usage --time-period Start=$(date -u -d \u0026quot;$TODAY\u0026quot; '+%Y-%m-01'),End=$(date -u +\u0026quot;%Y-%m-%d\u0026quot; --date=\u0026quot;+1 day\u0026quot;) --granularity MONTHLY --metrics UnblendedCost --output text   注意：CE服务是按api调用次数收费\nAWS Cost Explorer USE1-APIRequest $0.05 $0.01 per API Request 5.000 Request $0.05  steps to make ec2 access from outside  create vpc create internet gateway add route to the gateway into the routetable  RequestTimeTooSkewed error with S3 upload https://github.com/aws/aws-sdk-js/issues/399 https://aws.amazon.com/blogs/developer/clock-skew-correction/\ncloudfront set Origin Custom Headers https://w3guy.com/solution-font-origin-http-cdn-domain-blocked-loading-cors-policy/\n","date":1511441566,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511441566,"objectID":"2b9ce00fb2f3a520ff4f334760374e55","permalink":"https://wubigo.com/post/2014-02-03-aws-notes/","publishdate":"2017-11-23T20:52:46+08:00","relpermalink":"/post/2014-02-03-aws-notes/","section":"post","summary":"查看本月费用  The Cost Explorer API enables you to programmatically query your cost and usage data. You can query for aggregated data such as total monthly costs or total daily usage. You can also query for granular data, such as the number of daily write operations for Amazon DynamoDB database tables in your production environment   Linux\naws ce get-cost-and-usage --time-period Start=$(date -u -d \u0026quot;$TODAY\u0026quot; '+%Y-%m-01'),End=$(date -u +\u0026quot;%Y-%m-%d\u0026quot; --date=\u0026quot;+1 day\u0026quot;) --granularity MONTHLY --metrics UnblendedCost --output text   注意：CE服务是按api调用次数收费","tags":["AWS"],"title":"Aws Notes","type":"post"},{"authors":null,"categories":[],"content":" 前提条件 配置AWS aws configure list Name Value Type Location ---- ----- ---- -------- profile \u0026lt;not set\u0026gt; None None access_key ****************s-ok shared-credentials-file secret_key ****************-key shared-credentials-file region local config-file ~/.aws/config  ~/.aws/config\n[default] output = json region = local  ~/.aws/credentials\n[default] aws_access_key_id = any-id-is-ok aws_secret_access_key = fake-key  启动aws本地服务 localstack start  创建EC2 配置 mkdir ec2 cd ec2 touch ec2.tf  ec2.tf\nprovider \u0026quot;aws\u0026quot; { profile = \u0026quot;default\u0026quot; region = \u0026quot;us-east-1\u0026quot; endpoints { ec2 = \u0026quot;http://localhost:4597\u0026quot; sts = \u0026quot;http://localhost:4592\u0026quot; } } resource \u0026quot;aws_instance\u0026quot; \u0026quot;example\u0026quot; { ami = \u0026quot;ami-2757f631\u0026quot; instance_type = \u0026quot;t2.micro\u0026quot; }  aws localstack provider provider \u0026quot;aws\u0026quot; { access_key = \u0026quot;mock_access_key\u0026quot; region = \u0026quot;us-east-1\u0026quot; secret_key = \u0026quot;mock_secret_key\u0026quot; s3_force_path_style = true skip_credentials_validation = true skip_metadata_api_check = true skip_requesting_account_id = true endpoints { apigateway = \u0026quot;http://localhost:4567\u0026quot; cloudformation = \u0026quot;http://localhost:4581\u0026quot; cloudwatch = \u0026quot;http://localhost:4582\u0026quot; dynamodb = \u0026quot;http://localhost:4569\u0026quot; es = \u0026quot;http://localhost:4578\u0026quot; firehose = \u0026quot;http://localhost:4573\u0026quot; iam = \u0026quot;http://localhost:4593\u0026quot; kinesis = \u0026quot;http://localhost:4568\u0026quot; lambda = \u0026quot;http://localhost:4574\u0026quot; route53 = \u0026quot;http://localhost:4580\u0026quot; redshift = \u0026quot;http://localhost:4577\u0026quot; s3 = \u0026quot;http://localhost:4572\u0026quot; secretsmanager = \u0026quot;http://localhost:4584\u0026quot; ses = \u0026quot;http://localhost:4579\u0026quot; sns = \u0026quot;http://localhost:4575\u0026quot; sqs = \u0026quot;http://localhost:4576\u0026quot; ssm = \u0026quot;http://localhost:4583\u0026quot; stepfunctions = \u0026quot;http://localhost:4585\u0026quot; sts = \u0026quot;http://localhost:4592\u0026quot; } }  初始化  下载SP\nterraform init  应用配置\nterraform apply   成功执行后terraform.tfstate自动生成，该文件记录被管理资源的ID\n 显示结果\nterraform show   检查结果 awslocal ec2 describe-instances  销毁资源 terraform destroy  配置 .terraformrc\nplugin_cache_dir = \u0026quot;$HOME/.terraform.d/plugin-cache\u0026quot; disable_checkpoint = true   Third-party Plugins  These third-party providers must be manually installed,\nsince terraform init cannot automatically download them\n~/.terraform.d/plugin  https://stackoverflow.com/questions/50944395/use-pre-installed-terraform-plugins-instead-of-downloading-them-with-terraform-i/59015322#59015322\ninit terraform init -input=false -plugin-dir=/usr/lib/custom-terraform-plugins/windows_amd64  ","date":1511421061,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511421061,"objectID":"dcedb933db8e4ca23b71439ac8fa78b2","permalink":"https://wubigo.com/post/terraform-notes/","publishdate":"2017-11-23T15:11:01+08:00","relpermalink":"/post/terraform-notes/","section":"post","summary":"前提条件 配置AWS aws configure list Name Value Type Location ---- ----- ---- -------- profile \u0026lt;not set\u0026gt; None None access_key ****************s-ok shared-credentials-file secret_key ****************-key shared-credentials-file region local config-file ~/.aws/config  ~/.aws/config\n[default] output = json region = local  ~/.aws/credentials\n[default] aws_access_key_id = any-id-is-ok aws_secret_access_key = fake-key  启动aws本地服务 localstack start  创建EC2 配置 mkdir ec2 cd ec2 touch ec2.tf  ec2.tf\nprovider \u0026quot;aws\u0026quot; { profile = \u0026quot;default\u0026quot; region = \u0026quot;us-east-1\u0026quot; endpoints { ec2 = \u0026quot;http://localhost:4597\u0026quot; sts = \u0026quot;http://localhost:4592\u0026quot; } } resource \u0026quot;aws_instance\u0026quot; \u0026quot;example\u0026quot; { ami = \u0026quot;ami-2757f631\u0026quot; instance_type = \u0026quot;t2.","tags":["LOCALSTACK","TERRAFORM"],"title":"Terraform Notes","type":"post"},{"authors":null,"categories":[],"content":" 标准设备  Bridge: A Linux bridge behaves like a network switch. It forwards packets between interfaces that are connected to it. It\u0026rsquo;s usually used for forwarding packets on routers, on gateways, or between VMs and network namespaces on a host. It also supports STP, VLAN filter, and multicast snooping. TUN: TUN (network Tunnel) devices work at the IP level or layer three level of the network stack and are usually point-to-point connections. A typical use for a TUN device is establishing VPN connections since it gives the VPN software a chance to encrypt the data before it gets put on the wire. Since a TUN device works at layer three it can only accept IP packets and in some cases only IPv4. If you need to run any other protocol over a TUN device you’re out of luck. Additionally because TUN devices work at layer three they can’t be used in bridges and don\u0026rsquo;t typically support broadcasting. TAP: TAP (terminal access point) devices, in contrast, work at the Ethernet level or layer two and therefore behave very much like a real network adaptor. Since they are running at layer two they can transport any layer three protocol and aren’t limited to point-to-point connections. TAP devices can be part of a bridge and are commonly used in virtualization systems to provide virtual network adaptors to multiple guest machines. Since TAP devices work at layer two they will forward broadcast traffic which normally makes them a poor choice for VPN connections as the VPN link is typically much narrower than a LAN network (and usually more expensive). VETH: Virtual Ethernet interfaces are essentially a virtual equivalent of a patch cable, what goes in one end comes out the other. When either device is down, the link state of the pair is down.  网络名字空间 Network namespaces allows different processes to have different views of the network and different aspects of networking can be isolated between processes\nhttps://gist.github.com/mtds/4c4925c2aa022130e4b7c538fdd5a89f\n","date":1510364562,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510364562,"objectID":"6d09fb6ba05862fa7884d011c0f863b4","permalink":"https://wubigo.com/post/virtual-networking-on-linux/","publishdate":"2017-11-11T09:42:42+08:00","relpermalink":"/post/virtual-networking-on-linux/","section":"post","summary":"标准设备  Bridge: A Linux bridge behaves like a network switch. It forwards packets between interfaces that are connected to it. It\u0026rsquo;s usually used for forwarding packets on routers, on gateways, or between VMs and network namespaces on a host. It also supports STP, VLAN filter, and multicast snooping. TUN: TUN (network Tunnel) devices work at the IP level or layer three level of the network stack and are usually point-to-point connections.","tags":["NFV","LINUX","NETWORK"],"title":"Linux中的虚拟网络设施","type":"post"},{"authors":null,"categories":[],"content":" disable Taskbar thumbnail preview on Windows https://www.windowscentral.com/how-disable-taskbar-thumbnail-preview-windows-10\n","date":1506669961,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506669961,"objectID":"df61cfea5c04d0c5a243098d0839a4e4","permalink":"https://wubigo.com/post/win10-customize/","publishdate":"2017-09-29T15:26:01+08:00","relpermalink":"/post/win10-customize/","section":"post","summary":"disable Taskbar thumbnail preview on Windows https://www.windowscentral.com/how-disable-taskbar-thumbnail-preview-windows-10","tags":["WINDOWS"],"title":"Win10 Customize","type":"post"},{"authors":null,"categories":[],"content":" 最近在做智慧工地的项目，“智慧工地”的建设很大一部分信息主要是来自于工程BIM模型。 大部分情况下，BIM模型的精确度决定了“智慧工地”的开展程度。 “智慧工地”建设中，BIM模型的应用主要集中在以下几个方面：\n 工程量的统计：分析各施工流水段各材料的工程量，如混凝土的工程量。\n 施工模拟：施工进度计划与BIM模型相关联，对施工过程进行模拟。将实际工程进度与模拟进度进行对比，可以直观的看出工程是否滞后。\n 可视化交底：通过BIM的可视化特点，对施工方案进行模拟，对施工人员进行3D动画交底，提高了交底的可行性。\n 节点分析： 对复杂节点进行BIM建模，通过模型对复杂节点进行分析。\n 综合管线碰撞检测： 检测预留孔洞、机电、设备管线安装碰撞。\n  从具体实现的角度对BIM和CAD做个比较\n    CAD BIM     工具集 2D(3D通过划线) 3D   连接过程 直接(2D对象直接连接) 间接(3D对象通过参数装配)   视图 高度具体的可视化视图 动态流线型视图，可随意放大，缩小   知识复用  MODEL(部件从MODEL抽取，而且部件的修改同时同步到MODEL)    基于WEB的3D建模工具  https://www.onshape.com/ Lagoa(基于云端的3D建模APP，被Autodesk6千万美元收购) https://www.vectary.com/ https://clara.io/  ","date":1504914766,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504914766,"objectID":"298b657e214758877416cdcc41d63cfb","permalink":"https://wubigo.com/post/bim-vs-cad/","publishdate":"2017-09-09T07:52:46+08:00","relpermalink":"/post/bim-vs-cad/","section":"post","summary":" 最近在做智慧工地的项目，“智慧工地”的建设很大一部分信息主要是来自于工程BIM模型。 大部分情况下，BIM模型的精确度决定了“智慧工地”的开展程度。 “智慧工地”建设中，BIM模型的应用主要集中在以下几个方面：\n 工程量的统计：分析各施工流水段各材料的工程量，如混凝土的工程量。\n 施工模拟：施工进度计划与BIM模型相关联，对施工过程进行模拟。将实际工程进度与模拟进度进行对比，可以直观的看出工程是否滞后。\n 可视化交底：通过BIM的可视化特点，对施工方案进行模拟，对施工人员进行3D动画交底，提高了交底的可行性。\n 节点分析： 对复杂节点进行BIM建模，通过模型对复杂节点进行分析。\n 综合管线碰撞检测： 检测预留孔洞、机电、设备管线安装碰撞。\n  从具体实现的角度对BIM和CAD做个比较\n    CAD BIM     工具集 2D(3D通过划线) 3D   连接过程 直接(2D对象直接连接) 间接(3D对象通过参数装配)   视图 高度具体的可视化视图 动态流线型视图，可随意放大，缩小   知识复用  MODEL(部件从MODEL抽取，而且部件的修改同时同步到MODEL)    基于WEB的3D建模工具  https://www.onshape.com/ Lagoa(基于云端的3D建模APP，被Autodesk6千万美元收购) https://www.vectary.com/ https://clara.io/  ","tags":["3D","AR"],"title":"BIM和CAD的对比","type":"post"},{"authors":null,"categories":["IT"],"content":" On the other hand, convolution is most typically done with 3x3 windows and no stride (stride 1).\nMATPLOTLIB Matplot is a python library that help us to plot data. The easiest and basic plots are line, scatter and histogram plots.\n Line plot is better when x axis is time. Scatter is better when there is correlation between two variables Histogram is better when we need to see distribution of numerical data. Customization: Colors,labels,thickness of line, title, opacity, grid, figsize, ticks of axis and linestyle  How TensorBoard gets data from TensorFlow The first step in using TensorBoard is acquiring data from your TensorFlow run. For this, you need summary ops. Summary ops are ops, like tf.matmul or tf.nn.relu, which means they take in tensors, produce tensors, and are evaluated from within a TensorFlow graph. However, summary ops have a twist: the Tensors they produce contain serialized protobufs, which are written to disk and sent to TensorBoard. To visualize the summary data in TensorBoard, you should evaluate the summary op, retrieve the result, and then write that result to disk using a summary.FileWriter.\nlearning rate Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. The lower the value, the slower we travel along the downward slope. While this might be a good idea (using a low learning rate) in terms of making sure that we do not miss any local minima, it could also mean that we’ll be taking a long time to converge — especially if we get stuck on a plateau region The following formula shows the relationship.\nnew_weight = existing_weight — learning_rate * gradient  Typically learning rates are configured naively at random by the user. At best, the user would leverage on past experiences (or other types of learning material) to gain the intuition on what is the best value to use in setting learning rates.\nAs such, it’s often hard to get it right. The below diagram demonstrates the different scenarios one can fall into when configuring the learning rate\nmost common ways to prevent overfitting in neural networks  Get more training data. Reduce the capacity of the network. Add weight regularization. Add dropout  activation function A function that takes the input signal and generates an output signal, but takes into account some kind of threshold is called an activation function\nCNN feature map state of art AI https://www.stateoftheart.ai/\nhttp://colah.github.io/posts/2014-07-Understanding-Convolutions/\nReferences  [1] https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10 [2] http://wubigo.com/2017/01/numpy-notes/  ","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"5b3065c2f225a6419d8aeb3abb124d09","permalink":"https://wubigo.com/post/2017-08-01-deep_learning_with_python/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/post/2017-08-01-deep_learning_with_python/","section":"post","summary":"On the other hand, convolution is most typically done with 3x3 windows and no stride (stride 1).\nMATPLOTLIB Matplot is a python library that help us to plot data. The easiest and basic plots are line, scatter and histogram plots.\n Line plot is better when x axis is time. Scatter is better when there is correlation between two variables Histogram is better when we need to see distribution of numerical data.","tags":null,"title":"Deep Learning with Python","type":"post"},{"authors":null,"categories":[],"content":" Overlay vs underlay in summary An Underlay network is the physical network responsible for the delivery of packets like DWDM, L2, L3, MPLS, or internet, etc. while an overlay is a logical network that uses network virtualization to built connectivity on top of physical infrastructure using tunneling encapsulations such as VXLAN, GRE, IPSec.\nhttps://telcocloudbridge.com/blog/overlay-vs-underlay-networks/\n","date":1501401726,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501401726,"objectID":"ef0d0be647c398e2cc70094cb2d0c2ba","permalink":"https://wubigo.com/post/overlay-vs-underlay-networks/","publishdate":"2017-07-30T16:02:06+08:00","relpermalink":"/post/overlay-vs-underlay-networks/","section":"post","summary":"Overlay vs underlay in summary An Underlay network is the physical network responsible for the delivery of packets like DWDM, L2, L3, MPLS, or internet, etc. while an overlay is a logical network that uses network virtualization to built connectivity on top of physical infrastructure using tunneling encapsulations such as VXLAN, GRE, IPSec.\nhttps://telcocloudbridge.com/blog/overlay-vs-underlay-networks/","tags":["SDN"],"title":"Overlay vs Underlay Networks","type":"post"},{"authors":null,"categories":[],"content":"Some things worth noting in br_add_if:\n Only ethernet like devices can be added to bridge, as bridge is a layer 2 device. Bridges cannot be added to a bridge. New interface is set to promiscuous mode: dev_set_promiscuity(dev, 1)  https://goyalankit.com/blog/linux-bridge\n","date":1501202178,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501202178,"objectID":"4fd405e9b253ff208d5d75a489baf20d","permalink":"https://wubigo.com/post/linux-bridge/","publishdate":"2017-07-28T08:36:18+08:00","relpermalink":"/post/linux-bridge/","section":"post","summary":"Some things worth noting in br_add_if:\n Only ethernet like devices can be added to bridge, as bridge is a layer 2 device. Bridges cannot be added to a bridge. New interface is set to promiscuous mode: dev_set_promiscuity(dev, 1)  https://goyalankit.com/blog/linux-bridge","tags":["SDN","NFV"],"title":"Linux Bridge","type":"post"},{"authors":null,"categories":null,"content":" 节点维护 kubectl drain \u0026lt;node name\u0026gt;  维护有DaemonSet-managed pod的节点\nkubectl drain \u0026lt;node name\u0026gt; --delete-local-data --force --ignore-daemonsets kubectl delete node \u0026lt;node name\u0026gt; sudo iptables -F sudo iptables -S  create a regular pod 必须使用\u0026ndash;restart=Never\nkubectl run -it curl --image=curlimages/curl:7.72.0 --restart=Never -- sh   Never acts like a cronjob which is scheduled immediately. Always creates a deployment and the deployment monitors the pod and restarts in case of failure.  kubeadm install mirror in china apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update apt-cache madison kubeadm apt install kubeadm=1.18.3-00 apt-get install -y kubelet kubeadm kubectl  join node kubeadm token create --print-join-command  swapoff kubelet服务不会正常启动，如果交换分区没有关闭\ndpkg-query -L kubelet  docker Entrypoint vs k8s command     docker k8s     entry ENTRYPOINT command   arguments CMD args    k8s command and args override the default Entrypoint and Cmd\nDockerfile\nFROM alpine:3.8 RUN apk add --no-cache curl ethtool \u0026amp;\u0026amp; rm -rf /var/cache/apk/* CMD [\u0026quot;--version\u0026quot;] ENTRYPOINT [\u0026quot;curl\u0026quot;]  cmd-override-pod.yaml\napiVersion: v1 kind: Pod metadata: name: command-override labels: purpose: override-command spec: containers: - name: command-override-container image: bigo/curl:v1 command: [\u0026quot;curl\u0026quot;] args: [\u0026quot;--help\u0026quot;] restartPolicy: Never  docker run -it bigo/curl:v1 curl 7.61.1 (x86_64-alpine-linux-musl) libcurl/7.61.1 LibreSSL/2.0.0 zlib/1.2.11 libssh2/1.8.0 nghttp2/1.32.0 Release-Date: 2018-09-05  kubectl apply -f cmd-override-pod.yaml kubectl logs command-override Usage: curl [options...] \u0026lt;url\u0026gt; --abstract-unix-socket \u0026lt;path\u0026gt; Connect via abstract Unix domain socket --anyauth Pick any authentication method -a, --append Append to target file when uploading  工具POD apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28 command: - sleep - \u0026quot;13600\u0026quot; imagePullPolicy: IfNotPresent restartPolicy: Always  busybox:latest has bug on nslookup\ndocker network create test 32024cd09daca748f8254468f4f00893afc2e1173c378919b1f378ed719f1618 docker run -dit --name nginx --network test nginx:alpine 7feaf1f0b4f3d421603bbb984854b753c7cbc6b581dd0a304d3b8fccf8c6604b $ docker run -it --rm --network test busybox:1.28 nslookup nginx Server: 127.0.0.11 Address 1: 127.0.0.11 Name: nginx Address 1: 172.22.0.2 nginx.test docker stop nginx docker network rm test  kubectl exec -ti busybox -- nslookup kubernetes.default kubectl run -it --image busybox test --restart=Never --rm nslookup kubernetes.default  无选择器服务 使用场景：\n 通过SERVICE连接到外部服务 连接到另一个名字空间或集群 迁移过程中访问遗留系统  步骤\n 创建服务\nkind: Service apiVersion: v1 metadata: name: ext-db spec: ports: - protocol: TCP port: 80 targetPort: 3316  手动创建一个端点\nkind: Endpoints apiVersion: v1 metadata: name: my-service subsets: - addresses: - ip: 10.8.0.2 ports: - port: 3316   kube-proxy mode kubectl get cm kube-proxy -n kube-system -o yaml \u0026gt; kube-proxy.yaml sed -i s/mode:\u0026quot;\u0026quot;/mode:\u0026quot;ipvs/ kube-proxy.yaml sec -i s/creationTimestamp:*// kube-proxy.yaml sed -i s/resourceVersion: \u0026quot;*\u0026quot;// kube-proxy.yaml kubectl apply -f kube-proxy.yaml sudo ipvsadm -Ln ... IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.96.0.1:443 rr -\u0026gt; 192.168.1.11:6443 Masq 1 1 0 TCP 10.96.0.10:53 rr -\u0026gt; 10.2.0.129:53 Masq 1 0 0 -\u0026gt; 10.2.0.132:53 Masq 1 0 0 TCP 10.99.128.143:44134 rr -\u0026gt; 10.2.12.103:44134 Masq 1 0 0 TCP 10.101.148.51:8080 rr -\u0026gt; 10.2.12.102:8080 Masq 1 0 0 TCP 10.101.148.51:9093 rr -\u0026gt; 10.2.12.102:9093 Masq 1 0 0 TCP 10.101.148.51:15010 rr -\u0026gt; 10.2.12.102:15010 Masq 1 0 0 TCP 10.101.148.51:15011 rr -\u0026gt; 10.2.12.102:15011 Masq 1 0 0 TCP 10.102.2.50:443 rr -\u0026gt; 10.2.0.131:8443 Masq 1 0 0 UDP 10.96.0.10:53 rr -\u0026gt; 10.2.0.129:53 Masq 1 0 0 -\u0026gt; 10.2.0.132:53 Masq 1 0 0 ...  Creating sample user  Create Service Account  dashboard-adminuser.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system   Create ClusterRoleBinding  asumming that cluster-admin exists(provisioned by kubeadmin or kops)\nadminuser-bind-clusteramdin.yaml\napiVersion: rbac.authorization.K8S.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.K8S.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system  kubectl apply -f dashboard-adminuser.yaml   login with Bearer Token\nkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')   multi-tenant K8S clusters at network-level:  Namespaces Ingress rules allow/deny and ingress/egress Network Policies Network-aware Zones  Architect a multi-tenant system with kubernetes I don\u0026rsquo;t think there is one document out there really summaries everything. The link below is a bit old but can help outline some of the basics on how they build on K8S. Ultimately the primitives are the same but they abstract namespaces a bit and build it around RBAC. Coupled with a default vxlan (isolated) SDN plugin and their ingress routing, its a compelling multi-tenant solution that provides isolation and quotes at multiple levels.\nOpenshift really just adds some glue (a lot of it being devleoper workflow) on top of Kubernetes. What is nice is that RedHat continues to try and upstream features of origin into K8S where it makes sense.\nhttps://blog.openshift.com/building-kubernetes-bringing-google-scale-container-orchestration-to-the-enterprise/ https://www.reddit.com/r/kubernetes/comments/6qp24h/ask_kubernetes_how_would_you_architect_a/\n","date":1499904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499904000,"objectID":"a16545f8df9259854a34ab4a1e650274","permalink":"https://wubigo.com/post/k8s-notes/","publishdate":"2017-07-13T00:00:00Z","relpermalink":"/post/k8s-notes/","section":"post","summary":"节点维护 kubectl drain \u0026lt;node name\u0026gt;  维护有DaemonSet-managed pod的节点\nkubectl drain \u0026lt;node name\u0026gt; --delete-local-data --force --ignore-daemonsets kubectl delete node \u0026lt;node name\u0026gt; sudo iptables -F sudo iptables -S  create a regular pod 必须使用\u0026ndash;restart=Never\nkubectl run -it curl --image=curlimages/curl:7.72.0 --restart=Never -- sh   Never acts like a cronjob which is scheduled immediately. Always creates a deployment and the deployment monitors the pod and restarts in case of failure.  kubeadm install mirror in china apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl https://mirrors.","tags":["K8S","PAAS"],"title":"K8S notes","type":"post"},{"authors":null,"categories":null,"content":" proxy config for Windows  ~/.atom/.apmrc https-proxy = http://192.168.0.119:3128/ http-proxy = http://192.168.0.119:3128/  Set Up \u0026amp; Use Atom as a Markdown Editor https://www.portent.com/blog/content-strategy/atom-markdown.htm\n","date":1493769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493769600,"objectID":"66349127f34fa3acce2c6902605a99b7","permalink":"https://wubigo.com/post/2017-05-03-atomnote/","publishdate":"2017-05-03T00:00:00Z","relpermalink":"/post/2017-05-03-atomnote/","section":"post","summary":"proxy config for Windows  ~/.atom/.apmrc https-proxy = http://192.168.0.119:3128/ http-proxy = http://192.168.0.119:3128/  Set Up \u0026amp; Use Atom as a Markdown Editor https://www.portent.com/blog/content-strategy/atom-markdown.htm","tags":null,"title":"atom note","type":"post"},{"authors":null,"categories":[],"content":" # side car proxy\n 方法1  Namespace labels\nkubectl label ns servicea istio-injection=enabled  Istio watches over all the deployments and adds the side car container to our pods.This is achieved by leveraging what is called MutatingAdmissionWebhooks, this feature was introduced in Kubernetes 1.9. So before the resources get created, the web hook intercepts the requests, checks if “Istio injection” is enabled for that namespace, and then adds the side car container to the pod\n istioctl command line tool   PILOT = ENVOY CONTROL PLANE API SERVER\nPilot maintains a canonical representation of services in the mesh that is independent of the underlying platform. Platform-specific adapters in Pilot are responsible for populating this canonical model appropriately. For example, the Kubernetes adapter in Pilot implements the necessary controllers to watch the Kubernetes API server for changes to the pod registration information, ingress resources, and third-party resources that store traffic management rules. This data is translated into the canonical representation. An Envoy-specific configuration is then generated based on the canonical representation\nPilot enables service discovery, dynamic updates to load balancing pools and routing tables.\nYou can specify high-level traffic management rules through Pilot’s Rule configuration. These rules are translated into low-level configurations and distributed to Envoy instances\nK8S KUBE-PROXY Kubernetes services take care of maintaining the list of Pod endpoints it can route traffic to. And usually kube-proxy does the load balancing between these pod endpoints. ENVOY client side load balancing do not want kube-proxy to load balance, we want to get the list of Pod endpoints and load balance it ourselves. For this we can use a “headless service”, which will just return the list of endpoints.\n Client-side Load Balancing  Many are familiar with what server-side load balancing is but the lesser known, client-side load balancing, has begun to climb in popularity due to SOA and microservices. Instead of relying on another service to distribute the load, the client itself, is responsible for deciding where to send the traffic also using an algorithm like round-robin. It can either discover the instances, via service discovery, or can be configured with a predefined list. Netflix Ribbon is an example of a client-side load balancer.\n安装  启用代理envoy（pilot.sidecar=true）\nhelm install --debug install/kubernetes/helm/istio --name istio --namespace istio-system --set security.enabled=false --set ingress.enabled=false --set gateways.istio-ingressgateway.enabled=false --set gateways.istio-egressgateway.enabled=false --set galley.enabled=false --set mixer.enabled=false --set prometheus.enabled=false --set global.proxy.envoyStatsd.enabled=false --set sidecarInjectorWebhook.enabled=false --set pilot.sidecar=true  检查POD\n  istio-pilot包含两个容器： discovery 和 istio-proxy\nkubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE istio-pilot-786dc4c88d-vnsr9 2/2 Running 0 15m   检查代理\nkubectl exec -it -n istio-system istio-pilot-786dc4c88d-vnsr9 -c istio-proxy -- bash # cd /etc/istio/proxy/ # ls envoy.yaml envoy_pilot.yaml.tmpl envoy_policy.yaml.tmpl envoy_telemetry.yaml.tmpl # ps fax PID TTY STAT TIME COMMAND 64 pts/2 Ss 0:00 bash 74 pts/2 R+ 0:00 \\_ ps fax 1 ? Ssl 0:00 /usr/local/bin/pilot-agent proxy --serviceCluster istio-pilot --templateFile /etc/istio/proxy/envoy_pilot.yaml.tmpl --controlPlaneAuthPolicy NONE 15 ? Sl 0:14 /usr/local/bin/envoy -c /etc/istio/proxy/envoy.yaml --restart-epoch 0 --drain-time-s 2 --parent-shutdown-time-s 3 --service-cluster istio-pilot --service-node sidecar~10.2.12.70  检查 discovery\nkubectl exec -it -n istio-system istio-pilot-786dc4c88d-vnsr9 -c discovery -- bash # ls -l /etc/istio/config/ total 0 lrwxrwxrwx 1 root root 11 Mar 30 06:52 mesh -\u0026gt; ..data/mesh # ps fax PID TTY STAT TIME COMMAND 61 pts/0 Ss 0:00 bash 71 pts/0 R+ 0:00 \\_ ps fax 1 ? Ssl 1:55 /usr/local/bin/pilot-discovery discovery  检查日志\nPodUID=${kubectl get pod -n istio-system istio-pilot-786dc4c88d-vnsr9 -o=jsonpath='{.metadata.uid}}' scp vm4:/var/log/pods/50f3507c-52b8-11e9-9372-08002775f493/istio-proxy/1.log ~./  检查proxy by adminPort\n  进入容器查看\nkubectl exec -it -n istio-system istio-pilot-786dc4c88d-vnsr9 -c discovery -- bash #curl http://localhost:15000/  或本地代理\nkubectl port-forward -n istio-system istio-pilot-786dc4c88d-vnsr9 15000:15000  pilot地址\nistio-pilot:release-1.0-latest-daily没有把服务端口通过EXPOSE暴露， 通过inspect查找\nkubectl exec -n istio-system istio-pilot-786dc4c88d-ls2z6 -c discovery env | grep \u0026quot;ISTIO_PILOT\u0026quot; ISTIO_PILOT_PORT=tcp://10.111.94.9:15010 ISTIO_PILOT_PORT_8080_TCP_ADDR=10.111.94.9 ISTIO_PILOT_SERVICE_PORT_HTTP_MONITORING=9093 ISTIO_PILOT_PORT_15010_TCP_PROTO=tcp ISTIO_PILOT_PORT_15010_TCP_PORT=15010 ISTIO_PILOT_SERVICE_PORT=15010 ISTIO_PILOT_PORT_15011_TCP=tcp://10.111.94.9:15011 ISTIO_PILOT_PORT_15011_TCP_PROTO=tcp ISTIO_PILOT_PORT_9093_TCP_PROTO=tcp ISTIO_PILOT_SERVICE_PORT_HTTP_LEGACY_DISCOVERY=8080 ISTIO_PILOT_PORT_15011_TCP_PORT=15011 ISTIO_PILOT_PORT_8080_TCP=tcp://10.111.94.9:8080 ISTIO_PILOT_PORT_8080_TCP_PROTO=tcp ISTIO_PILOT_SERVICE_PORT_HTTPS_XDS=15011 ISTIO_PILOT_PORT_9093_TCP=tcp://10.111.94.9:9093 ISTIO_PILOT_SERVICE_PORT_GRPC_XDS=15010 ISTIO_PILOT_PORT_8080_TCP_PORT=8080 ISTIO_PILOT_PORT_9093_TCP_ADDR=10.111.94.9 ISTIO_PILOT_SERVICE_HOST=10.111.94.9 ISTIO_PILOT_PORT_15010_TCP=tcp://10.111.94.9:15010 ISTIO_PILOT_PORT_15010_TCP_ADDR=10.111.94.9 ISTIO_PILOT_PORT_15011_TCP_ADDR=10.111.94.9 ISTIO_PILOT_PORT_9093_TCP_PORT=9093  docker inspect --format='{{range .Config.Env}}{{println .}}{{end}}' istio-pilot docker inspect --format='{{range .Config.Env}}{{println .}}{{end}}' ab92d1c866ce | grep \u0026quot;ISTIO_PILOT_*\u0026quot; ISTIO_PILOT_PORT=tcp://10.111.94.9:15010 ISTIO_PILOT_PORT_8080_TCP_ADDR=10.111.94.9 ISTIO_PILOT_SERVICE_PORT_HTTP_MONITORING=9093 ISTIO_PILOT_PORT_15010_TCP_PROTO=tcp ISTIO_PILOT_PORT_15010_TCP_PORT=15010 ISTIO_PILOT_SERVICE_PORT=15010 ISTIO_PILOT_PORT_15011_TCP=tcp://10.111.94.9:15011 ISTIO_PILOT_PORT_15011_TCP_PROTO=tcp ISTIO_PILOT_PORT_9093_TCP_PROTO=tcp ISTIO_PILOT_SERVICE_PORT_HTTP_LEGACY_DISCOVERY=8080 ISTIO_PILOT_PORT_15011_TCP_PORT=15011 ISTIO_PILOT_PORT_8080_TCP=tcp://10.111.94.9:8080 ISTIO_PILOT_PORT_8080_TCP_PROTO=tcp ISTIO_PILOT_SERVICE_PORT_HTTPS_XDS=15011 ISTIO_PILOT_PORT_9093_TCP=tcp://10.111.94.9:9093 ISTIO_PILOT_SERVICE_PORT_GRPC_XDS=15010 ISTIO_PILOT_PORT_8080_TCP_PORT=8080 ISTIO_PILOT_PORT_9093_TCP_ADDR=10.111.94.9 ISTIO_PILOT_SERVICE_HOST=10.111.94.9 ISTIO_PILOT_PORT_15010_TCP=tcp://10.111.94.9:15010 ISTIO_PILOT_PORT_15010_TCP_ADDR=10.111.94.9 ISTIO_PILOT_PORT_15011_TCP_ADDR=10.111.94.9 ISTIO_PILOT_PORT_9093_TCP_PORT=9093  kubectl exec -it -n istio-system istio-pilot-786dc4c88d-vnsr9 -c discovery -- bash #cat /etc/istio/config/mesh | grep discoveryAddress   pilot-agent      default debug     \u0026ndash;log_output_level default:info default:debug   \u0026ndash;log_stacktrace_level default:none default:debug    Comma-separated minimum per-scope logging level of messages to output, in the form of \u0026lt;scope\u0026gt;:\u0026lt;level\u0026gt;,\u0026lt;scope\u0026gt;:\u0026lt;level\u0026gt;,... where scope can be one of [default, model, rbac] and level can be one of [debug, info, warn, error, fatal, none] (default `default:info`)   调试istio-discovery\nkubectl get deployments -n istio-system -o json \u0026gt; istio.k8s.deployment.json   discovery调试信息\u0026ndash;log_output_level\n \u0026quot;args\u0026quot;: [ \u0026quot;discovery\u0026quot;, \u0026quot;--log_output_level\u0026quot;, \u0026quot;default:debug\u0026quot; ]  proxy调试信息(/usr/local/bin/proxy -l debug)\nproxy被pilot-agent启动，所以调试日志还是和discovery一样\n \u0026quot;args\u0026quot;: [ \u0026quot;proxy\u0026quot;, \u0026quot;--serviceCluster\u0026quot;, \u0026quot;istio-pilot\u0026quot;, \u0026quot;--templateFile\u0026quot;, \u0026quot;/etc/istio/proxy/envoy_pilot.yaml.tmpl\u0026quot;, \u0026quot;--controlPlaneAuthPolicy\u0026quot;, \u0026quot;NONE\u0026quot;, \u0026quot;--log_output_level\u0026quot;, \u0026quot;default:debug\u0026quot; ]  kubectl apply -f istio.k8s.deployment.json  kubectl exec -it -n istio-system istio-pilot-84678c759f-qjbf4 -c discovery -- bash root@istio-pilot-84678c759f-qjbf4:/# ps -fax PID TTY STAT TIME COMMAND 28 pts/0 Ss 0:00 bash 39 pts/0 R+ 0:00 \\_ ps -fax 1 ? Ssl 0:28 /usr/local/bin/pilot-discovery discovery --log_output_level default:debug   下载配置\nkubectl cp istio-system/istio-pilot-b8d58697f-5nthh:etc/istio/proxy/envoy.yaml ./ -c istio-proxy  PodUID=${kubectl get pod -n istio-system istio-pilot-786dc4c88d-vnsr9 -o=jsonpath='{.metadata.uid}' kubectl cp istio-system/istio-pilot-b8d58697f-5nthh:/etc/istio/proxy/envoy.yaml ./ -c istio-proxy  Adding Kubernetes registry adapter 2019-04-03T06:43:56.839512Z\tinfo\tPrimary Cluster name: Kubernetes 2019-04-03T06:43:56.839600Z\tinfo\tService controller watching namespace \u0026quot;\u0026quot; for service, endpoint, nodes and pods, refresh 60000000000 gc 4 @4.096s 4%: 0.043+22+4.4 ms clock, 0.087+1.2/6.0/13+8.9 ms cpu, 5-\u0026gt;5-\u0026gt;3 MB, 6 MB goal, 2 P 2019-04-03T06:43:56.852472Z\tdebug\tempty Webhook API endpoint. 2019-04-03T06:43:56.875696Z\tinfo\tads\tStarting ADS server with throttle=25 burst=100 2019-04-03T06:43:56.879233Z\tinfo\tSetting up event handlers 2019-04-03T06:43:56.879495Z\tinfo\tDiscovery service started at http=[::]:8080 grpc=[::]:15010   ","date":1493535895,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493535895,"objectID":"8af54d26b1dec578465ae13bb710819c","permalink":"https://wubigo.com/post/k8s-istio-discovery-proxy/","publishdate":"2017-04-30T15:04:55+08:00","relpermalink":"/post/k8s-istio-discovery-proxy/","section":"post","summary":"# side car proxy\n 方法1  Namespace labels\nkubectl label ns servicea istio-injection=enabled  Istio watches over all the deployments and adds the side car container to our pods.This is achieved by leveraging what is called MutatingAdmissionWebhooks, this feature was introduced in Kubernetes 1.9. So before the resources get created, the web hook intercepts the requests, checks if “Istio injection” is enabled for that namespace, and then adds the side car container to the pod","tags":["K8S"],"title":"K8s Istio Pilot as envoy control place","type":"post"},{"authors":null,"categories":[],"content":" 特性  与hadoop集成，支持MR数据读取 二级索引 支持长行（最多20亿的列） 动态schema更改 bulk load 其他数据源如hadoop with sstableloader， CSV importing with cqlsh DTCS优化时序数据性能  DB CATEGORY BY CAP  CA  To primarily support consistency and availability means that you’re likely using two-phase commit for distributed transactions. It means that the system will block when a network partition occurs, so it may be that your system is limited to a single data center cluster in an attempt to mitigate this. If your application needs only this level of scale, this is easy to manage and allows you to rely on familiar, simple structures.\n CP  To primarily support consistency and partition tolerance, you may try to advance your architecture by setting up data shards in order to scale. Your data will be consistent, but you still run the risk of some data becoming unavailable if nodes fail.\n AP  To primarily support availability and partition tolerance, your system may return inaccurate data, but the system will always be available, even in the face of network partitioning. DNS is perhaps the most popular example of a system that is massively scalable, highly available, and partition tolerant.\n Cassandra uses a special primary key called a composite key (or compound key) to represent wide rows, also called partitions. The composite key consists of a partition key, plus an optional set of clustering columns. The partition key is used to determine the nodes on which rows are stored and can itself consist of multiple columns. The clustering columns are used to control how data is sorted for storage within a partition. Cassandra also supports an additional construct called a static column, which is for storing data that is not part of the primary key but is shared by every row in a partition  Server-Side Denormalization with Materialized Views Historically, denormalization in Cassandra has required designing and managing multiple tables using techniques we will introduce momentarily. Beginning with the 3.0 release, Cassandra provides a feature known as materialized views which allows us to create multiple denormalized views of data based on a base table design. Cassandra manages materialized views on the server, including the work of keeping the views in sync with the table\nMaterialized views simplify application development: instead of the application having to keep multiple denormalized tables in sync, Cassandra takes on the responsibility of updating views in order to keep them consistent with the base table\nPrimary Keys Are Forever After you create a table, there is no way to modify the primary key, because this controls how data is distributed within the cluster, and even more importantly, how it is stored on disk.\n","date":1493370487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493370487,"objectID":"283f625d2a671c49b6886482df3aff47","permalink":"https://wubigo.com/post/cassandra/","publishdate":"2017-04-28T17:08:07+08:00","relpermalink":"/post/cassandra/","section":"post","summary":"特性  与hadoop集成，支持MR数据读取 二级索引 支持长行（最多20亿的列） 动态schema更改 bulk load 其他数据源如hadoop with sstableloader， CSV importing with cqlsh DTCS优化时序数据性能  DB CATEGORY BY CAP  CA  To primarily support consistency and availability means that you’re likely using two-phase commit for distributed transactions. It means that the system will block when a network partition occurs, so it may be that your system is limited to a single data center cluster in an attempt to mitigate this.","tags":["SHELL","NOSQL"],"title":"Cassandra","type":"post"},{"authors":null,"categories":[],"content":" \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-zuul\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  gateway with routing @EnableZuulServer is used when you want to build your own routing service and not use any Zuul prebuilt capabilities. An example of this would be if you wanted to use Zuul to integrate with a service discovery engine other than Eureka (for example, Consul). We’ll only use the @EnableZuulServer annotation in this book.\nThe Zuul proxy server is designed by default to work on the Spring products. As such, Zuul will automatically use Eureka to look up services by their service IDs and then use Netflix Ribbon to do client-side load balancing of requests from within Zuul.\n","date":1493370487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493370487,"objectID":"ecac105488c8c9fb88e6f4fa40351b6b","permalink":"https://wubigo.com/post/spring-gateway/","publishdate":"2017-04-28T17:08:07+08:00","relpermalink":"/post/spring-gateway/","section":"post","summary":"\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-zuul\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  gateway with routing @EnableZuulServer is used when you want to build your own routing service and not use any Zuul prebuilt capabilities. An example of this would be if you wanted to use Zuul to integrate with a service discovery engine other than Eureka (for example, Consul). We’ll only use the @EnableZuulServer annotation in this book.\nThe Zuul proxy server is designed by default to work on the Spring products.","tags":["SPRING","MICROSERVICE"],"title":"Spring Gateway","type":"post"},{"authors":null,"categories":[],"content":" http://localhost:8080/oauth/token\ncurl -u eagleeye:thisissecret -i -H \u0026lsquo;Accept:application/json\u0026rsquo; -d \u0026ldquo;grant_type=password\u0026amp;scope=webclient\u0026amp;username=will\u0026amp;password=pass\u0026rdquo; -H \u0026ldquo;Content-Type: application/x-www-form-urlencoded\u0026rdquo; -X POST http://localhost:8080/oauth/token\naccess protected resource ","date":1493370487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493370487,"objectID":"132484b899dcf0aa9734b6ac65080af0","permalink":"https://wubigo.com/post/spring-oauth2/","publishdate":"2017-04-28T17:08:07+08:00","relpermalink":"/post/spring-oauth2/","section":"post","summary":" http://localhost:8080/oauth/token\ncurl -u eagleeye:thisissecret -i -H \u0026lsquo;Accept:application/json\u0026rsquo; -d \u0026ldquo;grant_type=password\u0026amp;scope=webclient\u0026amp;username=will\u0026amp;password=pass\u0026rdquo; -H \u0026ldquo;Content-Type: application/x-www-form-urlencoded\u0026rdquo; -X POST http://localhost:8080/oauth/token\naccess protected resource ","tags":["SHELL","MYSQL"],"title":"Spring Oauth2","type":"post"},{"authors":null,"categories":[],"content":" 提示： 以下操作是在VirtualBox虚机环境，并做如下配置\n 网络  下拉高级设置，在\u0026rdquo;Adapter Type\u0026rdquo;选择PCnet-FAST III\u0026rdquo;, 而不是默认的e1000 (Intel PRO/1000). 另外\u0026rdquo;Promiscuous Mode\u0026rdquo;必须设置为\u0026rdquo;Allow All\u0026rdquo;. 否则通过网桥连接的容器无法工作, 因为虚拟网卡 会过滤掉掉所有带有不同MAC的数据包。\n 多网卡  每块网卡都要做上述调整\n准备  安装util-linux\nsudo apt install util-linux   /etc/network/interface\ncat interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s3 iface enp0s3 inet static address 192.168.1.10 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 192.168.1.1 auto enp0s8 iface enp0s8 inet static address 192.168.1.16 netmask 255.255.255.0 dns-nameservers 192.168.1.1  ip route default via 192.168.1.1 dev enp0s3 onlink 169.254.0.0/16 dev enp0s3 scope link metric 1000 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.1.0/24 dev enp0s3 proto kernel scope link src 192.168.1.10 192.168.1.0/24 dev enp0s8 proto kernel scope link src 192.168.1.16  使用NAT  docker host network  assign a second ip to host interface\nexport SIP=192.168.1.117 sudo ip addr add $SIP/24 dev enp0s3  bind container to SIP host network\ndocker run -it --name web -p ${SIP}:80:80 nginx:1.14-alpine sudo iptables -L DOCKER -v -n Chain DOCKER (1 references) pkts bytes target prot opt in out source destination 7 528 ACCEPT tcp -- !docker0 docker0 0.0.0.0/0 172.17.0.2 tcp dpt:80    sudo iptables -t nat -I POSTROUTING -s 172.17.0.2 \\ -j SNAT --to-source 192.168.1.119 sudo iptables -t nat -L -n -v  使用LINUX网桥  查看网卡的ip\nifconfig enp0s8 enp0s3 Link encap:Ethernet HWaddr 08:00:27:4e:18:68 inet addr:192.168.1.16  创建网桥br-enp0s8并把enp0s8的IP分配给网桥，把enp0s8连接到网桥\nsudo brctl addbr br-enp0s8 sudo ip link set br-enp0s8 up sudo ip addr add 192.168.1.111/24 dev br-enp0s8; \\ ip addr del 192.168.1.111/24 dev enp0s8; \\ brctl addif br-enp0s8 enp0s8; \\ ip route del default; \\ ip route add default via 192.168.1.1 dev br-enp0s8 ifconfig br-enp0s8 br-enp0s8 Link encap:Ethernet HWaddr 08:00:27:4e:18:68 inet addr:192.168.1.16 ifconfig enp0s8 enp0s8 Link encap:Ethernet HWaddr 08:00:27:4e:18:68   br-enp0s8和enp0s8拥有相同的HWaddr(Mac地址)\n 确认网络是否对外连接正常\nip route default via 192.168.1.1 dev br-enp0s8 169.254.0.0/16 dev enp0s3 scope link metric 1000 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.1.0/24 dev enp0s3 proto kernel scope link src 192.168.1.10 192.168.1.0/24 dev br-enp0s8 proto kernel scope link src 192.168.1.16  curl -IL https://wubigo.com HTTP/1.1 200 OK  启动容器\ndocker run -it --rm --name web -p 80 nginx:1.14-alpine  创建veth接口对web-int/web-ext:\nsudo ip link add web-int type veth peer name web-ext  连接veth一端web-ext到网桥\nsudo brctl addif br-enp0s8 web-ext  连接veth的另一端web-int连接到容器的网络名字空间\nsudo ip link set netns $(docker-pid web) dev web-int sudo nsenter -t $(docker-pid web) -n ip link set web-int up sudo nsenter -t $(docker-pid web) -n ip addr add 192.168.1.117/24 dev web-int  检查容器已经连接到web-int并且ip地址正确分配\ndocker exec -it web ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:84 errors:0 dropped:0 overruns:0 frame:0 TX packets:21 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:9396 (9.1 KiB) TX bytes:2348 (2.2 KiB) web-int Link encap:Ethernet HWaddr 5A:1D:90:CF:6B:2C inet addr:192.168.1.117 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  docker exec -it web ip route default via 172.17.0.1 dev eth0 172.17.0.0/16 dev eth0 scope link src 172.17.0.2 192.168.1.0/24 dev web-int scope link src 192.168.1.117  设置web-int为容器路由默认接口\nsudo nsenter -t $(docker-pid web) -n ip route del default sudo nsenter -t $(docker-pid web) -n ip route add default via 192.168.1.1 dev web-int  测试清理\ndocker rm web sudo ip link set br-enp0s8 down sudo brctl delbr br-enp0s8   ","date":1493075455,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493075455,"objectID":"16cf3f49bb97c1d9ad4d0ad448b4417c","permalink":"https://wubigo.com/post/connect-container-to-host-network/","publishdate":"2017-04-25T07:10:55+08:00","relpermalink":"/post/connect-container-to-host-network/","section":"post","summary":"提示： 以下操作是在VirtualBox虚机环境，并做如下配置\n 网络  下拉高级设置，在\u0026rdquo;Adapter Type\u0026rdquo;选择PCnet-FAST III\u0026rdquo;, 而不是默认的e1000 (Intel PRO/1000). 另外\u0026rdquo;Promiscuous Mode\u0026rdquo;必须设置为\u0026rdquo;Allow All\u0026rdquo;. 否则通过网桥连接的容器无法工作, 因为虚拟网卡 会过滤掉掉所有带有不同MAC的数据包。\n 多网卡  每块网卡都要做上述调整\n准备  安装util-linux\nsudo apt install util-linux   /etc/network/interface\ncat interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s3 iface enp0s3 inet static address 192.168.1.10 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 192.168.1.1 auto enp0s8 iface enp0s8 inet static address 192.168.1.16 netmask 255.","tags":["DOCKER","NETWORK"],"title":"容器多种方式链接宿主网络","type":"post"},{"authors":null,"categories":[],"content":" 业务 公司专注于建筑信息化行业二十余年，业务领域正逐步由招投标阶段拓展至工程项目的全生命周期， 产品从单一的预算软件扩展到工程造价、工程施工等多个业务板块，涵盖工具软件类、解决方案类、大数 据、移动互联网、云计算、智能硬件设备、产业金融服务等多种业务形态；服务的客户从中国境内拓展到 全球一百多个国家和地区；累计为行业二十余万家企业、千余万产品使用者提供专业化服务。 根据业务阶段及服务客户不同，公司业务划分为四大业务板块，分别为数字造价业务板块、数字施工 业务板块、创新业务板块和生态业务板块；根据业务区域不同，又分为国内业务和海外业务。\n造价 数字造价业务板块属于公司成熟业务，主要为建设工程造价（工程成本、工程量计算）提供工具类软 件产品及数据服务，包括工程计价业务线、工程算量业务线和工程信息业务线等。经过二十多年发展，公 司在国内该业务领域有较高的市场占有率，竞争优势明显。报告期内，数字造价业务中的主要产品线正在 逐步推进云转型，其商业模式正由销售软件产品逐步转向提供服务的SaaS模式。\n施工 数字施工业务板块是公司重点突破的成长业务，目前已经形成数字施工整体解决方案。在2019年6月 发布的广联达数字项目管理（BIM+智慧工地）平台基础上，结合广联达对建造业务的理解，已经开发出 覆盖岗位级、项目级、企业级的多个数字化应用系统，为施工企业数字化转型提供一站式服务。数字项目 管理平台解决了数据互通，促进了产品融合和价值提升，业务的协同效应和整合优势日益显现。该板块业 务的商业模式主要为提供平台化解决方案，销售自主软件产品。\n创新 创新业务板块属于公司孵化业务，在新空间、新客户、新业务、新模式等方面实现公司业务探索和布局， 目前主要包括规建管一体化平台、建设方一体化平台、全装定制一体化平台等业务，上述业务以项目 的形式展开，尚未形成规模化销售。\n生态 生态业务板块主要包括产业新金融、工程教育等业务。产业新金融业务依托公司专业应用系统的精准 数据及大数据服务，探索为建筑产业企业客户提供供应链金融服务的新模式；工程教育业务则围绕建筑类 院校相关需求，提供建筑实训课程产品销售及相关服务。 海外业务方面，一部分依托2014年全资收购的芬兰子公司机电专业BIM相关业务，形成MagiCAD产品线， 主要覆盖欧美等市场；另一部分为数字造价业务的国际化系列产品Cubicost，主要开拓香港，新加坡、马 来西亚和印尼等东南亚市场。\n市场 数字施工业务全年实现收入8.53亿元，同比增长29.58%。报告期内，公司进一步融合原BIM与 智慧工地相关产品，发布针对项目的数字项目管理（BIM+智慧工地）平台，在该平台基础上结合应用组 件，实现“平台+组件”的业务形态，从而为项目管理提供可以快速规模化的综合解决方案。 数字项目管理（BIM+智慧工地）平台将施工现场系统和硬件设备集成到一个平台，将产生的数据汇 总建模，形成数据中心，实现统一主数据、统一BI、统一入口、统一技术标准和数据接口。基于此平台， 可以为施工项目部提供整体的数字化解决方案，从项目的生产要素（人员、机械、物料、工艺方法等）及 管理要素（进度、质量、安全、成本等）出发，通过提供标准化产品，为施工企业带来三个转化。一是作 业数字化，即利用数字化技术实现项目信息实时传递与留痕，保证施工现场施工员、技术员、质量员、安 全员等多岗位工作结果有据可依的同时，还能收集工地现场数据，让管理更加立体。二是管理系统化，即 全面接入施工现场塔吊、施工电梯、深基坑、闸机等多设备信息，并将作业在线数据按照不同管理维度（如 进度管理、安全管理、质量管理）抽提给项目部各管理层，实现统一数据标准，达成业务动态协同。三是 决策智慧化，BIM+智慧工地平台将先进技术应用到项目管理，覆盖质量安全巡检、生产任务排分、安全 教育、技术交底、物料验收等各种场景，筛选出有效数据后供项目负责人制定战略规划，合理高效决策， 并及时预警风险。 报告期内，在数字施工业务企业级、项目级、岗位级三大类产品中，基于数字项目管理平台的项目级 产品营业收入占比过半，“平台+组件”项目级产品的推广取得了阶段性的成果。项目级产品全年新增企业 客户超过1100家，其中特、一级企业逾990家；新增项目超过3200个，其中特、一级企业项目逾2900个。 报告期内，数字施工业务的企业级和岗位级产品线也取得良好进展。企业级产品价值获得更多客户认 可，客户覆盖率和应用率进一步提升；岗位级产品持续迭代和打磨，产品的规模化拓展正在推进过程中。\n","date":1491359552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491359552,"objectID":"362a1ff392e144a244f64f66c00b8ba1","permalink":"https://wubigo.com/post/glodon-biz/","publishdate":"2017-04-05T10:32:32+08:00","relpermalink":"/post/glodon-biz/","section":"post","summary":"业务 公司专注于建筑信息化行业二十余年，业务领域正逐步由招投标阶段拓展至工程项目的全生命周期， 产品从单一的预算软件扩展到工程造价、工程施工等多个业务板块，涵盖工具软件类、解决方案类、大数 据、移动互联网、云计算、智能硬件设备、产业金融服务等多种业务形态；服务的客户从中国境内拓展到 全球一百多个国家和地区；累计为行业二十余万家企业、千余万产品使用者提供专业化服务。 根据业务阶段及服务客户不同，公司业务划分为四大业务板块，分别为数字造价业务板块、数字施工 业务板块、创新业务板块和生态业务板块；根据业务区域不同，又分为国内业务和海外业务。\n造价 数字造价业务板块属于公司成熟业务，主要为建设工程造价（工程成本、工程量计算）提供工具类软 件产品及数据服务，包括工程计价业务线、工程算量业务线和工程信息业务线等。经过二十多年发展，公 司在国内该业务领域有较高的市场占有率，竞争优势明显。报告期内，数字造价业务中的主要产品线正在 逐步推进云转型，其商业模式正由销售软件产品逐步转向提供服务的SaaS模式。\n施工 数字施工业务板块是公司重点突破的成长业务，目前已经形成数字施工整体解决方案。在2019年6月 发布的广联达数字项目管理（BIM+智慧工地）平台基础上，结合广联达对建造业务的理解，已经开发出 覆盖岗位级、项目级、企业级的多个数字化应用系统，为施工企业数字化转型提供一站式服务。数字项目 管理平台解决了数据互通，促进了产品融合和价值提升，业务的协同效应和整合优势日益显现。该板块业 务的商业模式主要为提供平台化解决方案，销售自主软件产品。\n创新 创新业务板块属于公司孵化业务，在新空间、新客户、新业务、新模式等方面实现公司业务探索和布局， 目前主要包括规建管一体化平台、建设方一体化平台、全装定制一体化平台等业务，上述业务以项目 的形式展开，尚未形成规模化销售。\n生态 生态业务板块主要包括产业新金融、工程教育等业务。产业新金融业务依托公司专业应用系统的精准 数据及大数据服务，探索为建筑产业企业客户提供供应链金融服务的新模式；工程教育业务则围绕建筑类 院校相关需求，提供建筑实训课程产品销售及相关服务。 海外业务方面，一部分依托2014年全资收购的芬兰子公司机电专业BIM相关业务，形成MagiCAD产品线， 主要覆盖欧美等市场；另一部分为数字造价业务的国际化系列产品Cubicost，主要开拓香港，新加坡、马 来西亚和印尼等东南亚市场。\n市场 数字施工业务全年实现收入8.53亿元，同比增长29.58%。报告期内，公司进一步融合原BIM与 智慧工地相关产品，发布针对项目的数字项目管理（BIM+智慧工地）平台，在该平台基础上结合应用组 件，实现“平台+组件”的业务形态，从而为项目管理提供可以快速规模化的综合解决方案。 数字项目管理（BIM+智慧工地）平台将施工现场系统和硬件设备集成到一个平台，将产生的数据汇 总建模，形成数据中心，实现统一主数据、统一BI、统一入口、统一技术标准和数据接口。基于此平台， 可以为施工项目部提供整体的数字化解决方案，从项目的生产要素（人员、机械、物料、工艺方法等）及 管理要素（进度、质量、安全、成本等）出发，通过提供标准化产品，为施工企业带来三个转化。一是作 业数字化，即利用数字化技术实现项目信息实时传递与留痕，保证施工现场施工员、技术员、质量员、安 全员等多岗位工作结果有据可依的同时，还能收集工地现场数据，让管理更加立体。二是管理系统化，即 全面接入施工现场塔吊、施工电梯、深基坑、闸机等多设备信息，并将作业在线数据按照不同管理维度（如 进度管理、安全管理、质量管理）抽提给项目部各管理层，实现统一数据标准，达成业务动态协同。三是 决策智慧化，BIM+智慧工地平台将先进技术应用到项目管理，覆盖质量安全巡检、生产任务排分、安全 教育、技术交底、物料验收等各种场景，筛选出有效数据后供项目负责人制定战略规划，合理高效决策， 并及时预警风险。 报告期内，在数字施工业务企业级、项目级、岗位级三大类产品中，基于数字项目管理平台的项目级 产品营业收入占比过半，“平台+组件”项目级产品的推广取得了阶段性的成果。项目级产品全年新增企业 客户超过1100家，其中特、一级企业逾990家；新增项目超过3200个，其中特、一级企业项目逾2900个。 报告期内，数字施工业务的企业级和岗位级产品线也取得良好进展。企业级产品价值获得更多客户认 可，客户覆盖率和应用率进一步提升；岗位级产品持续迭代和打磨，产品的规模化拓展正在推进过程中。","tags":["BIM"],"title":"Glodon Biz","type":"post"},{"authors":null,"categories":[],"content":" the architecture of gRPC is layered: The lowest layer is the transport: gRPC uses HTTP/2 as its transport protocol. HTTP/2 provides the same basic semantics as HTTP 1.1 (the version with which nearly all developers are familiar), but aims to be more efficient and more secure. The new features in HTTP/2 that are most obvious at first glance are (1) that it can multiplex many parallel requests over the same network connection and (2) that it allows full-duplex bidirectional communication. We’ll learn more about HTTP/2 and the ways it differs from and improves on HTTP 1.1 later in the book.\nThe next layer is the channel: This is a thin abstraction over the transport. The channel defines calling conventions and implements the mapping of an RPC onto the underlying transport. At this layer, a gRPC call consists of a client-provided service name and method name, optional request metadata (key-value pairs), and zero or more request messages. A call is completed when the server provides optional response header metadata, zero or more response messages, and response trailer metadata. The trailer metadata indicates the final disposition of the call: whether it was a success or a failure. At this layer, there is no knowledge of interface constraints, data types, or message encoding. A message is just a sequence of zero or more bytes. A call may have any number of request and response messages.\nThe last layer is the stub: The stub layer is where interface constraints and data types are defined. Does a method accept exactly one request message or a stream of request messages? What kind of data is in each response message and how is it encoded? The answers to these questions are provided by the stub. The stub marries the IDL-defined interfaces to a channel. The stub code is generated from the IDL. The channel layer provides the ABI that these generated stubs use.\nStreaming When a very large amount of data must be exchanged, this can mean significant memory pressure on both the client process and the server process. And it means that operations must typically impose hard limits on the size of request and response messages, to prevent resource exhaustion. Streaming alleviates this by allowing the request or response to be an arbitrarily long sequence of messages. The cumulative total size of a request or response stream may be incredibly large, but clients and servers do not need to store the entire stream in memory. Instead, they can operate on a subset of data, even as little as just one message at a time.\nNot only does gRPC support streaming, but it also supports full-duplex bidirectional streams. Bidirectional means that the client can use a stream to upload an arbitrary amount of request data and the server can use a stream to send back an arbitrary amount of response data, all in the same RPC. The novel part is the “full-duplex” part. Most request-response protocols, including HTTP 1.1 are “half-duplex.” They support bidirectional communication (HTTP 1.1 even supports bidirectional streaming), but the two directions cannot be used at the same time. A request must first be fully uploaded before the server begins responding; only after the client is done transmitting can the server then reply with its full response. gRPC is built on HTTP/2, which explicitly supports full-duplex streams, which means that the client can upload request data at the same time the server is sending back response data. This is very powerful and eliminates the need for things like web sockets, which is an extension of HTTP 1.1, to allow full-duplex communication over an HTTP 1.1 connection. Thanks to streaming, applications can build very sophisticated conversational protocols on top of gRPC.\n","date":1491170022,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491170022,"objectID":"4e37a31092cf539cc02f79edb1e32e0c","permalink":"https://wubigo.com/post/rpc-grpc/","publishdate":"2017-04-03T05:53:42+08:00","relpermalink":"/post/rpc-grpc/","section":"post","summary":"the architecture of gRPC is layered: The lowest layer is the transport: gRPC uses HTTP/2 as its transport protocol. HTTP/2 provides the same basic semantics as HTTP 1.1 (the version with which nearly all developers are familiar), but aims to be more efficient and more secure. The new features in HTTP/2 that are most obvious at first glance are (1) that it can multiplex many parallel requests over the same network connection and (2) that it allows full-duplex bidirectional communication.","tags":["RPC"],"title":"RPC GRPC","type":"post"},{"authors":null,"categories":[],"content":"使用一个没有被占用的网段设置DOCKER_GATEWAY\nexport DOCKER_GATEWAY=172.28.0.1  URL=https://github.com/istio/istio/releases/download/1.1.1/istio-1.1.1-linux.tar.gz curl -L \u0026quot;$URL\u0026quot; | tar xz cd istio-1.1.1 docker-compose -f install/consul/istio.yaml up -d  Configure kubectl to use mapped local port for the API server:\nkubectl config set-context istio --cluster=istio kubectl config set-cluster istio --server=http://localhost:8080 kubectl config use-context istio  docker-compose -f samples/bookinfo/platform/consul/bookinfo.yaml up -d  kubectl apply -f samples/bookinfo/platform/consul/destination-rule-all.yaml kubectl get destinationrules -o yaml  kubectl apply -f samples/bookinfo/platform/consul/virtual-service-all-v1.yaml  docker-compose -f bookinfo.yaml exec details-v1 sh #cat /etc/resolv.conf search service.consul nameserver 127.0.0.11 options ndots:0  docker run -it --rm --network consul_istiomesh busybox:glibc #cat /etc/resolv.conf  destinationrules\napiVersion: v1 items: - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;DestinationRule\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;details\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;details.service.consul\u0026quot;,\u0026quot;subsets\u0026quot;:[{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v1\u0026quot;},{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v2\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v2\u0026quot;}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:15:43Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: details namespace: default resourceVersion: \u0026quot;106\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/details uid: 4666f266-5530-11e9-bf95-0242ac1c000d spec: host: details.service.consul subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;DestinationRule\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;productpage\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;productpage.service.consul\u0026quot;,\u0026quot;subsets\u0026quot;:[{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v1\u0026quot;}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:15:43Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: productpage namespace: default resourceVersion: \u0026quot;103\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/productpage uid: 465ee98f-5530-11e9-bf95-0242ac1c000d spec: host: productpage.service.consul subsets: - labels: version: v1 name: v1 - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;DestinationRule\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;ratings\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;ratings.service.consul\u0026quot;,\u0026quot;subsets\u0026quot;:[{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v1\u0026quot;}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:15:43Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: ratings namespace: default resourceVersion: \u0026quot;105\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/ratings uid: 4662363d-5530-11e9-bf95-0242ac1c000d spec: host: ratings.service.consul subsets: - labels: version: v1 name: v1 - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;DestinationRule\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;reviews\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;reviews.service.consul\u0026quot;,\u0026quot;subsets\u0026quot;:[{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v1\u0026quot;},{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v2\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v2\u0026quot;},{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v3\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v3\u0026quot;}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:15:43Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: reviews namespace: default resourceVersion: \u0026quot;104\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/reviews uid: 46605c8c-5530-11e9-bf95-0242ac1c000d spec: host: reviews.service.consul subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 - labels: version: v3 name: v3 kind: List metadata: resourceVersion: \u0026quot;\u0026quot; selfLink: \u0026quot;\u0026quot;  VirtualService\napiVersion: v1 items: - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;details\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;hosts\u0026quot;:[\u0026quot;details.service.consul\u0026quot;],\u0026quot;http\u0026quot;:[{\u0026quot;route\u0026quot;:[{\u0026quot;destination\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;details.service.consul\u0026quot;,\u0026quot;subset\u0026quot;:\u0026quot;v1\u0026quot;}}]}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:17:57Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: details namespace: default resourceVersion: \u0026quot;110\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/details uid: 95f21f5f-5530-11e9-bf95-0242ac1c000d spec: hosts: - details.service.consul http: - route: - destination: host: details.service.consul subset: v1 - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;productpage\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;hosts\u0026quot;:[\u0026quot;productpage.service.consul\u0026quot;],\u0026quot;http\u0026quot;:[{\u0026quot;route\u0026quot;:[{\u0026quot;destination\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;productpage.service.consul\u0026quot;,\u0026quot;subset\u0026quot;:\u0026quot;v1\u0026quot;}}]}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:17:57Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: productpage namespace: default resourceVersion: \u0026quot;107\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/productpage uid: 95ea84fc-5530-11e9-bf95-0242ac1c000d spec: hosts: - productpage.service.consul http: - route: - destination: host: productpage.service.consul subset: v1 - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;ratings\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;hosts\u0026quot;:[\u0026quot;ratings.service.consul\u0026quot;],\u0026quot;http\u0026quot;:[{\u0026quot;route\u0026quot;:[{\u0026quot;destination\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;ratings.service.consul\u0026quot;,\u0026quot;subset\u0026quot;:\u0026quot;v1\u0026quot;}}]}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:17:57Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: ratings namespace: default resourceVersion: \u0026quot;109\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/ratings uid: 95ee32e2-5530-11e9-bf95-0242ac1c000d spec: hosts: - ratings.service.consul http: - route: - destination: host: ratings.service.consul subset: v1 - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;reviews\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;hosts\u0026quot;:[\u0026quot;reviews.service.consul\u0026quot;],\u0026quot;http\u0026quot;:[{\u0026quot;route\u0026quot;:[{\u0026quot;destination\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;reviews.service.consul\u0026quot;,\u0026quot;subset\u0026quot;:\u0026quot;v1\u0026quot;}}]}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:17:57Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: reviews namespace: default resourceVersion: \u0026quot;108\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/reviews uid: 95eb9df1-5530-11e9-bf95-0242ac1c000d spec: hosts: - reviews.service.consul http: - route: - destination: host: reviews.service.consul subset: v1 kind: List metadata: resourceVersion: \u0026quot;\u0026quot; selfLink: \u0026quot;\u0026quot;  ","date":1491118301,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491118301,"objectID":"b4198015380af687e4523385cda01e51","permalink":"https://wubigo.com/post/istio-pilot-docker/","publishdate":"2017-04-02T15:31:41+08:00","relpermalink":"/post/istio-pilot-docker/","section":"post","summary":"使用一个没有被占用的网段设置DOCKER_GATEWAY\nexport DOCKER_GATEWAY=172.28.0.1  URL=https://github.com/istio/istio/releases/download/1.1.1/istio-1.1.1-linux.tar.gz curl -L \u0026quot;$URL\u0026quot; | tar xz cd istio-1.1.1 docker-compose -f install/consul/istio.yaml up -d  Configure kubectl to use mapped local port for the API server:\nkubectl config set-context istio --cluster=istio kubectl config set-cluster istio --server=http://localhost:8080 kubectl config use-context istio  docker-compose -f samples/bookinfo/platform/consul/bookinfo.yaml up -d  kubectl apply -f samples/bookinfo/platform/consul/destination-rule-all.yaml kubectl get destinationrules -o yaml  kubectl apply -f samples/bookinfo/platform/consul/virtual-service-all-v1.yaml  docker-compose -f bookinfo.yaml exec details-v1 sh #cat /etc/resolv.","tags":[],"title":"Istio Pilot Docker","type":"post"},{"authors":null,"categories":null,"content":" Microservice platform Spring-cloud VS Kubernetes https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/\n","date":1491091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491091200,"objectID":"29b330b2e5eaea4b52637c2de1262033","permalink":"https://wubigo.com/post/2017-04-02-microserviceplatformspring-cloudvskubernetes/","publishdate":"2017-04-02T00:00:00Z","relpermalink":"/post/2017-04-02-microserviceplatformspring-cloudvskubernetes/","section":"post","summary":"Microservice platform Spring-cloud VS Kubernetes https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/","tags":null,"title":"Microservice platform Spring-cloud VS Kubernetes","type":"post"},{"authors":null,"categories":[],"content":" Before Kubernetes version 1.11, the Kubernetes DNS service was based on kube-dns. Version 1.11 introduced CoreDNS to address some security and stability concerns with kube-dns.\nRegardless of the software handling the actual DNS records, both implementations work in a similar manner:\n A service named kube-dns and one or more pods are created. The kube-dns service listens for service and endpoint events from the Kubernetes API and updates its DNS records as needed. These events are triggered when you create, update or delete Kubernetes services and their associated pods. kubelet sets each new pod\u0026rsquo;s /etc/resolv.conf nameserver option to the cluster IP of the kube-dns service, with appropriate search options to allow for shorter hostnames to be used: Applications running in containers can then resolve hostnames such as example-service.namespace into the correct cluster IP addresses.  Kubernetes DNS Records  SVC    service.namespace.svc.cluster.local\n  POD    10.2.9.4.namespace.pod.cluster.local\n addressing a service in the same namespace\nnslookup other-svc  addressing a service in a different namespace\nnslookup other-svc.other-ns  Pod’s dnsPolicy Note: “Default” is not the default DNS policy. If dnsPolicy is not explicitly specified, then “ClusterFirst” is used.\n“ClusterFirst“: Any DNS query that does not match the configured cluster domain suffix, such as “www.kubernetes.io”, is forwarded to the upstream nameserver inherited from the node. Cluster administrators may have extra stub-domain and upstream DNS servers configured. See related discussion for details on how DNS queries are handled in those cases.\ncustomize pod dns with dnsConfig busybox has bug on nslookup of k8s svc addressing,\nuse alpine instead\nbigo@bigo-HP:~/wubigo.github.io$ kubectl run -it --image curl:v1 curl --restart=Never --rm -- sh If you don't see a command prompt, try pressing enter. / # nslookup kubernetes Name: kubernetes Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local / # nslookup nginx Name: nginx Address 1: 10.2.12.99 web-0.nginx.default.svc.cluster.local  coredns CM kubectl -n kube-system get configmap coredns -o yaml kubectl -n kube-system edit configmap coredns data: Corefile: | .:53 { log errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 loop reload loadbalance } kind: ConfigMap metadata: creationTimestamp: \u0026quot;2019-02-19T06:54:07Z\u0026quot; name: coredns namespace: kube-system resourceVersion: \u0026quot;561721\u0026quot; selfLink: /api/v1/namespaces/kube-system/configmaps/coredns uid: 2732a277-3413-11e9-86cc-08002775f493  ","date":1491060642,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491060642,"objectID":"5a7cba9adb40573c37beaa0bf81d912a","permalink":"https://wubigo.com/post/k8s-dns/","publishdate":"2017-04-01T23:30:42+08:00","relpermalink":"/post/k8s-dns/","section":"post","summary":"Before Kubernetes version 1.11, the Kubernetes DNS service was based on kube-dns. Version 1.11 introduced CoreDNS to address some security and stability concerns with kube-dns.\nRegardless of the software handling the actual DNS records, both implementations work in a similar manner:\n A service named kube-dns and one or more pods are created. The kube-dns service listens for service and endpoint events from the Kubernetes API and updates its DNS records as needed.","tags":["K8S"],"title":"K8s DNS","type":"post"},{"authors":null,"categories":[],"content":" set date FROM alpine:3.8 RUN apk add --no-cache tzdata \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV TZ Asia/Shanghai RUN ln -s /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone  docker run -it --rm -e TZ=Asia/Shanghai alpine:3.8 ash  创建/etc/localtime\nln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  ","date":1490944038,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490944038,"objectID":"b639ba3bccaf25e3b32bbf6c60c904ec","permalink":"https://wubigo.com/post/docker-alpine/","publishdate":"2017-03-31T15:07:18+08:00","relpermalink":"/post/docker-alpine/","section":"post","summary":" set date FROM alpine:3.8 RUN apk add --no-cache tzdata \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV TZ Asia/Shanghai RUN ln -s /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone  docker run -it --rm -e TZ=Asia/Shanghai alpine:3.8 ash  创建/etc/localtime\nln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  ","tags":["DOCKER"],"title":"Docker Alpine","type":"post"},{"authors":null,"categories":[],"content":" PodUID kubectl get pod \u0026lt;PID_NAME\u0026gt; -o=jsonpath='{.metadata.uid}'  POD on disk /var/lib/kubelet/pods/\u0026lt;PodUID\u0026gt;/\n/var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;\nls -l /var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;/ lrwxrwxrwx 1 root root 165 3月 30 06:52 0.log -\u0026gt; /var/lib/docker/containers/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021-json.log  In a production cluster, logs are usually collected, aggregated, and shipped to a remote store where advanced analysis/search/archiving functions are supported. In kubernetes, the default cluster-addons includes a per-node log collection daemon, fluentd. To facilitate the log collection, kubelet creates symbolic links to all the docker containers logs under /var/log/containers with pod and container metadata embedded in the filename.\n/var/log/containers/__-.log`\n/var/log/containers/\nls -l  ","date":1490830896,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490830896,"objectID":"a96770cd1787cbef0328436c23093cc0","permalink":"https://wubigo.com/post/k8s-kubelet/","publishdate":"2017-03-30T07:41:36+08:00","relpermalink":"/post/k8s-kubelet/","section":"post","summary":"PodUID kubectl get pod \u0026lt;PID_NAME\u0026gt; -o=jsonpath='{.metadata.uid}'  POD on disk /var/lib/kubelet/pods/\u0026lt;PodUID\u0026gt;/\n/var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;\nls -l /var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;/ lrwxrwxrwx 1 root root 165 3月 30 06:52 0.log -\u0026gt; /var/lib/docker/containers/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021-json.log  In a production cluster, logs are usually collected, aggregated, and shipped to a remote store where advanced analysis/search/archiving functions are supported. In kubernetes, the default cluster-addons includes a per-node log collection daemon, fluentd. To facilitate the log collection, kubelet creates symbolic links to all the docker containers logs under /var/log/containers with pod and container metadata embedded in the filename.","tags":["K8S"],"title":"K8s Kubelet","type":"post"},{"authors":null,"categories":[],"content":" 你的老板，才是你最重要的人脉\n我相信这世界上，90% 的人都有过诅咒上司的一刻。偏执、变态、有病这些词，都是恨不得改成老板微信标签的定语。\n但是说大实话，年纪轻轻就有人促使你学会面对职场残酷的人，那不是一个贵人是什么？\n但是我们该如何面对这个残忍的贵人呢？我掏心掏肺给你一点方向。\n上司不是用来喜欢的 他不过是你的资源 最近老同事 Susan 给我打来了电话，很苦恼：我怎么就摊上了这样的破上司呢？\n不等我回答，她就直接自问自答：老板觉得我有能力，但是认为我对他不够尊重，就因为这样竟然就要封杀我！我要跳槽了！\n我笑了，你的情形再惨，惨得过孙悟空吗？毕竟唐僧动不动就给他念紧箍咒，没两天就叫他收拾包袱走人，他不也一样跟唐僧一起去取了个经回来吗。\n这无非是因为，孙悟空是足够聪明的猴子。\n他知道，唐僧是他老孙的一个重要资源，而不仅仅是他的上司。哪怕师傅再迂腐无能，再是非不分，他还是能调动观音菩萨、如来 佛祖来到身边的人。\n「要换做是我」，我笑着说，「尊重还不容易吗，态度好点就行了，直到他把行业诸多诀窍都教会我，再走。」\nSusan 开始若有所思了。她和大部分年轻人一样，对这个道理都不太懂。\n大部分人觉得：老板是资方，自己是劳方，天生就是对立的两端。毕竟在现实生活中逼你干活、骂你不对、给你 Mission impossible 的人就是他啊。\n但是聪明人却是这么看的：老板是小宇宙，自己也是小宇宙，只要懂得挖，他就是我的合作方，我的资源库。\n话说大名鼎鼎的格力董明珠，也有一个相处得不太顺当的前老板朱江洪。\n1991 年，朱江洪是格力的总经理，董明珠是一名普通的销售。一个技术狂，一个铁娘子，免不了争吵。\n然而争吵归争吵，董明珠还是在朱江洪的提拔下，从经营部长、大区经理、副总、总裁做到到格力集团董事长。两个人合作，把格力带进了千亿俱乐部，甚至突破了两千亿销售大关。\n董明珠就是这样一个足够机智的人。吵架归吵架，矛盾归矛盾，心底里骂过对方多少句「傻X」没关系，互相依赖、相互合作做出成绩来才是关键。\n每一个老板，无论多差劲，好歹都有一丝半点闪光点，否则他坐不到老板的位置上\n这样一个人，就是你人生长河中一个重要的资源库。\n多骂他几句「有病」，形成对立关系，无非是绝了自己获取人脉、信息、协助、建议的后路，这不是很笨吗？\n所有的职场恩怨，都起源于沟通不够 我刚开始工作的公司是个外企，沟通风格相当自由。\n上司和下属之间，有工作谈工作，没工作谈目标，没什么不能直接说的。很多人，哪怕跨了级，情同姐妹、称兄道弟的却比比皆是。\n然而当我到第一个民企工作的时候就感觉很压抑了。\n老板喜欢拍马屁，下属喜欢越位拍马屁，办公室流行着欲言又止的气氛，私下拉小群组帮派却热火朝天。所谓的人事厚黑学、信息不对称无处不在，我也算是开眼界了。\n然而我始终是一个好学宝宝，遇到问题和困难都喜欢探究个为什么。\n我发现，造成这么多职场政治的终极原因，无非就是沟通的问题。\n上司和下属之间沟通不明确，不拿出真心来说清楚要求，说一句留半句，剩下 9 句让你猜，这才是致命的低效源头\n俄国作家契科夫曾经写过小说《公务员之死》。这个倒霉蛋公务员就是不会沟通的典型。\n他不小心打喷嚏的时候把鼻涕喷在前座的将军身上，公务员连忙道歉，将军撇了撇嘴说「没关系，算了」。\n然而，公务员却把将军的「撇嘴」看成很生气，接下来每天都去将军那里道歉，将军烦不胜烦，更加不想理他。结果小公务员就一直提心吊胆，到最后竟然吓死了。\n这就是「说者无心，听者有意」的典型。小学语文课不好好做阅读理解，跑到职场上对老板表情做那么多阅读理解干嘛？\n上司很糟糕不跟你沟通，你不也很糟糕地没去主动找上司沟通吗？ 这世界上沟通是相互的，依赖别人终究被动，依赖自己才是王道\n糟糕上司永远没有最后，跳槽也解决不了 不知道大家发现了没有，身边的朋友，只要在一个公司遇到委屈，很可能就会跳槽跑路。\n他们常常寄望的是，下一个公司遇到一个圣人一样的妈妈老板，天天怕你冷了怕你饿了，更怕你的玻璃心掉一地碎了，像养蛙游戏的老母亲一样日日念叨着你快回来。\n实际上，很可能下一个老板更奇葩，更凶猛，KPI 更难完成。大概这世界上遇到好老板的几率，和媳妇遇到好婆婆的几率差不多低。\n大家熟知的偶像刘德华，在成为天王巨星之前也跟不少导演合作过了。\n80 年代和杜琪峰合作拍《鹿鼎记》，由始到终都被杜老板骂不如男一号梁朝伟。\n90 年代写了第一首歌《情是那么笨》，被知名作词人黄霑毫不留情地指责：「没见过写情这么笨的作词人。」\n当他进军歌坛的时候，还是被歌坛的老大哥谭咏麟一脸不客气地说他不会唱歌，劝他打消进军歌坛的念头。\n你看，哪怕是个有点名声的小鲜肉换各种角色都可能被前辈痛骂，何况你这个籍籍无名的小土豆，怎么可能通过跳槽就彻底换了一个毫不糟心的环境呢？\n糟糕上司哪里都有，如果你能死不要脸地撑到最后，妥妥用能力告诉他「老大，你看走眼了」，那才叫做真的威风\n职场说到底也是成年人的生意场，每个人都是自己的老板。 你的老板，你的同事，你的合作伙伴，都是你的供应商。\n他们身上的精华可以吸取，糟点可以避开，那你的装备才会越来越多，在升职加薪的路上一路打怪冲关。\n这年头流行让别人给自己「赋能」，现代管理学之父彼得·德鲁克比我们早数十年就看到上司的这个「功能」。他说：\n Making the strength of the boss productive is a key to the subordinate’s own effectiveness.\n 会用好上司的长处，才是你高效能的关键\n（文章来源于：维小维生素摘编）\n","date":1490173687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490173687,"objectID":"ea851af3bade29bf69c3197f95ac6ee4","permalink":"https://wubigo.com/post/2018-03-28-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%BA%E8%84%89/","publishdate":"2017-03-22T17:08:07+08:00","relpermalink":"/post/2018-03-28-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%BA%E8%84%89/","section":"post","summary":"你的老板，才是你最重要的人脉\n我相信这世界上，90% 的人都有过诅咒上司的一刻。偏执、变态、有病这些词，都是恨不得改成老板微信标签的定语。\n但是说大实话，年纪轻轻就有人促使你学会面对职场残酷的人，那不是一个贵人是什么？\n但是我们该如何面对这个残忍的贵人呢？我掏心掏肺给你一点方向。\n上司不是用来喜欢的 他不过是你的资源 最近老同事 Susan 给我打来了电话，很苦恼：我怎么就摊上了这样的破上司呢？\n不等我回答，她就直接自问自答：老板觉得我有能力，但是认为我对他不够尊重，就因为这样竟然就要封杀我！我要跳槽了！\n我笑了，你的情形再惨，惨得过孙悟空吗？毕竟唐僧动不动就给他念紧箍咒，没两天就叫他收拾包袱走人，他不也一样跟唐僧一起去取了个经回来吗。\n这无非是因为，孙悟空是足够聪明的猴子。\n他知道，唐僧是他老孙的一个重要资源，而不仅仅是他的上司。哪怕师傅再迂腐无能，再是非不分，他还是能调动观音菩萨、如来 佛祖来到身边的人。\n「要换做是我」，我笑着说，「尊重还不容易吗，态度好点就行了，直到他把行业诸多诀窍都教会我，再走。」\nSusan 开始若有所思了。她和大部分年轻人一样，对这个道理都不太懂。\n大部分人觉得：老板是资方，自己是劳方，天生就是对立的两端。毕竟在现实生活中逼你干活、骂你不对、给你 Mission impossible 的人就是他啊。\n但是聪明人却是这么看的：老板是小宇宙，自己也是小宇宙，只要懂得挖，他就是我的合作方，我的资源库。\n话说大名鼎鼎的格力董明珠，也有一个相处得不太顺当的前老板朱江洪。\n1991 年，朱江洪是格力的总经理，董明珠是一名普通的销售。一个技术狂，一个铁娘子，免不了争吵。\n然而争吵归争吵，董明珠还是在朱江洪的提拔下，从经营部长、大区经理、副总、总裁做到到格力集团董事长。两个人合作，把格力带进了千亿俱乐部，甚至突破了两千亿销售大关。\n董明珠就是这样一个足够机智的人。吵架归吵架，矛盾归矛盾，心底里骂过对方多少句「傻X」没关系，互相依赖、相互合作做出成绩来才是关键。\n每一个老板，无论多差劲，好歹都有一丝半点闪光点，否则他坐不到老板的位置上\n这样一个人，就是你人生长河中一个重要的资源库。\n多骂他几句「有病」，形成对立关系，无非是绝了自己获取人脉、信息、协助、建议的后路，这不是很笨吗？\n所有的职场恩怨，都起源于沟通不够 我刚开始工作的公司是个外企，沟通风格相当自由。\n上司和下属之间，有工作谈工作，没工作谈目标，没什么不能直接说的。很多人，哪怕跨了级，情同姐妹、称兄道弟的却比比皆是。\n然而当我到第一个民企工作的时候就感觉很压抑了。\n老板喜欢拍马屁，下属喜欢越位拍马屁，办公室流行着欲言又止的气氛，私下拉小群组帮派却热火朝天。所谓的人事厚黑学、信息不对称无处不在，我也算是开眼界了。\n然而我始终是一个好学宝宝，遇到问题和困难都喜欢探究个为什么。\n我发现，造成这么多职场政治的终极原因，无非就是沟通的问题。\n上司和下属之间沟通不明确，不拿出真心来说清楚要求，说一句留半句，剩下 9 句让你猜，这才是致命的低效源头\n俄国作家契科夫曾经写过小说《公务员之死》。这个倒霉蛋公务员就是不会沟通的典型。\n他不小心打喷嚏的时候把鼻涕喷在前座的将军身上，公务员连忙道歉，将军撇了撇嘴说「没关系，算了」。\n然而，公务员却把将军的「撇嘴」看成很生气，接下来每天都去将军那里道歉，将军烦不胜烦，更加不想理他。结果小公务员就一直提心吊胆，到最后竟然吓死了。\n这就是「说者无心，听者有意」的典型。小学语文课不好好做阅读理解，跑到职场上对老板表情做那么多阅读理解干嘛？\n上司很糟糕不跟你沟通，你不也很糟糕地没去主动找上司沟通吗？ 这世界上沟通是相互的，依赖别人终究被动，依赖自己才是王道\n糟糕上司永远没有最后，跳槽也解决不了 不知道大家发现了没有，身边的朋友，只要在一个公司遇到委屈，很可能就会跳槽跑路。\n他们常常寄望的是，下一个公司遇到一个圣人一样的妈妈老板，天天怕你冷了怕你饿了，更怕你的玻璃心掉一地碎了，像养蛙游戏的老母亲一样日日念叨着你快回来。\n实际上，很可能下一个老板更奇葩，更凶猛，KPI 更难完成。大概这世界上遇到好老板的几率，和媳妇遇到好婆婆的几率差不多低。\n大家熟知的偶像刘德华，在成为天王巨星之前也跟不少导演合作过了。\n80 年代和杜琪峰合作拍《鹿鼎记》，由始到终都被杜老板骂不如男一号梁朝伟。\n90 年代写了第一首歌《情是那么笨》，被知名作词人黄霑毫不留情地指责：「没见过写情这么笨的作词人。」\n当他进军歌坛的时候，还是被歌坛的老大哥谭咏麟一脸不客气地说他不会唱歌，劝他打消进军歌坛的念头。\n你看，哪怕是个有点名声的小鲜肉换各种角色都可能被前辈痛骂，何况你这个籍籍无名的小土豆，怎么可能通过跳槽就彻底换了一个毫不糟心的环境呢？\n糟糕上司哪里都有，如果你能死不要脸地撑到最后，妥妥用能力告诉他「老大，你看走眼了」，那才叫做真的威风\n职场说到底也是成年人的生意场，每个人都是自己的老板。 你的老板，你的同事，你的合作伙伴，都是你的供应商。\n他们身上的精华可以吸取，糟点可以避开，那你的装备才会越来越多，在升职加薪的路上一路打怪冲关。\n这年头流行让别人给自己「赋能」，现代管理学之父彼得·德鲁克比我们早数十年就看到上司的这个「功能」。他说：\n Making the strength of the boss productive is a key to the subordinate’s own effectiveness.","tags":["CAREER","MANAGE"],"title":"最重要的人脉","type":"post"},{"authors":null,"categories":[],"content":" 模块 A module is a collection of related Go packages that are versioned together as a single unit.\nModules record precise dependency requirements and create reproducible builds.\n go.mod  A module is defined by a tree of Go source files with a go.mod file in the tree\u0026rsquo;s root directory. Module source code may be located outside of GOPATH. There are four directives: module, require, replace, exclude.\n显示当前的模块和依赖 go list -m all  显示特定模块的所有版本标签 go list -m -versions github.com/minio/cli  模块API说明书 go doc github.com/minio/cli  How to Use Modules  How to Install and Activate Module Support  Install the latest Go 1.11 release.   Once installed, you can then activate module support in one of two ways:\nInvoke the go command in a directory outside of the $GOPATH/src tree, with a valid go.mod file in the current directory or any parent of it and the environment variable GO111MODULE unset (or explicitly set to auto). Invoke the go command with GO111MODULE=on environment variable set.  https://blog.golang.org/using-go-modules\nhttps://semver.org/\n","date":1490166560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490166560,"objectID":"0780d10226636c9b3c4e5f80730c3cb2","permalink":"https://wubigo.com/post/lang-go-module/","publishdate":"2017-03-22T15:09:20+08:00","relpermalink":"/post/lang-go-module/","section":"post","summary":"模块 A module is a collection of related Go packages that are versioned together as a single unit.\nModules record precise dependency requirements and create reproducible builds.\n go.mod  A module is defined by a tree of Go source files with a go.mod file in the tree\u0026rsquo;s root directory. Module source code may be located outside of GOPATH. There are four directives: module, require, replace, exclude.\n显示当前的模块和依赖 go list -m all  显示特定模块的所有版本标签 go list -m -versions github.","tags":["LANG","GO"],"title":"Go Module","type":"post"},{"authors":null,"categories":[],"content":" Enable Helm in cluster  Create a Service Account tiller for the Tiller server (in the kube-system namespace). Service Accounts are meant for intra-cluster processes running in Pods.\n Bind the cluster-admin ClusterRole to this Service Account. ClusterRoleBindings to be applicable in all namespaces. Tiller to manage resources in all namespaces.\n Update the existing Tiller deployment (tiller-deploy) to associate its pod with the Service Account tiller.\nkubectl create serviceaccount tiller --namespace kube-system kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}'  or\ncat tiller-clusterrolebinding.yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.K8S.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026quot;\u0026quot; docker pull registry.cn-beijing.aliyuncs.com/k4s/tiller:v2.12.3 kubectl create -f tiller-clusterrolebinding.yaml # Update the existing tiller-deploy deployment with the Service Account helm init --service-account tiller --upgrade   ","date":1489828359,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489828359,"objectID":"16ad83a5ce3bc4eddd51e55285a3b095","permalink":"https://wubigo.com/post/k8s-helm-setup/","publishdate":"2017-03-18T17:12:39+08:00","relpermalink":"/post/k8s-helm-setup/","section":"post","summary":"Enable Helm in cluster  Create a Service Account tiller for the Tiller server (in the kube-system namespace). Service Accounts are meant for intra-cluster processes running in Pods.\n Bind the cluster-admin ClusterRole to this Service Account. ClusterRoleBindings to be applicable in all namespaces. Tiller to manage resources in all namespaces.\n Update the existing Tiller deployment (tiller-deploy) to associate its pod with the Service Account tiller.\nkubectl create serviceaccount tiller --namespace kube-system kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}'  or","tags":["K8S"],"title":"K8s Helm Setup","type":"post"},{"authors":null,"categories":[],"content":" Configuring Nodes to Authenticate to a Private Registry  Note: Kubernetes as of now only supports the auths and HttpHeaders section of docker config. This means credential helpers (credHelpers or credsStore) are not supported.\n Docker stores keys for private registries in the $HOME/.dockercfg or $HOME/.docker/config.json file. If there are files in the search paths list below, kubelet uses it as the credential provider when pulling images.\n {\u0026ndash;root-dir:-/var/lib/kubelet}/config.json {cwd of kubelet}/config.json ${HOME}/.docker/config.json /.docker/config.json {\u0026ndash;root-dir:-/var/lib/kubelet}/.dockercfg {cwd of kubelet}/.dockercfg ${HOME}/.dockercfg /.dockercfg  ~/.docker/config.json\n\u0026quot;auths\u0026quot;: {\t\u0026quot;registry.cn-hangzhou.aliyuncs.com\u0026quot;: { \u0026quot;auth\u0026quot;: \u0026quot;d3ViaWdvOjEyMzEyMwo=\u0026quot; } }, \u0026quot;HttpHeaders\u0026quot;: { \u0026quot;User-Agent\u0026quot;: \u0026quot;Docker-Client/17.03.3-ce (linux)\u0026quot; }   convert the base64-encoded auth data to a readable format\necho \u0026quot;d3ViaWdvOjEyMzEyMwo=\u0026quot; | base64 --decode  nodes=$(kubectl get nodes -o jsonpath='{range.items[*].metadata}{.name} {end}') for n in $nodes; do scp ~/.docker/config.json root@$n:/var/lib/kubelet/config.json; done   https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nhttps://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n","date":1489704286,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489704286,"objectID":"620e8ba367120ada5a564b558a531ccb","permalink":"https://wubigo.com/post/k8s-private-registry/","publishdate":"2017-03-17T06:44:46+08:00","relpermalink":"/post/k8s-private-registry/","section":"post","summary":"Configuring Nodes to Authenticate to a Private Registry  Note: Kubernetes as of now only supports the auths and HttpHeaders section of docker config. This means credential helpers (credHelpers or credsStore) are not supported.\n Docker stores keys for private registries in the $HOME/.dockercfg or $HOME/.docker/config.json file. If there are files in the search paths list below, kubelet uses it as the credential provider when pulling images.\n {\u0026ndash;root-dir:-/var/lib/kubelet}/config.json {cwd of kubelet}/config.","tags":["K8S"],"title":"K8s Private Registry","type":"post"},{"authors":null,"categories":[],"content":" ssh client config ~/.ssh/config\nhost * StrictHostKeyChecking no  Enables forwarding of the authentication agent connection  client config .ssh/config\nForwardAgent yes  Enable ssh-agent on main device\n  .bashrc\nSSH_ENV=\u0026quot;$HOME/.ssh/environment\u0026quot; function start_agent { echo \u0026quot;Initialising new SSH agent...\u0026quot; /usr/bin/ssh-agent | sed 's/^echo/#echo/' \u0026gt; \u0026quot;${SSH_ENV}\u0026quot; echo succeeded chmod 600 \u0026quot;${SSH_ENV}\u0026quot; . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null /usr/bin/ssh-add; } # Source SSH settings, if applicable if [ -f \u0026quot;${SSH_ENV}\u0026quot; ]; then . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null #ps ${SSH_AGENT_PID} doesn't work under cywgin ps -ef | grep ${SSH_AGENT_PID} | grep ssh-agent$ \u0026gt; /dev/null || { start_agent; } else start_agent; fi  http://mah.everybody.org/docs/ssh\n","date":1489656084,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489656084,"objectID":"785f669f0bbe895ddc21b4d02aad8ae6","permalink":"https://wubigo.com/post/linux-ssh/","publishdate":"2017-03-16T17:21:24+08:00","relpermalink":"/post/linux-ssh/","section":"post","summary":"ssh client config ~/.ssh/config\nhost * StrictHostKeyChecking no  Enables forwarding of the authentication agent connection  client config .ssh/config\nForwardAgent yes  Enable ssh-agent on main device\n  .bashrc\nSSH_ENV=\u0026quot;$HOME/.ssh/environment\u0026quot; function start_agent { echo \u0026quot;Initialising new SSH agent...\u0026quot; /usr/bin/ssh-agent | sed 's/^echo/#echo/' \u0026gt; \u0026quot;${SSH_ENV}\u0026quot; echo succeeded chmod 600 \u0026quot;${SSH_ENV}\u0026quot; . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null /usr/bin/ssh-add; } # Source SSH settings, if applicable if [ -f \u0026quot;${SSH_ENV}\u0026quot; ]; then .","tags":["LINUX","SSH"],"title":"Linux SSH","type":"post"},{"authors":null,"categories":[],"content":" setup external ETCD  install docker, kubelet, and kubeadm Configure the kubelet to be a service manager for etcd Create configuration files for kubeadm  /tmp/${HOST0}/kubeadmcfg.yaml\napiVersion: \u0026quot;kubeadm.k8s.io/v1beta1\u0026quot; kind: ClusterConfiguration etcd: local: serverCertSANs: - \u0026quot;192.168.1.10\u0026quot; peerCertSANs: - \u0026quot;192.168.1.10\u0026quot; extraArgs: initial-cluster: infra0=https://192.168.1.10:2380 initial-cluster-state: new name: infra0 listen-peer-urls: https://192.168.1.10:2380 listen-client-urls: https://192.168.1.10:2379 advertise-client-urls: https://192.168.1.10:2379 initial-advertise-peer-urls: https://192.168.1.10:2380   Generate the certificate authority\nsudo kubeadm init phase certs etcd-ca export HOST0=\u0026quot;192.168.1.10\u0026quot; sudo kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml sudo kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml sudo kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml sudo kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml  Create the static pod manifests\nkubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml   /etc/kubernetes/manifests\napiVersion: v1 kind: Pod metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026quot;\u0026quot; creationTimestamp: null labels: component: etcd tier: control-plane name: etcd namespace: kube-system spec: containers: - command: - etcd - --advertise-client-urls=https://192.168.1.10:2379 - --initial-advertise-peer-urls=https://192.168.1.10:2380 - --initial-cluster=infra0=https://192.168.1.10:2380 - --initial-cluster-state=new - --listen-client-urls=https://192.168.1.10:2379 - --listen-peer-urls=https://192.168.1.10:2380 - --name=infra0 - --cert-file=/etc/kubernetes/pki/etcd/server.crt - --client-cert-auth=true - --data-dir=/var/lib/etcd - --key-file=/etc/kubernetes/pki/etcd/server.key - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt image: k8s.gcr.io/etcd:3.2.24 imagePullPolicy: IfNotPresent livenessProbe: exec: command: - /bin/sh - -ec - ETCDCTL_API=3 etcdctl --endpoints=https://[192.168.1.10]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo failureThreshold: 8 initialDelaySeconds: 15 timeoutSeconds: 15 name: etcd resources: {} volumeMounts: - mountPath: /var/lib/etcd name: etcd-data - mountPath: /etc/kubernetes/pki/etcd name: etcd-certs hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/kubernetes/pki/etcd type: DirectoryOrCreate name: etcd-certs - hostPath: path: /var/lib/etcd type: DirectoryOrCreate name: etcd-data status: {}   ensure the etcd pod is running\njournalctl -xeu kubelet remote_image.go:112] PullImage \u0026quot;k8s.gcr.io/etcd:3.2.24\u0026quot; from image service failed: rpc error: code = Unknown desc = Error response fro 3月 16 14:41:53 bigo-vm2 kubelet[4221]: E0316 14:41:53.537292 4221 kuberuntime_image.go:51] Pull image \u0026quot;k8s.gcr.io/etcd:3.2.24\u0026quot; failed: rpc error: code = Unknown desc = Error response from daemon: Get 3月 16 14:41:53 bigo-vm2 kubelet[4221]: E0316 14:41:53.537393 4221 kuberuntime_manager.go:749] container start failed: ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: Get htt 3月 16 14:41:53 bigo-vm2 kubelet[4221]: E0316 14:41:53.537469 4221 pod_workers.go:190] Error syncing pod 30ecbbae123bb7b8baaa2f08cb762164 (\u0026quot;etcd-bigo-vm2_kube-system(30ecbbae123bb7b8baaa2f08cb762164)\u0026quot;)  docker pull mirrorgooglecontainers/etcd:3.2.24 docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24  Check the cluster health\ndocker run --rm -it \\ --net host \\ -v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:3.2.24 etcdctl \\ --cert-file /etc/kubernetes/pki/etcd/peer.crt \\ --key-file /etc/kubernetes/pki/etcd/peer.key \\ --ca-file /etc/kubernetes/pki/etcd/ca.crt \\ --endpoints https://${HOST0}:2379 cluster-health   Set up the first control plane node  Copy the following files from any node from the etcd cluster\nexport CONTROL_PLANE=\u0026quot;192.168.1.9\u0026quot; scp /etc/kubernetes/pki/etcd/ca.crt \u0026quot;${CONTROL_PLANE}\u0026quot;: scp /etc/kubernetes/pki/apiserver-etcd-client.crt \u0026quot;${CONTROL_PLANE}\u0026quot;: scp /etc/kubernetes/pki/apiserver-etcd-client.key \u0026quot;${CONTROL_PLANE}\u0026quot;:   kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable apiServer: certSANs: - \u0026quot;192.168.1.9\u0026quot; controlPlaneEndpoint: \u0026quot;192.168.1.9:6443\u0026quot; etcd: external: endpoints: - https://192.168.1.10:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key networking: podSubnet: \u0026quot;10.2.0.0/16\u0026quot;  kubeadm init --pod-network-cidr 10.2.0.0/16 --config kubeadm-config.yaml -v 4 kubeadm init --config kubeadm-config.yaml -v 4  setup CNI ssh $ETCD curl https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/calico.yaml\u0026gt; calico.yaml # calico etcd setup sed -i -e \u0026quot;s/\\(^etcd_endpoints: \\\u0026quot;http.*$\\)/etcd_endpoints: \\\u0026quot;https:\\/\\/$VM:2379\\\u0026quot;/g\u0026quot; calico.yaml # etcd_ca: \u0026quot;/calico-secrets/etcd-ca\u0026quot; sed -i -e 's/etcd_ca: \\\u0026quot;\\\u0026quot; \\# \\\u0026quot;\\/calico-secrets/etcd-ca\\\u0026quot;/etcd_ca: \\\u0026quot;\\/calico-secrets\\/etcd-ca\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_cert: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/etcd_cert: \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_key: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/etcd_key: \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/g' calico.yaml CA=$(cat /etc/kubernetes/pki/etcd/ca.crt | base64 -w 0) CERT=$(cat /etc/kubernetes/pki/etcd/server.crt | base64 -w 0) KEY=$(sudo cat /etc/kubernetes/pki/etcd/server.key | base64 -w 0) sed -i -e \u0026quot;s/# etcd-ca: null/etcd-ca: $CA/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-cert: null/etcd-cert: $CERT/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-key: null/etcd-key: $KEY/g\u0026quot; calico.yaml  join a node https://blog.scottlowe.org/2018/08/21/bootstrapping-etcd-cluster-with-tls-using-kubeadm/ https://github.com/kelseyhightower/standalone-kubelet-tutorial\n","date":1489641812,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489641812,"objectID":"68d7fe0fb30aa6cb3cc71de26d0b5669","permalink":"https://wubigo.com/post/k8s-ha-setup-with-kubeadm/","publishdate":"2017-03-16T13:23:32+08:00","relpermalink":"/post/k8s-ha-setup-with-kubeadm/","section":"post","summary":"setup external ETCD  install docker, kubelet, and kubeadm Configure the kubelet to be a service manager for etcd Create configuration files for kubeadm  /tmp/${HOST0}/kubeadmcfg.yaml\napiVersion: \u0026quot;kubeadm.k8s.io/v1beta1\u0026quot; kind: ClusterConfiguration etcd: local: serverCertSANs: - \u0026quot;192.168.1.10\u0026quot; peerCertSANs: - \u0026quot;192.168.1.10\u0026quot; extraArgs: initial-cluster: infra0=https://192.168.1.10:2380 initial-cluster-state: new name: infra0 listen-peer-urls: https://192.168.1.10:2380 listen-client-urls: https://192.168.1.10:2379 advertise-client-urls: https://192.168.1.10:2379 initial-advertise-peer-urls: https://192.168.1.10:2380   Generate the certificate authority\nsudo kubeadm init phase certs etcd-ca export HOST0=\u0026quot;192.168.1.10\u0026quot; sudo kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.","tags":["K8S"],"title":"K8s HA Setup With Kubeadm","type":"post"},{"authors":null,"categories":[],"content":"tl;dr\nuget https://osdn.net/projects/systemrescuecd/storage/releases/6.0.2/systemrescuecd-6.0.2.iso sudo mount -t tmpfs tmpfs /takeover/ sudo mount -o loop,ro -t iso9660 ~/systemrescuecd-6.0.2.iso /mnt/cd cp -rf /mnt/cd/* /takeover/ curl -L https://www.busybox.net/downloads/binaries/1.26.2-defconfig-multiarch/busybox-x86_64 \u0026gt; busybox chmod u+x /takeover/busybox git clone https://github.com/marcan/takeover.sh.git gcc takeover.sh/fakeinit.c -o ./fakeinit  ","date":1489242916,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489242916,"objectID":"aa638212ded38327c0c22535695dd9d8","permalink":"https://wubigo.com/post/reinstall-a-running-linux-system-via-ssh/","publishdate":"2017-03-11T22:35:16+08:00","relpermalink":"/post/reinstall-a-running-linux-system-via-ssh/","section":"post","summary":"tl;dr\nuget https://osdn.net/projects/systemrescuecd/storage/releases/6.0.2/systemrescuecd-6.0.2.iso sudo mount -t tmpfs tmpfs /takeover/ sudo mount -o loop,ro -t iso9660 ~/systemrescuecd-6.0.2.iso /mnt/cd cp -rf /mnt/cd/* /takeover/ curl -L https://www.busybox.net/downloads/binaries/1.26.2-defconfig-multiarch/busybox-x86_64 \u0026gt; busybox chmod u+x /takeover/busybox git clone https://github.com/marcan/takeover.sh.git gcc takeover.sh/fakeinit.c -o ./fakeinit  ","tags":["LINUX"],"title":"通过SSH远程修复linux","type":"post"},{"authors":null,"categories":[],"content":" 准备 docker pull istio/proxyv2:1.0.6 docker tag istio/proxyv2:1.0.6 gcr.io/istio-release/proxyv2:release-1.0-latest-daily docker push registry.cn-beijing.aliyuncs.com/co1/istio_proxyv2:1.0.6 docker pull istio/pilot:1.0.6 docker tag istio/pilot:1.0.6 gcr.io/istio-release/pilot:release-1.0-latest-daily docker pull istio/mixer:1.0.6 docker tag istio/mixer:1.0.6 gcr.io/istio-release/mixer:release-1.0-latest-daily docker pull istio/galley:1.0.6 docker tag istio/galley:1.0.6 gcr.io/istio-release/galley:release-1.0-latest-daily docker pull istio/citadel:1.0.6 docker tag istio/citadel:1.0.6 gcr.io/istio-release/citadel:release-1.0-latest-daily docker pull istio/sidecar_injector:1.0.6 docker tag istio/sidecar_injector:1.0.6 gcr.io/istio-release/sidecar_injector:release-1.0-latest-daily git clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6  安装 Istio by default uses LoadBalancer service object types. Some platforms do not support LoadBalancer service objects. For platforms lacking LoadBalancer support, install Istio with NodePort support instead with the flags \u0026ndash;set gateways.istio-ingressgateway.type=NodePort \u0026ndash;set gateways.istio-egressgateway.type=NodePort appended to the end of the Helm operation.\nhelm install install/kubernetes/helm/istio --name istio --namespace istio-system --set gateways.istio-ingressgateway.type=NodePort --set gateways.istio-egressgateway.type=NodePort  精简安装 helm install --debug install/kubernetes/helm/istio --name istio --namespace istio-system --set security.enabled=false --set ingress.enabled=false --set gateways.istio-ingressgateway.enabled=false --set gateways.istio-egressgateway.enabled=false --set galley.enabled=false --set mixer.enabled=false --set prometheus.enabled=false --set global.proxy.envoyStatsd.enabled=false --set pilot.sidecar=true --set sidecarInjectorWebhook.enabled=false  kubectl label namespace default istio-injection=enabled kubectl describe ns default -n istio-system  RESOURCES: ==\u0026gt; v1beta1/ClusterRoleBinding NAME AGE istio-pilot-istio-system 4s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE istio-pilot 1 1 1 0 3s ==\u0026gt; v1alpha3/Gateway NAME AGE istio-autogenerated-k8s-ingress 3s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE istio-pilot-754ccc994f-zzkj9 0/1 Pending 0 2s ==\u0026gt; v1/ConfigMap NAME DATA AGE istio 1 5s istio-sidecar-injector 1 4s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE istio-pilot-service-account 1 4s ==\u0026gt; v1beta1/ClusterRole NAME AGE istio-pilot-istio-system 4s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-pilot ClusterIP 10.96.216.216 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 4s ==\u0026gt; v2beta1/HorizontalPodAutoscaler NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE istio-pilot Deployment/istio-pilot \u0026lt;unknown\u0026gt;/80% 1 5 0 2s  Ensure the istio-pilot-* Kubernetes pod is deployed and its container is up and running:\nkubectl get pods -n istio-system  MountVolume.SetUp failed for volume \u0026quot;certs\u0026quot; : secret \u0026quot;istio.istio-sidecar-injector-service-account\u0026quot; not found  the missing secret is created by the citadel pod which isn\u0026rsquo;t running due to the the \u0026ndash;set security.enabled=false flag, setting that to true starts citadel and the secret is created and then pilot will start.\n删除 helm del --purge istio kubectl -n istio-system delete job --all kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-system kubectl get customresourcedefinitions.apiextensions.k8s.io |grep istio | xargs kubectl delete customresourcedefinitions.apiextensions.k8s.io  运行配置 kubectl get cm -n istio-system istio -o yaml \u0026gt; istio.config awk '{gsub(/\\\\n/,\u0026quot;\\n\u0026quot;)}1' istio.config  or\nkubectl exec -it istio-pilot -c discovery -n istio-system -- bash #cat /etc/istio/config/mesh | grep discoveryAddress kubectl get svc/istio-pilot -n istio-system -o yaml apiVersion: v1 kind: Service metadata: creationTimestamp: \u0026quot;2018-03-29T11:04:04Z\u0026quot; labels: app: istio-pilot chart: pilot-1.0.6 heritage: Tiller release: istio name: istio-pilot namespace: istio-system resourceVersion: \u0026quot;467151\u0026quot; selfLink: /api/v1/namespaces/istio-system/services/istio-pilot uid: 5de2a2d8-5212-11e9-b518-08002775f493 spec: clusterIP: 10.108.66.176 ports: - name: grpc-xds port: 15010 protocol: TCP targetPort: 15010 - name: https-xds port: 15011 protocol: TCP targetPort: 15011 - name: http-legacy-discovery port: 8080 protocol: TCP targetPort: 8080 - name: http-monitoring port: 9093 protocol: TCP targetPort: 9093 selector: istio: pilot sessionAffinity: None type: ClusterIP status: loadBalancer: {} kubectl port-forward svc/istio-pilot -n istio-system 15010:15010  ","date":1488616718,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488616718,"objectID":"ebf88da19207ac64b970ca310f802061","permalink":"https://wubigo.com/post/k8s-mesh-istio/","publishdate":"2017-03-04T16:38:38+08:00","relpermalink":"/post/k8s-mesh-istio/","section":"post","summary":"准备 docker pull istio/proxyv2:1.0.6 docker tag istio/proxyv2:1.0.6 gcr.io/istio-release/proxyv2:release-1.0-latest-daily docker push registry.cn-beijing.aliyuncs.com/co1/istio_proxyv2:1.0.6 docker pull istio/pilot:1.0.6 docker tag istio/pilot:1.0.6 gcr.io/istio-release/pilot:release-1.0-latest-daily docker pull istio/mixer:1.0.6 docker tag istio/mixer:1.0.6 gcr.io/istio-release/mixer:release-1.0-latest-daily docker pull istio/galley:1.0.6 docker tag istio/galley:1.0.6 gcr.io/istio-release/galley:release-1.0-latest-daily docker pull istio/citadel:1.0.6 docker tag istio/citadel:1.0.6 gcr.io/istio-release/citadel:release-1.0-latest-daily docker pull istio/sidecar_injector:1.0.6 docker tag istio/sidecar_injector:1.0.6 gcr.io/istio-release/sidecar_injector:release-1.0-latest-daily git clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6  安装 Istio by default uses LoadBalancer service object types. Some platforms do not support LoadBalancer service objects.","tags":["K8S"],"title":"K8S微服务治理","type":"post"},{"authors":null,"categories":[],"content":" GITHUB两种主要的pull request的开发模式\n分叉拉取模式 任何开发人员可以在项目源仓库(upstream)分叉，然后仓库该分叉(origin)到本地文件系统进行开发 测试，测试完毕提交到分叉origin，并发送pull request到源仓库upstream, 源仓库维护人员评审 更改，并最终决定是否合并该更改到源仓库\n在发送pull request之前，好几个开发人员共同为一个特性协作开发， 互相从对方的仓库拉取代码。 这时，从对方的仓库拉取代码简化重新定义一个remote，该remote把本地的分叉指向对方仓库地址。\n https://github.com/wubigo/wubigo.github.io 单击Fork按钮(右上角)\n GITHUB把该仓库代码复制到自己的github账号，建立分叉仓库\n 打开git命令行客户端，把分叉仓库克隆到本地环境\ngit clone https://github.com/$USER_NAME/wubigo.github.io.git cd wubigo.github.io git remote add upstream git@github.com:wubigo/wubigo.github.io.git # Never push to upstream master git remote set-url --push upstream no_push # Confirm that your remotes make sense: git remote -v origin\thttps://github.com/Fuang/wubigo.github.io.git (fetch) origin\thttps://github.com/Fuang/wubigo.github.io.git (push) upstream\tgit@github.com:wubigo/wubigo.github.io.git (fetch) upstream\tgit@github.com:wubigo/wubigo.github.io.git (push)  同步本地代码到upstream\ngit fetch upstream git checkout master git rebase upstream/master git push  查看各个分支的最新提交ID\ngit branch -av * master a3b4a2b Update 2016-02-03-k8s local development setup.md remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/jekyll 439d951 Merge branch 'master' of github.com:wubigo/igo.github.io remotes/origin/master a3b4a2b Update 2016-02-03-k8s local development setup.md remotes/upstream/jekyll 439d951 Merge branch 'master' of github.com:wubigo/igo.github.io remotes/upstream/master a38ea93 deploy:github fork\u0026amp;pull model  从分叉创建pull request\n  原则上你可以在你分叉的任何分支或提交ID上向upstream仓库发出pull request， 但实际开发中，建议自建主题分支。\ngit branch add-project git add . git commit -m \u0026quot;add project\u0026quot; git push   进入github页面  点击add-project分支后面的compare \u0026amp; pull request按钮\n确认base repo是upstream， head repo是分叉\ncompare分支选择自建的add-project\n","date":1488585981,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488585981,"objectID":"64c4e271393e7ccf69cf6202c0f2d498","permalink":"https://wubigo.com/post/git-forkpull-model/","publishdate":"2017-03-04T08:06:21+08:00","relpermalink":"/post/git-forkpull-model/","section":"post","summary":"GITHUB两种主要的pull request的开发模式\n分叉拉取模式 任何开发人员可以在项目源仓库(upstream)分叉，然后仓库该分叉(origin)到本地文件系统进行开发 测试，测试完毕提交到分叉origin，并发送pull request到源仓库upstream, 源仓库维护人员评审 更改，并最终决定是否合并该更改到源仓库\n在发送pull request之前，好几个开发人员共同为一个特性协作开发， 互相从对方的仓库拉取代码。 这时，从对方的仓库拉取代码简化重新定义一个remote，该remote把本地的分叉指向对方仓库地址。\n https://github.com/wubigo/wubigo.github.io 单击Fork按钮(右上角)\n GITHUB把该仓库代码复制到自己的github账号，建立分叉仓库\n 打开git命令行客户端，把分叉仓库克隆到本地环境\ngit clone https://github.com/$USER_NAME/wubigo.github.io.git cd wubigo.github.io git remote add upstream git@github.com:wubigo/wubigo.github.io.git # Never push to upstream master git remote set-url --push upstream no_push # Confirm that your remotes make sense: git remote -v origin\thttps://github.com/Fuang/wubigo.github.io.git (fetch) origin\thttps://github.com/Fuang/wubigo.github.io.git (push) upstream\tgit@github.com:wubigo/wubigo.github.io.git (fetch) upstream\tgit@github.com:wubigo/wubigo.github.io.git (push)  同步本地代码到upstream\ngit fetch upstream git checkout master git rebase upstream/master git push  查看各个分支的最新提交ID","tags":["GIT","SCM"],"title":"分叉拉取模式(fork and pull model)","type":"post"},{"authors":null,"categories":[],"content":" 准备  创建角色和授权\nkubectl create clusterrolebinding \u0026quot;cluster-admin-faas\u0026quot; \\ --clusterrole=cluster-admin \\ --user=\u0026quot;cluster-admin-faas\u0026quot;  分别为FAAS核心服务和函数创建名字空间\nkubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml  创建凭证\n# generate a random password PASSWORD=$(head -c 12 /dev/urandom | shasum| cut -d' ' -f1) kubectl -n openfaas create secret generic basic-auth \\ --from-literal=basic-auth-user=admin \\ --from-literal=basic-auth-password=\u0026quot;$PASSWORD\u0026quot;  在本地helm仓库增加openfaas\nhelm repo add openfaas https://openfaas.github.io/faas-netes/ \u0026quot;openfaas\u0026quot; has been added to your repositories   开始安装 helm repo update \\ \u0026amp;\u0026amp; helm upgrade openfaas --install openfaas/openfaas \\ --namespace openfaas \\ --set basic_auth=true \\ --set functionNamespace=openfaas-fn  默认通过NodePorts方式访问openfaas控制台\nkubectl -n openfaas get svc -o wide |grep external gateway-external NodePort 10.105.222.28 \u0026lt;none\u0026gt; 8080:31112/TCP 91m app=gateway export OPENFAAS_URL=http://\u0026lt;pod-node-ip\u0026gt;:31112 curl OPENFAAS_URL USERNAME=$(kubectl get secrets basic-auth -n openfaas -o yaml | grep basic-auth-user) PASSWD=$(kubectl get secrets basic-auth -n openfaas -o yaml | grep basic-auth-password) PASSWD=$(echo '$PASSWD' | base64 --decode)  验证安装结果  通过浏览器访问openfaas\ncurl http://\u0026lt;pod-node-ip\u0026gt;:31112   输入上面的用户名和密码进入openfaas控制台\n 安装openfaas客户端\ncurl -sSL https://cli.openfaas.com | sh echo -n $PASSWORD | faas-cli login -g $OPENFAAS_URL -u admin --password-stdin   Removing the OpenFaaS All control plane components can be cleaned up with helm:\nhelm delete --purge openfaas kubectl delete namespace/openfaas kubectl delete namespace/openfaas-fn  ","date":1488501955,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488501955,"objectID":"12eeed65e38554ed2601db2d66499d71","permalink":"https://wubigo.com/post/serverless-computing-why-how/","publishdate":"2017-03-03T08:45:55+08:00","relpermalink":"/post/serverless-computing-why-how/","section":"post","summary":"准备  创建角色和授权\nkubectl create clusterrolebinding \u0026quot;cluster-admin-faas\u0026quot; \\ --clusterrole=cluster-admin \\ --user=\u0026quot;cluster-admin-faas\u0026quot;  分别为FAAS核心服务和函数创建名字空间\nkubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml  创建凭证\n# generate a random password PASSWORD=$(head -c 12 /dev/urandom | shasum| cut -d' ' -f1) kubectl -n openfaas create secret generic basic-auth \\ --from-literal=basic-auth-user=admin \\ --from-literal=basic-auth-password=\u0026quot;$PASSWORD\u0026quot;  在本地helm仓库增加openfaas\nhelm repo add openfaas https://openfaas.github.io/faas-netes/ \u0026quot;openfaas\u0026quot; has been added to your repositories   开始安装 helm repo update \\ \u0026amp;\u0026amp; helm upgrade openfaas --install openfaas/openfaas \\ --namespace openfaas \\ --set basic_auth=true \\ --set functionNamespace=openfaas-fn  默认通过NodePorts方式访问openfaas控制台","tags":["SERVERLESS","FAAS"],"title":"无服务器计算环境OPENFAAS搭建","type":"post"},{"authors":null,"categories":[],"content":"","date":1488447914,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488447914,"objectID":"64d52d939f2a482b199b94cb83c3ade7","permalink":"https://wubigo.com/post/docker-network-host/","publishdate":"2017-03-02T17:45:14+08:00","relpermalink":"/post/docker-network-host/","section":"post","summary":"","tags":[],"title":"Docker Network Host","type":"post"},{"authors":null,"categories":[],"content":"","date":1488446262,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488446262,"objectID":"c11870dfe8c3bf61b1af1185dea9ef23","permalink":"https://wubigo.com/post/docker_network_bridge/","publishdate":"2017-03-02T17:17:42+08:00","relpermalink":"/post/docker_network_bridge/","section":"post","summary":"","tags":[],"title":"Docker_network_bridge","type":"post"},{"authors":null,"categories":[],"content":" 容器网络 容器网络方案 = 接入 + 流控 + 通道\ndocker默认的网络 桥接网络\nDocker网络macvlan 网络macvlan\nDocker宿主网络 宿主网络\nDocker覆盖网络 宿主端口绑定 绑定方式： -p\n绑定形式\n ip:hostPort:containerPort| ip::containerPort | hostPort:containerPort | containerPort\n containerPort必须指定\ndocker run --rm --name web -p 80:80 -v /home/bigo/site:/usr/share/nginx/html:ro -d nginx:1.14-alpine  docker 会为端口绑定的容器自动启动docker-proxy进程\ndocker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.2 -container-port 80  ","date":1488424251,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488424251,"objectID":"7c5ad0c4ff20287ebd8ffa813ae04338","permalink":"https://wubigo.com/post/docker_network/","publishdate":"2017-03-02T11:10:51+08:00","relpermalink":"/post/docker_network/","section":"post","summary":" 容器网络 容器网络方案 = 接入 + 流控 + 通道\ndocker默认的网络 桥接网络\nDocker网络macvlan 网络macvlan\nDocker宿主网络 宿主网络\nDocker覆盖网络 宿主端口绑定 绑定方式： -p\n绑定形式\n ip:hostPort:containerPort| ip::containerPort | hostPort:containerPort | containerPort\n containerPort必须指定\ndocker run --rm --name web -p 80:80 -v /home/bigo/site:/usr/share/nginx/html:ro -d nginx:1.14-alpine  docker 会为端口绑定的容器自动启动docker-proxy进程\ndocker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.2 -container-port 80  ","tags":["DOCKER"],"title":"Docker网络","type":"post"},{"authors":null,"categories":null,"content":" Getting an SSL Certificate and CloudFront Create CloudFront Distribution Navigate to CloudFront in your AWS console and click \u0026ldquo;Create Distribution\u0026rdquo;. Click \u0026ldquo;Get Started\u0026rdquo; under the Web option (not the RTMP). You\u0026rsquo;ll arrive on the Create Distribution page. Here you need to change three things: 1. Click inside the input field for \u0026ldquo;Origin Domain Name\u0026rdquo;. A list of your Amazon S3 buckets should pop up. Select the S3 bucket you want to use. 2. Scroll to the \u0026ldquo;Alternate Domain Names (CNAMEs)\u0026rdquo; field and enter your domain/subdomain you\u0026rsquo;re using 3. Scroll down to SSL Certificate and change the option to \u0026ldquo;Custom SSL Certificate\u0026rdquo;, then select the certificate you just created in the drop-down list. Scroll the rest of the way down and click \u0026ldquo;Create Distribution\u0026rdquo;.\nChange Distribution CNAME ","date":1488412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488412800,"objectID":"2366dc628bf609e0193702a5b12908f5","permalink":"https://wubigo.com/post/2017-03-02-s3withcustomdomainnameviahttps/","publishdate":"2017-03-02T00:00:00Z","relpermalink":"/post/2017-03-02-s3withcustomdomainnameviahttps/","section":"post","summary":"Getting an SSL Certificate and CloudFront Create CloudFront Distribution Navigate to CloudFront in your AWS console and click \u0026ldquo;Create Distribution\u0026rdquo;. Click \u0026ldquo;Get Started\u0026rdquo; under the Web option (not the RTMP). You\u0026rsquo;ll arrive on the Create Distribution page. Here you need to change three things: 1. Click inside the input field for \u0026ldquo;Origin Domain Name\u0026rdquo;. A list of your Amazon S3 buckets should pop up. Select the S3 bucket you want to use.","tags":null,"title":"S3 access with custom domain name via https","type":"post"},{"authors":null,"categories":[],"content":" 介绍 Macvlan支持从一个上层物理接口创建子接口，每个子接口有自己独立的MAC和IP地址。 应用程序，容器或虚机可以绑定到子接口，用子接口的IP和物理网络直接通信。\n 好处\n 现有的很多网络监控设备还不支持虚拟网络设备的监控，Macvlan支持 不需要新建iptable，nat，route单独管理容器网络  不足\n 交换机的每个端口上能连接的不同MAC有策略上限 网卡上过多的MAC会影响性能 Macvlan只支持LINUX   准备  需要4.0以上的内核\nuname -r 4.15.0-45-generic  加载macvlan模块\nsudo modprobe macvlan lsmod | grep macvlan ... macvlan 24576 0 ...  配置网卡为混杂模式\n     主机 IP     PC 192.168.1.5/24   VM1 192.168.1.10/24   Container1 192.168.1.128/25    MACVLAN四种工作模式  Macvlan VEPA Macvlan Bridge Macvlan Passthru  创建macvlan ip addr show enp0s3 enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000 link/ether 08:00:27:c0:91:4c brd ff:ff:ff:ff:ff:ff inet 192.168.1.10/24 ip route default via 192.168.1.1 dev enp0s3 onlink  docker network create -d macvlan \\ --subnet=192.168.1.0/24 \\ --ip-range=192.168.1.128/25 \\ --gateway=192.168.1.1 \\ -o parent=enp0s3 macLan  docker run --rm -itd --network macLan --name web nginx:1.14-alpine  macvlan限制  despite having the ability to communicate with other guests and other external hosts on the network, the guest cannot communicate with its own host.\nThis situation is actually not an error — it is the defined behavior of macvtap. Due to the way in which the host\u0026rsquo;s physical Ethernet is attached to the macvtap bridge, traffic into that bridge from the guests that is forwarded to the physical interface cannot be bounced back up to the host\u0026rsquo;s IP stack. Additionally, traffic from the host\u0026rsquo;s IP stack that is sent to the physical interface cannot be bounced back up to the macvtap bridge for forwarding to the guests.\n 变通方法：\n 预留ip \u0026ndash;aux-address=\u0026ldquo;macvlan_bridge=192.168.1.199\u0026rdquo;\ndocker network create -d macvlan \\ --subnet=192.168.1.0/24 \\ --ip-range=192.168.1.128/25 \\ --gateway=192.168.1.1 \\ -o parent=enp0s3 macLan \\ --aux-address=\u0026quot;macvlan_bridge=192.168.1.199\u0026quot;  创建一个新的macvlan\nsudo ip link add macvlan_bridge link enp0s3 type macvlan mode bridge sudo ip addr add 192.168.1.199/32 dev macvlan_bridge sudo ip link set macvlan_bridge up  指示宿主通过macvlan_bridge访问部署在宿主上的容器网络\nip route add 192.168.1.128/27 dev macvlan_bridge   ","date":1488336572,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488336572,"objectID":"62f31bcf9c334866d594a59d86fd714a","permalink":"https://wubigo.com/post/docker_network_macvlan/","publishdate":"2017-03-01T10:49:32+08:00","relpermalink":"/post/docker_network_macvlan/","section":"post","summary":"介绍 Macvlan支持从一个上层物理接口创建子接口，每个子接口有自己独立的MAC和IP地址。 应用程序，容器或虚机可以绑定到子接口，用子接口的IP和物理网络直接通信。\n 好处\n 现有的很多网络监控设备还不支持虚拟网络设备的监控，Macvlan支持 不需要新建iptable，nat，route单独管理容器网络  不足\n 交换机的每个端口上能连接的不同MAC有策略上限 网卡上过多的MAC会影响性能 Macvlan只支持LINUX   准备  需要4.0以上的内核\nuname -r 4.15.0-45-generic  加载macvlan模块\nsudo modprobe macvlan lsmod | grep macvlan ... macvlan 24576 0 ...  配置网卡为混杂模式\n     主机 IP     PC 192.168.1.5/24   VM1 192.168.1.10/24   Container1 192.168.1.128/25    MACVLAN四种工作模式  Macvlan VEPA Macvlan Bridge Macvlan Passthru  创建macvlan ip addr show enp0s3 enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000 link/ether 08:00:27:c0:91:4c brd ff:ff:ff:ff:ff:ff inet 192.","tags":["CRI","DOCKER","NETWORK"],"title":"Docker网络macvlan","type":"post"},{"authors":null,"categories":null,"content":" How to allow unsafe ports in Chrome http://douglastarr.com/how-to-allow-unsafe-ports-in-chrome\nconfig $ cat _config.yml port: 6000  build for production $rm site.tar (remove before build otherwise tar-self is repackaged too) $JEKYLL_ENV=production bundle exec jekyll build $tar zcvf site.tar _site/*  replace gem source mirror gem sources --remove https://rubygems.org/ gem sources -a http://ruby.taobao.org/ gem sources -l http://ruby.taobao.org/  ","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"17dbab90328213a2dc7d8d2b1255dd35","permalink":"https://wubigo.com/post/2017-03-01-jekyll/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/post/2017-03-01-jekyll/","section":"post","summary":" How to allow unsafe ports in Chrome http://douglastarr.com/how-to-allow-unsafe-ports-in-chrome\nconfig $ cat _config.yml port: 6000  build for production $rm site.tar (remove before build otherwise tar-self is repackaged too) $JEKYLL_ENV=production bundle exec jekyll build $tar zcvf site.tar _site/*  replace gem source mirror gem sources --remove https://rubygems.org/ gem sources -a http://ruby.taobao.org/ gem sources -l http://ruby.taobao.org/  ","tags":null,"title":"jekyll notes","type":"post"},{"authors":null,"categories":[],"content":"  We can no longer assign a public IP address to your instance\nThe auto-assign public IP address feature for this instance is disabled because you specified multiple network interfaces. Public IPs can only be assigned to instances with one network interface. To re-enable the auto-assign public IP address feature, please specify only the eth0 network interface.\n ip MAC=`curl http://169.254.169.254/latest/meta-data/mac` curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/${MAC}/local-ipv4s  配置第二块网卡 ip a | grep ^[[:digit:]] tee -a /etc/network/interfaces.d/50-cloud-init.cfg \u0026lt;\u0026lt;-'EOF' auto eth1 iface eth1 inet dhcp EOF tee -a /etc/dhcp/dhclient-enter-hooks.d/restrict-default-gw \u0026lt;\u0026lt;-'EOF' case ${interface} in eth0) ;; *) unset new_routers ;; esac EOF systemctl restart networking  访问互联网 必须分配并关联EIP\n","date":1488270409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488270409,"objectID":"b29ae9ce5872c2d296311d8199959881","permalink":"https://wubigo.com/post/aws-ec2-secondary-network-interface/","publishdate":"2017-02-28T16:26:49+08:00","relpermalink":"/post/aws-ec2-secondary-network-interface/","section":"post","summary":"We can no longer assign a public IP address to your instance\nThe auto-assign public IP address feature for this instance is disabled because you specified multiple network interfaces. Public IPs can only be assigned to instances with one network interface. To re-enable the auto-assign public IP address feature, please specify only the eth0 network interface.\n ip MAC=`curl http://169.254.169.254/latest/meta-data/mac` curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/${MAC}/local-ipv4s  配置第二块网卡 ip a | grep ^[[:digit:]] tee -a /etc/network/interfaces.","tags":["AWS","SDN"],"title":"Aws EC2 多网卡配置","type":"post"},{"authors":null,"categories":[],"content":"","date":1488266530,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488266530,"objectID":"5a24b2063e7f22ac0652b42c9695b2bf","permalink":"https://wubigo.com/post/effective-coding-python/","publishdate":"2017-02-28T15:22:10+08:00","relpermalink":"/post/effective-coding-python/","section":"post","summary":"","tags":[],"title":"Effective Coding Python","type":"post"},{"authors":null,"categories":[],"content":" Namecheap不支持根域(apex doamin/naked domain/bare domain)的ALIAS记录\n在代码仓库的根目录创建CNAME文件 CNME文件包括根域域名\ntee CNAME \u0026lt;\u0026lt; EOF wubigo.com EOF  在DNS控制台创建域名记录 一条指向根域的A记录，一条指向子域的CNAME记录 AN A Record for @(apex doamin) and a cname record(Alias) for github 不要在github页使用通配符 dns 记录! 否则没有指定的子域将有被别人使用的风险\n确认域名配置成功 dig www.wubigo.com +nostats +nocomments +nocmd ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.3-P4-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.wubigo.com +nostats +nocomments +nocmd ;; global options: +cmd ;www.wubigo.com.\tIN\tA www.wubigo.com.\t1\tIN\tCNAME\twubigo.github.io. wubigo.github.io.\t3462\tIN\tA\t185.199.111.153 wubigo.github.io.\t3462\tIN\tA\t185.199.110.153 wubigo.github.io.\t3462\tIN\tA\t185.199.109.153 wubigo.github.io.\t3462\tIN\tA\t185.199.108.153  ","date":1487998304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487998304,"objectID":"d5e6285fd00911e2570af33a609fbf1e","permalink":"https://wubigo.com/post/dns-config-for-github-pages/","publishdate":"2017-02-25T12:51:44+08:00","relpermalink":"/post/dns-config-for-github-pages/","section":"post","summary":"Namecheap不支持根域(apex doamin/naked domain/bare domain)的ALIAS记录\n在代码仓库的根目录创建CNAME文件 CNME文件包括根域域名\ntee CNAME \u0026lt;\u0026lt; EOF wubigo.com EOF  在DNS控制台创建域名记录 一条指向根域的A记录，一条指向子域的CNAME记录 AN A Record for @(apex doamin) and a cname record(Alias) for github 不要在github页使用通配符 dns 记录! 否则没有指定的子域将有被别人使用的风险\n确认域名配置成功 dig www.wubigo.com +nostats +nocomments +nocmd ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.3-P4-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.wubigo.com +nostats +nocomments +nocmd ;; global options: +cmd ;www.wubigo.com.\tIN\tA www.wubigo.com.\t1\tIN\tCNAME\twubigo.github.io. wubigo.github.io.\t3462\tIN\tA\t185.199.111.153 wubigo.github.io.\t3462\tIN\tA\t185.199.110.153 wubigo.github.io.\t3462\tIN\tA\t185.","tags":["DNS","GITHUB"],"title":"DNS配置Github Pages","type":"post"},{"authors":null,"categories":["IT"],"content":" setup prometheus  prepare pv for prometheus  https://wubigo.com/post/2018-01-11-kubectlcheatsheet/#pvc\u0026ndash;using-local-pv\n install\nhelm install --name prometheus1 stable/prometheus --set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false   ","date":1487852920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487852920,"objectID":"c720be75ff6e1de6ced199d874f29384","permalink":"https://wubigo.com/post/k8s-monitor/","publishdate":"2017-02-23T20:28:40+08:00","relpermalink":"/post/k8s-monitor/","section":"post","summary":" setup prometheus  prepare pv for prometheus  https://wubigo.com/post/2018-01-11-kubectlcheatsheet/#pvc\u0026ndash;using-local-pv\n install\nhelm install --name prometheus1 stable/prometheus --set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false   ","tags":["K8S","MONITOR"],"title":"K8S Monitor","type":"post"},{"authors":null,"categories":["IT"],"content":"/etc/default/locale\nupdate-locale LANG=zh_CN.UTF-8 # File generated by update-locale LANG=\u0026quot;en_US.UTF-8\u0026quot; LC_NUMERIC=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TIME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MONETARY=\u0026quot;zh_CN.UTF-8\u0026quot; LC_PAPER=\u0026quot;zh_CN.UTF-8\u0026quot; LC_NAME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_ADDRESS=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TELEPHONE=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MEASUREMENT=\u0026quot;zh_CN.UTF-8\u0026quot; LC_IDENTIFICATION=\u0026quot;zh_CN.UTF-8\u0026quot; LANGUAGE=\u0026quot;zh_CN:en_US:en\u0026quot;  ","date":1487805691,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487805691,"objectID":"f7dd75f40aee72579bcfb6730c1fd8e9","permalink":"https://wubigo.com/post/ubungu-chinese/","publishdate":"2017-02-23T07:21:31+08:00","relpermalink":"/post/ubungu-chinese/","section":"post","summary":"/etc/default/locale\nupdate-locale LANG=zh_CN.UTF-8 # File generated by update-locale LANG=\u0026quot;en_US.UTF-8\u0026quot; LC_NUMERIC=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TIME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MONETARY=\u0026quot;zh_CN.UTF-8\u0026quot; LC_PAPER=\u0026quot;zh_CN.UTF-8\u0026quot; LC_NAME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_ADDRESS=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TELEPHONE=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MEASUREMENT=\u0026quot;zh_CN.UTF-8\u0026quot; LC_IDENTIFICATION=\u0026quot;zh_CN.UTF-8\u0026quot; LANGUAGE=\u0026quot;zh_CN:en_US:en\u0026quot;  ","tags":["LINUX"],"title":"Ubungu Chinese locale","type":"post"},{"authors":null,"categories":[],"content":"  Everything a containerized application writes to stdout and stderr is handled and redirected somewhere by a container engine. For example, the Docker container engine redirects those two streams to a logging driver\n The docker logs command is not available for drivers other than json-file and journald.\ndocker-compose日志 docker-compose -f docker-compose-0.7.1.yml logs -f  logging driver To configure the Docker daemon to default to a specific logging driver, set the value of log-driver to the name of the logging driver in the daemon.json file\nThe default logging driver is json-file. Thus, the default output for commands such as docker inspect  is JSON\n{ \u0026quot;log-driver\u0026quot;: \u0026quot;json-file\u0026quot;, \u0026quot;log-opts\u0026quot;: { \u0026quot;max-size\u0026quot;: \u0026quot;10m\u0026quot;, \u0026quot;max-file\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;labels\u0026quot;: \u0026quot;bigo.logg\u0026quot;, \u0026quot;env\u0026quot;: \u0026quot;os\u0026quot; } }   check the current driver\ndocker info --format '{{.LoggingDriver}}'   json-file the default location\n/var/lib/docker/containers/\u0026lt;INSTANCE_ID\u0026gt;/\u0026lt;INSTANCE_ID\u0026gt;-json.log\nor check log location via\ndocker inspect --format='{{.LogPath}}' $INSTANCE_ID  Supported logging drivers    Driver Description     none No logs are available for the container and docker logs does not return any output.   json-file The logs are formatted as JSON. The default logging driver for Docker.   local Writes logs messages to local filesystem in binary files using Protobuf.   syslog Writes logging messages to the syslog facility. The syslog daemon must be running on the host machine.   journald Writes log messages to journald. The journald daemon must be running on the host machine.   gelf Writes log messages to a Graylog Extended Log Format (GELF) endpoint such as Graylog or Logstash.   fluentd Writes log messages to fluentd (forward input). The fluentd daemon must be running on the host machine.   awslogs Writes log messages to Amazon CloudWatch Logs.    覆盖ENTRYPOINT  方法1\ndocker run -it --rm --entrypoint /bin/sh curl:v1  方法2 通过Dockerfile从镜像再建一个镜像，并重定义ENTRYPOINT\n  view real time logging of Docker containers docker logs -f  ","date":1487291065,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487291065,"objectID":"0cb058a8ab8dee7bbcf361b377ca4d6a","permalink":"https://wubigo.com/post/docker-logging/","publishdate":"2017-02-17T08:24:25+08:00","relpermalink":"/post/docker-logging/","section":"post","summary":"Everything a containerized application writes to stdout and stderr is handled and redirected somewhere by a container engine. For example, the Docker container engine redirects those two streams to a logging driver\n The docker logs command is not available for drivers other than json-file and journald.\ndocker-compose日志 docker-compose -f docker-compose-0.7.1.yml logs -f  logging driver To configure the Docker daemon to default to a specific logging driver, set the value of log-driver to the name of the logging driver in the daemon.","tags":["DOCKER"],"title":"Docker日志","type":"post"},{"authors":null,"categories":[],"content":" 虚拟包版本 Untagged revisions can be referred to using a \u0026ldquo;pseudo-version\u0026rdquo; like v0.0.0-yyyymmddhhmmss-abcdefabcdef, where the time is the commit time in UTC and the final suffix is the prefix of the commit has\ngo get github.com/vladimirvivien/go4vl@40b41ba go get: upgraded github.com/vladimirvivien/go4vl v0.0.1 =\u0026gt; v0.0.2-0.20211216162907-40b41ba86c5c  类型转换操作  For every type T, there is a corresponding conversion operation T(x) that converts the value x to \u0026gt;type T. A conversion from one type to another is allowed if both have the same underlying type, or \u0026gt;if both are unnamed pointer types that point to variables of the same underlying type; these \u0026gt;conversions change the type but not the representation of the value. If x is assignable to T, a \u0026gt;conversion is permitted but is usually redundant.\n blank identifier To ignore one of the values, assign it to the blank identifier :\nlinks, _ := findLinks(url) // errors ignored for _, url := range os.Args[1:] {  import ImportDeclaration = \u0026quot;import\u0026quot; ImportSpec ImportSpec = [ \u0026quot;.\u0026quot; | \u0026quot;_\u0026quot; | Identifier ] ImportPath  According to language spec it depends on the implementation how\nimport path (string) is interpreted but in real life it’s path\nrelative package’s vendor directory or go env GOPATH/src\n 定制的包名\n# github.com/mlowicki/main.go package main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/mlowicki/b\u0026quot; ) func main() { fmt.Println(c.B) } # github.com/mlowicki/b/b.go package c var B = \u0026quot;b\u0026quot;  初始化\nimport _ \u0026quot;math\u0026quot;   doesn’t require to use package math in importing file but init function(s) from imported package will be executed anyway (package and it dependencies will be initialized). It’s useful if we’re interested only in bootstrapping work done by imported package but we don’t reference any exported identifiers from it.\nHow to check if a map contains a key func TestMap(t *testing.T){ attended := map[string]string{ \u0026quot;Ann\u0026quot;: \u0026quot;t\u0026quot;, \u0026quot;Joe\u0026quot;: \u0026quot;r\u0026quot;, } val, ok := attended[\u0026quot;mm\u0026quot;] fmt.Print(val, ok) }  if statements in Go can include both a condition and an initialization statement. The example above uses both:\n initializes two variables - val will receive either the value of \u0026ldquo;foo\u0026rdquo; from the map or a \u0026ldquo;zero value\u0026rdquo; (in this case the empty string) and ok will receive a bool that will be set to true if \u0026ldquo;foo\u0026rdquo; was actually present in the map\n evaluates ok, which will be true if \u0026ldquo;foo\u0026rdquo; was in the map\n  Constructors two accepted best practices:\n Make the zero value of your struct a sensible default. (While this looks strange to most people coming from \u0026ldquo;traditional\u0026rdquo; oop it often works and is really convenient). Provide a function func New() YourTyp or if you have several such types in your package functions func NewYourType1() YourType1 and so on.  Document if a zero value of your type is usable or not (in which case it has to be set up by one of the New\u0026hellip; functions\nempty interface (interface{} is an empty interface) An interface is two things: it is a set of methods, but it is also a type  The interface{} type, the empty interface is the interface that has no methods.\nSince there is no implements keyword, all types implement at least zero methods, and satisfying an interface is done automatically,all types satisfy the empty interface.\nThat means that if you write a function that takes an interface{} value as a parameter, you can supply that function with any value.\nMethod\u0026rsquo;s receiver  go don\u0026rsquo;t user special name like( this or self ) for the receiver\nfunc (p Point) MoveBy(factor float){ p.X += factor //-\u0026gt; this.X += factor }  Pointer receiver\n  project-layout Internal app/pkg Directory Clarification Using /internal/pkg is about consistency if you use the /pkg pattern. The public shared code goes in \u0026lsquo;/pkg\u0026rsquo; and the private shared code goes in /internal/pkg\nhttps://github.com/golang-standards/project-layout/issues/9\nvendor package go help gopath find . -name grpc | xargs rm -rf /go/pkg/mod/google.golang.org$ ls genproto@v0.0.0-20180817151627-c66870c02cf8 genproto@v0.0.0-20190127191240-7cdc0b958d75 grpc@v1.19.0 grpc@v1.19.1  Vendor directories do not affect the placement of new repositories being checked out for the first time by \u0026lsquo;go get\u0026rsquo;: those are always placed in the main GOPATH, never in a vendor subtree\nremove installed package go clean -i -n google.golang.org/grpc... cd /home/bigo/go/src/google.golang.org/grpc rm -f grpc.test grpc.test.exe rm -f /home/bigo/go/pkg/linux_amd64/google.golang.org/grpc.a cd /home/bigo/go/src/google.golang.org/grpc/balancer rm -f balancer.test balancer.test.exe rm -f /home/bigo/go/pkg/linux_amd64/google.golang.org/grpc/balancer.a cd /home/bigo/go/src/google.golang.org/grpc/balancer/base rm -f base.test base.test.exe rm -f /home/bigo/go/pkg/linux_amd64/google.golang.org/grpc/balancer/base.a cd /home/bigo/go/src/google.golang.org/grpc/balancer/grpclb rm -f grpclb.test grpclb.test.exe  identifier export In Go, a simple rule governs which identifiers are exported and which are not: exported identifiers start with an upper-case letter\nPackage-level names like the types and constants declared in one file of a package are visible to all the other files of the package, as if the source code were all in a single file\nCSP “Another lineage among Go’s ancestors, and one that makes Go distinctive among recent programming languages, is a sequence of little-known research languages developed at Bell Labs, all inspired by the concept of communicating sequential processes (CSP) from Tony Hoare’s seminal 1978 paper on the foundations of concurrency. In CSP, a program is a parallel composition of processes that have no shared state; the processes communicate and synchronize using channels.”\nhindsight “As a recent high-level language, Go has the benefit of hindsight, and the basics are done well: it has garbage collection, a package system, first-class functions, lexical scope, a system call interface, and immutable strings in which text is generally encoded in UTF-8. But it has comparatively few features and is unlikely to add more. For instance, it has no implicit numeric conversions, no constructors or destructors, no operator overloading, no default parameter values, no inheritance, no generics, no exceptions, no macros, no function annotations, and no thread-local storage”\ngofmt Go does not require semicolons at the ends of statements or declarations, except where two or more appear on the same line. In effect, newlines following certain tokens are converted into semicolons, so where newlines are placed matters to proper parsing of Go code. For instance, the opening brace { of the function must be on the same line as the end of the func declaration, not on a line by itself, and in the expression x + y, a newline is permitted after but not before the + operator.\npoint var  “The variable to which p points is written *p. The expression *p yields the value of that variable, an int, but since *p denotes a variable, it may also appear on the left-hand side of an assignment, in which case the assignment updates the variable.” “Expressions that denote variables are the only expressions to which the address-of operator \u0026amp; may be applied.” “Each time we take the address of a variable or copy a pointer, we create new aliases or ways to identify the same variable. For example, *p is an alias for v.”  Function Values have types declare a var with function value as type * func(int) int -\u0026gt; type\nvar f func(int) int  nil pointer dereference package main import \u0026quot;fmt\u0026quot; var f func(int) int func main() { //fmt.Println(f(2)) fmt.Println(f(2)) f = func(i int) int { if i == 0 { return 1 } return i * f(i-1) } fmt.Println(f(2)) } panic: runtime error: invalid memory address or nil pointer dereference[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x4850f0]goroutine 1 [running]:main.main()\t/home/bigo/go/src/github.com/gopl.io/ch5/t2/main.go:9 +0x30exit status 2Process exiting with code: 1  Capturing Iteration Variables “The problem of iteration variable capture is most often encountered when using the go statement or with defer since both may delay the execution of a function value until after the loop has finished. But the problem is not inherent to go or defer.”\nvar s1 [3]int = [3]int{1, 2, 3} var fs []func() func main() { for _, v := range s1 { //fmt.Printf(\u0026quot;%d\\n\u0026quot;, v) fs = append(fs, func() { fmt.Printf(\u0026quot;%d\\n\u0026quot;, v) //v is caputed and shared }) } for _, f := range fs { f() } fs = nil for _, v := range s1 { i := v fs = append(fs, func() { fmt.Printf(\u0026quot;%d\\n\u0026quot;, i) }) } for _, f := range fs { f() } }  The reason is a consequence of the scope rules for loop variables. In the program immediately above, the for loop introduces a new lexical block in which the variable dir is declared. All function values created by this loop “capture” and share the same variable—an addressable storage location, not its value at that particular moment. The value of dir is updated in successive iterations, so by the time the cleanup functions are called, the dir variable has been updated several times by the now-completed for loop.\nexpression in go/defer must be function call  express anonymous function as function call()\nvar s1 [3]string = [3]string{\u0026quot;go\u0026quot;, \u0026quot;func\u0026quot;, \u0026quot;value\u0026quot;} func f(s string) { fmt.Println(s) } func main() { fmt.Println(\u0026quot;s escapsed\u0026quot;) for _, s := range s1 { defer func() { fmt.Println(s) }() } fmt.Println(\u0026quot;work expected\u0026quot;) for _, s := range s1 { defer func(s string) { fmt.Println(s) }(s) } for _, s := range s1 { defer f(s) } for _, s := range s1 { go func(s string) { fmt.Println(\u0026quot;h %s\u0026quot;, s) }(s) } for _, s := range s1 { go f(s) } }   install go-1.10 ***vscode will report go-runtime read-only error which has been installed via snap\nsudo add-apt-repository ppa:gophers/archive sudo apt-get update sudo apt-get install golang-1.10-go export GOROOT=\u0026quot;/usr/lib/go-1.10\u0026quot; export GOPATH=$HOME/go export PATH=\u0026quot;$PATH:$GOPATH/bin:$GOROOT/bin/\u0026quot; go version  GOPATH directory cd $GOPATH ls src bin pkg     DIR      pkg 存放编译过的包   bin 存放可执行程序(package main)    import path两个功能 go help importpath   在本地文件系统$GOPATH/pkg/$GOOS_$GOARCH/查找\nimport \u0026quot;github.com/google/go-github/github\u0026quot; ... go/src/wubigo.com/cloud/github_client/main.go:7:2: cannot find package \u0026quot;github.com/google/go-github/github\u0026quot; in any of: /usr/lib/go-1.11/src/github.com/google/go-github/github (from $GOROOT) /home/bigo/go/src/github.com/google/go-github/github (from $GOPATH) Process exiting with code: 1 ...  如果没找到，需要go get通过安装\ngo get github.com/google/go-github/github  import path custom domain name\ncurl golang.org/x/tools/cmd/rename \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026quot;Content-Type\u0026quot; content=\u0026quot;text/html; charset=utf-8\u0026quot;/\u0026gt; \u0026lt;meta name=\u0026quot;go-import\u0026quot; content=\u0026quot;golang.org/x/tools git https://go.googlesource.com/tools\u0026quot;\u0026gt; \u0026lt;meta name=\u0026quot;go-source\u0026quot; content=\u0026quot;golang.org/x/tools https://github.com/golang/tools/ https://github.com/golang/tools/tree/master{/dir} https://github.com/golang/tools/blob/master{/dir}/{file}#L{line}\u0026quot;\u0026gt; \u0026lt;meta http-equiv=\u0026quot;refresh\u0026quot; content=\u0026quot;0; url=https://godoc.org/golang.org/x/tools/cmd/rename\u0026quot;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Nothing to see here; \u0026lt;a href=\u0026quot;https://godoc.org/golang.org/x/tools/cmd/rename\u0026quot;\u0026gt;move along\u0026lt;/a\u0026gt;. \u0026lt;/body\u0026gt;    Canonical import paths\n The syntax is simple: put an identifying comment on the package line\nhttps://github.com/golang/tools/blob/master/cmd/gorename/main.go\npackage main // import \u0026quot;golang.org/x/tools/cmd/gorename\u0026quot;  struct tags  List of well-known struct tags     Tag Documentation     xml https://godoc.org/encoding/xml   json https://godoc.org/encoding/json   asn1 https://godoc.org/encoding/asn1   reform https://godoc.org/gopkg.in/reform.v1   bigquery https://godoc.org/cloud.google.com/go/bigquery   datastore https://godoc.org/cloud.google.com/go/datastore   spanner https://godoc.org/cloud.google.com/go/spanner   bson https://godoc.org/labix.org/v2/mgo/bson   gorm https://godoc.org/github.com/jinzhu/gorm   yaml https://godoc.org/gopkg.in/yaml.v2    field tags A field tag is a string of metadata associated at compile time with the field of a struct:\nYear int `json:\u0026quot;released\u0026quot;` Color bool `json:\u0026quot;color,omitempty\u0026quot;`  A field tag may be any literal string , but it is conventionally interpreted as a space-separated list of key:\u0026ldquo;value\u0026rdquo; pairs; since they contain double quotation marks, field tags are usually written with raw string literals. The json key control s the behavior of the encoding/json package, and other encoding/\u0026hellip; packages follow this convention. The first part of the json field tag specifies an alternative JSON name for the Go field\n","date":1486815889,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486815889,"objectID":"0c7407e9740d27a2603d0206c57a357f","permalink":"https://wubigo.com/post/lang-go-notes/","publishdate":"2017-02-11T20:24:49+08:00","relpermalink":"/post/lang-go-notes/","section":"post","summary":"虚拟包版本 Untagged revisions can be referred to using a \u0026ldquo;pseudo-version\u0026rdquo; like v0.0.0-yyyymmddhhmmss-abcdefabcdef, where the time is the commit time in UTC and the final suffix is the prefix of the commit has\ngo get github.com/vladimirvivien/go4vl@40b41ba go get: upgraded github.com/vladimirvivien/go4vl v0.0.1 =\u0026gt; v0.0.2-0.20211216162907-40b41ba86c5c  类型转换操作  For every type T, there is a corresponding conversion operation T(x) that converts the value x to \u0026gt;type T. A conversion from one type to another is allowed if both have the same underlying type, or \u0026gt;if both are unnamed pointer types that point to variables of the same underlying type; these \u0026gt;conversions change the type but not the representation of the value.","tags":["LANG","GO"],"title":"GO NOTES","type":"post"},{"authors":null,"categories":[],"content":" 准备  搭建测试环境\n可以参考从源代码构件K8S开发环境\n  ","date":1485396540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485396540,"objectID":"242bbaa1bf1a06a806159c8d68e30383","permalink":"https://wubigo.com/post/k8s_cni_l2_network_on_bare_metal/","publishdate":"2017-01-26T10:09:00+08:00","relpermalink":"/post/k8s_cni_l2_network_on_bare_metal/","section":"post","summary":" 准备  搭建测试环境\n可以参考从源代码构件K8S开发环境\n  ","tags":["K8S","CNI","NETWORK"],"title":"K8SCNI之L2 网络实现","type":"post"},{"authors":null,"categories":[],"content":" COOKIE \u0026amp; HTTP SESSION H5 addition that adds a key/value store to browsers and cookies\nstateful session Some examples of scaling stateful sessions:\nOnce you run multiple backend processes on a server: A Redis daemon (on that server) for session storage. Once you run on multiple servers: A dedicated server running Redis just for session storage. Once you run on multiple servers, in multiple clusters: Sticky sessions.  JWT session  Stateless JWT: A JWT token that contains the session data, encoded directly into the token. Stateful JWT: A JWT token that contains just a reference or ID for the session. The session data is stored server-side. Session token/cookie: A standard (optionally signed) session ID, like web frameworks have been using for a long time. The session data is stored server-side.  Stop using JWT for sessions http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/\n","date":1483584360,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483584360,"objectID":"a05f4c9e433afc07c2f3cb8ddd3f162c","permalink":"https://wubigo.com/post/http-session-management/","publishdate":"2017-01-05T10:46:00+08:00","relpermalink":"/post/http-session-management/","section":"post","summary":"COOKIE \u0026amp; HTTP SESSION H5 addition that adds a key/value store to browsers and cookies\nstateful session Some examples of scaling stateful sessions:\nOnce you run multiple backend processes on a server: A Redis daemon (on that server) for session storage. Once you run on multiple servers: A dedicated server running Redis just for session storage. Once you run on multiple servers, in multiple clusters: Sticky sessions.  JWT session  Stateless JWT: A JWT token that contains the session data, encoded directly into the token.","tags":["HTTP"],"title":"HTTP Session Management","type":"post"},{"authors":null,"categories":null,"content":" Early neural networks Although the core ideas of neural networks were investigated in toy forms as early as the 1950s, the approach took decades to really get started. For a long time, the missing piece was a lack of an efficient way to train large neural networks. This changed in the mid-1980s, as multiple people independently rediscovered the \u0026ldquo;backpropagation\u0026rdquo; algorithm, a way to train chains of parametric operations using gradient descent optimization (later in the book, we will go on to precisely define these concepts), and started applying it to neural networks. The first successful practical application of neural nets came in 1989 from Bell Labs, when Yann LeCun combined together the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of handwritten digits classification. The resulting network, dubbed \u0026ldquo;LeNet\u0026rdquo;, was used by the US Post Office in the 1990s to automate the reading of ZIP codes on mail envelopes\n逻辑回归(Logistic regression) Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes).\nIn logistic regression, the dependent variable is binary or dichotomous, i.e. it only contains data coded as 1 (TRUE, success, pregnant, etc.) or 0 (FALSE, failure, non-pregnant, etc.).\nThe goal of logistic regression is to find the best fitting (yet biologically reasonable) model to describe the relationship between the dichotomous characteristic of interest (dependent variable = response or outcome variable) and a set of independent (predictor or explanatory) variables. Logistic regression generates the coefficients (and its standard errors and significance levels) of a formula to predict a logit transformation of the probability of presence of the characteristic of interest\nhttps://www.medcalc.org/manual/logistic_regression.php\n神经网络 一个分类算法，其输出是样本属于某类别的概率值 P(y==k|x;Θ)\nactivation function is the sigmoid function self.activation_function = lambda x: scipy.special.expit(x) Instead of the usual def() definitions, we use the magic lambda to create a function there and then, quickly and easily. The function here takes x and returns scipy.special.expit(x) which is the sigmoid function. Functions created with lambda are nameless or anonymous, but here we’ve assigned it to the name self.activation_function(). All this means is that whenever someone needs to user the activation function, all they need to do is call self.activation_function()\n","date":1483401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483401600,"objectID":"4ba4deb87812a03bcc1368ba258e2527","permalink":"https://wubigo.com/post/2017-01-03-machinelearningbasic/","publishdate":"2017-01-03T00:00:00Z","relpermalink":"/post/2017-01-03-machinelearningbasic/","section":"post","summary":"Early neural networks Although the core ideas of neural networks were investigated in toy forms as early as the 1950s, the approach took decades to really get started. For a long time, the missing piece was a lack of an efficient way to train large neural networks. This changed in the mid-1980s, as multiple people independently rediscovered the \u0026ldquo;backpropagation\u0026rdquo; algorithm, a way to train chains of parametric operations using gradient descent optimization (later in the book, we will go on to precisely define these concepts), and started applying it to neural networks.","tags":null,"title":"machine learning basic","type":"post"},{"authors":null,"categories":null,"content":" along an axis Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1)\n\u0026gt;\u0026gt;\u0026gt;print np.arange(12) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=1) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=0)  ","date":1483401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483401600,"objectID":"79fe544dceb4c0e98497a333d4b12f8f","permalink":"https://wubigo.com/post/2017-02-03-numpynotes/","publishdate":"2017-01-03T00:00:00Z","relpermalink":"/post/2017-02-03-numpynotes/","section":"post","summary":" along an axis Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1)\n\u0026gt;\u0026gt;\u0026gt;print np.arange(12) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=1) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=0)  ","tags":null,"title":"numpy notes","type":"post"},{"authors":null,"categories":null,"content":" The State Of Front-End Frameworks []()\nLet\u0026rsquo;s Encrypt now holds 35% of the market https://nettrack.info/ssl_certificate_issuers.html\nCode together in real time with Teletype for Atom https://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html\nHow Firefox Got Fast Again https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster/\nDo you need a VPN https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"9a5b10ed10e57e19a16308435c006b94","permalink":"https://wubigo.com/post/2017-01-01-hacknewsfavorites2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post/2017-01-01-hacknewsfavorites2017/","section":"post","summary":"The State Of Front-End Frameworks []()\nLet\u0026rsquo;s Encrypt now holds 35% of the market https://nettrack.info/ssl_certificate_issuers.html\nCode together in real time with Teletype for Atom https://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html\nHow Firefox Got Fast Again https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster/\nDo you need a VPN https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn","tags":null,"title":"Hacknews favorites 2017","type":"post"},{"authors":null,"categories":null,"content":" discourse-setup discourse-setup script does, more-or-less, is just copy samples/standalone.yml to containers/app.yml and edit a bunch of stuff in response to the answers to a bunch of questions\n$cp samples/standalone.yml containers/app.yml  Discourse app.yml AWS setup example\nDiscourse app.yml AWS setup\nhow-to-specify-a-different-port-not-80-during-install\nfix these settings after bootstrapping, edit the /containers/app.yml then rebuild to take effect\n./launcher rebuild app  ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"3abac9b183c99b7bd38bc7e32bcbd0b5","permalink":"https://wubigo.com/post/2017-01-01-discoursenotes/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post/2017-01-01-discoursenotes/","section":"post","summary":" discourse-setup discourse-setup script does, more-or-less, is just copy samples/standalone.yml to containers/app.yml and edit a bunch of stuff in response to the answers to a bunch of questions\n$cp samples/standalone.yml containers/app.yml  Discourse app.yml AWS setup example\nDiscourse app.yml AWS setup\nhow-to-specify-a-different-port-not-80-during-install\nfix these settings after bootstrapping, edit the /containers/app.yml then rebuild to take effect\n./launcher rebuild app  ","tags":null,"title":"discourse notes","type":"post"},{"authors":null,"categories":null,"content":" Adding swap memory If your system has less than 1GB memory, you may run into errors. To overcome this, configure a larger amount of swap memory:\ndd if=/dev/zero of=/var/swap bs=1k count=1024k mkswap /var/swap swapon /var/swap echo '/var/swap swap swap default 0 0' \u0026gt;\u0026gt; /etc/fstab  default-setting /var/www/ghost/versions/1.18.2/core/server/data/schema/default-settings.json  blog mail configuration using Amazon SES Ghost is a blogging platform written in nodejs. Edit the config.js file at the ghost root directory\nmail: { from: 'from@email.com', transport: 'SMTP', options: { host: \u0026quot;your-amazon-host\u0026quot;, port: 465, service: \u0026quot;SES\u0026quot;, auth: { user: \u0026quot;your-amazon-user\u0026quot;, pass: \u0026quot;your-amazon-password\u0026quot; } } }  Amazon SES credential can be generated from amazon control panel. From address must be registered and verified as sender.\nfavicon.ico cp /home/ubuntu/favicon.ico /var/www/ghost/versions/1.18.2/core/server/public/  ","date":1483056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483056000,"objectID":"1e116d493d15f537536b7ff2446fe23c","permalink":"https://wubigo.com/post/2016-12-30-ghostnotes/","publishdate":"2016-12-30T00:00:00Z","relpermalink":"/post/2016-12-30-ghostnotes/","section":"post","summary":"Adding swap memory If your system has less than 1GB memory, you may run into errors. To overcome this, configure a larger amount of swap memory:\ndd if=/dev/zero of=/var/swap bs=1k count=1024k mkswap /var/swap swapon /var/swap echo '/var/swap swap swap default 0 0' \u0026gt;\u0026gt; /etc/fstab  default-setting /var/www/ghost/versions/1.18.2/core/server/data/schema/default-settings.json  blog mail configuration using Amazon SES Ghost is a blogging platform written in nodejs. Edit the config.js file at the ghost root directory","tags":null,"title":"ghost notes","type":"post"},{"authors":null,"categories":null,"content":" good practise There are several common things and These all take practice. 1. Organization skills/project management skills: ability to pull together internal/external resources, ability to create clear plans, ability to build relationships with other teams to make cross-team collaboration easier 2. Communication skills: ability to deescalate and mediate conflicts fairly, ability to give both positive and negative feedback regularly, ability to create psychological safety, ability to deliver bad news while maintaining psychological safety, ability to run efficient meetings, ability to make unpopular decisions while still commanding the team\u0026rsquo;s respect, ability to engage with different personality types and communication styles 3. Delegation skills: ability to translate ambiguous directives from above into actionable tasks for the team, ability to arrange tasks so that individuals have the creative freedom to figure out how to do it, predictability in decision-making, ability to shield team from external pressure, ability to give negative feedback immediately, ability to fire low performers before they fester into team morale and culture problems 4. Coaching skills: ability to give targeted advice, ability to match employee\u0026rsquo;s own aspirations and growth objectives to projects, enough technical expertise to point people in the right direction, ability to evaluate skills and hire for them\nDealing with ambiguity and developing resilience http://www.melanieallen.co.uk/articles/dealing-with-ambiguity/\nThe 2 Mental Shifts Highly Successful People Make https://medium.com/personal-growth/the-2-mental-shifts-highly-successful-people-make-7089450c2d7c\nwhy your programmers just want to code https://hackernoon.com/why-your-programmers-just-want-to-code-36da9973388e\nThree Powerful Conversations Managers Must Have To Develop Their People  Starting with kindergarten, tell me about your life\n Spot their lighthouse and bring it into focus “The idea is to try to get employees to start to talk to you about their dreams, or three to five of them if they don\u0026rsquo;t really want to commit to one idea. None of it should be time-bound — no 10-year plans. Ask what this person would be doing at the pinnacle of his or her career — when they’re feeling challenged, engaged and not wanting anything else.”\n Create a career action plan\n  leading snowflakes http://leadingsnowflakes.com/ Show How, Don\u0026rsquo;t Tell What - A Management Style\nProgressing from tech to leadership https://lcamtuf.blogspot.hk/2018/02/on-leadership.html\nShare your Manager README https://matthewnewkirk.com/2017/09/20/share-your-manager-readme/\nHow to Win Friends and Influence People https://www.amazon.com/How-Friends-Influence-People-Chinese\nBooks I read this year https://www.gatesnotes.com/About-Bill-Gates/Best-Books-2017\n","date":1483056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483056000,"objectID":"0f689655e2902582d054924032be3cbd","permalink":"https://wubigo.com/post/2016-12-30-softwareleadweekly/","publishdate":"2016-12-30T00:00:00Z","relpermalink":"/post/2016-12-30-softwareleadweekly/","section":"post","summary":"good practise There are several common things and These all take practice. 1. Organization skills/project management skills: ability to pull together internal/external resources, ability to create clear plans, ability to build relationships with other teams to make cross-team collaboration easier 2. Communication skills: ability to deescalate and mediate conflicts fairly, ability to give both positive and negative feedback regularly, ability to create psychological safety, ability to deliver bad news while maintaining psychological safety, ability to run efficient meetings, ability to make unpopular decisions while still commanding the team\u0026rsquo;s respect, ability to engage with different personality types and communication styles 3.","tags":null,"title":"software lead weekly","type":"post"},{"authors":null,"categories":[],"content":"RDMA (Remote Direct Memory Access), TOE (TCP Offload Engine), and OpenOnload. More recently, DPDK (Data Plane Development Kit) has been used in some applications to bypass the kernel, and then there are new emerging initiatives such as FD.io (Fast Data Input Output) based on VPP (Vector Packet Processing). More will likely emerge in the future.\nTechnologies like RDMA and TOE create a parallel stack in the kernel and solve the first problem (namely, the \u0026ldquo;kernel is too slow\u0026rdquo;) while OpenOnload, DPDK and FD.io (based on VPP) move networking into Linux user space to address both speed and technology plug-in requirements. When technologies are built in the Linux user space, the need for changes to the kernel is avoided, eliminating the extra effort required to convince the Linux kernel community about the usefulness of the bypass technologies and their adoption via upstreaming into the Linux kernel.\n","date":1482977759,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482977759,"objectID":"99566989824e91dfc183e1c6f9ea5012","permalink":"https://wubigo.com/post/kernel-bypass-networking/","publishdate":"2016-12-29T10:15:59+08:00","relpermalink":"/post/kernel-bypass-networking/","section":"post","summary":"RDMA (Remote Direct Memory Access), TOE (TCP Offload Engine), and OpenOnload. More recently, DPDK (Data Plane Development Kit) has been used in some applications to bypass the kernel, and then there are new emerging initiatives such as FD.io (Fast Data Input Output) based on VPP (Vector Packet Processing). More will likely emerge in the future.\nTechnologies like RDMA and TOE create a parallel stack in the kernel and solve the first problem (namely, the \u0026ldquo;kernel is too slow\u0026rdquo;) while OpenOnload, DPDK and FD.","tags":["SDN","NFV"],"title":"Kernel Bypass Networking","type":"post"},{"authors":null,"categories":[],"content":" QUERY aws dynamodb scan --table-name \u0026quot;orders\u0026quot;  ","date":1482217742,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482217742,"objectID":"5967be7e4296e3f9c0e9dc15a90bdedb","permalink":"https://wubigo.com/post/aws-dynamodb-notes/","publishdate":"2016-12-20T15:09:02+08:00","relpermalink":"/post/aws-dynamodb-notes/","section":"post","summary":" QUERY aws dynamodb scan --table-name \u0026quot;orders\u0026quot;  ","tags":["AWS","DB"],"title":"Aws Dynamodb Notes","type":"post"},{"authors":null,"categories":[],"content":" 本文所说的新手是指还在12个月免费期内的用户\n只要有一张信用卡，就可以注册一个AWS账号。\n对于AWS新注册账号，有12个月的免费使用额度\n具体额度如下：\n服务\n   服务 额度 当月使用统计     服务器 750小时 t2.micro    硬盘 30GB    硬盘快照 1GB    网盘 5G    数据库 25G    函数计算 1百万次调用     下图我这个月的使用额度\n计算和存储 从上面的图可以看出，AWS免费额度里面比较鸡肋的是\n网盘快照的额度太少，走常规的操作系统镜像备份是要\n花钱的，因为一个最小的ubuntu实例镜像就是8G，\n如何做到保存自己的最新工作成果，而又额外使用快照从而\n节省存储费用呢？\n解决办法如下：\n硬盘外挂  创建EC2\n 创建一块硬盘，小于20G即可，并把该硬盘外挂到EC2\n 在外挂的硬盘里面保存自己的操作数据\n 使用user_data初始化包括安装常用软件包，自动外挂硬盘等\n 使用完EC2销毁即可(卸载外挂硬盘千万不要销毁外挂的硬盘)\n  上面的操作可以使用基础设施配置工具(ansible, terraform, pupport等)\n进行自动化管理。\n下面以terraform为例介绍操作步骤：\n前提  安装AWS SDK\n 配置AWS访问权限\n  EC2  启动并外挂硬盘  使用容器镜像 AWS提供500M的镜像存储空间，可以把自己的数据保存到容器镜像\nEC2带宽费用 EIP双向收费，策略是使用S3+VPC中转\nIPv4: Data transferred “in” to and “out” from public or Elastic IPv4 address is charged at $0.01/GB in each direction. IPv6: Data transferred “in” to and “out” from an IPv6 address in a different VPC is charged at $0.01/GB in each direction.  搭建S3 web站点 web hosting\nSQS vs SNS vs kinesis https://medium.com/better-programming/moving-messages-in-aws-comparing-kinesis-sqs-and-sns-32cb5d2f89d5\n","date":1482186688,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482186688,"objectID":"8a13c51bd42e552b947be572782c4fbf","permalink":"https://wubigo.com/post/aws-new-guy-cheat-sheet/","publishdate":"2016-12-20T06:31:28+08:00","relpermalink":"/post/aws-new-guy-cheat-sheet/","section":"post","summary":"本文所说的新手是指还在12个月免费期内的用户\n只要有一张信用卡，就可以注册一个AWS账号。\n对于AWS新注册账号，有12个月的免费使用额度\n具体额度如下：\n服务\n   服务 额度 当月使用统计     服务器 750小时 t2.micro    硬盘 30GB    硬盘快照 1GB    网盘 5G    数据库 25G    函数计算 1百万次调用     下图我这个月的使用额度\n计算和存储 从上面的图可以看出，AWS免费额度里面比较鸡肋的是\n网盘快照的额度太少，走常规的操作系统镜像备份是要\n花钱的，因为一个最小的ubuntu实例镜像就是8G，\n如何做到保存自己的最新工作成果，而又额外使用快照从而\n节省存储费用呢？\n解决办法如下：\n硬盘外挂  创建EC2\n 创建一块硬盘，小于20G即可，并把该硬盘外挂到EC2\n 在外挂的硬盘里面保存自己的操作数据\n 使用user_data初始化包括安装常用软件包，自动外挂硬盘等\n 使用完EC2销毁即可(卸载外挂硬盘千万不要销毁外挂的硬盘)\n  上面的操作可以使用基础设施配置工具(ansible, terraform, pupport等)","tags":["AWS"],"title":"公有云羊毛党使用秘籍(新手篇)","type":"post"},{"authors":null,"categories":null,"content":" twitter social network dataset http://law.di.unimi.it/datasets.php\nA comparison of state-of-the-art graph processing systems https://code.facebook.com/posts/319004238457019/a-comparison-of-state-of-the-art-graph-processing-systems/\nLinkedin graph database https://engineering.linkedin.com/teams/data/projects/search-and-discovery https://engineering.linkedin.com/teams/data\nWhy I left Apache Spark GraphX and returned to HBase for my graph database https://lbartkowski.wordpress.com/2015/04/21/why-i-left-apache-spark-graphx-and-returned-to-hbase-for-my-graph-database/\n","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"59adc335c5a4340a46dc2c9bf3b60801","permalink":"https://wubigo.com/post/2016-11-01-graphprocessing/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/post/2016-11-01-graphprocessing/","section":"post","summary":"twitter social network dataset http://law.di.unimi.it/datasets.php\nA comparison of state-of-the-art graph processing systems https://code.facebook.com/posts/319004238457019/a-comparison-of-state-of-the-art-graph-processing-systems/\nLinkedin graph database https://engineering.linkedin.com/teams/data/projects/search-and-discovery https://engineering.linkedin.com/teams/data\nWhy I left Apache Spark GraphX and returned to HBase for my graph database https://lbartkowski.wordpress.com/2015/04/21/why-i-left-apache-spark-graphx-and-returned-to-hbase-for-my-graph-database/","tags":null,"title":"Graph Processing","type":"post"},{"authors":null,"categories":[],"content":" 1. Single Responsibility Principle Definition: The single responsibility principle is also known as the single-function principle, that is, there is no more than one reason for the class change. In layman’s terms, a class is only responsible for one responsibility. Principle: If a class has too many responsibilities, it is equivalent to coupling these responsibilities together. A change in responsibilities may weaken or continue the ability of this class to perform other duties. This coupling can lead to fragile designs, and when changes occur, the design suffers unexpected damage. And if you want to avoid this happening, you should follow the principle of single responsibility as much as possible. The core of this principle is decoupling and enhancing cohesion. The origin of the problem: T is responsible for two different duties: responsibility P1, responsibility P2. When the class T needs to be modified due to the change in the requirement of the responsibility P1, it may cause the P2 function that was originally functioning normally to malfunction. That is to say, duties P1 and P2 are coupled together. Cause: No programmer is unclear that high cohesion and low coupling procedures should be written, but many couplings often occur inadvertently because the responsibility spreads , for some reason, a duty is divided into granularity. A lot of responsibilities. Solution: Comply with different responsibilities and package different responsibilities into different classes or modules Advantage: You can reduce the complexity of a class. A class is only responsible for one responsibility, and its logic is definitely much simpler than being responsible for multiple duties; Improve the readability of the class and improve the maintainability of the system; The risk caused by the change is reduced, and the change is inevitable. If the single responsibility principle is followed well, when modifying a function, the impact on other functions can be significantly reduced.\n2. Liskov Substitution Principle LSP definition: One of the basic principles of object-oriented design where any base class can appear, subclasses must appear. LSP is the cornerstone of inheritance and reuse. Only when the derived class can replace the base class, the function of the software unit is not affected, the base class can be truly reused, and the derived class can also add new behavior based on the base class. . (ie, if the parent class is a part of a function module, the child class is used instead of the parent class and the function module can run normally) [The child class instance can replace the parent class instance] Inheritance contains such a meaning: all methods that have been implemented in the parent class (as opposed to abstract methods) are actually setting a series of specifications and contracts, although it does not mandate that all subclasses must follow these Contracts, but if subclasses arbitrarily modify these non-abstract methods, they will cause damage to the entire inheritance system. The principle of Lee’s replacement is to express this meaning. Inheritance, as one of the three characteristics of object-oriented, brings great convenience to programming, but also brings disadvantages. For example, using inheritance can bring intrusion to the program, the portability of the program is reduced, and the coupling between objects is increased. If a class is inherited by other classes, all the children must be considered when the class needs to be modified. Classes, and after the parent class is modified, all functions involving subclasses may fail. The actual situation: In actual programming, we often complete the new functions by rewriting the methods of the parent class, so that although the writing is simple, the reusability of the whole inheritance system will be poor, especially when the polymorphism is used more frequently. The chances of running an error are very high. If you want to override the method of the parent class, the more common practice is: the original parent class and subclass inherit a more common base class, the original inheritance relationship is removed, and the relationship of dependency, aggregation, combination, etc. is used instead. Popular meaning: Subclasses can extend the functionality of the parent class, but not the original function of the parent class. Subclasses can implement abstract methods of the parent class, but they cannot override the non-abstract methods of the parent class. Subclasses can add their own unique methods. When a subclass’s method overrides the parent’s method, the method’s precondition (that is, the method’s formal parameter) is more lenient than the parent class’s input parameter. When a subclass’s method implements the abstract method of the parent class, the postcondition of the method (that is, the return value of the method) is stricter than the parent class.\n3. Dependency Inversion Principle Definition: High-level modules should not rely on low-level modules, both of which should rely on their abstractions; abstractions should not rely on details; details should rely on abstractions. The difference: For process-oriented development, the upper layer calls the lower layer, and the upper layer depends on the lower layer. When the lower layer changes drastically, the upper layer also needs to change, which leads to the reduced reusability of the module and greatly increases the development cost. Object-oriented development solves this problem very well. In general, the probability of abstract change is small, and the user program depends on abstraction, and the implementation details depend on abstraction. Even if the implementation details are constantly changing, the client program does not need to change as long as the abstraction does not change. This greatly reduces the coupling between the client program and the implementation details. The origin of the problem: Class A directly depends on class B. If you want to change class A to dependent class C, you must do so by modifying the code of class A. In this scenario, class A is generally a high-level module responsible for complex business logic; class B and class C are low-level modules that are responsible for basic atomic operations; if class A is modified, it will introduce unnecessary risks to the program. Solution: Modify class A to rely on interface I. Class B and class C each implement interface I. Class A indirectly communicates with class B or class C through interface I, which greatly reduces the chance of modifying class A. The principle of dependency inversion is based on the fact that abstract things (interfaces or abstract classes) are much more stable than the variability of details. Architectures built on abstraction are much more stable than those built on details (specific implementation classes). There are three ways to pass dependencies, interface passing, constructor passing, and set methods passing Programming requirements: Low-level modules should have abstract classes or interfaces, or both. The declared type of a variable is as much as possible an abstract class or interface. Follow the principle of Richter replacement when using inheritance.\n4. Interface Isolation Principle ISP (ISP — Interface Segregation Principle) Definition: The client should not rely on interfaces it does not need; the dependency of one class on another should be based on the smallest interface. The origin of the problem: Class A depends on class B through interface I. Class C depends on class D through interface I. If interface I is not the smallest interface for class A and class B, then class B and class D must implement methods they do not need. Solution: Split the bloated interface I into separate interfaces, and class A and class C respectively establish dependencies with the interfaces they need. Interface isolation principle Therefore: Create a single interface, do not build a large and bloated interface, try to refine the interface, the method in the interface is as small as possible. That is, we need to create a dedicated interface for each class, rather than trying to build a very large interface for all classes that depend on it. 4 Note: The interface is as small as possible, but limited. Thinning the interface can increase the flexibility of programming. It is not a fact, but if it is too small, it will cause too many interfaces and complicate the design. So be sure to be moderate. Customizing a service for an interface-dependent class exposes only the methods it needs to the calling class, and the methods it doesn’t need are hidden. Only by focusing on providing a custom service for a module can a minimum dependency be established. Improve cohesion and reduce external interaction. Make the interface do the most things with the fewest methods.\n5. Law of Demeter Definition: The Law of Demeter is also known as the Least Knowledge Principle (LKP), which means that an object should have as little knowledge as possible about other objects and not speak to strangers. English abbreviated as: LoD. In layman’s terms, the less a class knows about the class it depends on, the better. That is to say, for the dependent class, no matter how complicated the logic is, the logic is encapsulated inside the class as much as possible, and the public method provided by the external does not leak any information. Another way of saying : only communicate with direct friends Direct friend: Classes in member variables, method parameters, method return values, Indirect friends: Class in local variables The origin of the problem: The closer the relationship between a class and a class, the greater the degree of coupling. When one class changes, the effect on another class is greater. Solution: Try to reduce the coupling between classes and classes. Note: The original intention of Dimit’s law is to reduce the coupling between classes. Since each class reduces unnecessary dependencies, it can indeed reduce the coupling relationship. But everything has a degree, although it can avoid communication with indirect classes, but to communicate, it is bound to be connected through an “intermediary.” Excessive use of the Dimit principle will result in a large number of such intermediaries and delivery classes, resulting in increased system complexity. Therefore, when using the Dimitte rule, we must repeatedly weigh the balance, so that the structure is clear, but also high cohesion and low coupling.\n6. Open-Closed Principle Definition: A software entity such as classes, modules, and functions should be open to extensions and closed to modifications. The origin of the problem: During the software life cycle, when the original code of the software needs to be modified due to changes, upgrades, and maintenance, it may introduce errors into the old code, or it may cause us to refactor the entire function. There is code that has been retested. Solution: When software needs to change, try to implement changes by extending the behavior of the software entities, rather than modifying the existing code to implement the changes. To sum up: Use the abstract build framework to extend the details with implementation. Because the abstract flexibility is good and the adaptability is wide, as long as the abstraction is reasonable, the software architecture can be basically kept stable. And the variable details in the software, we use the abstraction-derived implementation class to expand, when the software needs to change, we only need to re-derive an implementation class to expand according to the requirements. Of course, the premise is that our abstraction should be reasonable, and we must be forward-looking and predictive of changes in demand.\n","date":1472139424,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472139424,"objectID":"2e42703f0300838ec21239b16729e3cb","permalink":"https://wubigo.com/post/6-principles-of-software-design/","publishdate":"2016-08-25T23:37:04+08:00","relpermalink":"/post/6-principles-of-software-design/","section":"post","summary":"1. Single Responsibility Principle Definition: The single responsibility principle is also known as the single-function principle, that is, there is no more than one reason for the class change. In layman’s terms, a class is only responsible for one responsibility. Principle: If a class has too many responsibilities, it is equivalent to coupling these responsibilities together. A change in responsibilities may weaken or continue the ability of this class to perform other duties.","tags":["OOP"],"title":"6 Principles of Software Design","type":"post"},{"authors":null,"categories":[],"content":" 安装客户端 sudo apt-get install m2crypto pip install shadowsocks  /etc/shadowsocks.json\n{ \u0026quot;server\u0026quot;:\u0026quot;4.1.33.104\u0026quot;, \u0026quot;server_port\u0026quot;:8388, \u0026quot;local_address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;local_port\u0026quot;:1080, \u0026quot;password\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;timeout\u0026quot;:600, \u0026quot;method\u0026quot;:\u0026quot;aes-256-cfb\u0026quot; }  sslocal -c /etc/shadowsocks.json # To run in the background sudo sslocal -c /etc/shadowsocks.json -d start  配置代理 sudo apt-get install polipo  /etc/polipo/config\nlogSyslog = true logFile = /var/log/polipo/polipo.log logSyslog = true logFile = /var/log/polipo/polipo.log proxyAddress = \u0026quot;0.0.0.0\u0026quot; socksParentProxy = \u0026quot;127.0.0.1:1080\u0026quot; socksProxyType = socks5 chunkHighMark = 50331648 objectHighMark = 16384 serverMaxSlots = 64 serverSlots = 16 serverSlots1 = 32  sudo service polipo stop sudo service polipo start export http_proxy=http://127.0.0.1:8123 export https_proxy=https://127.0.0.1:8123 git config --global http.proxy socks5://localhost:1080 git config --global https.proxy socks5://localhost:1080 git config --global http.https://github.com.proxy socks5://127.0.0.1:1080 git config --global https.https://github.com.proxy socks5://127.0.0.1:1080 git config --global --unset http.proxy git config --global --unset https.proxy  /etc/apt/apt.conf\nAcquire::http::Proxy \u0026quot;http://127.0.0.1:8123\u0026quot;; Acquire::https::Proxy \u0026quot;https://127.0.0.1:8123\u0026quot;;  config docker daemon\n测试 curl www.google.com git clone golang.org/x/lint/golint docker pull k8s.gcr.io/kube-apiserver:v1.15.10 apt update  ","date":1463963518,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463963518,"objectID":"e9f4c5c775008f18dd7d382c9aa471a7","permalink":"https://wubigo.com/post/ubuntu-vpn-client/","publishdate":"2016-05-23T08:31:58+08:00","relpermalink":"/post/ubuntu-vpn-client/","section":"post","summary":"安装客户端 sudo apt-get install m2crypto pip install shadowsocks  /etc/shadowsocks.json\n{ \u0026quot;server\u0026quot;:\u0026quot;4.1.33.104\u0026quot;, \u0026quot;server_port\u0026quot;:8388, \u0026quot;local_address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;local_port\u0026quot;:1080, \u0026quot;password\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;timeout\u0026quot;:600, \u0026quot;method\u0026quot;:\u0026quot;aes-256-cfb\u0026quot; }  sslocal -c /etc/shadowsocks.json # To run in the background sudo sslocal -c /etc/shadowsocks.json -d start  配置代理 sudo apt-get install polipo  /etc/polipo/config\nlogSyslog = true logFile = /var/log/polipo/polipo.log logSyslog = true logFile = /var/log/polipo/polipo.log proxyAddress = \u0026quot;0.0.0.0\u0026quot; socksParentProxy = \u0026quot;127.0.0.1:1080\u0026quot; socksProxyType = socks5 chunkHighMark = 50331648 objectHighMark = 16384 serverMaxSlots = 64 serverSlots = 16 serverSlots1 = 32  sudo service polipo stop sudo service polipo start export http_proxy=http://127.","tags":["VPN"],"title":"Ubuntu Vpn Client","type":"post"},{"authors":null,"categories":[],"content":" push to a mirror repository push to github at same time when a commit is pushed to gitlab\nProtected Branches By default, protected branches are designed to:\n prevent their creation, if not already created, from everybody except Maintainers prevent pushes from everybody except Maintainers prevent anyone from force pushing to the branch prevent anyone from deleting the branch  Project members permissions NOTE:\nIn GitLab 11.0, the Master role was renamed to Maintainer The following table depicts the various user permission levels in a project.\nAction Guest Reporter Developer Maintainer Owner\nCreate new issue ✓ ✓ ✓ ✓ ✓\nCreate confidential issue ✓ ✓ ✓ ✓ ✓\nView confidential issues (✓) ✓ ✓ ✓ ✓\nLeave comments ✓ ✓ ✓ ✓ ✓\nSee related issues ✓ ✓ ✓ ✓ ✓\nSee a list of jobs ✓ ✓ ✓ ✓ ✓\nSee a job log ✓ ✓ ✓ ✓ ✓\nDownload and browse job artifacts ✓ ✓ ✓ ✓ ✓\nView wiki pages ✓ ✓ ✓ ✓ ✓\nPull project code\n✓ ✓ ✓ ✓\nDownload project\n✓ ✓ ✓ ✓\nAssign issues\n✓ ✓ ✓ ✓\nAssign merge requests\n✓ ✓ ✓\nLabel issues and merge requests\n✓ ✓ ✓ ✓\nCreate code snippets\n✓ ✓ ✓ ✓\nManage issue tracker\n✓ ✓ ✓ ✓\nManage labels\n✓ ✓ ✓ ✓\nSee a commit status\n✓ ✓ ✓ ✓\nSee a container registry\n✓ ✓ ✓ ✓\nSee environments\n✓ ✓ ✓ ✓\nSee a list of merge requests\n✓ ✓ ✓ ✓\nManage related issues [STARTER]\n✓ ✓ ✓ ✓\nLock issue discussions\n✓ ✓ ✓ ✓\nLock merge request discussions\n✓ ✓ ✓\nCreate new environments\n✓ ✓ ✓\nStop environments\n✓ ✓ ✓\nManage/Accept merge requests\n✓ ✓ ✓\nCreate new merge request\n✓ ✓ ✓\nCreate new branches\n✓ ✓ ✓\nPush to non-protected branches\n✓ ✓ ✓\nForce push to non-protected branches\n✓ ✓ ✓\nRemove non-protected branches\n✓ ✓ ✓\nAdd tags\n✓ ✓ ✓\nWrite a wiki\n✓ ✓ ✓\nCancel and retry jobs\n✓ ✓ ✓\nCreate or update commit status\n✓ ✓ ✓\nUpdate a container registry\n✓ ✓ ✓\nRemove a container registry image\n✓ ✓ ✓\nCreate/edit/delete project milestones\n✓ ✓ ✓\nUse environment terminals\n✓ ✓\nAdd new team members\n✓ ✓\nPush to protected branches\n✓ ✓\nEnable/disable branch protection\n✓ ✓\nTurn on/off protected branch push for devs\n✓ ✓\nEnable/disable tag protections\n✓ ✓\nRewrite/remove Git tags\n✓ ✓\nEdit project\n✓ ✓\nAdd deploy keys to project\n✓ ✓\nConfigure project hooks\n✓ ✓\nManage Runners\n✓ ✓\nManage job triggers\n✓ ✓\nManage variables\n✓ ✓\nManage GitLab Pages\n✓ ✓\nManage GitLab Pages domains and certificates\n✓ ✓\nRemove GitLab Pages\n✓\nManage clusters\n✓ ✓\nEdit comments (posted by any user)\n✓ ✓\nSwitch visibility level\n✓\nTransfer project to another namespace\n✓\nRemove project\n✓\nDelete issues\n✓\nRemove pages\n✓\nForce push to protected branches\nRemove protected branches\nView project Audit Events\n✓ ✓\nProject features permissions\n","date":1461834487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461834487,"objectID":"1e4e47a6c6acb1c121b28bd0a762ec85","permalink":"https://wubigo.com/post/gitlab-notes/","publishdate":"2016-04-28T17:08:07+08:00","relpermalink":"/post/gitlab-notes/","section":"post","summary":"push to a mirror repository push to github at same time when a commit is pushed to gitlab\nProtected Branches By default, protected branches are designed to:\n prevent their creation, if not already created, from everybody except Maintainers prevent pushes from everybody except Maintainers prevent anyone from force pushing to the branch prevent anyone from deleting the branch  Project members permissions NOTE:\nIn GitLab 11.0, the Master role was renamed to Maintainer The following table depicts the various user permission levels in a project.","tags":["SHELL","GIT"],"title":"Gitlab Notes","type":"post"},{"authors":null,"categories":[],"content":" LETTUCE VS JEDIS While Jedis is easy to use and supports a vast number of Redis features, it is not thread safe and needs connection pooling to work in a multi-threaded environment. Connection pooling comes at the cost of a physical connection per Jedis instance which increases the number of Redis connections.\nLettuce, on the other hand, is built on netty (https://netty.io/) and connection instances can be shared across multiple threads. So a multi-threaded application can use a single connection regardless the number of concurrent threads that interact with Lettuce.\nSYNC VS ASYNC One other reason we opted to go with Lettuce was that it facilitates asynchronicity from building the client on top of netty that is a multithreaded, event-driven I/O framework. Asynchronous methodologies allow you to utilize better system resources, instead of wasting threads waiting for network or disk I/O. Threads can be fully utilised to perform other work instead.\nFor the purpose of having a concurrently processing system, it’s convenient, in this scenario, to have all communication handled asynchronously. There are scenarios where this might not be the case, where you have quick running tasks and try to access data that has just been invalidated by a different task.\nConnecting to Redis spring-boot-starter-data-redis resolves Lettuce by default. Spring provides LettuceConnectionFactory to get connections. To get pooled connection factory we need to provide commons-pool2 on the classpath\n \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  redis-cli CONFIG GET databases INFO keyspace  ","date":1461834487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461834487,"objectID":"8a6d1d482c97039b3a5e348f7fd96ad9","permalink":"https://wubigo.com/post/redis-with-spring-boot-v2/","publishdate":"2016-04-28T17:08:07+08:00","relpermalink":"/post/redis-with-spring-boot-v2/","section":"post","summary":"LETTUCE VS JEDIS While Jedis is easy to use and supports a vast number of Redis features, it is not thread safe and needs connection pooling to work in a multi-threaded environment. Connection pooling comes at the cost of a physical connection per Jedis instance which increases the number of Redis connections.\nLettuce, on the other hand, is built on netty (https://netty.io/) and connection instances can be shared across multiple threads.","tags":["CACHE","REDIS"],"title":"Redis With Spring Boot V2","type":"post"},{"authors":null,"categories":[],"content":"  namespace  kube-logging.yaml\nkind: Namespace apiVersion: v1 metadata: name: kube-logging   headless service\nkubectl create -f kube-logging.yaml   elasticsearch_svc.yaml\nkind: Service apiVersion: v1 metadata: name: elasticsearch namespace: kube-logging labels: app: elasticsearch spec: selector: app: elasticsearch clusterIP: None ports: - port: 9200 name: rest - port: 9300 name: inter-node   PROVISION local PV for EFK  local PV\n Creating the StatefulSet  elasticsearch_statefulset.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: name: es-cluster namespace: kube-logging spec: serviceName: elasticsearch replicas: 1 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: containers: - name: elasticsearch image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.4.3 resources: limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: rest protocol: TCP - containerPort: 9300 name: inter-node protocol: TCP volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data env: - name: cluster.name value: k8s-logs - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: discovery.zen.ping.unicast.hosts value: \u0026quot;es-cluster-0.elasticsearch\u0026quot; - name: discovery.zen.minimum_master_nodes value: \u0026quot;1\u0026quot; - name: ES_JAVA_OPTS value: \u0026quot;-Xms512m -Xmx512m\u0026quot; initContainers: - name: fix-permissions image: busybox command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;chown -R 1000:1000 /usr/share/elasticsearch/data\u0026quot;] securityContext: privileged: true volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data - name: increase-vm-max-map image: busybox command: [\u0026quot;sysctl\u0026quot;, \u0026quot;-w\u0026quot;, \u0026quot;vm.max_map_count=262144\u0026quot;] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;ulimit -n 65536\u0026quot;] securityContext: privileged: true volumeClaimTemplates: - metadata: name: data labels: app: elasticsearch spec: accessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ] storageClassName: local-hdd resources: requests: storage: 10Gi  elasticsearch--oss suffix ensures the open-source version of Elasticsearch the default version(without suffix) containing X-Pack\ncheck Elasticsearch is ready kubectl port-forward es-cluster-0 9200:9200 --namespace=kube-logging curl http://localhost:9200/_cluster/state?pretty | grep master_node ... { \u0026quot;cluster_name\u0026quot; : \u0026quot;k8s-logs\u0026quot;, \u0026quot;compressed_size_in_bytes\u0026quot; : 230, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;9dm998tzS-Ko45dGEOtnDQ\u0026quot;, \u0026quot;version\u0026quot; : 2, \u0026quot;state_uuid\u0026quot; : \u0026quot;oLlQU_qiT8iit4SYx9S1-g\u0026quot;, \u0026quot;master_node\u0026quot; : \u0026quot;icsCEGzzRCmITCGzGbsSSg\u0026quot;, \u0026quot;blocks\u0026quot; : { }, \u0026quot;nodes\u0026quot; : { \u0026quot;icsCEGzzRCmITCGzGbsSSg\u0026quot; : { \u0026quot;name\u0026quot; : \u0026quot;es-cluster-0\u0026quot;, \u0026quot;ephemeral_id\u0026quot; : \u0026quot;PoResMCsTyG99qfK-U44_w\u0026quot;, \u0026quot;transport_address\u0026quot; : \u0026quot;10.2.12.86:9300\u0026quot;, \u0026quot;attributes\u0026quot; : { } } }, \u0026quot;metadata\u0026quot; : { \u0026quot;cluster_uuid\u0026quot; : \u0026quot;9dm998tzS-Ko45dGEOtnDQ\u0026quot;, \u0026quot;templates\u0026quot; : { }, \u0026quot;indices\u0026quot; : { }, \u0026quot;index-graveyard\u0026quot; : { \u0026quot;tombstones\u0026quot; : [ ] } }, \u0026quot;routing_table\u0026quot; : { \u0026quot;indices\u0026quot; : { } }, \u0026quot;routing_nodes\u0026quot; : { \u0026quot;unassigned\u0026quot; : [ ], \u0026quot;nodes\u0026quot; : { \u0026quot;icsCEGzzRCmITCGzGbsSSg\u0026quot; : [ ] } }, \u0026quot;restore\u0026quot; : { \u0026quot;snapshots\u0026quot; : [ ] }, \u0026quot;snapshots\u0026quot; : { \u0026quot;snapshots\u0026quot; : [ ] }, \u0026quot;snapshot_deletions\u0026quot; : { \u0026quot;snapshot_deletions\u0026quot; : [ ] } } ...  Deploy Kibana kibana.yaml\napiVersion: v1 kind: Service metadata: name: kibana namespace: kube-logging labels: app: kibana spec: ports: - port: 5601 selector: app: kibana --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: kube-logging labels: app: kibana spec: replicas: 1 selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: containers: - name: kibana image: docker.elastic.co/kibana/kibana-oss:6.4.3 resources: limits: cpu: 1000m requests: cpu: 100m env: - name: ELASTICSEARCH_URL value: http://elasticsearch:9200 ports: - containerPort: 5601   检查kibana\nkubectl port-forward svc/kibana 5601:5601 -n kube-logging curl http://localhost:5601   Deploy Fluentd DaemonSet DaemonSet is a Kubernetes workload type that runs a copy of a given Pod on each Node in the Kubernetes cluster\n ServiceAccount  fluentd.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: fluentd namespace: kube-logging labels: app: fluentd --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: fluentd labels: app: fluentd rules: - apiGroups: - \u0026quot;\u0026quot; resources: - pods - namespaces verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd roleRef: kind: ClusterRole name: fluentd apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: fluentd namespace: kube-logging --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd namespace: kube-logging labels: app: fluentd spec: selector: matchLabels: app: fluentd template: metadata: labels: app: fluentd spec: serviceAccount: fluentd serviceAccountName: fluentd tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule restartPolicy: Never containers: - name: fluentd image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 env: - name: FLUENT_ELASTICSEARCH_HOST value: \u0026quot;elasticsearch.kube-logging.svc.cluster.local\u0026quot; - name: FLUENT_ELASTICSEARCH_PORT value: \u0026quot;9200\u0026quot; - name: FLUENT_ELASTICSEARCH_SCHEME value: \u0026quot;http\u0026quot; - name: FLUENT_UID value: \u0026quot;0\u0026quot; resources: limits: memory: 512Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers   tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule\n kubectl run fluent -it --image=fluent/fluentd-kubernetes-daemonset:v0.12-debian-elasticsearch --restart=Never Error attaching, falling back to logs: standard_init_linux.go:207: exec user process caused \u0026quot;no such file or directory\u0026quot; pod default/fluent terminated (Error)  https://www.digitalocean.com/community/tutorials/how-to-set-up-an-elasticsearch-fluentd-and-kibana-efk-logging-stack-on-kubernetes\n","date":1460873522,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460873522,"objectID":"77fe77e3b44b5a69df1a34de25eebca0","permalink":"https://wubigo.com/post/k8s-logging-efk/","publishdate":"2016-04-17T14:12:02+08:00","relpermalink":"/post/k8s-logging-efk/","section":"post","summary":"namespace  kube-logging.yaml\nkind: Namespace apiVersion: v1 metadata: name: kube-logging   headless service\nkubectl create -f kube-logging.yaml   elasticsearch_svc.yaml\nkind: Service apiVersion: v1 metadata: name: elasticsearch namespace: kube-logging labels: app: elasticsearch spec: selector: app: elasticsearch clusterIP: None ports: - port: 9200 name: rest - port: 9300 name: inter-node   PROVISION local PV for EFK  local PV\n Creating the StatefulSet  elasticsearch_statefulset.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: name: es-cluster namespace: kube-logging spec: serviceName: elasticsearch replicas: 1 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: containers: - name: elasticsearch image: docker.","tags":["K8S","EFK"],"title":"K8s日志EFK","type":"post"},{"authors":null,"categories":[],"content":" Dockerfile ENTRYPOINT有两种形式\n exec shell      exec(preferred) shell     ENTRYPOINT [\u0026ldquo;executable\u0026rdquo;, \u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] command param1 param2   Command line arguments to docker run  appended not being used     ENTRYPOINT will be started as a subcommand of /bin/sh -c   default N/A /bin/sh -c (start it with exec to sned stop signal)   CMD [“exec_cmd”, “p1_cmd”] exec_entry p1_entry exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry    ENTRYPOINT exec FROM alpine:3.8 ENTRYPOINT [\u0026quot;top\u0026quot;, \u0026quot;-b\u0026quot;]  因为没有sh进程，所以命令行没有环境变量替换。\n可以增加sh\nFROM alpine:3.8 ENTRYPOINT [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;] CMD [\u0026quot;exec top -b -u $UID\u0026quot;]  ENTRYPOINT shell CMD无效\nFROM alpine:3.8 ENTRYPOINT exec top -b  ENTRYPOINT作为sh的子命令执行即\n实际执行/bin/sh -c \u0026quot;exec top -b\u0026quot;\n验证\ndocker build -t cmd . docker run -it --rm cmd cmd1 cmd2 cmd3 Mem: 4750556K used, 4318692K free, 555320K shrd, 176952K buff, 1907592K cached CPU: 0% usr 0% sys 0% nic 99% idle 0% io 0% irq 0% sirq Load average: 0.40 0.28 0.22 4/973 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1524 0% 1 0% top -b  ENTRYPOINT [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] CMD [\u0026quot;exec java -jar $APP_FILE\u0026quot;]  类似执行如下指令\n/bin/sh -c \u0026quot;exec java -jar $APP_FILE\u0026quot;  ","date":1460510580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460510580,"objectID":"c230d1b1e6b16213b5cbce726a9cb373","permalink":"https://wubigo.com/post/docker-dockerfile-entrypoint/","publishdate":"2016-04-13T09:23:00+08:00","relpermalink":"/post/docker-dockerfile-entrypoint/","section":"post","summary":"Dockerfile ENTRYPOINT有两种形式\n exec shell      exec(preferred) shell     ENTRYPOINT [\u0026ldquo;executable\u0026rdquo;, \u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] command param1 param2   Command line arguments to docker run  appended not being used     ENTRYPOINT will be started as a subcommand of /bin/sh -c   default N/A /bin/sh -c (start it with exec to sned stop signal)   CMD [“exec_cmd”, “p1_cmd”] exec_entry p1_entry exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry    ENTRYPOINT exec FROM alpine:3.","tags":["DOCKER"],"title":"Docker Dockerfile ENTRYPOINT","type":"post"},{"authors":null,"categories":[],"content":" 关闭网络连接 个性化设置  性能模式\n 任务栏\n 通知\n 文件夹\n  关闭激活服务 禁用激活服务 https://www.wikihow.com/Turn-Off-Windows-Activation\n关闭自动激活 https://www.intowindows.com/how-to-turn-off-automatic-activation-in-windows-10/\n注意 个性化设置必须在关闭激活服务之前完成\n禁止用户修改密码 net users net user user_cmp /PasswordChg:No  关闭后台服务 turn off the background app functionality\nStart \u0026gt; Settings \u0026gt; Privacy \u0026gt; Background apps  ","date":1459725807,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459725807,"objectID":"16b5898ff3e0308e151341d8dc24abdb","permalink":"https://wubigo.com/post/wins-turn-off-windows-activation/","publishdate":"2016-04-04T07:23:27+08:00","relpermalink":"/post/wins-turn-off-windows-activation/","section":"post","summary":" 关闭网络连接 个性化设置  性能模式\n 任务栏\n 通知\n 文件夹\n  关闭激活服务 禁用激活服务 https://www.wikihow.com/Turn-Off-Windows-Activation\n关闭自动激活 https://www.intowindows.com/how-to-turn-off-automatic-activation-in-windows-10/\n注意 个性化设置必须在关闭激活服务之前完成\n禁止用户修改密码 net users net user user_cmp /PasswordChg:No  关闭后台服务 turn off the background app functionality\nStart \u0026gt; Settings \u0026gt; Privacy \u0026gt; Background apps  ","tags":["WINDOWS","OS"],"title":"购买笔记本电脑的正确打开姿势","type":"post"},{"authors":null,"categories":null,"content":" log-based vs memory-based broker “Thus, in situations where messages may be expensive to process and you want to parallelize processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. On the other hand, in situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.”\n“the throughput of a log remains more or less constant, since every message is written to disk anyway [18]. This behavior is in contrast to messaging systems that keep messages in memory by default and only write them to disk if the queue grows too large: such systems are fast when queues are short and become much slower when they start writing to”\n","date":1458518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458518400,"objectID":"f2461a9a52c992d8848c5d9cb177bea0","permalink":"https://wubigo.com/post/2016-03-21-streamdataprocessing/","publishdate":"2016-03-21T00:00:00Z","relpermalink":"/post/2016-03-21-streamdataprocessing/","section":"post","summary":"log-based vs memory-based broker “Thus, in situations where messages may be expensive to process and you want to parallelize processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. On the other hand, in situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.”","tags":null,"title":"streaming note","type":"post"},{"authors":null,"categories":[],"content":" RequiresUser annotation  Requires the current Subject to be an application user for the annotated class/instance/method to be accessed or invoked. This is less restrictive than the RequiresAuthentication annotation.\n Shiro defines a \u0026ldquo;user\u0026rdquo; as a Subject that is either \u0026ldquo;remembered\u0026rdquo; or authenticated:\n An authenticated user is a Subject that has successfully logged in (proven their identity) during their current session. A remembered user is any Subject that has proven their identity at least once, although not necessarily during their current session, and asked the system to remember them.  Note however that when a new session is created for the corresponding user, that user\u0026rsquo;s identity would be remembered, but they are NOT considered authenticated\n","date":1458376099,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458376099,"objectID":"cc3a43badfe2fbbee3d41aa8b2593009","permalink":"https://wubigo.com/post/uaa-shiro-notes/","publishdate":"2016-03-19T16:28:19+08:00","relpermalink":"/post/uaa-shiro-notes/","section":"post","summary":"RequiresUser annotation  Requires the current Subject to be an application user for the annotated class/instance/method to be accessed or invoked. This is less restrictive than the RequiresAuthentication annotation.\n Shiro defines a \u0026ldquo;user\u0026rdquo; as a Subject that is either \u0026ldquo;remembered\u0026rdquo; or authenticated:\n An authenticated user is a Subject that has successfully logged in (proven their identity) during their current session. A remembered user is any Subject that has proven their identity at least once, although not necessarily during their current session, and asked the system to remember them.","tags":["UAA","SHIRO"],"title":"UAA Shiro Notes","type":"post"},{"authors":null,"categories":null,"content":" Backends for Frontends https://www.thoughtworks.com/insights/blog/bff-soundcloud\n","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"5626be8c6b83784a26e59051195a8098","permalink":"https://wubigo.com/post/2016-03-01-backendsforfrontends/","publishdate":"2016-03-01T00:00:00Z","relpermalink":"/post/2016-03-01-backendsforfrontends/","section":"post","summary":"Backends for Frontends https://www.thoughtworks.com/insights/blog/bff-soundcloud","tags":null,"title":"Backends for Frontends","type":"post"},{"authors":null,"categories":[],"content":"#!/usr/bin/env bash set -e EXITCODE=0 # bits of this were adapted from lxc-checkconfig # see also https://github.com/lxc/lxc/blob/lxc-1.0.2/src/lxc/lxc-checkconfig.in possibleConfigs=( '/proc/config.gz' \u0026quot;/boot/config-$(uname -r)\u0026quot; \u0026quot;/usr/src/linux-$(uname -r)/.config\u0026quot; '/usr/src/linux/.config' ) if [ $# -gt 0 ]; then CONFIG=\u0026quot;$1\u0026quot; else : ${CONFIG:=\u0026quot;${possibleConfigs[0]}\u0026quot;} fi if ! command -v zgrep \u0026amp;\u0026gt; /dev/null; then zgrep() { zcat \u0026quot;$2\u0026quot; | grep \u0026quot;$1\u0026quot; } fi kernelVersion=\u0026quot;$(uname -r)\u0026quot; kernelMajor=\u0026quot;${kernelVersion%%.*}\u0026quot; kernelMinor=\u0026quot;${kernelVersion#$kernelMajor.}\u0026quot; kernelMinor=\u0026quot;${kernelMinor%%.*}\u0026quot; is_set() { zgrep \u0026quot;CONFIG_$1=[y|m]\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_in_kernel() { zgrep \u0026quot;CONFIG_$1=y\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_as_module() { zgrep \u0026quot;CONFIG_$1=m\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } color() { local codes=() if [ \u0026quot;$1\u0026quot; = 'bold' ]; then codes=( \u0026quot;${codes[@]}\u0026quot; '1' ) shift fi if [ \u0026quot;$#\u0026quot; -gt 0 ]; then local code= case \u0026quot;$1\u0026quot; in # see https://en.wikipedia.org/wiki/ANSI_escape_code#Colors black) code=30 ;; red) code=31 ;; green) code=32 ;; yellow) code=33 ;; blue) code=34 ;; magenta) code=35 ;; cyan) code=36 ;; white) code=37 ;; esac if [ \u0026quot;$code\u0026quot; ]; then codes=( \u0026quot;${codes[@]}\u0026quot; \u0026quot;$code\u0026quot; ) fi fi local IFS=';' echo -en '\\033['\u0026quot;${codes[*]}\u0026quot;'m' } wrap_color() { text=\u0026quot;$1\u0026quot; shift color \u0026quot;$@\u0026quot; echo -n \u0026quot;$text\u0026quot; color reset echo } wrap_good() { echo \u0026quot;$(wrap_color \u0026quot;$1\u0026quot; white): $(wrap_color \u0026quot;$2\u0026quot; green)\u0026quot; } wrap_bad() { echo \u0026quot;$(wrap_color \u0026quot;$1\u0026quot; bold): $(wrap_color \u0026quot;$2\u0026quot; bold red)\u0026quot; } wrap_warning() { wrap_color \u0026gt;\u0026amp;2 \u0026quot;$*\u0026quot; red } check_flag() { if is_set_in_kernel \u0026quot;$1\u0026quot;; then wrap_good \u0026quot;CONFIG_$1\u0026quot; 'enabled' elif is_set_as_module \u0026quot;$1\u0026quot;; then wrap_good \u0026quot;CONFIG_$1\u0026quot; 'enabled (as module)' else wrap_bad \u0026quot;CONFIG_$1\u0026quot; 'missing' EXITCODE=1 fi } check_flags() { for flag in \u0026quot;$@\u0026quot;; do echo -n \u0026quot;- \u0026quot;; check_flag \u0026quot;$flag\u0026quot; done } check_command() { if command -v \u0026quot;$1\u0026quot; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then wrap_good \u0026quot;$1 command\u0026quot; 'available' else wrap_bad \u0026quot;$1 command\u0026quot; 'missing' EXITCODE=1 fi } check_device() { if [ -c \u0026quot;$1\u0026quot; ]; then wrap_good \u0026quot;$1\u0026quot; 'present' else wrap_bad \u0026quot;$1\u0026quot; 'missing' EXITCODE=1 fi } check_distro_userns() { source /etc/os-release 2\u0026gt;/dev/null || /bin/true if [[ \u0026quot;${ID}\u0026quot; =~ ^(centos|rhel)$ \u0026amp;\u0026amp; \u0026quot;${VERSION_ID}\u0026quot; =~ ^7 ]]; then # this is a CentOS7 or RHEL7 system grep -q \u0026quot;user_namespace.enable=1\u0026quot; /proc/cmdline || { # no user namespace support enabled wrap_bad \u0026quot; (RHEL7/CentOS7\u0026quot; \u0026quot;User namespaces disabled; add 'user_namespace.enable=1' to boot command line)\u0026quot; EXITCODE=1 } fi } if [ ! -e \u0026quot;$CONFIG\u0026quot; ]; then wrap_warning \u0026quot;warning: $CONFIG does not exist, searching other paths for kernel config ...\u0026quot; for tryConfig in \u0026quot;${possibleConfigs[@]}\u0026quot;; do if [ -e \u0026quot;$tryConfig\u0026quot; ]; then CONFIG=\u0026quot;$tryConfig\u0026quot; break fi done if [ ! -e \u0026quot;$CONFIG\u0026quot; ]; then wrap_warning \u0026quot;error: cannot find kernel config\u0026quot; wrap_warning \u0026quot; try running this script again, specifying the kernel config:\u0026quot; wrap_warning \u0026quot; CONFIG=/path/to/kernel/.config $0 or $0 /path/to/kernel/.config\u0026quot; exit 1 fi fi wrap_color \u0026quot;info: reading kernel config from $CONFIG ...\u0026quot; white echo echo 'Generally Necessary:' echo -n '- ' cgroupSubsystemDir=\u0026quot;$(awk '/[, ](cpu|cpuacct|cpuset|devices|freezer|memory)[, ]/ \u0026amp;\u0026amp; $3 == \u0026quot;cgroup\u0026quot; { print $2 }' /proc/mounts | head -n1)\u0026quot; cgroupDir=\u0026quot;$(dirname \u0026quot;$cgroupSubsystemDir\u0026quot;)\u0026quot; if [ -d \u0026quot;$cgroupDir/cpu\u0026quot; -o -d \u0026quot;$cgroupDir/cpuacct\u0026quot; -o -d \u0026quot;$cgroupDir/cpuset\u0026quot; -o -d \u0026quot;$cgroupDir/devices\u0026quot; -o -d \u0026quot;$cgroupDir/freezer\u0026quot; -o -d \u0026quot;$cgroupDir/memory\u0026quot; ]; then echo \u0026quot;$(wrap_good 'cgroup hierarchy' 'properly mounted') [$cgroupDir]\u0026quot; else if [ \u0026quot;$cgroupSubsystemDir\u0026quot; ]; then echo \u0026quot;$(wrap_bad 'cgroup hierarchy' 'single mountpoint!') [$cgroupSubsystemDir]\u0026quot; else echo \u0026quot;$(wrap_bad 'cgroup hierarchy' 'nonexistent??')\u0026quot; fi EXITCODE=1 echo \u0026quot; $(wrap_color '(see https://github.com/tianon/cgroupfs-mount)' yellow)\u0026quot; fi if [ \u0026quot;$(cat /sys/module/apparmor/parameters/enabled 2\u0026gt;/dev/null)\u0026quot; = 'Y' ]; then echo -n '- ' if command -v apparmor_parser \u0026amp;\u0026gt; /dev/null; then echo \u0026quot;$(wrap_good 'apparmor' 'enabled and tools installed')\u0026quot; else echo \u0026quot;$(wrap_bad 'apparmor' 'enabled, but apparmor_parser missing')\u0026quot; echo -n ' ' if command -v apt-get \u0026amp;\u0026gt; /dev/null; then echo \u0026quot;$(wrap_color '(use \u0026quot;apt-get install apparmor\u0026quot; to fix this)')\u0026quot; elif command -v yum \u0026amp;\u0026gt; /dev/null; then echo \u0026quot;$(wrap_color '(your best bet is \u0026quot;yum install apparmor-parser\u0026quot;)')\u0026quot; else echo \u0026quot;$(wrap_color '(look for an \u0026quot;apparmor\u0026quot; package for your distribution)')\u0026quot; fi EXITCODE=1 fi fi flags=( NAMESPACES {NET,PID,IPC,UTS}_NS CGROUPS CGROUP_CPUACCT CGROUP_DEVICE CGROUP_FREEZER CGROUP_SCHED CPUSETS MEMCG KEYS VETH BRIDGE BRIDGE_NETFILTER NF_NAT_IPV4 IP_NF_FILTER IP_NF_TARGET_MASQUERADE NETFILTER_XT_MATCH_{ADDRTYPE,CONNTRACK,IPVS} IP_NF_NAT NF_NAT NF_NAT_NEEDED # required for bind-mounting /dev/mqueue into containers POSIX_MQUEUE ) check_flags \u0026quot;${flags[@]}\u0026quot; if [ \u0026quot;$kernelMajor\u0026quot; -lt 4 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 4 -a \u0026quot;$kernelMinor\u0026quot; -lt 8 ]; then check_flags DEVPTS_MULTIPLE_INSTANCES fi echo echo 'Optional Features:' { check_flags USER_NS check_distro_userns } { check_flags SECCOMP } { check_flags CGROUP_PIDS } { CODE=${EXITCODE} check_flags MEMCG_SWAP MEMCG_SWAP_ENABLED if [ -e /sys/fs/cgroup/memory/memory.memsw.limit_in_bytes ]; then echo \u0026quot; $(wrap_color '(cgroup swap accounting is currently enabled)' bold black)\u0026quot; EXITCODE=${CODE} elif is_set MEMCG_SWAP \u0026amp;\u0026amp; ! is_set MEMCG_SWAP_ENABLED; then echo \u0026quot; $(wrap_color '(cgroup swap accounting is currently not enabled, you can enable it by setting boot option \u0026quot;swapaccount=1\u0026quot;)' bold black)\u0026quot; fi } { if is_set LEGACY_VSYSCALL_NATIVE; then echo -n \u0026quot;- \u0026quot;; wrap_bad \u0026quot;CONFIG_LEGACY_VSYSCALL_NATIVE\u0026quot; 'enabled' echo \u0026quot; $(wrap_color '(dangerous, provides an ASLR-bypassing target with usable ROP gadgets.)' bold black)\u0026quot; elif is_set LEGACY_VSYSCALL_EMULATE; then echo -n \u0026quot;- \u0026quot;; wrap_good \u0026quot;CONFIG_LEGACY_VSYSCALL_EMULATE\u0026quot; 'enabled' elif is_set LEGACY_VSYSCALL_NONE; then echo -n \u0026quot;- \u0026quot;; wrap_bad \u0026quot;CONFIG_LEGACY_VSYSCALL_NONE\u0026quot; 'enabled' echo \u0026quot; $(wrap_color '(containers using eglibc \u0026lt;= 2.13 will not work. Switch to' bold black)\u0026quot; echo \u0026quot; $(wrap_color ' \u0026quot;CONFIG_VSYSCALL_[NATIVE|EMULATE]\u0026quot; or use \u0026quot;vsyscall=[native|emulate]\u0026quot;' bold black)\u0026quot; echo \u0026quot; $(wrap_color ' on kernel command line. Note that this will disable ASLR for the,' bold black)\u0026quot; echo \u0026quot; $(wrap_color ' VDSO which may assist in exploiting security vulnerabilities.)' bold black)\u0026quot; # else Older kernels (prior to 3dc33bd30f3e, released in v4.40-rc1) do # not have these LEGACY_VSYSCALL options and are effectively # LEGACY_VSYSCALL_EMULATE. Even older kernels are presumably # effectively LEGACY_VSYSCALL_NATIVE. fi } if [ \u0026quot;$kernelMajor\u0026quot; -lt 4 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 4 -a \u0026quot;$kernelMinor\u0026quot; -le 5 ]; then check_flags MEMCG_KMEM fi if [ \u0026quot;$kernelMajor\u0026quot; -lt 3 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 3 -a \u0026quot;$kernelMinor\u0026quot; -le 18 ]; then check_flags RESOURCE_COUNTERS fi if [ \u0026quot;$kernelMajor\u0026quot; -lt 3 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 3 -a \u0026quot;$kernelMinor\u0026quot; -le 13 ]; then netprio=NETPRIO_CGROUP else netprio=CGROUP_NET_PRIO fi flags=( BLK_CGROUP BLK_DEV_THROTTLING IOSCHED_CFQ CFQ_GROUP_IOSCHED CGROUP_PERF CGROUP_HUGETLB NET_CLS_CGROUP $netprio CFS_BANDWIDTH FAIR_GROUP_SCHED RT_GROUP_SCHED IP_NF_TARGET_REDIRECT IP_VS IP_VS_NFCT IP_VS_PROTO_TCP IP_VS_PROTO_UDP IP_VS_RR ) check_flags \u0026quot;${flags[@]}\u0026quot; if ! is_set EXT4_USE_FOR_EXT2; then check_flags EXT3_FS EXT3_FS_XATTR EXT3_FS_POSIX_ACL EXT3_FS_SECURITY if ! is_set EXT3_FS || ! is_set EXT3_FS_XATTR || ! is_set EXT3_FS_POSIX_ACL || ! is_set EXT3_FS_SECURITY; then echo \u0026quot; $(wrap_color '(enable these ext3 configs if you are using ext3 as backing filesystem)' bold black)\u0026quot; fi fi check_flags EXT4_FS EXT4_FS_POSIX_ACL EXT4_FS_SECURITY if ! is_set EXT4_FS || ! is_set EXT4_FS_POSIX_ACL || ! is_set EXT4_FS_SECURITY; then if is_set EXT4_USE_FOR_EXT2; then echo \u0026quot; $(wrap_color 'enable these ext4 configs if you are using ext3 or ext4 as backing filesystem' bold black)\u0026quot; else echo \u0026quot; $(wrap_color 'enable these ext4 configs if you are using ext4 as backing filesystem' bold black)\u0026quot; fi fi echo '- Network Drivers:' echo ' - \u0026quot;'$(wrap_color 'overlay' blue)'\u0026quot;:' check_flags VXLAN | sed 's/^/ /' echo ' Optional (for encrypted networks):' check_flags CRYPTO CRYPTO_AEAD CRYPTO_GCM CRYPTO_SEQIV CRYPTO_GHASH \\ XFRM XFRM_USER XFRM_ALGO INET_ESP INET_XFRM_MODE_TRANSPORT | sed 's/^/ /' echo ' - \u0026quot;'$(wrap_color 'ipvlan' blue)'\u0026quot;:' check_flags IPVLAN | sed 's/^/ /' echo ' - \u0026quot;'$(wrap_color 'macvlan' blue)'\u0026quot;:' check_flags MACVLAN DUMMY | sed 's/^/ /' echo ' - \u0026quot;'$(wrap_color 'ftp,tftp client in container' blue)'\u0026quot;:' check_flags NF_NAT_FTP NF_CONNTRACK_FTP NF_NAT_TFTP NF_CONNTRACK_TFTP | sed 's/^/ /' # only fail if no storage drivers available CODE=${EXITCODE} EXITCODE=0 STORAGE=1 echo '- Storage Drivers:' echo ' - \u0026quot;'$(wrap_color 'aufs' blue)'\u0026quot;:' check_flags AUFS_FS | sed 's/^/ /' if ! is_set AUFS_FS \u0026amp;\u0026amp; grep -q aufs /proc/filesystems; then echo \u0026quot; $(wrap_color '(note that some kernels include AUFS patches but not the AUFS_FS flag)' bold black)\u0026quot; fi [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'btrfs' blue)'\u0026quot;:' check_flags BTRFS_FS | sed 's/^/ /' check_flags BTRFS_FS_POSIX_ACL | sed 's/^/ /' [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'devicemapper' blue)'\u0026quot;:' check_flags BLK_DEV_DM DM_THIN_PROVISIONING | sed 's/^/ /' [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'overlay' blue)'\u0026quot;:' check_flags OVERLAY_FS | sed 's/^/ /' [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'zfs' blue)'\u0026quot;:' echo -n \u0026quot; - \u0026quot;; check_device /dev/zfs echo -n \u0026quot; - \u0026quot;; check_command zfs echo -n \u0026quot; - \u0026quot;; check_command zpool [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 EXITCODE=$CODE [ \u0026quot;$STORAGE\u0026quot; = 1 ] \u0026amp;\u0026amp; EXITCODE=1 echo check_limit_over() { if [ $(cat \u0026quot;$1\u0026quot;) -le \u0026quot;$2\u0026quot; ]; then wrap_bad \u0026quot;- $1\u0026quot; \u0026quot;$(cat $1)\u0026quot; wrap_color \u0026quot; This should be set to at least $2, for example set: sysctl -w kernel/keys/root_maxkeys=1000000\u0026quot; bold black EXITCODE=1 else wrap_good \u0026quot;- $1\u0026quot; \u0026quot;$(cat $1)\u0026quot; fi } echo 'Limits:' check_limit_over /proc/sys/kernel/keys/root_maxkeys 10000 echo exit $EXITCODE   curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh \u0026gt; check-config.sh\n https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh\n","date":1456394317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456394317,"objectID":"dd1820587bd381fc88912b6de01b433f","permalink":"https://wubigo.com/post/docker-check-config/","publishdate":"2016-02-25T17:58:37+08:00","relpermalink":"/post/docker-check-config/","section":"post","summary":"#!/usr/bin/env bash set -e EXITCODE=0 # bits of this were adapted from lxc-checkconfig # see also https://github.com/lxc/lxc/blob/lxc-1.0.2/src/lxc/lxc-checkconfig.in possibleConfigs=( '/proc/config.gz' \u0026quot;/boot/config-$(uname -r)\u0026quot; \u0026quot;/usr/src/linux-$(uname -r)/.config\u0026quot; '/usr/src/linux/.config' ) if [ $# -gt 0 ]; then CONFIG=\u0026quot;$1\u0026quot; else : ${CONFIG:=\u0026quot;${possibleConfigs[0]}\u0026quot;} fi if ! command -v zgrep \u0026amp;\u0026gt; /dev/null; then zgrep() { zcat \u0026quot;$2\u0026quot; | grep \u0026quot;$1\u0026quot; } fi kernelVersion=\u0026quot;$(uname -r)\u0026quot; kernelMajor=\u0026quot;${kernelVersion%%.*}\u0026quot; kernelMinor=\u0026quot;${kernelVersion#$kernelMajor.}\u0026quot; kernelMinor=\u0026quot;${kernelMinor%%.*}\u0026quot; is_set() { zgrep \u0026quot;CONFIG_$1=[y|m]\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_in_kernel() { zgrep \u0026quot;CONFIG_$1=y\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_as_module() { zgrep \u0026quot;CONFIG_$1=m\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } color() { local codes=() if [ \u0026quot;$1\u0026quot; = 'bold' ]; then codes=( \u0026quot;${codes[@]}\u0026quot; '1' ) shift fi if [ \u0026quot;$#\u0026quot; -gt 0 ]; then local code= case \u0026quot;$1\u0026quot; in # see https://en.","tags":["DOCKER"],"title":"Docker Check Config","type":"post"},{"authors":null,"categories":["IT"],"content":" K8S网络基础\nK8S简介 K8S是自动化部署和监控容器的容器编排和管理工具。各大云厂商和应用开发平台都提供基于K8S的容器服务。 如果觉得K8S托管服务不容易上手或者和本公司的业务场景不很匹配，现在也有很多工具帮助在自己的数据 中心或私有云平台搭建K8S运行环境。\n Minikube kops kubeadm  如果你想搭建一个测试环境，请参考\n 从K8S源代码构建容器集群(支持最新稳定版V1.13.3) 一个脚步部署K8S  Kubernetes主要构件:\n 主节点： 主要的功能包括管理工作节点集群，服务部署，服务发现，工作调度，负载均衡等。 工作节点： 应用负载执行单元。 服务规范： 无状态服务，有状态服务，守护进程服务，定时任务等。   K8S网络基础 K8S网络模型\n 每一个POD拥有独立的IP地址 任何两个POD之间都可以互相通信且不通过NAT 集群每个节点上的代理(KUBELET)可以和该节点上的所有POD通信  K8S网络模型从网络端口分配的角度为容器建立一个干净的，向后兼容的规范，极大的方便和简化应用从虚拟机往容器迁移的流程。\nK8S解决的网络问题：\n 容器间通信问题： 由POD和localhost通信解决\n POD间通信问题： 由CNI解决 POD和服务的通信问题： 由SERVICE解决 外部系统和SERVICE的通信问题： 由SERVICE解决  ","date":1456313943,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456313943,"objectID":"896fb0dffd51ac94259bf63b206f5f45","permalink":"https://wubigo.com/post/k8s-network-basic/","publishdate":"2016-02-24T19:39:03+08:00","relpermalink":"/post/k8s-network-basic/","section":"post","summary":" K8S网络基础\nK8S简介 K8S是自动化部署和监控容器的容器编排和管理工具。各大云厂商和应用开发平台都提供基于K8S的容器服务。 如果觉得K8S托管服务不容易上手或者和本公司的业务场景不很匹配，现在也有很多工具帮助在自己的数据 中心或私有云平台搭建K8S运行环境。\n Minikube kops kubeadm  如果你想搭建一个测试环境，请参考\n 从K8S源代码构建容器集群(支持最新稳定版V1.13.3) 一个脚步部署K8S  Kubernetes主要构件:\n 主节点： 主要的功能包括管理工作节点集群，服务部署，服务发现，工作调度，负载均衡等。 工作节点： 应用负载执行单元。 服务规范： 无状态服务，有状态服务，守护进程服务，定时任务等。   K8S网络基础 K8S网络模型\n 每一个POD拥有独立的IP地址 任何两个POD之间都可以互相通信且不通过NAT 集群每个节点上的代理(KUBELET)可以和该节点上的所有POD通信  K8S网络模型从网络端口分配的角度为容器建立一个干净的，向后兼容的规范，极大的方便和简化应用从虚拟机往容器迁移的流程。\nK8S解决的网络问题：\n 容器间通信问题： 由POD和localhost通信解决\n POD间通信问题： 由CNI解决 POD和服务的通信问题： 由SERVICE解决 外部系统和SERVICE的通信问题： 由SERVICE解决  ","tags":["K8S","NETWORK"],"title":"K8S网络基础","type":"post"},{"authors":null,"categories":[],"content":" REMOVE ROLE delete policy before delete role\naws iam list-roles aws iam list-role-policies --role-name api-executor aws iam delete-role-policy --role-name api-executor -policy-name \u0026quot;log-writer\u0026quot; aws iam delete-role --role-name pizza-api-executor  ADD ROLE POLICY aws iam put-role-policy \\ --role-name pizza-api-executor \\ --policy-name PizzaApiDynamoDB \\ --policy-document file://./roles/dynamodb.json  You need to provide a path to dynamodb.json with the file:// prefix. If you are providing an absolute path, keep in mind that you will have three slashes after file:. The first two are for file://, and the third one is from the absolute path, because it starts with a slash.\n","date":1455949823,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455949823,"objectID":"65a3bd0a087d29d461650cde1e7b892a","permalink":"https://wubigo.com/post/aws-iam-notes/","publishdate":"2016-02-20T14:30:23+08:00","relpermalink":"/post/aws-iam-notes/","section":"post","summary":"REMOVE ROLE delete policy before delete role\naws iam list-roles aws iam list-role-policies --role-name api-executor aws iam delete-role-policy --role-name api-executor -policy-name \u0026quot;log-writer\u0026quot; aws iam delete-role --role-name pizza-api-executor  ADD ROLE POLICY aws iam put-role-policy \\ --role-name pizza-api-executor \\ --policy-name PizzaApiDynamoDB \\ --policy-document file://./roles/dynamodb.json  You need to provide a path to dynamodb.json with the file:// prefix. If you are providing an absolute path, keep in mind that you will have three slashes after file:.","tags":["AWS","IAM"],"title":"Aws Iam Notes","type":"post"},{"authors":null,"categories":null,"content":" cost effective network solutions In general , commercial product is better. Nuage is software-based ,while 华为，华三 are hardware-based. Nuage support container, bare metal and VM\n","date":1455235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455235200,"objectID":"02b5377f7051d7bda55ee55bcbc7e96e","permalink":"https://wubigo.com/post/2016-02-12-openstacknotes/","publishdate":"2016-02-12T00:00:00Z","relpermalink":"/post/2016-02-12-openstacknotes/","section":"post","summary":"cost effective network solutions In general , commercial product is better. Nuage is software-based ,while 华为，华三 are hardware-based. Nuage support container, bare metal and VM","tags":null,"title":"openstack notes","type":"post"},{"authors":null,"categories":[],"content":" NAT GATEWAY 数据备份S3 Infrequent Access Tier, All Storage / Month $0.0125 per GB\n Archive 50 TB into Amazon S3  If you perform a one-time migration of 50 TB of 16 MB files into Amazon S3 in US East (Ohio), it costs you the following to use DataSync: (50 TB copied into S3 * 1024 GB * $0.0125/GB) + (1 S3 LIST request * $0.005 / 1000) + (50 TB / 16 MB S3 PUT requests * $0.005 / 1000) = $640 + $0 + $16.38 = $656.38\nTotal data copied by AWS DataSync per month: 20000 TB x 1024 GB in a TB = 20480000 GB Pricing calculations\n20,480,000 GB x 0.0125 USD = 256,000.00 USD\nAWS DataSync Pricing (monthly): 256,000.00 USD AWS DataSync Pricing (yearly): 256,000.00 * 12 = 3,072,000 USD\n数据存储S3 Frequent Access Tier, Over 500 TB / Month $0.021 per GB\n20,480,000 GB x 0.021 USD = 512,000.00 USD + READ REQUEST * $0.005 / 1000\n","date":1454768688,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454768688,"objectID":"e171bf4d50d80109ee6630e855b9c2f2","permalink":"https://wubigo.com/post/aws-billing-notes/","publishdate":"2016-02-06T22:24:48+08:00","relpermalink":"/post/aws-billing-notes/","section":"post","summary":"NAT GATEWAY 数据备份S3 Infrequent Access Tier, All Storage / Month $0.0125 per GB\n Archive 50 TB into Amazon S3  If you perform a one-time migration of 50 TB of 16 MB files into Amazon S3 in US East (Ohio), it costs you the following to use DataSync: (50 TB copied into S3 * 1024 GB * $0.0125/GB) + (1 S3 LIST request * $0.005 / 1000) + (50 TB / 16 MB S3 PUT requests * $0.","tags":["AWS"],"title":"Aws Billing Notes","type":"post"},{"authors":null,"categories":[],"content":" 典型使用场景  单一公开子网   公开子网和私有子网   企业数据中心+公开子网   企业数据中心  公网网关 An Internet gateway is a fully managed AWS service that performs bi-direction source and destination network address translation for your EC2 instances. Optionally, a VPC may use a virtual private gateway to grant instances secure access to a user’s corporate network via VPN or direct connect links. Instances in a subnet can also be granted outbound only Internet access through a NAT gateway.\nsecurity group  security group as the source for a rule  When you specify a security group as the source for a rule, traffic is allowed from the network interfaces that are associated with the source security group for the specified protocol and port. Adding a security group as a source does not add rules from the source security group\npublic subnet A public subnet is a subnet that\u0026rsquo;s associated with a route table that has a route to an Internet gateway\nif your subnet is not associated to a specific route table, then by default it’s going to the main route table\nInternet Gateway An Internet Gateway is a logical connection between an Amazon VPC and the Internet. It is nota physical device. Only one can be associated with each VPC. It does not limit the bandwidth of Internet connectivity. (The only limitation on bandwidth is the size of the Amazon EC2 instance, and it applies to all traffic \u0026ndash; internal to the VPC and out to the Internet.)\nIf a VPC does not have an Internet Gateway, then the resources in the VPC cannot be accessed from the Internet (unless the traffic flows via a corporate network and VPN/Direct Connect).\nA subnet is deemed to be a Public Subnet if it has a Route Table that directs traffic to the Internet Gateway.\nNAT Instance A NAT Instance is an Amazon EC2 instance configured to forward traffic to the Internet. It can be launched from an existing AMI, or can be configured via User Data like this:\n#!/bin/sh echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward echo 0 \u0026gt; /proc/sys/net/ipv4/conf/eth0/send_redirects /sbin/iptables -t nat -A POSTROUTING -o eth0 -s 0.0.0.0/0 -j MASQUERADE /sbin/iptables-save \u0026gt; /etc/sysconfig/iptables mkdir -p /etc/sysctl.d/ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/nat.conf net.ipv4.ip_forward = 1 net.ipv4.conf.eth0.send_redirects = 0 EOF  Instances in a private subnet that want to access the Internet can have their Internet-bound traffic forwarded to the NAT Instance via a Route Table configuration. The NAT Instance will then make the request to the Internet (since it is in a Public Subnet) and the response will be forwarded back to the private instance.\nTraffic sent to a NAT Instance will typically be sent to an IP address that is not associated with the NAT Instance itself (it will be destined for a server on the Internet). Therefore, it is important to turn off the Source/Destination Check option on the NAT Instance otherwise the traffic will be blocked.\nNAT Gateway AWS introduced a NAT Gateway Service that can take the place of a NAT Instance. The benefits of using a NAT Gateway service are:\nIt is a fully-managed service \u0026ndash; just create it and it works automatically, including fail-over It can burst up to 10 Gbps (a NAT Instance is limited to the bandwidth associated with the EC2 instance type) However:\nSecurity Groups cannot be associated with a NAT Gateway You\u0026rsquo;ll need one in each AZ since they only operate in a single AZ\nRoute table Every VPC is attached to an implicit router. This router is not visible to the user and is fully managed and scaled by AWS. What is visible is the route table associated with each subnet, which is used by the VPC router to determine the allowed routes for outbound network traffic leaving a subnet.\nevery route table contains a default local route to facilitate communication between instances in the same VPC, even across subnets. This intra-VPC local route is implied and cannot be changed.\nIn the case of the main route table that is associated with a default subnet, there will also be a route out to the Internet via the default gateway for the VPC.\nevery subnet must be associated with a route table. If the association is not explicitly defined, then a subnet will be implicitly associated with the main route table.\nand as such each subnet can route to each other. The fact that the subnets are in different AZs is irrelevant.\nVPC Endpoints There are two types of VPC endpoints: interface endpoints and gateway endpoints. Create the type of VPC endpoint required by the supported service\n Interface Endpoints (Powered by AWS PrivateLink)\n Gateway Endpoints\n  ","date":1454715579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454715579,"objectID":"ade4c47647b96b72389dd3a711c7ed8c","permalink":"https://wubigo.com/post/aws-vpc-notes/","publishdate":"2016-02-06T07:39:39+08:00","relpermalink":"/post/aws-vpc-notes/","section":"post","summary":"典型使用场景  单一公开子网   公开子网和私有子网   企业数据中心+公开子网   企业数据中心  公网网关 An Internet gateway is a fully managed AWS service that performs bi-direction source and destination network address translation for your EC2 instances. Optionally, a VPC may use a virtual private gateway to grant instances secure access to a user’s corporate network via VPN or direct connect links. Instances in a subnet can also be granted outbound only Internet access through a NAT gateway.","tags":["AWS","VPC"],"title":"Aws VPC 基础","type":"post"},{"authors":null,"categories":["IT"],"content":" 更新到最新正式发布版V1.13.3\nMain external dependencies  go docker cri cni  external-dependencies\nKUBEADM IS CURRENTLY IN BETA\nkubeadm maturity build k8s  docker v17.03\nsudo apt-get install docker-ce=17.03.3~ce-0~ubuntu-xenial docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.11.7 k8s.gcr.io/kube-apiserver-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.7 k8s.gcr.io/kube-controller-manager-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.11.7 k8s.gcr.io/kube-scheduler-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.11.7 k8s.gcr.io/kube-proxy-amd64:v1.11.7 docker pull mirrorgooglecontainers/pause:3.1 docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 docker pull mirrorgooglecontainers/etcd-amd64:3.2.18 docker tag mirrorgooglecontainers/etcd-amd64:3.2.18 k8s.gcr.io/etcd-amd64:3.2.18 docker pull coredns/coredns:1.1.3 docker tag coredns/coredns:1.1.3 k8s.gcr.io/coredns:1.1.3  cri-tools v1.11.0\ngit clone https://github.com/kubernetes-sigs/cri-tools.git $GOPATH/src/github.com/kubernetes-sigs/ git checkout tags/v1.13.0 -b v1.13.0 make $GOPATH/bin/crictl -version cp $GOPATH/bin/cri* /usr/local/bin/  install-go-1.10\n checkout v1.11.7\ngit clone https://github.com/kubernetes/kubernetes.git $GOPATH/src/k8s.io/ git fetch --all git checkout tags/v1.11.7 -b v1.11.7  or\ngit clone -b v1.11.7 https://github.com/kubernetes/kubernetes.git  LOCAL ETCD INTEGRATION\n+++ source /home/bigo/go/src/k8s.io/kubernetes/hack/lib/etcd.sh ++++ ETCD_VERSION=3.2.24 ++++ ETCD_HOST=127.0.0.1 ++++ ++++ KUBE_INTEGRATION_ETCD_URL=http://127.0.0.1:2379  build v1.11.7\ncd kubernetes git remote add upstream https://github.com/kubernetes/kubernetes.git git remote set-url --push upstream no_push git fetch upstream git tag|grep v1.11.7 git checkout tags/v1.11.7 -b \u0026lt;branch_name\u0026gt; docker pull mirrorgooglecontainers/kube-cross:v1.10.7-1 docker tag mirrorgooglecontainers/kube-cross:v1.10.7-1 k8s.gcr.io/kube-cross:v1.10.7-1  export ETCD_HOST=192.168.1.9 export KUBE_INTEGRATION_ETCD_URL=http://$ETCD_HOST:2379 bash -x ./build/run.sh make \u0026gt; run.log 2\u0026gt;\u0026amp;1 ./_output/dockerized/bin/linux/amd64/kubeadm version| grep v1.11.7   or\nmake quick-release ./_output/release-stage/client/linux-amd64/kubernetes/client/bin/kubeadm version| grep v1.11.7  deploy K8S with kubeadm  install kubectl\nsudo cp ./_output/release-stage/client/linux-amd64/kubernetes/client/bin/kubectl /usr/bin/ sudo cp ./_output/release-stage/server/linux-amd64/kubernetes/server/bin/kubeadm /usr/bin/ sudo cp ./_output/release-stage/server/linux-amd64/kubernetes/server/bin/kubelet /usr/bin/  kubeadm kubectl bash completion\nkubeadm completion bash \u0026gt; ~/.kube/kubeadm_completion.bash.inc echo \u0026quot;source '$HOME/.kube/kubeadm_completion.bash.inc'\\n\u0026quot; \u0026gt;\u0026gt; $HOME/.bashrc  install kubelet service\nsudo cp ./build/debs/kubelet.service /etc/systemd/system/kubelet.service sudo mkdir -p /etc/kubernetes/manifests sudo mkdir -p /etc/systemd/system/kubelet.service.d/ sudo cp ./build/debs/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sudo systemctl daemon-reload sudo systemctl enable kubelet --now sudo systemctl start kubelet sudo useradd -G systemd-journal $USER journalctl -xeu kubelet  disable swap\nsudo swapoff -a  build cni v0.6.0\ngit clone -b v0.6.0 https://github.com/containernetworking/cni.git cd cni ./build.sh mkdir -p /opt/cni/bin cp bin/* /opt/cni/bin/  Configure NetworkManager for calio\n  NetworkManager manipulates the routing table for interfaces in the default network namespace where Calico veth pairs are anchored for connections to containers. This can interfere with the Calico agent’s ability to route correctly. Create the following configuration file at /etc/NetworkManager/conf.d/calico.conf to prevent NetworkManager from interfering with the interfaces:\n[keyfile] unmanaged-devices=interface-name:cali*;interface-name:tunl*   bootstrap a secure Kubernetes cluster debug level with -v\nsudo kubeadm init --kubernetes-version=v1.11.7 --pod-network-cidr 10.2.0.0/16 -v 4  configure kubectl\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  calico setup\n  calico etcd setup\nkubectl apply -f calico.yaml (https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/hosted/calico.yaml)   kubectl completion code for bash\n# Installing bash completion on Linux kubectl completion bash \u0026gt; ~/.kube/kubectl.bash.inc printf \u0026quot; # Kubectl shell completion source '$HOME/.kube/kubectl.bash.inc' \u0026quot; \u0026gt;\u0026gt; $HOME/.bashrc source $HOME/.bashrc  Control plane node isolation\n  By default, the cluster will not schedule pods on the master for security reasons. If you want to be able to schedule pods on the master, e.g. for a single-machine Kubernetes cluster for development, run:\nkubectl taint nodes --all node-role.kubernetes.io/master-  view cluster config kubectl describe configmaps kubeadm-config -n kube-system journalctl -xe | grep -i etcd  or\ncd /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml  ETCD liveness probe kubectl describe pods etcd-bigo-vm3 -n kube-system Liveness: exec [/bin/sh -ec ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo] sudo curl -v -l https://127.0.0.1:2379/v3/keys --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/healthcheck-client.crt --key /etc/kubernetes/pki/etcd/healthcheck-client.key  kubectl exec -it etcd-bigo-vm1 -n kube-system -- sh ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-cl ient.key member list  join a node  install docker v17.03 IPVS proxier load IPVS mod install ebtables socat\napt install ebtables socat  install kubelet service\n get token\nkubeadm token list  get token-ca-cert-hash\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //'    all in one shell\n deploy-work-node.sh\n token recreate By default, tokens expire after 24 hours. Joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the master node:\nkubeadm token create  Deploying the Dashboard\n  sa-admin-user.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system  rb-admin-user.yaml\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system  kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml  Bearer Token\nkubectl proxy kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/   depoly nginx and verify  creates a Deployment object and an associated ReplicaSet object with 2 pods\nkubectl run nginx1-14 --generator=run-pod/v1 --labels=\u0026quot;run=nginx1.14\u0026quot; --image=nginx:1.14-alpine --port=80 POD_IP=$(kubectl get pods -o wide | grep nginx1-14 | awk '{print $6}' | head -n 1) curl $POD_IP kubectl get pods -o wide | grep nginx1-14 | awk '{print $6}' | head -n 2 |xargs printf -- 'http://%s\\n'|xargs curl  Kubernetes requires a none-stop app/CMD Docker container stop automatically after running **K8S will restart it at default if a container stop **\ntest/curl/Dockerfile\n***let kubectl never restart container\nFROM alpine:3.8 RUN apk add --no-cache curl CMD [\u0026quot;sh\u0026quot;] docker build . docker tag curl-alpine:1.0 kubectl run curl -it --image=curl-alpine:1.0 --restart=Never sh   tear down cluster\nkubectl drain \u0026lt;node name\u0026gt; --delete-local-data --force --ignore-daemonsets kubectl delete node \u0026lt;node name\u0026gt;   Then, on the node being removed, reset all kubeadm installed state:\nkubeadm reset  The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:\niptables -F \u0026amp;\u0026amp; iptables -t nat -F \u0026amp;\u0026amp; iptables -t mangle -F \u0026amp;\u0026amp; iptables -X  If you want to reset the IPVS tables, you must run the following command:\nipvsadm -C  sudo kubeadm init phase etcd local \u0026ndash;config=configfile.yaml -v4\n\u0026ndash;kubernetes-version=v1.11.7\nkubeadm init \u0026ndash;config\netcd: local: serverCertSANs: - \u0026quot;0.0.0.0\u0026quot; peerCertSANs: - \u0026quot;0.0.0.0\u0026quot; extraArgs: listen-client-urls: --listen-client-urls=https://0.0.0.0:2379  ","date":1454470707,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454470707,"objectID":"105972d42fa16a684ab40158e6690e53","permalink":"https://wubigo.com/post/k8s-local-development-setup/","publishdate":"2016-02-03T11:38:27+08:00","relpermalink":"/post/k8s-local-development-setup/","section":"post","summary":"Setup a local development environment from source code with kubeadm","tags":["PAAS","K8S"],"title":"K8S local development setup from source code","type":"post"},{"authors":null,"categories":null,"content":" Microservices at Netflix Scale https://gotocon.com/dl/goto-amsterdam-2016/slides/RuslanMeshenberg_MicroservicesAtNetflixScaleFirstPrinciplesTradeoffsLessonsLearned.pdf\nsecuring microservice with UAA\nuser accounting and authorizing service(UAA) Using JWT authentication without manually forwarding JWTs from request to internal request forces microservices to call other microservices over the gateway, which involves additional internal requests per one master requests. But even with forwarding, it’s not possible to cleanly separate user and machine authentication.\nJWT (JSON Web Token) JWT (JSON Web Token) is an industry standard, easy-to-use method for securing applications in a microservices architecture.\nTokens are generated by the gateway, and sent to the underlying microservices: as they share a common secret key, microservices are able to validate the token, and authenticate users using that token.\nThose tokens are self-sufficient: they have both authentication and authorization information, so microservices do not need to query a database or an external system. This is important in order to ensure a scalable architecture\n","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"3c82f77b18f4006263734d8d02804c2e","permalink":"https://wubigo.com/post/2016-02-01-microservice-notes/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/post/2016-02-01-microservice-notes/","section":"post","summary":"Microservices at Netflix Scale https://gotocon.com/dl/goto-amsterdam-2016/slides/RuslanMeshenberg_MicroservicesAtNetflixScaleFirstPrinciplesTradeoffsLessonsLearned.pdf\nsecuring microservice with UAA\nuser accounting and authorizing service(UAA) Using JWT authentication without manually forwarding JWTs from request to internal request forces microservices to call other microservices over the gateway, which involves additional internal requests per one master requests. But even with forwarding, it’s not possible to cleanly separate user and machine authentication.\nJWT (JSON Web Token) JWT (JSON Web Token) is an industry standard, easy-to-use method for securing applications in a microservices architecture.","tags":["MICROSERVICE"],"title":"microservice notes","type":"post"},{"authors":null,"categories":[],"content":" 端口绑定 By default, when you create or run a container using docker create or docker run, it does not publish any of its ports to the outside world. To make a port available to services outside of Docker, or to Docker containers which are not connected to the container’s network, use the \u0026ndash;publish or -p flag. This creates a firewall rule which maps a container port to a port on the Docker host to the outside world\ndocker port \u0026lt;container-name\u0026gt;  容器组件 $ ps fxa | grep docker 1046 ? Ssl 86:30 /usr/bin/dockerd -H fd:// 1129 ? Ssl 61:37 \\_ docker-containerd --config /var/run/docker/containerd/containerd.toml 4370 ? Sl 0:01 | \\_ docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/187543746f3caaac62254b8aee40a7c5c8060d54722fa631a7cdfadd0722c71a -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc 4352 ? Sl 0:00 \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 443 -container-ip 172.17.0.3 -container-port 443 4364 ? Sl 0:00 \\_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.3 -container-port 80 3897 pts/1 S+ 0:00 \\_ grep --color=auto docker  Multi-stage builds in Docker only support for Doceker version \u0026gt; 17.05\nhttps://blog.alexellis.io/mutli-stage-docker-builds/\nFROM golang:1.10 as builder # build env and make target FROM alpine:latest WORKDIR /root/ COPY --from=builder ./  busybox nslookup busybox:latest has bug on nslookup\ndocker network create test 32024cd09daca748f8254468f4f00893afc2e1173c378919b1f378ed719f1618 docker run -dit --name nginx --network test nginx:alpine 7feaf1f0b4f3d421603bbb984854b753c7cbc6b581dd0a304d3b8fccf8c6604b $ docker run -it --rm --network test busybox:1.28 nslookup nginx Server: 127.0.0.11 Address 1: 127.0.0.11 Name: nginx Address 1: 172.22.0.2 nginx.test docker stop nginx docker network rm test  docker proxy /etc/systemd/system/docker.service.d/https-proxy.conf\n[Service] Environment=\u0026quot;HTTP_PROXY=http://127.0.0.1:33351/\u0026quot; Environment=\u0026quot;HTTPS_PROXY=http://127.0.0.1:33351/\u0026quot;  sudo systemctl daemon-reload sudo systemctl restart docker systemctl show --property=Environment docker  docker clean up disk space  delete volumes $ docker volume rm $(docker volume ls -qf dangling=true) $ docker volume ls -qf dangling=true | xargs -r docker volume rm  docker rmi $(docker images --filter \u0026quot;dangling=true\u0026quot; -q --no-trunc) docker rmi $(docker images | grep \u0026quot;none\u0026quot; | awk '/ / { print $3 }') docker rm $(docker ps -qa --no-trunc --filter \u0026quot;status=exited\u0026quot;)   Caution\ndocker system prune -a  ubuntu docker Post-installation steps  check to docker log for warning\njournalctl -xu docker journalctl -xu docker.service  check-config\n   curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh \u0026gt; check-config.sh\n docker run -d --name web httpd:2.4.38-alpine docker run --name mysql -e MYSQL_ROOT_PASSWORD=mysql -d mysql:5.5 docker run -it --name curl bigo/curl:v1 sudo pipework br1 web 192.168.1.117/24 sudo pipework br1 mysql 192.168.1.118/24 sudo pipework br1 curl 192.168.1.119/24 docker exec -it curl curl 192.168.117 docker logs web 192.168.1.119 - - [28/Feb/2019:10:09:15 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 45 192.168.1.119 - - [28/Feb/2019:10:15:43 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 45  pipework eth2 web\n文件 CONTAINDER_ID = $(docker run -d image) NS_PID = $(head -n 1 /sys/fs/cgroup/devices/docker/$CONTAINDER_ID/tasks) LOCAL_PAIR_VETH=veth\u0026lt;NO\u0026gt;pl\u0026lt;NS_PID\u0026gt; GUEST_PAIR_VETH=veth\u0026lt;NO\u0026gt;pg\u0026lt;NS_PID\u0026gt; ip link set veth1pl1452 master br1 ip link set veth1pl1452 up ip link set veth1pg1452 netns 1452 ip netns exec 1452 ip link set veth1pg1452 name eth1 ip netns exec 1452 ip -4 addr add 192.168.1.118/24 dev eth1 ip netns exec 1452 ip -4 link set eth1 up  为容器配置路由\nsudo nsenter -t $(docker-pid web) -n ip route del default sudo nsenter -t $(docker-pid web) -n ip route add default via 192.168.1.1 dev eth0  容器间通信  icc inter-container communication\ndocker network create --driver bridge --subnet 192.168.200.0/24 --ip-range 192.168.200.0/24 -o \u0026quot;com.docker.network.bridge.enable_icc\u0026quot;=\u0026quot;false\u0026quot; no-icc-network  enable_ip_masquerade\n  是否允许NAT使用宿主的IP掩蔽来自容器访问宿主外的网络包的SOURCE IP\ncom.docker.network.bridge.enable_ip_masquerade  改变默认的数据存储位置和驱动  配置  daemon.json\n{ \u0026quot;data-root\u0026quot;: \u0026quot;/mnt/docker\u0026quot;, \u0026quot;storage-driver\u0026quot;: \u0026quot;overlay2\u0026quot; }   移动数据\nsystemctl stop docker mv /var/lib/docker/* /mnt/docker/ systemctl start docker   定位容器的VETH接口 docker exec CID sudo ethtool -S eth0 NIC statistics: peer_ifindex: 7 sudo ip link | grep 7   capture all incoming IP traffic destined to the node except local traffic\n sudo tcpdump -i enp0s25 tcp -n sudo tcpdump -i enp0s25 dst host 192.168.1.5 and not src net 192.168.1.0/24  [1] https://www.securitynik.com/2016/12/docker-networking-internals-how-docker_16.html\n","date":1453713065,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453713065,"objectID":"cb36c07a249c5cfc1646698d05813209","permalink":"https://wubigo.com/post/docker-notes/","publishdate":"2016-01-25T17:11:05+08:00","relpermalink":"/post/docker-notes/","section":"post","summary":"端口绑定 By default, when you create or run a container using docker create or docker run, it does not publish any of its ports to the outside world. To make a port available to services outside of Docker, or to Docker containers which are not connected to the container’s network, use the \u0026ndash;publish or -p flag. This creates a firewall rule which maps a container port to a port on the Docker host to the outside world","tags":["DOCKER"],"title":"Docker Notes","type":"post"},{"authors":null,"categories":[],"content":"","date":1449322208,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1449322208,"objectID":"2f1e4183b542ba37ad2c2a22293ea15e","permalink":"https://wubigo.com/post/software-development-teams/","publishdate":"2015-12-05T21:30:08+08:00","relpermalink":"/post/software-development-teams/","section":"post","summary":"","tags":["DEV"],"title":"Software Development Teams","type":"post"},{"authors":null,"categories":null,"content":" git-changelog-maven-plugin \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;se.bjurr.gitchangelog\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;git-changelog-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.50\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;GenerateGitChangelog\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;generate-sources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;git-changelog\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- A file on filesystem //--\u0026gt; \u0026lt;file\u0026gt;CHANGELOG.md\u0026lt;/file\u0026gt; \u0026lt;toRef\u0026gt;HEAD\u0026lt;/toRef\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;  get a copy of mustache template and save as changelog.mustache under the project home directory https://github.com/tomasbjerre/git-changelog-lib/tree/master/src/test/resources/templates  mvn compile to create the CHANGELOG.md mvn compile  upload the CHANGELOG.md to nginx as a release not config nginx support browser MD mime.types text/markdown md;  reload nginx and check the release note as text use template with StrapDown.js to render Markdown as html \t\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;title\u0026gt;Bigo release\u0026lt;/title\u0026gt; \u0026lt;xmp theme=\u0026quot;united\u0026quot; style=\u0026quot;display:none;\u0026quot;\u0026gt; \u0026lt;/xmp\u0026gt; \u0026lt;script src=\u0026quot;http://strapdownjs.com/v/0.2/strapdown.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt;  ","date":1443830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443830400,"objectID":"d65862ec22ce872b3ad144e0f9cedc9c","permalink":"https://wubigo.com/post/2015-10-03-nginx-git-log-as-relasenote/","publishdate":"2015-10-03T00:00:00Z","relpermalink":"/post/2015-10-03-nginx-git-log-as-relasenote/","section":"post","summary":"git-changelog-maven-plugin \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;se.bjurr.gitchangelog\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;git-changelog-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.50\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;GenerateGitChangelog\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;generate-sources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;git-changelog\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- A file on filesystem //--\u0026gt; \u0026lt;file\u0026gt;CHANGELOG.md\u0026lt;/file\u0026gt; \u0026lt;toRef\u0026gt;HEAD\u0026lt;/toRef\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;  get a copy of mustache template and save as changelog.mustache under the project home directory https://github.com/tomasbjerre/git-changelog-lib/tree/master/src/test/resources/templates  mvn compile to create the CHANGELOG.md mvn compile  upload the CHANGELOG.md to nginx as a release not config nginx support browser MD mime.types text/markdown md;  reload nginx and check the release note as text use template with StrapDown.","tags":null,"title":"Git log as release note","type":"post"},{"authors":null,"categories":[],"content":" INNER JOIN = JOIN INNER JOIN is the default if you don\u0026rsquo;t specify the type when you use the word JOIN\nINNER JOIN is ANSI syntax that you should use.\nWhy Use the New Syntax for SQL Joins?\n Join conditions are separate from filtering conditions\n Easier to join multiple tables\n  ","date":1440553244,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440553244,"objectID":"0fb0a23a3f0a87534bfd8ebc453f5403","permalink":"https://wubigo.com/post/having-multiple-tables-in-from-and-using-join/","publishdate":"2015-08-26T09:40:44+08:00","relpermalink":"/post/having-multiple-tables-in-from-and-using-join/","section":"post","summary":" INNER JOIN = JOIN INNER JOIN is the default if you don\u0026rsquo;t specify the type when you use the word JOIN\nINNER JOIN is ANSI syntax that you should use.\nWhy Use the New Syntax for SQL Joins?\n Join conditions are separate from filtering conditions\n Easier to join multiple tables\n  ","tags":["SQL"],"title":"Having Multiple Tables in FROM and Using JOIN","type":"post"},{"authors":null,"categories":null,"content":" Configuring A records with DNS provider @ 185.199.108.153 @ 185.199.109.153 @ 185.199.110.153 @ 185.199.111.153  dig the custom domain to confirm DNS setup $dig +noall +answer wubigo.com wubigo.com.\t285\tIN\tA\t185.199.110.153 wubigo.com.\t285\tIN\tA\t185.199.108.153 wubigo.com.\t285\tIN\tA\t185.199.111.153 wubigo.com.\t285\tIN\tA\t185.199.109.153  https://help.github.com/articles/setting-up-an-apex-domain/#configuring-a-records-with-your-dns-provider\n","date":1438560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438560000,"objectID":"9033a7185a446a46f551aacb1f1b98d7","permalink":"https://wubigo.com/post/2015-08-03-github-notes/","publishdate":"2015-08-03T00:00:00Z","relpermalink":"/post/2015-08-03-github-notes/","section":"post","summary":"Configuring A records with DNS provider @ 185.199.108.153 @ 185.199.109.153 @ 185.199.110.153 @ 185.199.111.153  dig the custom domain to confirm DNS setup $dig +noall +answer wubigo.com wubigo.com.\t285\tIN\tA\t185.199.110.153 wubigo.com.\t285\tIN\tA\t185.199.108.153 wubigo.com.\t285\tIN\tA\t185.199.111.153 wubigo.com.\t285\tIN\tA\t185.199.109.153  https://help.github.com/articles/setting-up-an-apex-domain/#configuring-a-records-with-your-dns-provider","tags":null,"title":"github notes","type":"post"},{"authors":null,"categories":null,"content":" GET vs. POST HTTP POST requests supply additional data from the client (browser) to the server in the message body. In contrast, GET requests include all required data in the URL. Forms in HTML can use either method by specifying method=\u0026ldquo;POST\u0026rdquo; or method=\u0026ldquo;GET\u0026rdquo; (default) in the  element. The method specified determines how form data is submitted to the server. When the method is GET, all form data is encoded into the URL, appended to the action URL as query string parameters. With POST, form data appears within the message body of the HTTP request\nAuthors of services which use the HTTP protocol SHOULD NOT use GET based forms for the submission of sensitive data, because this will cause this data to be encoded in the Request-URI. Many existing servers, proxies, and user agents will log the request URI in some place where it might be visible to third parties. Servers can use POST-based form submission instead\nFinally, an important consideration when using GET for AJAX requests is that some browsers - IE in particular - will cache the results of a GET request. So if you, for example, poll using the same GET request you will always get back the same results, even if the data you are querying is being updated server-side. One way to alleviate this problem is to make the URL unique for each request by appending a timestamp.\nRestrictions on form data length Yes, since form data is in the URL and URL length is restricted. A safe URL length limit is often 2048 characters but varies by browser and web server.\nFrom the Dropbox developer blog:\n a browser doesn’t know exactly what a particular HTML form does, but if the form is submitted via HTTP GET, the browser knows it’s safe to automatically retry the submission if there’s a network error. For forms that use HTTP POST, it may not be safe to retry so the browser asks the user for confirmation first.  A \u0026ldquo;GET\u0026rdquo; request is often cacheable, whereas a \u0026ldquo;POST\u0026rdquo; request can hardly be. For query systems this may have a considerable efficiency impact, especially if the query strings are simple, since caches might serve the most frequent queries.\nhttp://www.diffen.com/difference/GET-vs-POST-HTTP-Requests\nrotate catalina out without restarting tomcat The catalina.out file is created by a shell redirection, ex \u0026ldquo;\u0026gt;\u0026gt; catalina.out 2\u0026gt;\u0026amp;1\u0026rdquo;. This catches anything written to System.out and System.err and places it into the catalina.out file. Given this, a good way to rotate catalina.out is to alter the script to pipe the output to a log rotation script rather than directly to a file. This will allow you to rotate the logs without restarting Tomcat and without copying the entire contents of the log to another file. It\u0026rsquo;s a pretty simple change to catalina.sh and it is described at this link.\n[http://marc.info/?l=tomcat-user\u0026amp;m=105640876032532\u0026amp;w=2](http://marc.info/?l=tomcat-user\u0026amp;m=105640876032532\u0026amp;w=2) [https://dzone.com/articles/how-rotate-tomcat-catalinaout](https://dzone.com/articles/how-rotate-tomcat-catalinaout) cat /dev/null \u0026gt; logs/catalina.out  tomcat connector config  \u0026lt;Connector address=\u0026quot;127.0.0.1\u0026quot; port=\u0026quot;8080\u0026quot; protocol=\u0026quot;org.apache.coyote.http11.Http11Nio2Protocol\u0026quot; connectionTimeout=\u0026quot;30000\u0026quot; redirectPort=\u0026quot;8443\u0026quot; executor=\u0026quot;tomcatThreadPool\u0026quot; minProcessors=\u0026quot;100\u0026quot; maxProcessors=\u0026quot;300\u0026quot; enableLookups=\u0026quot;false\u0026quot; acceptCount=\u0026quot;500\u0026quot; maxPostSize=\u0026quot;-1\u0026quot; disableUploadTimeout=\u0026quot;false\u0026quot; connectionUploadTimeout=\u0026quot;600000\u0026quot; compression=\u0026quot;on\u0026quot; compressionMinSize=\u0026quot;2048\u0026quot; noCompressionUserAgents=\u0026quot;gozilla, traviata\u0026quot; acceptorThreadCount=\u0026quot;2\u0026quot; compressableMimeType=\u0026quot;text/html,text/xml,text/plain,text/css,text/javascript,application/javascript\u0026quot; URIEncoding=\u0026quot;utf-8\u0026quot;/\u0026gt;  A CharacterEncodingFilter sets the body encoding, but not the URI encoding. Need to set URIEncoding=\u0026ldquo;UTF-8\u0026rdquo; as an attribute in all the connectors in Tomcat server.xml\nThe request.setCharacterEncoding(\u0026ldquo;UTF-8\u0026rdquo;); only sets the encoding of the request body (which is been used by POST requests), not the encoding of the request URI (which is been used by GET requests).\nenabling gzip with nginx and verifying that it\u0026rsquo;s working curl -H \u0026quot;Accept-Encoding: gzip,deflate\u0026quot; -I http://web/resource  https://www.digitalocean.com/community/tutorials/how-to-add-the-gzip-module-to-nginx-on-ubuntu-16-04\n","date":1438560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438560000,"objectID":"5096f8e4f05b72c0d0fc1652271e313c","permalink":"https://wubigo.com/post/2015-08-03-http/","publishdate":"2015-08-03T00:00:00Z","relpermalink":"/post/2015-08-03-http/","section":"post","summary":"GET vs. POST HTTP POST requests supply additional data from the client (browser) to the server in the message body. In contrast, GET requests include all required data in the URL. Forms in HTML can use either method by specifying method=\u0026ldquo;POST\u0026rdquo; or method=\u0026ldquo;GET\u0026rdquo; (default) in the  element. The method specified determines how form data is submitted to the server. When the method is GET, all form data is encoded into the URL, appended to the action URL as query string parameters.","tags":null,"title":"http  TIL","type":"post"},{"authors":null,"categories":null,"content":" NGINX 1.12 ON UBUNTU 16 https://launchpad.net/~nginx/+archive/ubuntu/stable\nubuntu@ip-192-168-133-137:/etc/nginx$ nginx -V nginx version: nginx/1.12.1 built with OpenSSL 1.0.2g 1 Mar 2016 TLS SNI support enabled configure arguments: --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -fPIC -Wdate-time -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -fPIC' --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-proxy-temp-path=/var/lib/nginx/proxy --http-scgi-temp-path=/var/lib/nginx/scgi --http-uwsgi-temp-path=/var/lib/nginx/uwsgi --with-debug --with-pcre-jit --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_dav_module --with-http_slice_module --with-threads --with-http_addition_module --with-http_geoip_module=dynamic --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module=dynamic --with-http_sub_module --with-http_xslt_module=dynamic --with-stream=dynamic --with-stream_ssl_module --with-stream_ssl_preread_module --with-mail=dynamic --with-mail_ssl_module --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-auth-pam --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-dav-ext-module --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-echo --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-upstream-fair --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/ngx_http_substitutions_filter_module  nginxproxy.md\nStep 3 \u0026ndash; Configure /\nLet say we want to configure nginx to route requests for /, /blog, and /mail, respectively onto localhost:8080, localhost:8181, and localhost:8282.\n +--- host --------\u0026gt; node.js on localhost:8080 | users --\u0026gt; nginx --|--- host/blog ---\u0026gt; node.js on localhost:8181 | +--- host/mail ---\u0026gt; node.js on localhost:8282  To route /, you need to edit your nginx config file.\nIn the config file, find the server section:\nserver { listen 80; ... location / { ... } ... }  This section is simply telling nginx how it should serve HTTP requests.\nNow, change the location section to this snippet:\nserver { listen ...; ... location / { proxy_pass http://127.0.0.1:8080; } ... }  proxy_pass simply tells nginx to forward requests to / to the server listening on http://127.0.0.1:8080.\nStep 4 \u0026ndash; Reload nginx\u0026rsquo;s Configuration\nTo reload nginx\u0026rsquo;s configuration run: nginx -s reload on your machine.\nReferesh your browser. Do you see the output from your node.js application? If yes, you are all set. If no, there is a problem with your config.\nStep 5 \u0026ndash; Add /blog and /mail\nTo redirect /mail and /blog, you simply need to add new entries the location section in the config file:\nserver { listen ...; ... location / { proxy_pass http://127.0.0.1:8080; } location /blog { proxy_pass http://127.0.0.1:8181; } location /mail { proxy_pass http://127.0.0.1:8282; } ... }  Step 6 \u0026ndash; Reload Your nginx Configuration\nRun nginx -s reload on your machine.\nLog onto localhost:$PORT/blog in your browser. Do you see the output from your second node.js application?\nThen log onto localhost:$PORT/mail. Do you see the output from your third node.js application?\nIf yes \u0026amp; yes, you are all set. If no, there is a problem with your config.\nStep 7 \u0026ndash; Rewriting Requests\nNow as you might have noticed in Step 6, nginx sends the same HTTP request to your node.js web apps which results into a 404 error. Why? Because, your node.js web application serves requests from / not from /blog and /mail. But, nginx is sending requests to /blog and /mail.\nTo fix this issue, we need rewrite the URL so that it matches the URL you can serve on your node.js applications.\nTo correctly rewrite URLs change your config file to match the following snippet:\nserver { listen ...; ... location / { proxy_pass http://127.0.0.1:8080; } location /blog { rewrite ^/blog(.*) /$1 break; proxy_pass http://127.0.0.1:8181; } location /mail { rewrite ^/mail(.*) /$1 break; proxy_pass http://127.0.0.1:8282; } ... }  This rewrite commands are simple regular expressions that transform strings like /blogWHAT_EVER and /mailWHAT_EVER to /WHAT_EVER in the HTTP requests.\nStep 8 \u0026ndash; Reload and Test.\nAll set?\nExercise 1\nConfigure your nginx to redirect URLs from /google to http://www.google.com\nStep 9 (optional) \u0026ndash; Redirecting Based on Host Name\nLet say you want to host example1.com, example2.com, and example3.com on your machine, respectively to localhost:8080, localhost:8181, and localhost:8282.\nNote: Since you don\u0026rsquo;t have access to a DNS server, you should add domain name entries to your /etc/hosts (you can\u0026rsquo;t do this on CDF machines):\n\u0026hellip; 127.0.0.1 example1.com example2.com example3.com \u0026hellip; To proxy eaxmple1.com we can\u0026rsquo;t use the location part of the default server. Instead we need to add another server section with a server_name set to our virtual host (e.g., example1.com, \u0026hellip;), and then a simple location section that tells nginx how to proxy the requests:\nserver { listen 80; server_name example1.com; location / { proxy_pass http://127.0.0.1:8080; } } server { listen 80; server_name example2.com; location / { proxy_pass http://127.0.0.1:8181; } } server { listen 80; server_name example3.com; location / { proxy_pass http://127.0.0.1:8282; } }  ","date":1435881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435881600,"objectID":"0039d8c563b2aa08c54a20528746eafd","permalink":"https://wubigo.com/post/2015-06-03-nginx-proxy-rewrite/","publishdate":"2015-07-03T00:00:00Z","relpermalink":"/post/2015-06-03-nginx-proxy-rewrite/","section":"post","summary":"NGINX 1.12 ON UBUNTU 16 https://launchpad.net/~nginx/+archive/ubuntu/stable\nubuntu@ip-192-168-133-137:/etc/nginx$ nginx -V nginx version: nginx/1.12.1 built with OpenSSL 1.0.2g 1 Mar 2016 TLS SNI support enabled configure arguments: --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -fPIC -Wdate-time -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -fPIC' --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-proxy-temp-path=/var/lib/nginx/proxy --http-scgi-temp-path=/var/lib/nginx/scgi --http-uwsgi-temp-path=/var/lib/nginx/uwsgi --with-debug --with-pcre-jit --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_dav_module --with-http_slice_module --with-threads --with-http_addition_module --with-http_geoip_module=dynamic --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module=dynamic --with-http_sub_module --with-http_xslt_module=dynamic --with-stream=dynamic --with-stream_ssl_module --with-stream_ssl_preread_module --with-mail=dynamic --with-mail_ssl_module --add-dynamic-module=/build/nginx-aqArPM/nginx-1.","tags":null,"title":"nginx-proxy-rewrite","type":"post"},{"authors":null,"categories":null,"content":" update a fork on GitHub with upstream git fetch upstream git rebase upstream/master (sync upstream update to local master branch) git push (update the fork by push)  Moving a git repository $ git remote show origin $ git remote rm origin $ git remote add origin https://github.com/wubigo/wubigo.github.io.git $ git remote show origin $ git pull origin master  Branch from a previous commit using Git The magic can be done by git reset.\nCreate a new branch and switch to it (so all of your latest commits are stored here)\ngit checkout -b your_new_branch\nSwitch back to your previous working branch (assume it\u0026rsquo;s master)\ngit checkout master\nRemove the latest x commits, keep master clean\ngit reset \u0026ndash;hard HEAD~x # in your case, x = 3\nFrom this moment on, all the latest x commits are only in the new branch, not in your previous working branch (master) any more.\nshow head commit and relationship betwen local and upstream branch git branch -a -vv (HEAD detached at v0.6.9) 0ad6fa1 update data-plane-api to 4f4ee118c54b7a52393c7d45b94f6bd5040a7118 master 0ad6fa1 [origin/master] update data-plane-api to 4f4ee118c54b7a52393c7d45b94f6bd5040a7118 (#158) remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/master 0ad6fa1 update data-plane-api to 4f4ee118c54b7a52393c7d45b94f6bd5040a7118 (#158)  Ignoring an already checked-in directory git rm -r --cached \u0026lt;your directory\u0026gt;  The -r option causes the removal of all files under your directory. The \u0026ndash;cached option causes the files to only be removed from git\u0026rsquo;s index, not your working copy. By default git rm  would delete \npush a new local branch to a remote Git repository and track it git checkout -b \u0026lt;branch\u0026gt; | git branch \u0026lt;branch\u0026gt; git push -u origin \u0026lt;branch\u0026gt;  Adding Only Untracked Files git add -i. Type a (for \u0026ldquo;add untracked\u0026rdquo;), then * (for \u0026ldquo;all\u0026rdquo;), then q (to quit)\nDiscard all Changes not staged for commit git checkout \u0026ndash; .\nCreate a new empty branch and import from svn git checkout --orphan \u0026lt;branchname\u0026gt; git rm --cached -r . svn checkout git add . git commit -m \u0026quot;backup from svn tag\u0026quot; git push --set-upstream origin \u0026lt;branchname\u0026gt;  save username and password in git git config credential.helper store then git pull  ~/.git-credentials\nI delete a Git branch both locally and remotely Executive Summary $ git push -d origin  $ git branch -d  Delete Local Branch To delete the local branch use:\n$ git branch -d branch_name or use: $ git branch -D branch_name As of Git v1.7.0, you can delete a remote branch using $ git push origin \u0026ndash;delete \ngit without proxy method 1\n$ env|grep proxy http_proxy=http://192.168.0.119:3128/ socks_proxy=socks://192.168.0.119:3128/ https_proxy=https://192.168.0.119:3128/ $ unset http_proxy $ git pull  method 2(proxy for certain git urls/domains)\n@web:~/workspace/git/pub$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [http] proxy = \u0026quot;\u0026quot; [https] proxy = \u0026quot;\u0026quot;  https://www.andrewpage.me/tracking-down-bugs-with-git-bisect/ https://medium.com/@fredrikmorken/why-you-should-stop-using-git-rebase-5552bee4fed1\ncheck the list of tags on the upstream repo without cloning or fetchingn git ls-remote https://github.com/go-delve/delve git ls-remote --tags https://github.com/go-delve/delve  the difference between origin and upstream on GitHub  upstream generally refers to the original repo that you have forked (see also \u0026ldquo;Definition of “downstream” and “upstream”\u0026rdquo; for more on upstream term) origin is your fork: your own repo on GitHub, clone of the original repo of GitHub  From the GitHub page:\n When a repo is cloned, it has a default remote called origin that points to your fork on GitHub, not the original repo it was forked from. To keep track of the original repo, you need to add another remote named upstream  git remote add upstream git://github.com/user/repo.git  You will use upstream to fetch from the original repo (in order to keep your local copy in sync with the project you want to contribute to).\ngit fetch upstream\n(git fetch alone would fetch from origin by default, which is not what is needed here)\n","date":1433289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433289600,"objectID":"fa9ccaaa4a347547a004bff4879385df","permalink":"https://wubigo.com/post/2015-06-03-git-notes/","publishdate":"2015-06-03T00:00:00Z","relpermalink":"/post/2015-06-03-git-notes/","section":"post","summary":"update a fork on GitHub with upstream git fetch upstream git rebase upstream/master (sync upstream update to local master branch) git push (update the fork by push)  Moving a git repository $ git remote show origin $ git remote rm origin $ git remote add origin https://github.com/wubigo/wubigo.github.io.git $ git remote show origin $ git pull origin master  Branch from a previous commit using Git The magic can be done by git reset.","tags":null,"title":"git note","type":"post"},{"authors":null,"categories":null,"content":" understand of the HBase data model http://jimbojw.com/#understanding hbase\n","date":1428019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428019200,"objectID":"e8284e4c6ce95a8614ae8baf1b4a00f2","permalink":"https://wubigo.com/post/2015-04-03-hbase-notes/","publishdate":"2015-04-03T00:00:00Z","relpermalink":"/post/2015-04-03-hbase-notes/","section":"post","summary":"understand of the HBase data model http://jimbojw.com/#understanding hbase","tags":null,"title":"HBase notes","type":"post"},{"authors":null,"categories":[],"content":" cloud data management https://dataschool.com/data-governance\n三层数据仓库架构 Generally a data warehouses adopts a three-tier architecture. Following are the three tiers of the data warehouse architecture.\n Bottom Tier − The bottom tier of the architecture is the data warehouse database server. It is the relational database system. We use the back end tools and utilities to feed data into the bottom tier. These back end tools and utilities perform the Extract, Clean, Load, and refresh functions.\n Middle Tier − In the middle tier, we have the OLAP Server that can be implemented in either of the following ways.\n Top-Tier − This tier is the front-end client layer. This layer holds the query tools and reporting tools, analysis tools and data mining tools.\n  Star schema vs. Snowflake Schema     Star Schema Snowflake Schema     Understandability Easier for business users and analysts to query data Maybe more difficult for business users and analysts due to a number of tables they have to deal with   Dimension table Only has one dimension table for each dimension that groups related attributes. Dimension tables are not in the third normal form May have more than 1 dimension table for each dimension due to the further normalization of each dimension table. Dimension tables are in the third normal form (3NF)   Query complexity The query is very simple and easy to understand More complex query due to multiple foreign keys joins between dimension tables   Query performance High performance. The database engine can optimize and boost the query performance based on a predictable framework More foreign key joins, therefore, longer execution time of query in compare with star schema   When to use When dimension tables store a relatively small number of rows, space is not a big issue we can use star schema When dimension tables store a large number of rows with redundancy data and space is such an issue, we can choose snowflake schema to save space.   Foreign Key Joins Fewer Joins Higher number of joins   Data warehouse system Work best in any data warehouse/data mart Better for small data warehouse/ data mart    ETL工具    Vendor ETL Product Strengths Weaknesses     Informatica Data Integration Platform Highly rated by analystsExtensive product portfolio Reputation for high prices Overlapping products    基本概念  dimensions vs facts  It may help to think of dimensions as things or objects. A thing such as a product can exist without ever being involved in a business event. A dimension is your noun. It is something that can exist independent of a business event, such as a sale. Products, employees, equipment, are all things that exist. A dimension either does something, or has something done to it.\nEmployees sell, customers buy. Employees and customers are examples of dimensions, they do.\nProducts are sold, they are also dimensions as they have something done to them.\nFacts, are the verb. An entry in a fact table marks a discrete event that happens to something from the dimension table. A product sale would be recorded in a fact table. The event of the sale would be noted by what product was sold, which employee sold it, and which customer bought it. Product, Employee, and Customer are all dimensions that describe the event, the sale.\nIn addition fact tables also typically have some kind of quantitative data. The quantity sold, the price per item, total price, and so on.\n","date":1426040315,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426040315,"objectID":"d7490aa465c12cdb78e61643e26be628","permalink":"https://wubigo.com/post/data-warehouse/","publishdate":"2015-03-11T10:18:35+08:00","relpermalink":"/post/data-warehouse/","section":"post","summary":"cloud data management https://dataschool.com/data-governance\n三层数据仓库架构 Generally a data warehouses adopts a three-tier architecture. Following are the three tiers of the data warehouse architecture.\n Bottom Tier − The bottom tier of the architecture is the data warehouse database server. It is the relational database system. We use the back end tools and utilities to feed data into the bottom tier. These back end tools and utilities perform the Extract, Clean, Load, and refresh functions.","tags":["DATAWAREHOUSE","OLAP","ETL"],"title":"数据仓库","type":"post"},{"authors":null,"categories":null,"content":" 10 Highly Impactful Books You Should Definitely Check Out https://thoughtcatalog.com/ayodeji-awosika/2015/06/10-highly-impactful-books-you-should-definitely-check-out/\n","date":1425340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425340800,"objectID":"9fd7942affbbeffab10c766676cf5991","permalink":"https://wubigo.com/post/2015-03-03-10highly-impactful-books-you-should-definitely-check-out/","publishdate":"2015-03-03T00:00:00Z","relpermalink":"/post/2015-03-03-10highly-impactful-books-you-should-definitely-check-out/","section":"post","summary":"10 Highly Impactful Books You Should Definitely Check Out https://thoughtcatalog.com/ayodeji-awosika/2015/06/10-highly-impactful-books-you-should-definitely-check-out/","tags":null,"title":"10 Highly Impactful Books You Should Definitely Check Out","type":"post"},{"authors":null,"categories":[],"content":" cp data file cp /var/lib/mysql /data -Rf chown -R mysql:mysql /data/mysql  AppArmor /etc/apparmor.d/local/usr.sbin.mysqld\n/data/mysql r, /data/mysql/** rwk,  sudo systemctl reload apparmor  sudo as myql sudo -s -u mysql  mysql 时间类型支持微秒 MySQL permits fractional seconds for TIME, DATETIME, and TIMESTAMP values, with up to microseconds (6 digits) Mysql DATETIME(6) DATETIME[(fsp)] The fsp value, if given, must be in the range 0 to 6. A value of 0 signifies that there is no fractional part. If omitted, the default precision is 0. (This differs from the standard SQL default of 6, for compatibility with previous MySQL versions.)\n检查表大小 select table_schema as database_name, table_name, round( (data_length + index_length) / 1024 / 1024, 2) as total_size, round( (data_length) / 1024 / 1024, 2) as data_size, round( (index_length) / 1024 / 1024, 2) as index_size from information_schema.tables where table_schema not in ('information_schema', 'mysql', 'performance_schema' ,'sys') and table_type = 'BASE TABLE' -- and table_schema = 'your database name' order by total_size desc limit 10;  ","date":1422618240,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1422618240,"objectID":"271c8fa4615efa938fd84b659c1a6ab5","permalink":"https://wubigo.com/post/mysql-relocate-the-data-directory/","publishdate":"2015-01-30T19:44:00+08:00","relpermalink":"/post/mysql-relocate-the-data-directory/","section":"post","summary":"cp data file cp /var/lib/mysql /data -Rf chown -R mysql:mysql /data/mysql  AppArmor /etc/apparmor.d/local/usr.sbin.mysqld\n/data/mysql r, /data/mysql/** rwk,  sudo systemctl reload apparmor  sudo as myql sudo -s -u mysql  mysql 时间类型支持微秒 MySQL permits fractional seconds for TIME, DATETIME, and TIMESTAMP values, with up to microseconds (6 digits) Mysql DATETIME(6) DATETIME[(fsp)] The fsp value, if given, must be in the range 0 to 6. A value of 0 signifies that there is no fractional part.","tags":["MYSQL","SQL"],"title":"Mysql Relocate the Data Directory","type":"post"},{"authors":null,"categories":[],"content":" What is anycast? Anycast, also known as IP anycast, is a networking technique that allows for multiple machines to share the same IP address. Based on the location of the user request, the routers send it to the machine in the network that is closest. This is beneficial since, among other things, it reduces latency and increases redundancy. If a particular data center were to go offline, an anycasted IP would choose the best path for users and automatically redirect them to the next closest data center. The following outlines some of the pros and cons that are associated with configuring anycast.\nPros  Speed. Traffic going to an anycast node will be routed to the nearest node thus reducing latency between the client and the node itself. This ensures that speeds will be optimized no matter where the client is requesting information from. Redundancy. Anycast improves redundancy by placing multiple servers across the globe using the same IP. This allows for traffic to be rerouted to the next nearest server in the case that one server fails or goes offline. DDoS mitigation. DDoS attacks are caused by botnets which can generate so much traffic they overwhelm a typical Unicast machine. The benefit of having an anycast configuration in this situation is that each server is able to \u0026ldquo;absorb\u0026rdquo; a portion of the attack resulting in less strain on the server overall. Load balancing. Load balancing can be utilized in the case that there are multiple nodes all within the same geographic distance from the request. This takes some of the resource requirements off of a singular node and disperses them across multiple nodes.  Cons  Difficult to Implement. Implementing IP anycast is a complex endeavor that requires additional hardware, reliable upstream providers, and proper traffic routing  https://www.linuxjournal.com/magazine/ipv4-anycast-linux-and-quagga\n","date":1402123459,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1402123459,"objectID":"1f1423eb81561f2aadd4da1bd2592996","permalink":"https://wubigo.com/post/anycast-with-ipv4/","publishdate":"2014-06-07T14:44:19+08:00","relpermalink":"/post/anycast-with-ipv4/","section":"post","summary":"What is anycast? Anycast, also known as IP anycast, is a networking technique that allows for multiple machines to share the same IP address. Based on the location of the user request, the routers send it to the machine in the network that is closest. This is beneficial since, among other things, it reduces latency and increases redundancy. If a particular data center were to go offline, an anycasted IP would choose the best path for users and automatically redirect them to the next closest data center.","tags":["BGP","NETWORK"],"title":"泛播IPV4","type":"post"},{"authors":null,"categories":null,"content":" TZ $ export TZ=:/etc/localtime locale /etc/environment: LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8  1: set TZ=:/etc/localtime https://blog.packagecloud.io/eng/2017/02/21/set-environment-variable-save-thousands-of-system-calls/\nmysql utf8 For the recent version of MySQL,\ndefault-character-set = utf8 causes a problem. It\u0026rsquo;s deprecated I think.\nAs Justin Ball says in \u0026ldquo;Upgrade to MySQL 5.5.12 and now MySQL won’t start, you should:\nRemove that directive and you should be good. Then your configuration file (\u0026lsquo;/etc/my.cnf\u0026rsquo; for example) should look like that: [mysqld] collation-server = utf8_unicode_ci init-connect=\u0026lsquo;SET NAMES utf8\u0026rsquo; character-set-server = utf8 Restart MySQL. For making sure, your MySQL is UTF-8, run the following queries in your MySQL prompt: First query:\nmysql\u0026gt; show variables like \u0026lsquo;char%\u0026rsquo;;\ntomcat deploy for dev conf/Catalina/localhost/ROOT.xml \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;Context docBase=\u0026quot;/opt/etender\u0026quot; path=\u0026quot;\u0026quot; /\u0026gt; $TOMCAT_HOME/conf/server.xml \u0026lt;!-- \u0026lt;Context path=\u0026quot;/etender\u0026quot; docBase=\u0026quot;c:/WebRoot\u0026quot;\u0026gt; \u0026lt;/Context\u0026gt; --\u0026gt; \u0026lt;/Host\u0026gt; \u0026lt;/Engine\u0026gt; \u0026lt;/Service\u0026gt; \u0026lt;/Server\u0026gt;  show the last queries executed on MySQL temporarily If you prefer to output to a file:\nSET GLOBAL log_output = \u0026quot;FILE\u0026quot;; which is set by default. #set absolute path will report error,mysql.log=/var/lib/mysql/mysql.log SET GLOBAL general_log_file = \u0026quot;mysql.log\u0026quot;; SET GLOBAL general_log = 'ON'; tail -f /var/lib/mysql/mysql.log  Optimize Your Tomcat Installation on Ubuntu 14.04 hange JVM Heap Setting (-Xms -Xmx) of Tomcat – Configure setenv.sh file\ndefault no setenv.sh file under /bin directory. Have to create one with below parameters.\nXms=Xmx=1/2RAM( avoid having the costly memory allocation process running because the size of the allocated memory will be constant all the time) MaxPermSize=1/2mx\n$cat setenv.sh export CATALINA_OPTS=\u0026ldquo;$CATALINA_OPTS -Xms512m\u0026rdquo; export CATALINA_OPTS=\u0026ldquo;$CATALINA_OPTS -Xmx8192m\u0026rdquo; export CATALINA_OPTS=\u0026ldquo;$CATALINA_OPTS -XX:MaxPermSize=256m\u0026rdquo;\ncatalina.out to verify the setting in effect catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djdk.tls.ephemeralDHKeySize=2048 catalina.startup.VersionLoggerListener.log Command line argument: -Djava.protocol.handler.pkgs=org.apache.catalina.webresources org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Xms4192m org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Xmx4192m org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -XX:MaxPermSize=2000m\noptimize mysql showing current configuration variables mysql\u0026gt;SHOW VARIABLES LIKE \u0026lsquo;%max%\u0026rsquo;;\ninnodb_file_per_table = ON innodb_stats_on_metadata = OFF innodb_buffer_pool_instances = 8 innodb_buffer_pool_size = 1G query_cache_type = 0 query_cache_size = 0 innodb_log_file_size = 5242880 innodb_flush_log_at_trx_commit = 1 # may change to 2 or 0 innodb_flush_method = O_DIRECT\ntomcat upstat on ubuntu14.04 /etc/init/tomcat.conf description \u0026ldquo;Tomcat Server\u0026rdquo;\nstart on runlevel [2345] stop on runlevel [!2345] respawn respawn limit 10 5\nsetuid tomcat setgid tomcat\nenv JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/jre env CATALINA_HOME=/opt/tomcat\n# Modify these options as needed env JAVA_OPTS=\u0026ldquo;-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom\u0026rdquo; env CATALINA_OPTS=\u0026ldquo;-Xms512M -Xmx1024M -server -XX:+UseParallelGC\u0026rdquo;\nexec $CATALINA_HOME/bin/catalina.sh run\n# cleanup temp directory after stop post-stop script rm -rf $CATALINA_HOME/temp/* end script\nsudo initctl reload-configuration Tomcat is ready to be run. Start it with this command: sudo initctl start tomcat\nsudo sh -c \u0026lsquo;echo manual \u0026gt;\u0026gt; /etc/init/tomcat.override\u0026rsquo;\ndeploy web app as the root context in tomcat in the $CATALINA_BASE/conf/[enginename]/[hostname]/ROOT.XML \u0026lt;?xml version=\u0026ldquo;1.0\u0026rdquo; encoding=\u0026ldquo;UTF-8\u0026rdquo;?\u0026gt; authbind tomcat sudo apt install authbind sudo touch /etc/authbind/byport/{443,80} sudo chmod 500 /etc/authbind/byport/{443,80} sudo chown tomcat:tomcat /etc/authbind/byport/{443,80}\nConfigure Tomcat sudo sed -i \u0026rsquo;s/8080/80/g\u0026rsquo; /home/tomcat/apache-tomcat-8.0.35/conf/server.xml sudo sed -i \u0026rsquo;s/8443/443/g\u0026rsquo; /home/tomcat/apache-tomcat-8.0.35/conf/server.xml #TOMCAT_HOME/bin/setenv.sh export CATALINA_OPTS=\u0026ldquo;-Djava.net.preferIPv4Stack=true\u0026rdquo; #startup.sh: exec authbind \u0026ndash;deep \u0026ldquo;$PRGDIR\u0026rdquo;/\u0026ldquo;$EXECUTABLE\u0026rdquo; start \u0026ldquo;$@\u0026rdquo;\nupload big files with Nginx (Reverse proxy+SSL negotiation) and Tomcat solution 1: config nginx $TOMCAT_HOME/bin/server.xml\ndisableUploadTimeout=false In nginx.conf add: http { # at the END of this segment! client_max_body_size 1000m; }\nsolution 2 : config tomcat maxSwallowSize The maximum number of request body bytes (excluding transfer encoding overhead) that will be swallowed by Tomcat for an aborted upload. An aborted upload is when Tomcat knows that the request body is going to be ignored but the client still sends it. If Tomcat does not swallow the body the client is unlikely to see the response. If not specified the default of 2097152 (2 megabytes) will be used. A value of less than zero indicates that no limit should be enforced.\nMySql - changing innodb_file_per_table for a live db solution 1: mysql\u0026gt;set global innodb_file_per_table = 1 (set value to on doesn\u0026rsquo;t effect for mysql 5.5 )\nCross Origin Resource Sharing (CORS) with nginx  location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. # try_files $uri $uri/ =404; # Uncomment to enable naxsi on this location # include /etc/nginx/naxsi.rules add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS, PUT, DELETE';  remove Nginx Server Signature(reset server header in response) /etc/nginx/nginx.conf server_tokens off;  # 跨域的常见解决方法 目前来讲没有不依靠服务器端来跨域请求资源的技术 1.jsonp 需要目标服务器配合一个callback函数。 2.window.name+iframe 需要目标服务器响应window.name。 3.window.location.hash+iframe 同样需要目标服务器作处理。 4.html5的 postMessage+ifrme 这个也是需要目标服务器或者说是目标页面写一个postMessage，主要侧重于前端通讯。 5.CORS 需要服务器设置header ：Access-Control-Allow-Origin。 6.nginx反向代理 这个方法一般很少有人提及，但是他可以不用目标服务器配合，不过需要你搭建一个中转nginx服务器，用于转发请求。\nlocation / { #alias D:\\\\develop\\\\project1dir\\\\appDist\\\\; #此文件夹可以是项目打包后的上线代码文件，也可以是第二个项目源代码文件 # Frontend Server proxy_pass http://localhost:8002/; #前端服务器地址，比如gulp+browser-sync开启的服务器，能看到代码实时更新效果 } location /api/ { rewrite ^/api/(.*)$ /$1 break; #所有对后端的请求加一个api前缀方便区分，真正访问的时候移除这个前缀 # API Server proxy_pass http://serverB.com; #将真正的请求代理到serverB,即真实的服务器地址，ajax的url为/api/user/1的请求将会访问http://www.serverB.com/user/1 }  update time \tsudo ntpdate ntp.ubuntu.com   https://help.ubuntu.com/lts/serverguide/NTP.html  ","date":1399075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1399075200,"objectID":"c28933e174ef766366bbfa6f72dec90a","permalink":"https://wubigo.com/post/2014-05-03-doing_on_ubuntu/","publishdate":"2014-05-03T00:00:00Z","relpermalink":"/post/2014-05-03-doing_on_ubuntu/","section":"post","summary":"TZ $ export TZ=:/etc/localtime locale /etc/environment: LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8  1: set TZ=:/etc/localtime https://blog.packagecloud.io/eng/2017/02/21/set-environment-variable-save-thousands-of-system-calls/\nmysql utf8 For the recent version of MySQL,\ndefault-character-set = utf8 causes a problem. It\u0026rsquo;s deprecated I think.\nAs Justin Ball says in \u0026ldquo;Upgrade to MySQL 5.5.12 and now MySQL won’t start, you should:\nRemove that directive and you should be good. Then your configuration file (\u0026lsquo;/etc/my.cnf\u0026rsquo; for example) should look like that: [mysqld] collation-server = utf8_unicode_ci init-connect=\u0026lsquo;SET NAMES utf8\u0026rsquo; character-set-server = utf8 Restart MySQL.","tags":null,"title":"doing_on_ubuntu","type":"post"},{"authors":null,"categories":["IT"],"content":" The main point of a Content Distribution Network (CDN) is to put the content as close to the end-user as possible, thereby reducing the Distance component of the Round Trip Time (RTT) and speeding up the request. Simply serving static content from a s# sub-domain for CDN.\nThe advantages of serving content from such a sub-domain, however, are that\nThe sub-domain can be a cookie-less domain\nIf you use your cookies correctly (ie. don\u0026rsquo;t have any *.mydomain.com cookies), you can dramatically reduce the size (ie. number of packets sent) of the HTTP request, which would save on bandwidth and speed up requests significantly if you use cookies heavily on the main site. The page can benefit from a more simultaneous requests being made by the browser\nMost browsers will make simultaneous requests for page assets, like images, fonts, CSS, etc. The catch is that most browsers will only allow a limited number of open requests to a particular domain (somewhere around 5 I think). By spreading your assets across multiple sub-domains, you \u0026ldquo;trick\u0026rdquo; the browser, and allow more parallel requests, since the limit applies to each sub-domain.\nAdding an Alternate Domain Name Using the CloudFront Configure the DNS service for the domain to route traffic for the domain, such as example.com, to the CloudFront domain name for your distribution, such as d111111abcdef8.cloudfront.net. The method that you use depends on whether you\u0026rsquo;re using Amazon Route 53 as the DNS service provider for the domain:\nAmazon Route 53 Create an alias resource record set. With an alias resource record set, you don\u0026rsquo;t pay for Amazon Route 53 queries. In addition, you can create an alias resource record set for the root domain name (example.com), which DNS doesn\u0026rsquo;t allow for CNAMEs. For more information, see Routing Queries to an Amazon CloudFront Distribution in the Amazon Route 53 Developer Guide.\nAnother DNS service provider Use the method provided by your DNS service provider to add a CNAME resource record set to the hosted zone for your domain. This new CNAME resource record set will redirect DNS queries from your domain (for example, www.example.com) to the CloudFront domain name for your distribution (for example, d111111abcdef8.cloudfront.net). For more information, see the documentation provided by your DNS service provider.\nImportant If you already have an existing CNAME record for your domain name, update that resource record set or replace it with a new one that points to the CloudFront domain name for your distribution. In addition, confirm that your CNAME resource record set points to your distribution\u0026rsquo;s domain name and not to one of your origin servers.\nsteps to add cname record step 1: edit cloudfront distribution then add a Alternate Domain Name for cloudfront\nstep 2: add a CNAME resource record set by the DNS service provider\n","date":1396525143,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396525143,"objectID":"687bdf0d2dfbe65200fb105b462c6165","permalink":"https://wubigo.com/post/2014-04-03-cdn/","publishdate":"2014-04-03T19:39:03+08:00","relpermalink":"/post/2014-04-03-cdn/","section":"post","summary":"The main point of a Content Distribution Network (CDN) is to put the content as close to the end-user as possible, thereby reducing the Distance component of the Round Trip Time (RTT) and speeding up the request. Simply serving static content from a s# sub-domain for CDN.\nThe advantages of serving content from such a sub-domain, however, are that\nThe sub-domain can be a cookie-less domain\nIf you use your cookies correctly (ie.","tags":["IAAS","NETWORK"],"title":"cdn note","type":"post"},{"authors":null,"categories":null,"content":"1: all required dep on jsp tomcat\n\u0026lt;!-- Compile --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jstl\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Provided --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.embed\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat-embed-jasper\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;  ","date":1393804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393804800,"objectID":"c8218b6eddffe204e9229f561b8b4353","permalink":"https://wubigo.com/post/2014-03-03-boot/","publishdate":"2014-03-03T00:00:00Z","relpermalink":"/post/2014-03-03-boot/","section":"post","summary":"1: all required dep on jsp tomcat\n\u0026lt;!-- Compile --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jstl\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Provided --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.embed\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat-embed-jasper\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;  ","tags":null,"title":"SPRING-boot note","type":"post"},{"authors":null,"categories":null,"content":" Documenting the Web together https://blogs.windows.com/msedgedev/2017/10/18/documenting-web-together-mdn-web-docs/\nMDN https://developer.mozilla.org\nDEV DOCS https://devdocs.io\n","date":1393804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393804800,"objectID":"31a1ec70c56747fca48bf551e86a68e8","permalink":"https://wubigo.com/post/2014-03-03-web-api-reference/","publishdate":"2014-03-03T00:00:00Z","relpermalink":"/post/2014-03-03-web-api-reference/","section":"post","summary":"Documenting the Web together https://blogs.windows.com/msedgedev/2017/10/18/documenting-web-together-mdn-web-docs/\nMDN https://developer.mozilla.org\nDEV DOCS https://devdocs.io","tags":null,"title":"web API reference","type":"post"},{"authors":null,"categories":[],"content":" JAVA 基础 JAVA 基础\n","date":1393571995,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393571995,"objectID":"7d63ec946220b61371d44d011cc913cb","permalink":"https://wubigo.com/post/effective-coding-java/","publishdate":"2014-02-28T15:19:55+08:00","relpermalink":"/post/effective-coding-java/","section":"post","summary":"JAVA 基础 JAVA 基础","tags":["LANG","JAVA"],"title":"Effective Coding Java","type":"post"},{"authors":null,"categories":null,"content":" 1: ArangoDB automatically indexes the _key attribute in each collection. There is no need to index this attribute separately. Please note that a document\u0026rsquo;s _id attribute is derived from the _key attribute, and is thus implicitly indexed, too.\n2: Creating a database\nWe don’t want to mess with any existing data, so let’s start by creating a new database called “mydb”:\ndb.createDatabase(\u0026lsquo;mydb\u0026rsquo;).then( () =\u0026gt; console.log(\u0026lsquo;Database created\u0026rsquo;), err =\u0026gt; console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err) );\nBecause we’re trying to actually do something on the server, this action is asynchronous. All asynchronous methods in the ArangoDB driver return promises but you can also pass a node-style callback instead:\ndb.createDatabase(\u0026lsquo;mydb\u0026rsquo;, function (err) { if (!err) console.log(\u0026lsquo;Database created\u0026rsquo;); else console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err); });\nKeep in mind that the new database you’ve created is only available once the callback is called or the promise is resolved. Throughout this tutorial we’ll use the promise API because they’re available in recent versions of Node.js as well as most modern browsers.\n3: db = require(\u0026lsquo;arangojs\u0026rsquo;).Database; db = new Database(\u0026lsquo;http://127.0.0.1:8529'); db.useBasicAuth(\u0026lsquo;root\u0026rsquo;, \u0026lsquo;123123\u0026rsquo;); db.createDatabase(\u0026lsquo;mydb\u0026rsquo;).then( () =\u0026gt; console.log(\u0026lsquo;Database created\u0026rsquo;), err =\u0026gt; console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err) );\ncheck arangodb status /etc/init.d/arangodb3 status\nenable remote connection /etc/arangodb3/arangod.conf #endpoint = tcp://[::]:8529\n","date":1388707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388707200,"objectID":"eda9722e1bb5103bd8e54f8995cb5bb1","permalink":"https://wubigo.com/post/2014-01-03-arangodb/","publishdate":"2014-01-03T00:00:00Z","relpermalink":"/post/2014-01-03-arangodb/","section":"post","summary":"1: ArangoDB automatically indexes the _key attribute in each collection. There is no need to index this attribute separately. Please note that a document\u0026rsquo;s _id attribute is derived from the _key attribute, and is thus implicitly indexed, too.\n2: Creating a database\nWe don’t want to mess with any existing data, so let’s start by creating a new database called “mydb”:\ndb.createDatabase(\u0026lsquo;mydb\u0026rsquo;).then( () =\u0026gt; console.log(\u0026lsquo;Database created\u0026rsquo;), err =\u0026gt; console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err) );","tags":null,"title":"arrangodb note","type":"post"},{"authors":null,"categories":null,"content":" 自动化发布 很多网站选择周四作为发布日\n","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"ec2354ee671052fa4120517131a73ab8","permalink":"https://wubigo.com/post/2014-01-01-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/post/2014-01-01-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/","section":"post","summary":"自动化发布 很多网站选择周四作为发布日","tags":null,"title":"大型网站技术架构","type":"post"},{"authors":null,"categories":[],"content":" QEMU runs the guest code via the KVM kernel module. When working with KVM, QEMU also does I/O emulation, I/O device setup, live migration, and so on. QEMU opens the device file (/dev/kvm) exposed by the KVM kernel module and executes ioctls()on it\nKVM exposesa device file called /dev/kvmto applications to make use of the ioctls()provided. QEMU makes use of this device file to talk with KVM and to create, initialize, and manage the kernel mode context of virtual machines\nThe KVM or kernel-based virtual machine is not a full hypervisor; however, with the help of QEMU and emulators (a slightly modified QEMU for I/O device emulation and BIOS), it can become one. KVM needs hardware virtualization-capable processors to operate. Using these capabilities, KVM turns the standard Linux kernel into a hypervisor. When KVM runs virtual machines, every VM is a normal Linux process, which can obviously be scheduled to run on a CPU by the host kernel as with any other process present in the host kernel\nKVM APIs there are three main types of ioctl()s Three sets of ioctl make up the KVM API. The KVM API is a set of ioctls that are issued to control various aspects of a virtual machine. These ioctls belong to three classes: System ioctls: These query and set global attributes, which affect the whole KVM subsystem. In addition, a system ioctl is used to create virtual machines. VM ioctls: These query and set attributes that affect an entire virtual machine—for example, memory layout. In addition, a VM ioctl is used to create virtual CPUs (vCPUs). It runs VM ioctls from the same process (address space) that was used to create the VM. Vcpu ioctls: These query and set attributes that control the operation of a single virtual CPU. They run vCPU ioctls from the same thread that was used to create the vCPU.\n","date":1383618907,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383618907,"objectID":"bb0b758699de338eccfd77ff6733d086","permalink":"https://wubigo.com/post/kvm/","publishdate":"2013-11-05T10:35:07+08:00","relpermalink":"/post/kvm/","section":"post","summary":"QEMU runs the guest code via the KVM kernel module. When working with KVM, QEMU also does I/O emulation, I/O device setup, live migration, and so on. QEMU opens the device file (/dev/kvm) exposed by the KVM kernel module and executes ioctls()on it\nKVM exposesa device file called /dev/kvmto applications to make use of the ioctls()provided. QEMU makes use of this device file to talk with KVM and to create, initialize, and manage the kernel mode context of virtual machines","tags":["KVM","IAAS"],"title":"Kvm","type":"post"},{"authors":null,"categories":[],"content":" JAVA 开发规范 目标：\n 容易维护 健壮 可复用     分类 指导     编码 UTF-8   注释 支持Swagger   git模式 PR(pull-request) / merge request   代码静态检查 遵守风格规范   代码提交 必须通过review   测试 所有代码都有单元测试   集成测试 所有代码必须通过集成测试才能提交到分支或主干    编码风格\n","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"fcc1bd42b5d7776465e12b1a08ddb837","permalink":"https://wubigo.com/post/java-coding-guidelines/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/java-coding-guidelines/","section":"post","summary":"JAVA 开发规范 目标：\n 容易维护 健壮 可复用     分类 指导     编码 UTF-8   注释 支持Swagger   git模式 PR(pull-request) / merge request   代码静态检查 遵守风格规范   代码提交 必须通过review   测试 所有代码都有单元测试   集成测试 所有代码必须通过集成测试才能提交到分支或主干    编码风格","tags":["JAVA","CODE"],"title":"Java编码规范","type":"post"},{"authors":null,"categories":[],"content":" kafka broker connected remotely by ip config/server.properties\nadvertised.listeners=PLAINTEXT://172.16.16.5:9092 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=168  admin topic  list all topic\n./kafka-topics.sh --bootstrap-server 172.16.16.5:9092 --list  delete topic\n  delete.topic.enable option set to true\nDeleting a topic will also delete all its messages. This is not a reversible operation\n./kafka-topics.sh --bootstrap-server 172.16.16.5:9092 --delete --topic my-topic   describe topic\n./kafka-topics.sh --bootstrap-server 172.16.16.5:9092 --describe --topic alert1   admin consume group  list\n./kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --list  ./kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --describe --group group_id  ./kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --describe --group group_id --members CONSUMER-ID HOST CLIENT-ID #PARTITIONS consumer-2-c76c0c52-35e2-487e-9c63-39719cb2560b /192.168.200.67 consumer-2 0 consumer-2-25c4357a-38e4-4258-aa05-9072a824e03a /192.168.200.67 consumer-2 1  ./kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --describe --group group_id --members --verbose CONSUMER-ID HOST CLIENT-ID #PARTITIONS ASSIGNMENT consumer-2-c76c0c52-35e2-487e-9c63-39719cb2560b /192.168.200.67 consumer-2 0 - consumer-2-25c4357a-38e4-4258-aa05-9072a824e03a /192.168.200.67 consumer-2 1 users(0)  reset offset\n  test\nkafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --reset-offsets --group group_id --topic users --to-earliest --dry-run  execute\nkafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --reset-offsets --group group_id --topic users --to-earliest --execute  produce msg from console ./kafka-console-producer.sh --broker-list 172.16.16.5:9092 --topic users  consume msg from console  ./kafka-console-consumer.sh --bootstrap-server 172.16.16.5:9092 --topic users  spring boot with spring kafka Spring for Apache Kafka 2.0.x is not compatible with Spring Boot 2.1.x. You have to use Spring-Kafka 2.2.x. More over would be better to just rely on the dependency from Spring Boot per se. please, see https://start.spring.io/ for more info how properly start the project for Spring Boot.\noffset can\u0026rsquo;t be reset to earliest after log.rention period pass kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --reset-offsets --group timon-alert --topic rawdata --to-earliest --execute TOPIC PARTITION NEW-OFFSET rawdata 0 174  counters for number of messages received since start-up kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 172.16.16.5:9092 --topic testrawdata --time -1 --offsets 1  Kafka consumer list kafka-consumer-groups.sh --list --bootstrap-server 172.16.16.5:9092  zkServer.sh start kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties  delete consumer group kafka-consumer-groups.sh --delete --bootstrap-server 172.16.16.5:9092 --group timon-raw  ","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"295678ee8ea547fa703a2f7ad5092651","permalink":"https://wubigo.com/post/kafka-config/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/kafka-config/","section":"post","summary":"kafka broker connected remotely by ip config/server.properties\nadvertised.listeners=PLAINTEXT://172.16.16.5:9092 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.","tags":["KAFKA","MQ"],"title":"Kafka Config","type":"post"},{"authors":null,"categories":[],"content":"    rabbit kafka     创建时间 2007 2011   开发语言 erlang scala   AMQP SUPPORT NO   AGENT SMART(broker-centric) keeps track of consumer state dumb(producer-centric)   存储空间 in-memory disk   INGRESS VOLUME 20K messages/sec 100k/sec messages/sec   CONSUMERS mostly online(balancing load to many consumer) online and batch consumer   ROUTING exchange, binding simple   history N/A replay(删除by size 或时间)   数据压缩 N Y   SPRING SUPPORT weak strong   安全 RBAC backed by a built-in data store, LDAP JAAS role based access   管理 Web 和 CLI JMX 和 CLI    ","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"4c8acf1d549a1d6e8cb656a9afa85f2c","permalink":"https://wubigo.com/post/kafka-vs-rabbit/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/kafka-vs-rabbit/","section":"post","summary":"    rabbit kafka     创建时间 2007 2011   开发语言 erlang scala   AMQP SUPPORT NO   AGENT SMART(broker-centric) keeps track of consumer state dumb(producer-centric)   存储空间 in-memory disk   INGRESS VOLUME 20K messages/sec 100k/sec messages/sec   CONSUMERS mostly online(balancing load to many consumer) online and batch consumer   ROUTING exchange, binding simple   history N/A replay(删除by size 或时间)   数据压缩 N Y   SPRING SUPPORT weak strong   安全 RBAC backed by a built-in data store, LDAP JAAS role based access   管理 Web 和 CLI JMX 和 CLI    ","tags":["SHELL","MQ"],"title":"Kafka vs Rabbit","type":"post"},{"authors":null,"categories":[],"content":"阿里云Maven中央仓库为 阿里云云效 提供的公共代理仓库\n打开 maven 的配置文件（ windows 机器一般在 maven 安装目录的 conf/settings.xml ），\n在标签中添加 mirror 子节点:\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;aliyunmaven\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;阿里云公共仓库\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;https://maven.aliyun.com/repository/public\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt;  ","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"4aa905e02147651434be7da9bdcb46f9","permalink":"https://wubigo.com/post/maven-mirror/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/maven-mirror/","section":"post","summary":"阿里云Maven中央仓库为 阿里云云效 提供的公共代理仓库\n打开 maven 的配置文件（ windows 机器一般在 maven 安装目录的 conf/settings.xml ），\n在标签中添加 mirror 子节点:\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;aliyunmaven\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;阿里云公共仓库\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;https://maven.aliyun.com/repository/public\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt;  ","tags":["JAVA","MAVEN"],"title":"Maven Mirror","type":"post"},{"authors":null,"categories":[],"content":" mysqladmin status  MySQL is multithreaded, so there may be many clients issuing queries for a given table simultaneously. To minimize the problem with multiple client sessions having different states on the same table, the table is opened independently by each concurrent session. This uses additional memory but normally increases performance\nThe table_open_cache and max_connections system variables affect the maximum number of files the server keeps open. If you increase one or both of these values, you may run up against a limit imposed by your operating system on the per-process number of open file descriptors. Many operating systems permit you to increase the open-files limit, although the method varies widely from system to system. Consult your operating system documentation to determine whether it is possible to increase the limit and how to do so.\nTo determine whether your table cache is too small, check the Opened_tables status variable, which indicates the number of table-opening operations since the server started:\nmysql\u0026gt; SHOW GLOBAL STATUS LIKE 'Opened_tables';  ","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"abf5ecc089ebee4423bee7d57de0e064","permalink":"https://wubigo.com/post/mysql-tuning-on-many-tables/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/mysql-tuning-on-many-tables/","section":"post","summary":"mysqladmin status  MySQL is multithreaded, so there may be many clients issuing queries for a given table simultaneously. To minimize the problem with multiple client sessions having different states on the same table, the table is opened independently by each concurrent session. This uses additional memory but normally increases performance\nThe table_open_cache and max_connections system variables affect the maximum number of files the server keeps open. If you increase one or both of these values, you may run up against a limit imposed by your operating system on the per-process number of open file descriptors.","tags":["SHELL","MYSQL"],"title":"Mysql Tuning on Many Tables","type":"post"},{"authors":null,"categories":[],"content":"innodb-memcached-multiple-get-range-query  native partitioning in-place APIs\n","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"64459f7f959493592c0eafcf23409fc7","permalink":"https://wubigo.com/post/mysql-tuning-on-query/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/mysql-tuning-on-query/","section":"post","summary":"innodb-memcached-multiple-get-range-query  native partitioning in-place APIs","tags":["SHELL","MYSQL"],"title":"Mysql Tuning on Query","type":"post"},{"authors":null,"categories":[],"content":" install curl -O http://download.redis.io/redis-stable.tar.gz tar xzvf redis-stable.tar.gz cd redis-stable make make test sudo make install  config sudo mkdir /etc/redis sudo cp redis-stable/redis.conf /etc/redis sudo adduser --system --group --no-create-home redis sudo mkdir /var/lib/redis sudo chown redis:redis /var/lib/redis sudo chmod 770 /var/lib/redis  /etc/redis/redis.conf\nsupervised systemd dir /var/lib/redis # bind localhost  start redis-server /etc/redis/redis.conf  shutdown redis-cli shutdown redis-cli 127.0.0.1:6379\u0026gt; shutdown  run in docker docker run --name redis -network host -v /var/lib/redis:/data /etc/redis/redis.conf:/etc/redis/redis.conf -d redis:5.0-alpine3.9 redis-server /etc/redis/redis.conf --appendonly yes  https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-redis-on-ubuntu-16-04\n","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"bbf927940d7fbfc43485a951a3d53874","permalink":"https://wubigo.com/post/redis-install-ubuntu/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/redis-install-ubuntu/","section":"post","summary":"install curl -O http://download.redis.io/redis-stable.tar.gz tar xzvf redis-stable.tar.gz cd redis-stable make make test sudo make install  config sudo mkdir /etc/redis sudo cp redis-stable/redis.conf /etc/redis sudo adduser --system --group --no-create-home redis sudo mkdir /var/lib/redis sudo chown redis:redis /var/lib/redis sudo chmod 770 /var/lib/redis  /etc/redis/redis.conf\nsupervised systemd dir /var/lib/redis # bind localhost  start redis-server /etc/redis/redis.conf  shutdown redis-cli shutdown redis-cli 127.0.0.1:6379\u0026gt; shutdown  run in docker docker run --name redis -network host -v /var/lib/redis:/data /etc/redis/redis.","tags":["REDIS","MYSQL"],"title":"Redis Install Ubuntu","type":"post"},{"authors":null,"categories":[],"content":" [mysqld] server-id = 2 relay-log-index = slave-relay-bin.index relay-log = slave-relay-bin  mysql\u0026gt;CHANGE MASTER TO MASTER_HOST = 'db2',MASTER_PORT = 3306, MASTER_USER = 'repl_user', MASTER_PASSWORD = 'xyzzy';  Connecting the Master mysql\u0026gt; START SLAVE;  ","date":1367053687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367053687,"objectID":"1a359b933d9a8ed2d9167b5509a43eb6","permalink":"https://wubigo.com/post/mysql-slave/","publishdate":"2013-04-27T17:08:07+08:00","relpermalink":"/post/mysql-slave/","section":"post","summary":" [mysqld] server-id = 2 relay-log-index = slave-relay-bin.index relay-log = slave-relay-bin  mysql\u0026gt;CHANGE MASTER TO MASTER_HOST = 'db2',MASTER_PORT = 3306, MASTER_USER = 'repl_user', MASTER_PASSWORD = 'xyzzy';  Connecting the Master mysql\u0026gt; START SLAVE;  ","tags":["SHELL","MYSQL"],"title":"Mysql Slave","type":"post"},{"authors":null,"categories":[],"content":" Server and Operating System  Kernel – vm.swappiness  Disables swapping completely while 1 causes the kernel to perform the minimum amount of swapping\n# Set the swappiness value as root echo 1 \u0026gt; /proc/sys/vm/swappiness # Alternatively, using sysctl sysctl -w vm.swappiness=1 # Verify the change cat /proc/sys/vm/swappiness 1 # Alternatively, using sysctl sysctl vm.swappiness vm.swappiness = 1  Filesystems – XFS/ext4/ZFS     FILE SIZE mount option     EXT4 16TB noatime,data=writeback,barrier=0,nobh,errors=remount-ro   XFS 8EiB defaults,nobarrier    Disk Subsystem – I/O scheduler Most modern Linux distributions come with noop or deadline I/O schedulers by default, both providing better performance than the cfq and anticipatory ones\n# View the I/O scheduler setting. The value in square brackets shows the running scheduler cat /sys/block/sdb/queue/scheduler noop deadline [cfq] # Change the setting sudo echo noop \u0026gt; /sys/block/sdb/queue/scheduler  Disk Subsystem – Volume optimization separation of OS and data partitions, not just logically but physically, will improve database performance. The RAID level can also have an impact: RAID-5 should be avoided as the checksum needed to ensure integrity is costly\nSystem Architecture – NUMA settings the innodb_numa_interleave option to be available, MySQL must be compiled on a NUMA-enabled Linux system\n","date":1367053687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367053687,"objectID":"e8ab0b0043c9320b772863ab5fb384c2","permalink":"https://wubigo.com/post/mysql-tuning-on-os/","publishdate":"2013-04-27T17:08:07+08:00","relpermalink":"/post/mysql-tuning-on-os/","section":"post","summary":"Server and Operating System  Kernel – vm.swappiness  Disables swapping completely while 1 causes the kernel to perform the minimum amount of swapping\n# Set the swappiness value as root echo 1 \u0026gt; /proc/sys/vm/swappiness # Alternatively, using sysctl sysctl -w vm.swappiness=1 # Verify the change cat /proc/sys/vm/swappiness 1 # Alternatively, using sysctl sysctl vm.swappiness vm.swappiness = 1  Filesystems – XFS/ext4/ZFS     FILE SIZE mount option     EXT4 16TB noatime,data=writeback,barrier=0,nobh,errors=remount-ro   XFS 8EiB defaults,nobarrier    Disk Subsystem – I/O scheduler Most modern Linux distributions come with noop or deadline I/O schedulers by default, both providing better performance than the cfq and anticipatory ones","tags":["SHELL","MYSQL"],"title":"Mysql Tuning on OS","type":"post"},{"authors":null,"categories":[],"content":" mysql 8 测试环境快速搭建(WSL/root远程访问) sudo apt install -y mysql-server mysql --version sudo mysql mysql\u0026gt;CREATE USER 'root'@'%' IDENTIFIED BY '123'; mysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%'; mysql\u0026gt;flush PRIVILEGES  mysql.conf.d/mysqld.cnf :32:bind-address = 0.0.0.0\nsudo service mysql restart  Create the root user (yes, a new user because what exists is \u0026lsquo;root@localhost\u0026rsquo; which is local access only)\nroot用户本地登录 本地登录使用系统认证(auth_socket)\nsudo mysql  root用户远程登录 mysql -u root -p'123' -h 192.168.168.128  mysql\u0026gt; select user,host,plugin from mysql.user; +------------------+-----------+-----------------------+ | user | host | plugin | +------------------+-----------+-----------------------+ | root | % | caching_sha2_password | | debian-sys-maint | localhost | caching_sha2_password | | mysql.infoschema | localhost | caching_sha2_password | | mysql.session | localhost | caching_sha2_password | | mysql.sys | localhost | caching_sha2_password | | root | localhost | auth_socket | +------------------+-----------+-----------------------+ mysqldump miaosha \u0026gt; dump.sql mysql -u root -p'123' -h 192.168.168.128 miaosha \u0026lt; miaosha.sql  index buffer Depends on Storage Engine\nMyISAM (Caches Index Pages From .MYI files)\nSELECT FLOOR(SUM(index_length)/POWER(1024,2)) IndexSizesMB FROM information_schema.tables WHERE engine='MyISAM' AND table_schema NOT IN ('information_schema','performance_schema','mysql');  Subtract that from key_buffer_size.\nInnoDB (Caches Data and Index Pages)\nSELECT FLOOR(SUM(data_length+index_length)/POWER(1024,2)) InnoDBSizeMB FROM information_schema.tables WHERE engine='InnoDB';  Subtract that from innodb_buffer_pool_size.\nconvert all tables from InnoDB into MyISAM SET @DATABASE_NAME = 'guowang'; SELECT CONCAT('ALTER TABLE `', table_name, '` ENGINE=MyISAM;') AS sql_statements FROM information_schema.tables AS tb WHERE table_schema = @DATABASE_NAME AND `ENGINE` = 'InnoDB' AND `TABLE_TYPE` = 'BASE TABLE' ORDER BY table_name DESC;   mysql\u0026gt;show full processlist; mysql\u0026gt;Select concat('KILL ',id,';') from information_schema.processlist where user='root';  SHOW ENGINE INNODB STATUS  a method to find the best prefix length for a given column SELECT ROUND(SUM(LENGTH(`sno`)\u0026lt;10)*100/COUNT(`sno`),2) AS pct_length_10, ROUND(SUM(LENGTH(`sno`)\u0026lt;20)*100/COUNT(`sno`),2) AS pct_length_20, ROUND(SUM(LENGTH(`sno`)\u0026lt;50)*100/COUNT(`sno`),2) AS pct_length_50, ROUND(SUM(LENGTH(`sno`)\u0026lt;100)*100/COUNT(`sno`),2) AS pct_length_100 FROM `bs`;  show to file mysql --table -e \u0026quot;sELECT nbiot_create_time ,metric , totalTime FROM v_sel_g550 g WHERE g.nbiot_create_time \u0026gt; '2019-04-04'\u0026quot; -u root -p123456 guowang \u0026gt; guowang-status.txt  sELECT nbiot_create_time ,metric , totalTime FROM v_sel_g550 g\nWHERE g.nbiot_create_time \u0026gt; \u0026lsquo;2019-04-04\u0026rsquo;\nCREATE TABLE ` v_sel_g550` ( `id` int(32) NOT NULL AUTO_INCREMENT, `nbiot_create_time` timestamp, `metric` varchar(50), `totalTime` int, PRIMARY KEY (`id`) USING BTREE, ) ENGINE = InnoDB  ","date":1366621687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366621687,"objectID":"b5e999882747164a6eba2b96a8d6e5cc","permalink":"https://wubigo.com/post/mysql-5.7-innodb/","publishdate":"2013-04-22T17:08:07+08:00","relpermalink":"/post/mysql-5.7-innodb/","section":"post","summary":"mysql 8 测试环境快速搭建(WSL/root远程访问) sudo apt install -y mysql-server mysql --version sudo mysql mysql\u0026gt;CREATE USER 'root'@'%' IDENTIFIED BY '123'; mysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%'; mysql\u0026gt;flush PRIVILEGES  mysql.conf.d/mysqld.cnf :32:bind-address = 0.0.0.0\nsudo service mysql restart  Create the root user (yes, a new user because what exists is \u0026lsquo;root@localhost\u0026rsquo; which is local access only)\nroot用户本地登录 本地登录使用系统认证(auth_socket)\nsudo mysql  root用户远程登录 mysql -u root -p'123' -h 192.168.168.128  mysql\u0026gt; select user,host,plugin from mysql.","tags":["SHELL","MYSQL"],"title":"Mysql 5.7 InnoDB","type":"post"},{"authors":null,"categories":[],"content":" mysql install ubuntu 16.04 install mysql 5.7 at default\nsudo apt-get update sudo apt-get install mysql-server   Enable root remote connection\nmysql -u root -p mysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '\u0026lt;password\u0026gt;' WITH GRANT OPTION; mysql\u0026gt;FLUSH PRIVILEGES;   SHOW current setting mysql\u0026gt; SHOW VARIABLES WHERE Variable_name LIKE 'innodb%';  MySQL 5.7 has significantly better default values. the following variables are set by default:\n[mysqld] innodb_buffer_pool_instances=8 innodb_flush_method=O_DIRECT  setting of mysql 5.7 Use utf8mb4 to fully support Unicode [client] default-character-set=utf8mb4 [mysql] default-character-set=utf8mb4 [mysqld] character-set-client-handshake = FALSE character-set-server = utf8mb4 collation-server = utf8mb4_unicode_ci  mysql\u0026gt; SHOW VARIABLES WHERE Variable_name LIKE 'character\\_set\\_%' OR Variable_name LIKE 'collation%'; +--------------------------+--------------------+ | Variable_name | Value | +--------------------------+--------------------+ | character_set_client | utf8mb4 | | character_set_connection | utf8mb4 | | character_set_database | utf8mb4 | | character_set_filesystem | binary | | character_set_results | utf8mb4 | | character_set_server | utf8mb4 | | character_set_system | utf8 | | collation_connection | utf8mb4_unicode_ci | | collation_database | utf8mb4_unicode_ci | | collation_server | utf8mb4_unicode_ci | +--------------------------+--------------------+  Repair and optimize all tables to ensure there is no side effect.\nmysqlcheck -u root -p --auto-repair --optimize --all-databases  MySQL 5.7 Performance Tuning after installation  Basic settings  innodb_buffer_pool_size: this is the #1 setting to look at for any installation using InnoDB. The buffer pool is where data and indexes are cached: having it as large as possible will ensure you use memory and not disks for most read operations. Typical values are 5-6GB (8GB RAM), 20-25GB (32GB RAM), 100-120GB (128GB RAM).\ninnodb_log_file_size: this is the size of the redo logs. The redo logs are used to make sure writes are fast and durable and also during crash recovery. Up to MySQL 5.1, it was hard to adjust, as you wanted both large redo logs for good performance and small redo logs for fast crash recovery. Fortunately crash recovery performance has improved a lot since MySQL 5.5 so you can now have good write performance and fast crash recovery. Until MySQL 5.5 the total redo log size was limited to 4GB (the default is to have 2 log files). This has been lifted in MySQL 5.6.\nStarting with innodb_log_file_size = 512M (giving 1GB of redo logs) should give you plenty of room for writes. If you know your application is write-intensive and you are using MySQL 5.6, you can start with innodb_log_file_size = 4G.\nmax_connections: if you are often facing the ‘Too many connections’ error, max_connections is too low. It is very frequent that because the application does not close connections to the database correctly, you need much more than the default 151 connections. The main drawback of high values for max_connections (like 1000 or more) is that the server will become unresponsive if for any reason it has to run 1000 or more active transactions. Using a connection pool at the application level or a thread pool at the MySQL level can help here.\nhttps://www.percona.com/blog/2016/10/12/mysql-5-7-performance-tuning-immediately-after-installation/\n[mysqld] # other variables here innodb_buffer_pool_size = 1G # (adjust value here, 50%-70% of total RAM) innodb_log_file_size = 256M innodb_flush_log_at_trx_commit = 1 # may change to 2 or 0 innodb_flush_method = O_DIRECT     Variable Value     innodb_buffer_pool_size Start with 50% 70% of total RAM. Does not need to be larger than the database size   innodb_flush_log_at_trx_commit 1 (Default) 0/2 (more performance, less reliability)   innodb_log_file_size 128M – 2G (does not need to be larger than buffer pool)   innodb_flush_method O_DIRECT (avoid double buffering)    # InnoDB settings\n innodb_file_per_table  With MySQL 5.6, the default value is ON so you have nothing to do in most cases\n innodb_flush_log_at_trx_commit  the default setting of 1 means that InnoDB is fully ACID compliant. It is the best value when your primary concern is data safety, for instance on a master. However it can have a significant overhead on systems with slow disks because of the extra fsyncs that are needed to flush each change to the redo logs. Setting it to 2 is a bit less reliable because committed transactions will be flushed to the redo logs only once a second, but that can be acceptable on some situations for a master and that is definitely a good value for a replica. 0 is even faster but you are more likely to lose some data in case of a crash: it is only a good value for a replica.\n innodb_flush_method  this setting controls how data and logs are flushed to disk. Popular values are O_DIRECT when you have a hardware RAID controller with a battery-protected write-back cache and fdatasync (default value) for most other scenarios. sysbench is a good tool to help you choose between the 2 values.\n innodb_log_buffer_size  this is the size of the buffer for transactions that have not been committed yet. The default value (1MB) is usually fine but as soon as you have transactions with large blob/text fields, the buffer can fill up very quickly and trigger extra I/O load. Look at the Innodb_log_waits status variable and if it is not 0, increase innodb_log_buffer_size.\n# other setting\n query_cache_type  disables the query cache, as does setting query_cache_type=0. By default, the query cache is disabled.\n query_cache_size  the query cache is a well known bottleneck that can be seen even when concurrency is moderate. The best option is to disable it from day 1 by setting query_cache_size = 0 (now the default on MySQL 5.6) and to use other ways to speed up read queries: good indexing, adding replicas to spread the read load or using an external cache (memcache or redis for instance). If you have already built your MySQL application with the query cache enabled and if you have never noticed any problem, the query cache may be beneficial for you. So you should be cautious if you decide to disable it.\n log_bin  enabling binary logging is mandatory if you want the server to act as a replication master. If so, don’t forget to also set server_id to a unique value. It is also useful for a single server when you want to be able to do point-in-time recovery: restore your latest backup and apply the binary logs. Once created, binary log files are kept forever. So if you do not want to run out of disk space, you should either purge old files with PURGE BINARY LOGS or set expire_logs_days to specify after how many days the logs will be automatically purged.\nBinary logging however is not free, so if you do not need for instance on a replica that is not a master, it is recommended to keep it disabled.\n skip_name_resolve  when a client connects, the server will perform hostname resolution, and when DNS is slow, establishing the connection will become slow as well. It is therefore recommended to start the server with skip-name-resolve to disable all DNS lookups. The only limitation is that the GRANT statements must then use IP addresses only, so be careful when adding this setting to an existing system.\n","date":1366621687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366621687,"objectID":"bbc91ecfdca080a9a81e17e261103bb5","permalink":"https://wubigo.com/post/mysql5.7/","publishdate":"2013-04-22T17:08:07+08:00","relpermalink":"/post/mysql5.7/","section":"post","summary":"mysql install ubuntu 16.04 install mysql 5.7 at default\nsudo apt-get update sudo apt-get install mysql-server   Enable root remote connection\nmysql -u root -p mysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '\u0026lt;password\u0026gt;' WITH GRANT OPTION; mysql\u0026gt;FLUSH PRIVILEGES;   SHOW current setting mysql\u0026gt; SHOW VARIABLES WHERE Variable_name LIKE 'innodb%';  MySQL 5.7 has significantly better default values. the following variables are set by default:\n[mysqld] innodb_buffer_pool_instances=8 innodb_flush_method=O_DIRECT  setting of mysql 5.","tags":["SHELL","MYSQL"],"title":"Mysql5.7","type":"post"},{"authors":null,"categories":[],"content":" postgres in docker https://github.com/wubigo/docker-compose/blob/main/docker-compose-psql.yml\n启用pg_stat_statements docker volume inspect root_postgresql_data |grep \u0026quot;Mountpoint\u0026quot; echo \u0026quot;shared_preload_libraries = 'pg_stat_statements'\u0026quot; \u0026gt; /var/lib/docker/volumes/root_postgresql_data/_data/pgdata/postgresql.conf  INSTALL sudo apt-get install -y postgresql-client psql --version  TEST psql -h cmp.clv6a9yh.ap-northeast-1.rds.amazonaws.com -U postgres postgres=\u0026gt; \\list List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+-------------+-------------+----------------------- postgres | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | rdsadmin | rdsadmin | UTF8 | en_US.UTF-8 | en_US.UTF-8 | rdsadmin=CTc/rdsadmin template0 | rdsadmin | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/rdsadmin + | | | | | rdsadmin=CTc/rdsadmin template1 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres (4 rows)  SSH tunnelling to an RDS instance ssh -L 5432:mydb.myrdsinstance.eu-west-1.rds.amazonaws.com:5432 aws  ","date":1365234367,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1365234367,"objectID":"4e482a2577b705149c62445b32252b2a","permalink":"https://wubigo.com/post/postgres-notes/","publishdate":"2013-04-06T15:46:07+08:00","relpermalink":"/post/postgres-notes/","section":"post","summary":"postgres in docker https://github.com/wubigo/docker-compose/blob/main/docker-compose-psql.yml\n启用pg_stat_statements docker volume inspect root_postgresql_data |grep \u0026quot;Mountpoint\u0026quot; echo \u0026quot;shared_preload_libraries = 'pg_stat_statements'\u0026quot; \u0026gt; /var/lib/docker/volumes/root_postgresql_data/_data/pgdata/postgresql.conf  INSTALL sudo apt-get install -y postgresql-client psql --version  TEST psql -h cmp.clv6a9yh.ap-northeast-1.rds.amazonaws.com -U postgres postgres=\u0026gt; \\list List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+-------------+-------------+----------------------- postgres | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | rdsadmin | rdsadmin | UTF8 | en_US.UTF-8 | en_US.","tags":["DB","SQL"],"title":"Postgres Notes","type":"post"},{"authors":null,"categories":[],"content":" whether MySQL Server supports partitioning\nmysql -u root -p123456 -e \u0026quot;SHOW PLUGINS;\u0026quot; |grep partition   ","date":1363943287,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363943287,"objectID":"1d747e6830189cdd8c820f74ba9cae89","permalink":"https://wubigo.com/post/mysql-partition/","publishdate":"2013-03-22T17:08:07+08:00","relpermalink":"/post/mysql-partition/","section":"post","summary":" whether MySQL Server supports partitioning\nmysql -u root -p123456 -e \u0026quot;SHOW PLUGINS;\u0026quot; |grep partition   ","tags":["SHELL","MYSQL"],"title":"Mysql Partition","type":"post"},{"authors":null,"categories":[],"content":" show the last queries mysql\u0026gt;SET GLOBAL log_output = 'FILE'; mysql\u0026gt;SET GLOBAL general_log = 'ON'; mysql\u0026gt;SHOW VARIABLES WHERE Variable_name LIKE 'general_log_file'  Disable ONLY_FULL_GROUP_BY SHOW VARIABLES WHERE Variable_name LIKE 'sql_mode'; +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | Variable_name | Value | +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | sql_mode | ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |  [mysqld] sql_mode = \u0026quot;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\u0026quot;  mysql\u0026gt; SHOW CREATE TABLE \u0026lt;tablename\u0026gt;;  ","date":1361524087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361524087,"objectID":"8dea05fe1a8d04f747d2a60060f2edf7","permalink":"https://wubigo.com/post/mysql-5.7-sql-mode/","publishdate":"2013-02-22T17:08:07+08:00","relpermalink":"/post/mysql-5.7-sql-mode/","section":"post","summary":" show the last queries mysql\u0026gt;SET GLOBAL log_output = 'FILE'; mysql\u0026gt;SET GLOBAL general_log = 'ON'; mysql\u0026gt;SHOW VARIABLES WHERE Variable_name LIKE 'general_log_file'  Disable ONLY_FULL_GROUP_BY SHOW VARIABLES WHERE Variable_name LIKE 'sql_mode'; +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | Variable_name | Value | +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | sql_mode | ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |  [mysqld] sql_mode = \u0026quot;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\u0026quot;  mysql\u0026gt; SHOW CREATE TABLE \u0026lt;tablename\u0026gt;;  ","tags":["SHELL","MYSQL"],"title":"Mysql 5.7 SQL MODE","type":"post"},{"authors":null,"categories":[],"content":"[mysqld] log-bin = master-bin log-bin-index = master-bin.index server-id = 1  Grant the user to retrieve the binary log from the master\nmysql\u0026gt;CREATE USER repl_user; GRANT REPLICATION SLAVE ON *.* TO repl_user IDENTIFIED BY 'xyzzy';  ","date":1358845687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358845687,"objectID":"6b27567c18df15ccca99eb4fd341797b","permalink":"https://wubigo.com/post/mysql-master/","publishdate":"2013-01-22T17:08:07+08:00","relpermalink":"/post/mysql-master/","section":"post","summary":"[mysqld] log-bin = master-bin log-bin-index = master-bin.index server-id = 1  Grant the user to retrieve the binary log from the master\nmysql\u0026gt;CREATE USER repl_user; GRANT REPLICATION SLAVE ON *.* TO repl_user IDENTIFIED BY 'xyzzy';  ","tags":["SHELL","MYSQL"],"title":"Mysql Master","type":"post"},{"authors":null,"categories":null,"content":" API design: Choosing between names and identifiers in URLs API design: Choosing between names and identifiers in URLs\n","date":1357171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1357171200,"objectID":"b51bc744bcd6ae992033af60c36f65b3","permalink":"https://wubigo.com/post/2013-01-03-api_design/","publishdate":"2013-01-03T00:00:00Z","relpermalink":"/post/2013-01-03-api_design/","section":"post","summary":"API design: Choosing between names and identifiers in URLs API design: Choosing between names and identifiers in URLs","tags":null,"title":"API_design Note","type":"post"},{"authors":null,"categories":null,"content":" mapr downside with intermediate state “MapReduce’s approach of fully materializing intermediate state has downsides compared to Unix pipes: * A MapReduce job can only start when all tasks in the preceding jobs (that generate”“its inputs) have completed, whereas processes connected by a Unix pipe are started at the same time, with output being consumed as soon as it is produced. Skew or varying load on different machines means that a job often has a few straggler tasks that take much longer to complete than the others. Having to wait until all of the preceding job’s tasks have completed slows down the execution of the workflow as a whole. * Mappers are often redundant: they just read back the same file that was just written by a reducer, and prepare it for the next stage of partitioning and sorting. In many cases, the mapper code could be part of the previous reducer: if the reducer output was partitioned and sorted in the same way as mapper output, then reducers could be chained together directly, without interleaving with mapper stages. * “Storing intermediate state in a distributed filesystem means those files are replicated across several nodes, which is often overkill for such temporary data”\nDevelop Apache Spark Apps with IntelliJ IDEA on Windows OS https://www.linkedin.com/pulse/develop-apache-spark-apps-intellij-idea-windows-os-samuel-yee\nSpark notes http://stackoverflow.com/questions/40796818/how-to-append-a-resource-jar-for-spark-submit Set SPARK_PRINT_LAUNCH_COMMAND environment variable to have the complete Spark command printed out to the console, e.g. $ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell Spark Command: /Library/Ja\u0026hellip; Refer to Print Launch Command of Spark Scripts (or org.apache.spark.launcher.Main Standalone Application where this environment variable is actually used). Tip Avoid using scala.App trait for a Spark app docker run -v pwd:/data -e SPARK_PRINT_LAUNCH_COMMAND=1 -it heuermh/adam adam-shell Avoid using scala.App trait for a Spark application’s main class in Scala as\nreported in SPARK-4170 Closure problems when running Scala app that \u0026ldquo;extends\nApp\u0026rdquo;.\nRefer to Executing Main — runMain internal method in this document.\nMake sure to use the same version of Scala as the one used to build your distribution of Spark. Pre-built distributions of Spark 1.x use Scala 2.10, while pre-built distributions of Spark 2.0.x use Scala 2.11. 10 down vote\nSteps to install Spark(1.6.2-bin-hadoop2.6)prebuild in local mode on windows:\nInstall Java 7 or later. To test java installation is complete, open command prompt type javaand hit enter. If you receive a message \u0026lsquo;Java\u0026rsquo; is not recognized as an internal or external command. You need to configure your environment variables, JAVA_HOME and PATHto point to the path of jdk.\nDownload and install Scala.\nSet SCALA_HOME in Control Panel\\System and Security\\System goto \u0026ldquo;Adv System settings\u0026rdquo; and add %SCALA_HOME%\\bin in PATH variable in environment variables.\nInstall Python 2.6 or later from Python Download link.\nDownload SBT. Install it and set SBT_HOME as an environment variable with value as \u0026lt;\u0026gt;. Download winutils.exe from HortonWorks repo or git repo. Since we don\u0026rsquo;t have a local Hadoop installation on Windows we have to download winutils.exe and place it in a bindirectory under a created Hadoop home directory. Set HADOOP_HOME = \u0026lt;\u0026gt; in environment variable.and add it to path env We will be using a pre-built Spark package, so choose a Spark pre-built package for Hadoop Spark download. Download and extract it.\nSet SPARK_HOME and add %SPARK_HOME%\\bin in PATH variable in environment variables.\nRun command: spark-shell\nOpen http://localhost:4040/ in a browser to see the SparkContext web UI.\n$ cat rdd1.txt chr1 10016 chr1 10017 chr1 10018 chr1 20026 scala\u0026gt; val lines = sc.textFile(\u0026ldquo;/data/rdd1.txt\u0026rdquo;)\nscala\u0026gt; case class Chrom(name: String, value: Long)\ndefined class Chrom\nscala\u0026gt; val chroms = lines.map(_.split(\u0026rdquo;\\s+\u0026ldquo;)).map(r =\u0026gt; Chrom(r(0), r(1).toLong))\nchroms: org.apache.spark.rdd.RDD[Chrom] = MapPartitionsRDD[5] at map at :28\nscala\u0026gt; val df = chroms.toDF\n16/10/28 16:17:42 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n16/10/28 16:17:43 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\ndf: org.apache.spark.sql.DataFrame = [name: string, value: bigint]\nscala\u0026gt; df.show\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|name|value|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|chr1|10016|\n|chr1|10017|\n|chr1|10018|\n|chr1|20026|\n|chr1|20036|\n|chr1|30016|\n|chr1|30026|\n|chr2|40016|\n|chr2|40116|\n|chr2|50016|\n|chr3|70016|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\nscala\u0026gt; df.filter(\u0026lsquo;value \u0026gt; 30000).show\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|name|value|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|chr1|30016|\n|chr1|30026|\n|chr2|40016|\n|chr2|40116|\n|chr2|50016|\n|chr3|70016|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\nscala\u0026gt; case class Chrom2(name: String, value: Long, value: Long)\nscala\u0026gt; val chroms2 = rdd2.map(_.split(\u0026rdquo;\\s+\u0026ldquo;)).map(r =\u0026gt; Chrom2(r(0), r(1).toLong, r(2).toLong))\nchroms2: org.apache.spark.rdd.RDD[Chrom2] = MapPartitionsRDD[35] at map at :28\nscala\u0026gt; val df2=chroms2.toDF\ndf2: org.apache.spark.sql.DataFrame = [name: string, min: bigint \u0026hellip; 1 more field]\nscala\u0026gt; df.join(df2, Seq(\u0026ldquo;name\u0026rdquo;)).where($\u0026ldquo;value\u0026rdquo;.between($\u0026ldquo;min\u0026rdquo;, $\u0026ldquo;max\u0026rdquo;)).groupBy($\u0026ldquo;name\u0026rdquo;).count().show()\n$./bin/spark-shell \u0026ndash;packages com.databricks:spark-csv_2.11:1.2\n.0\nYour csv file does not have the same number of fields in each row - this cannot be parsed as is into a DataFrame\nAs of Spark 2.0.0, DataFrame - the flagship data abstraction of previous versions of Spark SQL - is currently a mere type alias for Dataset[Row] :\nA Dataset is local if it was created from local collections using SparkSession.emptyDataset or SparkSession.createDataset methods and their derivatives like toDF. If so, the queries on the Dataset can be optimized and run locally, i.e. without using Spark executors.\n","date":1349308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1349308800,"objectID":"1ca6d42c66a5123f3068b4aef27a8f87","permalink":"https://wubigo.com/post/2012-10-04-big-data-notes/","publishdate":"2012-10-04T00:00:00Z","relpermalink":"/post/2012-10-04-big-data-notes/","section":"post","summary":"mapr downside with intermediate state “MapReduce’s approach of fully materializing intermediate state has downsides compared to Unix pipes: * A MapReduce job can only start when all tasks in the preceding jobs (that generate”“its inputs) have completed, whereas processes connected by a Unix pipe are started at the same time, with output being consumed as soon as it is produced. Skew or varying load on different machines means that a job often has a few straggler tasks that take much longer to complete than the others.","tags":null,"title":"Develop Apache Spark Apps with IntelliJ IDEA on Windows OS","type":"post"},{"authors":null,"categories":null,"content":" update python to 2.7.11 on ubuntu apt_repository need python \u0026gt;2.7.9 ansible_galaxy has bug on ansible1.9.4 on proxy https://launchpad.net/~fkrull/+archive/ubuntu/deadsnakes-python2.7\n$sudo -E apt-add-repository ppa:fkrull/deadsnakes-python2.7 (deb http://ppa.launchpad.net/fkrull/deadsnakes-python2.7/ubuntu trusty main) $sudo -E apt-get install --only-upgrade python2.7 $python --version Python 2.7.11  Install $ sudo -E apt-get install software-properties-common $ sudo -E apt-add-repository ppa:ansible/ansible $ sudo -E apt-get update $ sudo -E apt-get install \u0026ndash;only-upgrade ansible Install latest ansible $git clone git://github.com/ansible/ansible.git \u0026ndash;recursive\n$virtualenv -p /home/whg/python2.7.11/bin/python python2.7.11\n$. ./python2.7.11/bin/activate $sudo pip install PyYAML Jinja2 httplib2 six Setting up Ansible to run out of checkout(~/.bashrc) export PATH=/home/ubuntu/ansible/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games export export PYTHONPATH=/home/ubuntu/ansible/lib: sudo PYTHONPATH=/home/ubuntu/ansible/lib ansible\nor /etc/sudoers\nDefaults env_reset\nDefaults env_keep += \u0026ldquo;PYTHONPATH\u0026rdquo;\nansible-galaxy(ansible \u0026gt; 2 otherwise of proxy bug) use role installed from galaxy $ansible-galaxy install angstwad.docker_ubuntu\n#create a play book to use the role\n hosts: all  sudo: yes\nroles:\n- angstwad.docker_ubuntu  #test to run the book\n$ansible-playbook t1.yml\nBuilding Role Scaffolding $ansible-galaxy init rolename\nBest Practices: Create a dedicated ansible node on the local net in the cloud to play book copy directory to remote node using scp dir remote:~/dir or using synchronize module instead of copy\nfacts about any system $ansible -m setup | less\n#facter comes as part of extra modules. to use the facter module, the \u0026ldquo;facter\u0026rdquo; and #\u0026ldquo;ruby-json\u0026rdquo; packages preinstalled on the target host.\n$ansible  -m facter | less\nvariables The following are the places from where Ansible accepts variables:\nThe default directory inside a role Inventory variables The host_vars and group_vars parameters defined in separate directories The host/group vars parameter defined in an inventory file Variables in playbooks and role parameters The vars directory inside a role and variables defined inside a play Extra variables provided with the -e option at runtime\nPatterns\nPatterns in Ansible are how we decide which hosts to manage.The following patterns address one or more groups. Groups separated by a colon indicate an “OR” configuration. This means the host may be in either one group or the other: hosts: name1:name2:group1:group2\nDisable SSH Host Key Checking For All Hosts set these options permanently in ~/.ssh/config (for the current user) or in /etc/ssh/ssh_config (for all users), either for all hosts or for a given set of IP addresses\nHost * StrictHostKeyChecking no UserKnownHostsFile=/dev/null\nAnsible looks for an ansible.cfg file in the following places, in this order:\nFile specified by the ANSIBLE_CONFIG environment variable\n./ansible.cfg (ansible.cfg in the current directory)\n~/.ansible.cfg (.ansible.cfg in your home directory)\n/etc/ansible/ansible.cfg\nAnsible will then move to the next task in the list, and go through these same four steps. It’s important to note that:\nAnsible runs each task in parallel across all hosts.\nAnsible waits until all hosts have completed a task before moving to the next task.\nAnsible runs the tasks in the order that you specify them.\nAnsible supports the ssh-agent program, so you don’t need to explicitly specify SSH key files in your inventory files. See “SSH Agent” for more details if you haven’t used ssh-agent before.\nYAML to get started with your first playbook:\nThe first line of a playbook should begin with \u0026ldquo;\u0026mdash; \u0026rdquo; (three hyphens) which indicates the beginning of the YAML document. Lists in YAML are represented with a hyphen followed by a white space. A playbook contains a list of plays; they are represented with \u0026ldquo;- \u0026ldquo;. Each play is an associative array, a dictionary, or a map in terms of key-value pairs. Indentations are important. All members of a list should be at the same indentation level. Each play can contain key-value pairs separated by \u0026ldquo;:\u0026rdquo; to denote hosts, variables, roles, tasks, and so on.\nrole dependencies pecify role dependency inside the meta subdirectory\nSafely limiting Ansible playbooks to a single machine There\u0026rsquo;s also a cute little trick that lets you specify a single host on the command line (or multiple hosts, I guess), without an intermediary inventory:\nansible-playbook -i \u0026ldquo;imac1-local,\u0026rdquo; user.yml Note the comma (,) at the end; this signals that it\u0026rsquo;s a list, not a file\n","date":1344038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1344038400,"objectID":"8d897195aec3cc72f94440472e18d4ed","permalink":"https://wubigo.com/post/2012-08-04-ansible/","publishdate":"2012-08-04T00:00:00Z","relpermalink":"/post/2012-08-04-ansible/","section":"post","summary":"update python to 2.7.11 on ubuntu apt_repository need python \u0026gt;2.7.9 ansible_galaxy has bug on ansible1.9.4 on proxy https://launchpad.net/~fkrull/+archive/ubuntu/deadsnakes-python2.7\n$sudo -E apt-add-repository ppa:fkrull/deadsnakes-python2.7 (deb http://ppa.launchpad.net/fkrull/deadsnakes-python2.7/ubuntu trusty main) $sudo -E apt-get install --only-upgrade python2.7 $python --version Python 2.7.11  Install $ sudo -E apt-get install software-properties-common $ sudo -E apt-add-repository ppa:ansible/ansible $ sudo -E apt-get update $ sudo -E apt-get install \u0026ndash;only-upgrade ansible Install latest ansible $git clone git://github.com/ansible/ansible.git \u0026ndash;recursive\n$virtualenv -p /home/whg/python2.","tags":null,"title":"ansible note","type":"post"},{"authors":null,"categories":[],"content":" 系统配置 /etc/sysctl.conf\nvm.swappiness = 1 vm.overcommit_memory = 1  改变数据目录 sudo install -o redis -g redis -d /mnt/redis-data \u0026gt; config get dir 1) \u0026quot;dir\u0026quot; 2) \u0026quot;/mnt/redis-data\u0026quot;  /lib/systemd/system/redis-server.service\n[Service] ReadWriteDirectories=-/mnt/redis-data  pidfile NOT FOUND FROM SYSTEMD /etc/redis/redis.conf\npidfile /var/run/redis/redis-server.pid  /lib/systemd/system/redis-server.service\nPIDFile=/run/redis/redis-server.pid  删除消费组 XGROUP DESTROY STREAM:TEST STRRAM:TEST:GROUP  Redis latency problems troubleshooting  Make sure you are not running slow commands that are blocking the server. Use the Redis Slow Log feature to check this. For EC2 users, make sure you use HVM based modern EC2 instances, like m3.medium. Otherwise fork() is too slow. Transparent huge pages must be disabled from your kernel. Use echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled to disable them, and restart your Redis process. If you are using a virtual machine, it is possible that you have an intrinsic latency that has nothing to do with Redis. Check the minimum latency you can expect from your runtime environment using ./redis-cli \u0026ndash;intrinsic-latency 100. Note: you need to run this command in the server not in the client. Enable and use the Latency monitor feature of Redis in order to get a human readable description of the latency events and causes in your Redis instance.  stop-writes-on-bgsave-error(save RDB snapshots) MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.\n$ redis-cli \u0026gt; config set stop-writes-on-bgsave-error no  Delete all the keys of the currently selected DB 127.0.0.1:6379\u0026gt;FLUSHALL or flushall  select database the number of databases is defined in the configuration file with the databases directive (the default value is 16). To switch between the databases, call SELECT. 127.0.0.1:6379\u0026gt;select \nDataType  DataType type = redisTemplate.type(key); if(DataType.NONE == type){ return null; }else if(DataType.STRING == type){ return super.redisTemplate.opsForValue().get(key); }else if(DataType.LIST == type){ return super.redisTemplate.opsForList().range(key, 0, -1); }else if(DataType.HASH == type){ return super.redisTemplate.opsForHash().entries(key); }else return null;  allowing remote connection to redis echo \u0026ldquo;bind 0.0.0.0\u0026rdquo; \u0026gt;\u0026gt; redis.conf\n1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.  ","date":1343864047,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1343864047,"objectID":"df442c9ce25d5fc85a7004cee2ecef13","permalink":"https://wubigo.com/post/redis-notes/","publishdate":"2012-08-02T07:34:07+08:00","relpermalink":"/post/redis-notes/","section":"post","summary":"系统配置 /etc/sysctl.conf\nvm.swappiness = 1 vm.overcommit_memory = 1  改变数据目录 sudo install -o redis -g redis -d /mnt/redis-data \u0026gt; config get dir 1) \u0026quot;dir\u0026quot; 2) \u0026quot;/mnt/redis-data\u0026quot;  /lib/systemd/system/redis-server.service\n[Service] ReadWriteDirectories=-/mnt/redis-data  pidfile NOT FOUND FROM SYSTEMD /etc/redis/redis.conf\npidfile /var/run/redis/redis-server.pid  /lib/systemd/system/redis-server.service\nPIDFile=/run/redis/redis-server.pid  删除消费组 XGROUP DESTROY STREAM:TEST STRRAM:TEST:GROUP  Redis latency problems troubleshooting  Make sure you are not running slow commands that are blocking the server. Use the Redis Slow Log feature to check this.","tags":["REDIS","CACHE"],"title":"Redis Notes","type":"post"},{"authors":null,"categories":null,"content":" B+树 vs. LSM树 RDBMS使用B+树专门针对磁盘存储而优化的N叉排序树 NoSQL使用LSM树\n","date":1341360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341360000,"objectID":"8dd0cb0fdbf378cdcc8265da15f2651c","permalink":"https://wubigo.com/post/2012-07-04-rdbms-nosql/","publishdate":"2012-07-04T00:00:00Z","relpermalink":"/post/2012-07-04-rdbms-nosql/","section":"post","summary":"B+树 vs. LSM树 RDBMS使用B+树专门针对磁盘存储而优化的N叉排序树 NoSQL使用LSM树","tags":["RDBMS","NOSQL"],"title":"RDBMS vs NoSQL","type":"post"},{"authors":null,"categories":[],"content":" 查看表状态 show table status FROM redis_db like 'point_value'; +-------------+--------+---------+------------+-----------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+-----------------+----------+----------------+---------+ | Name | Engine | Version | Row_format | Rows | Avg_row_length | Data_length | Max_data_length | Index_length | Data_free | Auto_increment | Create_time | Update_time | Check_time | Collation | Checksum | Create_options | Comment | +-------------+--------+---------+------------+-----------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+-----------------+----------+----------------+---------+ | point_value | InnoDB | 10 | Dynamic | 316755485 | 143 | 45420118016 | 0 | 0 | 3145728 | NULL | 2022-01-30 18:55:44 | 2022-02-06 18:17:56 | NULL | utf8_general_ci | NULL | | | +-------------+--------+---------+------------+-----------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+-----------------+----------+----------------+---------+ 1 row in set (0.00 sec)  或者\n select TABLE_NAME,TABLE_ROWS, AVG_ROW_LENGTH,DATA_LENGTH,INDEX_LENGTH,DATA_FREE from information_schema.TABLES WHERE TABLES.TABLE_SCHEMA ='redis_db'; +--------------------+------------+----------------+-------------+--------------+-----------+ | TABLE_NAME | TABLE_ROWS | AVG_ROW_LENGTH | DATA_LENGTH | INDEX_LENGTH | DATA_FREE | +--------------------+------------+----------------+-------------+--------------+-----------+ | EFPointValues | 18948888 | 223 | 4229955584 | 0 | 0 | | checksums | 2 | 8192 | 16384 | 0 | 0 | | point_value | 321024285 | 141 | 45420118016 | 0 | 3145728 | | point_value_source | 9398127 | 146 | 1376780288 | 0 | 5242880 | | stream | 2 | 8192 | 16384 | 0 | 0 | +--------------------+------------+----------------+-------------+--------------+-----------+  数据优化 https://docs.oracle.com/cd/E17952_01/mysql-5.6-en/data-size.html\n错误日志时间戳 [mysqld] log_timestamps = SYSTEM  innodb_file_per_table [mysqld] datadir = /data/mysql  ibdata1 是InnoDB 系统表空间， 使用innodb_file_per_table = 1 用户创建的表和索引按数据库单独存放。 假设用户创建了device数据库 和point表\nCREATE DATABASE device CREATE TABLE device.point (...) ENGINE=InnoDB  /data/mysql/device/point.ibd /data/mysql/device/point.frm\nshowing current configuration mysql\u0026gt;SHOW VARIABLES;   mysqldump -u root -h 192.168.76.62 -pgld --all-databases \u0026gt; dump.sql mysqldump -u root -h db -pgld --all-databases \u0026gt; dump.sql  then import data in mysql shell\nmysql\u0026gt;source dump.sql\nupdate the database if directly upgrade from 5.0 to 5.5\nmysql_upgrade -u root -p\nservice mysql restart\nA typical mysqldump command to move data from an external database to an Amazon RDS DB instance looks similar to the following:\nmysqldump -u  \u0026ndash;databases  \u0026ndash;single-transaction \u0026ndash;compress \u0026ndash;order-by-primary -p | mysql -u  \u0026ndash;port= \u0026ndash;host= -p\n\u0026ndash;single-transaction – Use to ensure that all of the data loaded from the local database is consistent with a single point in time. If there are other processes changing the data while mysqldump is reading it, using this option helps maintain data integrity. \u0026ndash;compress – Use to reduce network bandwidth consumption by compressing the data from the local database before sending it to Amazon RDS. \u0026ndash;order-by-primary – Use to reduce load time by sorting each table\u0026rsquo;s data by its primary key.\nYou must create any stored procedures, triggers, functions, or events manually in your Amazon RDS database. If you have any of these objects in the database that you are copying, then exclude them when you run mysqldump by including the following arguments with your mysqldump command: \u0026ndash;routines=0 \u0026ndash;triggers=0 \u0026ndash;events=0.\nhow to remove mysql completely\nsudo apt-get remove \u0026ndash;purge mysql-server mysql-client mysql-common sudo apt-get autoremove sudo apt-get autoclean then try to install it again.\nsudo apt-get install mysql-server if you installing with dpkg command and if it show any dependency on other package then run command :\nsudo apt-get install -f\nWhat is disabled by default is remote root access. If you want to enable that, run this SQL command locally:\nGRANT ALL PRIVILEGES ON . TO \u0026lsquo;root\u0026rsquo;@\u0026lsquo;%\u0026rsquo; IDENTIFIED BY \u0026lsquo;\u0026rsquo; WITH GRANT OPTION; FLUSH PRIVILEGES;\nEnable remote connection mysql 5.7\nmysqld config /etc/mysql/mysql.conf.d mysql client config /etc/mysql/conf.d\nAnd then find the following line and comment it out in your my.cnf file, which usually lives on /etc/mysql/my.cnf on Unix/OSX systems.\nIf it\u0026rsquo;s a Windows system, you can find it in the MySQL installation directory, usually something like C:\\Program Files\\MySQL\\MySQL Server 5.5\\ and the filename will be my.ini.\nChange line\n bind-address = 127.0.0.1  to\n #bind-address = 127.0.0.1  And restart the MySQL server for the changes to take effect.\nsudo apt-get install mysql-server\nupdate t_supplier_subproject set attachInfo=\u0026ldquo;;commit;\nupdate t_supplier set email=\u0026lsquo;gin_369@163.Cnn\u0026rsquo; where supplierID=7;\nselect * from t_supplier where userid=\u0026lsquo;6156354693465407499\u0026rsquo; and email=\u0026lsquo;gin_369@163.COM\u0026rsquo;;\nALTER TABLE t_quoted_adopt4tbq MODIFY adoptRemark VARCHAR(255);\n查询分包商数量 SELECT count(*) FROM etender.t_supplier;\nALTER TABLE etender.t_supplier CHANGE COLUMN email email VARCHAR(40) NULL DEFAULT \u0026ldquo; COMMENT \u0026lsquo;供应商邮箱\u0026rsquo;;\n新增列 询价类型 alter table t_project_query add column inquiryType varchar(255) DEFAULT \u0026lsquo;2\u0026rsquo; COMMENT \u0026lsquo;询价类型\u0026rsquo;;\nalter table t_quoted_billitem4tbq add column itemType varchar(255) DEFAULT \u0026lsquo;1\u0026rsquo; COMMENT \u0026lsquo;清单类型\u0026rsquo;;\nSELECT COUNT(*),t2.email AS \u0026lsquo;总包邮箱\u0026rsquo;, t1.userID FROM etender.t_user t2 LEFT JOIN etender.t_supplier t1 ON t1.userID = t2.userID WHERE t1.logicDelete !=1 GROUP BY t2.userID ;\nMaking a Copy of a Database $mysqldump -u root -pg1d etender \u0026gt; dump.sql $mysqladmin -u root -pg1d create hongq $mysql -u root -pg1d hongq \u0026lt; dump.sql  export to csv SELECT b.email,a.name supplierName,a.email supplierEmail,a.telephone,a.trade,a.level,a.address,a.contacts FROM t_supplier a LEFT JOIN t_user b ON a.userID = b.userid and a.logicDelete !=1 ORDER BY a.userID limit 1000,1000 INTO OUTFILE '/var/lib/mysql-files/subcon1000.csv' FIELDS TERMINATED BY ',' ENCLOSED BY '\u0026quot;' LINES TERMINATED BY '\\n'  Creating SSH Tunnel From Linux for mysql $ ssh -L 3306:rdb:3306 ubuntu@ec2\nuse ip instead of hostname to avoid channel X on ubuntu 16 \u0026ldquo;channel X: open failed: administratively prohibited\u0026rdquo;\n","date":1338766442,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338766442,"objectID":"789a5c54946ce08c97933508fe83a6e6","permalink":"https://wubigo.com/post/mysql-notes/","publishdate":"2012-06-04T07:34:02+08:00","relpermalink":"/post/mysql-notes/","section":"post","summary":"查看表状态 show table status FROM redis_db like 'point_value'; +-------------+--------+---------+------------+-----------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+-----------------+----------+----------------+---------+ | Name | Engine | Version | Row_format | Rows | Avg_row_length | Data_length | Max_data_length | Index_length | Data_free | Auto_increment | Create_time | Update_time | Check_time | Collation | Checksum | Create_options | Comment | +-------------+--------+---------+------------+-----------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+-----------------+----------+----------------+---------+ | point_value | InnoDB | 10 | Dynamic | 316755485 | 143 | 45420118016 | 0 | 0 | 3145728 | NULL | 2022-01-30 18:55:44 | 2022-02-06 18:17:56 | NULL | utf8_general_ci | NULL | | | +-------------+--------+---------+------------+-----------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+-----------------+----------+----------------+---------+ 1 row in set (0.","tags":["MYSQL","DBMS"],"title":"Mysql Notes","type":"post"},{"authors":null,"categories":["IT"],"content":" iptables规则配置  表与链   调用链顺序   检查内存 ram speed and type\n dmidecode\nsudo dmidecode --type memory # dmidecode 3.0 Getting SMBIOS data from sysfs. SMBIOS 2.6 present. Handle 0x003E, DMI type 17, 28 bytes Memory Device Array Handle: 0x003C Error Information Handle: Not Provided Total Width: Unknown Data Width: Unknown Size: No Module Installed Form Factor: DIMM Set: 1 Locator: XMM1 Bank Locator: Not Specified Type: DDR3 Type Detail: Synchronous Speed: Unknown Manufacturer: JEDEC ID: Serial Number: Asset Tag: Not Specified Part Number: Rank: Unknown  lshw\nsudo lshw -class memory memory:0 description: System Memory physical id: 3c slot: System board or motherboard *-bank:0 description: DIMM DDR3 Synchronous [empty] vendor: JEDEC ID: physical id: 0 slot: XMM1 *-bank:1 description: DIMM DDR3 Synchronous 1333 MHz (0.8 ns) product: M378B5273DH0-CH9 vendor: JEDEC ID:80 CE physical id: 1 serial: D3894765 slot: XMM2 size: 4GiB width: 64 bits clock: 1333MHz (0.8ns) *-bank:2 description: DIMM DDR3 Synchronous 1333 MHz (0.8 ns) product: M378B5273DH0-CH9 vendor: JEDEC ID:80 CE physical id: 2 serial: D4894765 slot: XMM3 size: 4GiB width: 64 bits clock: 1333MHz (0.8ns) *-bank:3 description: DIMM DDR3 Synchronous 1333 MHz (0.8 ns) product: 8JTF12864AZ-1G4G1 vendor: JEDEC ID:80 2C physical id: 3 serial: 07572436 slot: XMM4 size: 1GiB width: 64 bits clock: 1333MHz (0.8ns)   2R*8 代表双面，每面（RANK）8个芯片颗粒\ndownload manager sudo apt-get install uget uget-gtk  Enables forwarding of the authentication agent connection  client config .ssh/config\nForwardAgent yes  Enable ssh-agent on main device\n  .bashrc\nSSH_ENV=\u0026quot;$HOME/.ssh/environment\u0026quot; function start_agent { echo \u0026quot;Initialising new SSH agent...\u0026quot; /usr/bin/ssh-agent | sed 's/^echo/#echo/' \u0026gt; \u0026quot;${SSH_ENV}\u0026quot; echo succeeded chmod 600 \u0026quot;${SSH_ENV}\u0026quot; . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null /usr/bin/ssh-add; } # Source SSH settings, if applicable if [ -f \u0026quot;${SSH_ENV}\u0026quot; ]; then . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null #ps ${SSH_AGENT_PID} doesn't work under cywgin ps -ef | grep ${SSH_AGENT_PID} | grep ssh-agent$ \u0026gt; /dev/null || { start_agent; } else start_agent; fi  enter password to unlock your keyring 方法1 - set password-store to basic\ndpkg -L google-chrome-stable |grep desktop | xargs cp {1} ~/.local/share/applications  修改.local/share/applications/google-chrome.desktop\nExec=/usr/bin/google-chrome-stable --password-store=basic %U   seahorse\nseahorse   选择login，右键删除\nTCP DUMP  capture all incoming IP traffic destined to the node except local traffic\n sudo tcpdump -i enp0s25 tcp -n sudo tcpdump -i enp0s25 dst host 192.168.1.5 and not src net 192.168.1.0/24  终端窗口复制快捷键Ctrl-C  在命令终端窗口首选项里设置快捷键复制 -\u0026gt; Ctrl-C 设置终端驱动快捷键\nstty -a stty intr \\^k stty -a    Ctrl-K to interrupt current command\n ctrl-c.sh\n#!/usr/bin/env bash stty intr \\^k  network manager Linux systems which use a GUI often have a network manager running, which uses a dnsmasq instance running on a loopback address such as 127.0.0.1 or 127.0.1.1 to cache DNS requests, and adds this entry to /etc/resolv.conf. The dnsmasq service speeds up DNS look-ups and also provides DHCP services\n /usr/sbin/dnsmasq --no-resolv --keep-in-foreground --no-hosts --bind-interfaces --pid-file=/var/run/NetworkManager/dnsmasq.pid --listen-address=127.0.1.1 --cache-size=0 --conf-file=/dev/null --proxy-dnssec --enable-dbus=org.freedesktop.NetworkManager.dnsmasq --conf-dir=/etc/NetworkManager/dnsmasq.d  update git to 2.20 sudo add-apt-repository ppa:git-core/ppa sudo apt-get update sudo apt-get install git=2.20.1-0ppa1~ubuntu16.04.1  static ip sudo systemctl stop network-manager sudo systemctl disable network-manager.service echo \u0026quot;manual\u0026quot; | sudo tee /etc/init/network-manager.override  ubuntu18.04\nsudo systemctl stop NetworkManager-wait-online.service sudo systemctl disable NetworkManager-wait-online.service sudo systemctl stop NetworkManager-dispatcher.service sudo systemctl disable NetworkManager-dispatcher.service sudo systemctl stop network-manager.service sudo systemctl disable network-manager.service ystemctl unmask networking systemctl enable networking systemctl restart networking  cat /etc/network/interfaces auto enp0s25 iface enp0s25 inet static address 192.168.1.5 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 192.168.1.1  ubuntu18.04 only\necho \u0026quot;DNS=192.168.1.1\u0026gt;\u0026gt;/etc/systemd/resolved.conf systemctl restart systemd-resolved  ssh client config ~/.ssh/config\nhost * StrictHostKeyChecking no  instll docker v17.03 sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026quot; sudo apt-get update apt-cache madison docker-ce|grep 17.03 sudo apt-get install docker-ce=17.03.3~ce-0~ubuntu-xenial sudo usermod -aG docker $USER  Disable Chrome session restore popup Type chrome://flags/#infinite-session-restore in address bar (Crtl+L). Click on the right drop-down menu and change the \u0026lsquo;Default\u0026rsquo; value to \u0026lsquo;Disable\u0026rsquo;. Then restart Chrome to apply that setting\ndocker to reclaim disk space  remove untagged images\ndocker images --no-trunc | grep '\u0026lt;none\u0026gt;' | awk '{ print $3 }' | xargs -r docker rmi  Clean up dead and exited containers(use the -v flag to remove the volumes along the container)\ndocker ps --filter status=dead --filter status=exited -aq \\ | xargs docker rm -v  docker volume cleanup\ndocker volume ls -qf dangling=true | xargs -r docker volume rm   IPVS for i in ip_vs_sh ip_vs ip_vs_rr ip_vs_wrr; do sudo modprobe $i; done  change the runlevel on systemd for VM sudo systemctl enable multi-user.target sudo systemctl set-default multi-user.target  List files in package $dpkg -L docker-ce /usr/bin/docker-containerd /usr/bin/docker-proxy /usr/bin/docker /usr/bin/docker-runc /usr/bin/dockerd /usr/bin/docker-containerd-ctr /usr/bin/docker-containerd-shim /usr/bin/docker-init /etc/init.d/docker /etc/default/docker /etc/init/docker.conf /lib/systemd/system/docker.service /lib/systemd/system/docker.socket  Find the latest file by modified date find /path -printf '%T+ %p\\n' | sort -r | head  ghost systemd service /etc/systemd/system/ghost.service\nRunning sudo command: ln -sf /var/www/ghost/system/files/ghost_localhost.service /lib/systemd/system/ghost_localhost.service Running sudo command: systemctl daemon-reload  ls /lib/systemd/system/ghost* sudo systemctl stop ghost_localhost  Admin URL As per the SSL section above, admin.url can be used to specify a different protocol for your admin panel. It can also be used to specify a different hostname (domain name). It cannot be used to affect the path at which the admin panel is served (this is always /ghost/).\n\u0026quot;admin\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://example.com\u0026quot; }  ubuntu@ip-192-168-114-240:/lib/systemd/system$ sudo systemctl disable ghost_54-169-190-39.service Removed symlink /etc/systemd/system/multi-user.target.wants/ghost_54-169-190-39.service. Removed symlink /etc/systemd/system/ghost_54-169-190-39.service.  Rotate Tomcat catalina.out https://dzone.com/articles/how-rotate-tomcat-catalinaout\n","date":1338723543,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338723543,"objectID":"2d2de0feaad6fc1dd7970449aaf561cc","permalink":"https://wubigo.com/post/linux-notes/","publishdate":"2012-06-03T19:39:03+08:00","relpermalink":"/post/linux-notes/","section":"post","summary":"iptables规则配置  表与链   调用链顺序   检查内存 ram speed and type\n dmidecode\nsudo dmidecode --type memory # dmidecode 3.0 Getting SMBIOS data from sysfs. SMBIOS 2.6 present. Handle 0x003E, DMI type 17, 28 bytes Memory Device Array Handle: 0x003C Error Information Handle: Not Provided Total Width: Unknown Data Width: Unknown Size: No Module Installed Form Factor: DIMM Set: 1 Locator: XMM1 Bank Locator: Not Specified Type: DDR3 Type Detail: Synchronous Speed: Unknown Manufacturer: JEDEC ID: Serial Number: Asset Tag: Not Specified Part Number: Rank: Unknown  lshw","tags":["IAAS","LINUX"],"title":"linux note","type":"post"},{"authors":null,"categories":[],"content":" key_buffer_size  the size of the index buffers held in memory, which affects the speed of index reads\nrecommend: 25% or more of the available server memory\nA good way to determine whether to adjust the value is to compare the key_read_requests value, which is the total value of requests to read an index, and the key_reads values, the total number of requests that had to be read from disk.\n","date":1335085687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1335085687,"objectID":"cdc1b77fba72f3b0fddd83601d68cc60","permalink":"https://wubigo.com/post/mysql-5.7-myisam/","publishdate":"2012-04-22T17:08:07+08:00","relpermalink":"/post/mysql-5.7-myisam/","section":"post","summary":"key_buffer_size  the size of the index buffers held in memory, which affects the speed of index reads\nrecommend: 25% or more of the available server memory\nA good way to determine whether to adjust the value is to compare the key_read_requests value, which is the total value of requests to read an index, and the key_reads values, the total number of requests that had to be read from disk.","tags":["SHELL","MYSQL"],"title":"Mysql 5.7 MyISAM","type":"post"},{"authors":null,"categories":null,"content":" NOTICE\n Don\u0026rsquo;t put ca-key.pem into a Container Linux Config, it is recommended to store it in safe place. This key allows to generate as much certificates as possible.\n Keep key files in safe. Don\u0026rsquo;t forget to set proper file permissions, i.e. chmod 0600 server-key.pem.\n Certificates in this TLDR example have both server auth and client auth X509 V3 extensions and you can use them with servers and clients\u0026rsquo; authentication.\n generate keys and certificates for wildcard * address as well. They will work on any machine. It will simplify certificates routine but increase security risks.\n  check X509v3 Subject Alternative Name(HOST) issued in server.pem\nopenssl x509 -in server.pem -text |grep DNS  Generate self-signed certificates  Download cfssl\nmkdir ~/bin curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x ~/bin/{cfssl,cfssljson}  Initialize a certificate authority\nmkdir ~/cfssl cd ~/cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json  Certificate types which are used inside Container Linux\n client certificate is used to authenticate client by server. . server certificate is used by server and verified by client for server identity. peer certificate is used by cluster as members communicate with each other in both ways.  Configure CA options ca-csr.json(Certificate Signing Request (CSR))\n{ \u0026quot;CN\u0026quot;: \u0026quot;wubigo CA\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;ecdsa\u0026quot;, \u0026quot;size\u0026quot;: 256 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;BJ\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Bei Jing\u0026quot; } ] }   ca-config.json( set expiry to 8760h (1 year))\n{ \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;168h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;server\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot; ] }, \u0026quot;client\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;client auth\u0026quot; ] }, \u0026quot;peer\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } }   generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - ls ca-key.pem ca.csr ca.pem  Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within the CA\n Generate server certificate\ncfssl print-defaults csr \u0026gt; server.json  Most important values for server certificate are Common Name (CN) and hosts. substitute them accordingly:\n... \u0026quot;CN\u0026quot;: \u0026quot;server\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;192.168.1.6\u0026quot;, \u0026quot;wubigo.com\u0026quot;, \u0026quot;localhost\u0026quot;, \u0026quot;127.0.0.1\u0026quot; ], ...   generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server  result following files:\nserver-key.pem server.csr server.pem   Generate peer certificate\n  cfssl print-defaults csr \u0026gt; member1.json\nSubstitute CN and hosts values, for example:\n... \u0026quot;CN\u0026quot;: \u0026quot;member1\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;192.168.1.7\u0026quot;, \u0026quot;member1.wubigo.com\u0026quot;, \u0026quot;member1.local\u0026quot;, \u0026quot;member1\u0026quot; ], ...   generate member1 certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer member1.json | cfssljson -bare member1  member1-key.pem member1.csr member1.pem  Generate client certificate\ncfssl print-defaults csr \u0026gt; client.json  For client certificate ignore hosts values and set only Common Name (CN) to client value:\n... \u0026quot;CN\u0026quot;: \u0026quot;client\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;\u0026quot; ]\u0026quot;, ...  Generate client certificate:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client   get following files:\nclient-key.pem client.csr client.pem  ","date":1328054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1328054400,"objectID":"c75fad4581620ab0c699c16ce7e01a99","permalink":"https://wubigo.com/post/2012-02-01-tls-notes/","publishdate":"2012-02-01T00:00:00Z","relpermalink":"/post/2012-02-01-tls-notes/","section":"post","summary":"NOTICE\n Don\u0026rsquo;t put ca-key.pem into a Container Linux Config, it is recommended to store it in safe place. This key allows to generate as much certificates as possible.\n Keep key files in safe. Don\u0026rsquo;t forget to set proper file permissions, i.e. chmod 0600 server-key.pem.\n Certificates in this TLDR example have both server auth and client auth X509 V3 extensions and you can use them with servers and clients\u0026rsquo; authentication.","tags":null,"title":"TLS notes","type":"post"},{"authors":null,"categories":[],"content":" select myql 5.7 wget wget https://dev.mysql.com/get/mysql-apt-config_0.8.12-1_all.deb dpkg -i mysql-apt-config_0.8.12-1_all.deb  turns off the GPG check sources.list.d/mysql.list\ndeb [trusted=yes] http://repo.mysql.com/apt/ubuntu/ bionic mysql-5.7  install mysql and create admin user sudo apt update apt-cache policy mysql-server | grep 5.7 sudo apt install mysql-client=5.7.37-1ubuntu18.04 mysql-community-server=5.7.37-1ubuntu18.04 mysql -u root -p GRANT ALL PRIVILEGES ON *.* TO 'admin'@'%' IDENTIFIED BY 'pass' WITH GRANT OPTION  ","date":1327012653,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1327012653,"objectID":"da1aa62dc2a6903f12595ee0af768fe7","permalink":"https://wubigo.com/post/mysql-5-on-ubuntu20/","publishdate":"2012-01-20T06:37:33+08:00","relpermalink":"/post/mysql-5-on-ubuntu20/","section":"post","summary":" select myql 5.7 wget wget https://dev.mysql.com/get/mysql-apt-config_0.8.12-1_all.deb dpkg -i mysql-apt-config_0.8.12-1_all.deb  turns off the GPG check sources.list.d/mysql.list\ndeb [trusted=yes] http://repo.mysql.com/apt/ubuntu/ bionic mysql-5.7  install mysql and create admin user sudo apt update apt-cache policy mysql-server | grep 5.7 sudo apt install mysql-client=5.7.37-1ubuntu18.04 mysql-community-server=5.7.37-1ubuntu18.04 mysql -u root -p GRANT ALL PRIVILEGES ON *.* TO 'admin'@'%' IDENTIFIED BY 'pass' WITH GRANT OPTION  ","tags":["MYSQL","SQL"],"title":"install Mysql 5 on Ubuntu20","type":"post"},{"authors":null,"categories":["IT"],"content":" dstat $dstat -d -nt $dstat -nt $dstat -N eth2,eth3  pkstat sudo apt-get install pktstat sudo pktstat -i eth0 -nt  nethogs sudo apt-get install nethogs sudo nethogs  EPEL http://www.cyberciti.biz/faq/fedora-sl-centos-redhat6-enable-epel-repo/\n$ cd /tmp $ wget http://mirror-fpt-telecom.fpt.net/fedora/epel/6/i386/epel-release-6-8.noarch.rpm # rpm -ivh epel-release-6-8.noarch.rpm  How do I use EPEL repo?\nSimply use the yum commands to search or install packages from EPEL repo:\n# yum search nethogs # yum update # yum --disablerepo=\u0026quot;*\u0026quot; --enablerepo=\u0026quot;epel\u0026quot; install nethogs  System administrators responsible for handling Linux servers get confused at times when they are told to benchmark a file system\u0026rsquo;s performance. But the main reason that this confusion happens is because, it does not matter whatever tool you use to test the file system\u0026rsquo;s performance, what matter\u0026rsquo;s is the exact requirement.File system\u0026rsquo;s performance depends upon certain factors as follows.\nThe maximum rotational speed of your hard disk\nThe Allocated block size of a file system\nSeek Time\nThe performance rate of the file system\u0026rsquo;s metadata\nThe type of read/Write\nSeriously speaking its wonderful to realize that various different technologies made by different people and even different companies are working together in coordination inside a single box, and we call that box a computer. And its even more wonderful to realize that hard disk\u0026rsquo;s store\u0026rsquo;s almost all the information available in the world in digital format. Its a very complex thing to understand how really hard disks stores our data safely. Explaining different aspects of how a hard disk, and a file system on top of it, work together is beyond the scope of this article(But i will surely give it a try with couple of my posts about themwink)\nSo Lets begin our tutorial on file system benchmark test.\nIts advised that during this file system performance test, you must not run any other disk I/O intensive tasks. Otherwise your results about performance will be heavily deviated. Its better to stop all other process during this test.\nThe Simplest Performance Test Using dd command\nThe simplest read write performance test in Linux can be done with the help of dd command. This command is used to write or read from any block device in Linux. And you can do a lot of stuff with this command. The main plus point with this command, is that its readily available in almost all distributions out of the box. And is pretty easy to use.\nWith this dd command we will only be testing sequential read and sequential write.I will test the speed of my partition /dev/sda1 which is mounted on \u0026ldquo;/\u0026rdquo; (the only partition i have on my system)so can write the data to any where in my filesystem to test.\n[root@slashroot2 ~]# dd if=/dev/zero of=speetest bs=1M count=100 100+0 records in 100+0 records out 104857600 bytes (105 MB) copied, 0.0897865 seconds, 1.2 GB/s  In the above command you will be amazed to see that you have got 1.1GB/s. But dont be happy thats falsecheeky. Becasue the speed that dd reported to us is the speed with which data was cached to RAM memory, not to the disk. So we need to ask dd command to report the speed only after the data is synced with the disk.For that we need to run the below command.\n[root@slashroot2 ~]# dd if=/dev/zero of=speetest bs=1M count=100 conv=fdatasync 100+0 records in 100+0 records out 104857600 bytes (105 MB) copied, 2.05887 seconds, 50.9 MB/s  As you can clearly see that with the attribute fdatasync the dd command will show the status rate only after the data is completely written to the disk. So now we have the actual sequencial write speed. Lets go to an amount of data size thats larger than the RAM. Lets take 200MB of data in 64kb block size.\n[root@slashroot2 ~]# dd if=/dev/zero of=speedtest bs=64k count=3200 conv=fdatasync 3200+0 records in 3200+0 records out 209715200 bytes (210 MB) copied, 3.51895 seconds, 59.6 MB/s  as you can clearly see that the speed came to 59 MB/s. You need to note that ext3 bydefault if you do not specify the block size, gets formatted with a block size thats determined by the programes like mke2fs . You can verify yours with the following commands.\ntune2fs -l /dev/sda1 dumpe2fs /dev/sda1  For testing the sequential read speed with dd command, you need to run the below command as below.\n[root@myvm1 sarath]# dd if=speedtest of=/dev/null bs=64k count=24000 5200+0 records in 5200+0 records out 340787200 bytes (341 MB) copied, 3.42937 seconds, 99.4 MB/s  Performance Test using HDPARM\nNow lets use some other tool other than dd command for our tests. We will start with hdparm command to test the speed. Hdparm tool is also available out of the box in most of the linux distribution.\n[root@myvm1 ~]# hdparm -tT /dev/sda1 /dev/sda1: Timing cached reads: 5808 MB in 2.00 seconds = 2908.32 MB/sec Timing buffered disk reads: 10 MB in 3.12 seconds = 3.21 MB/sec  There are multiple things to understand here in the above hdparm results. the -t Option will show you the speed of reading from the cache buffer(Thats why its much much higher).\nThe -T option will show you the speed of reading without precached buffer(which from the above output is low 3.21 MB/sec as shown above. )\nthe hdparm output shows you both the cached reads and disk reads separately. As mentioned before hard disk seek time also matters a lot for your speed you can check your hard disk seek time with the following linux command. seek time is the time required by the hard disk to reach the sector where the data is stored.Now lets use this seeker tool to find out the seek time by the simple seek command.\n[root@slashroot2 ~]# seeker /dev/sda1 Seeker v3.0, 2009-06-17, http://www.linuxinsight.com/how_fast_is_your_disk.html Benchmarking /dev/sda1 [81915372 blocks, 41940670464 bytes, 39 GB, 39997 MB, 41 GiB, 41940 MiB] [512 logical sector size, 512 physical sector size] [1 threads] Wait 30 seconds.............................. Results: 87 seeks/second, 11.424 ms random access time (26606211 \u0026lt; offsets \u0026lt; 41937280284)  its clearly mentioned that my disk did a 86 seeks for sectors containing data per second. Thats ok for a desktop Linux machine but for servers its not at all ok.\nRead Write Benchmark Test using IOZONE:\nNow there is one tool out there in linux that will do all these test in one shot. Thats none other than \u0026ldquo;IOZONE\u0026rdquo;. We will do some benchmark test against my /dev/sda1 with the help of iozone.Computers or servers are always purchased keeping some purpose in mind. Some servers needs to be highend performance wise, some needs to be fast in sequencial reads,and some others are ordered keeping random reads in mind. IOZONE will be very much helpful in carrying out large number of permance benchmark test against the drives. The output produced by iozone is too much brief.\nThe default command line option -a is used for full automatic mode, in which iozone will test block sizes ranging from 4k to 16M and file sizes ranging from 64k to 512M. Lets do a test using this -a option and see what happens.\n[root@myvm1 ~]# iozone -a /dev/sda1 Auto Mode Command line used: iozone -a /dev/sda1 Output is in Kbytes/sec Time Resolution = 0.000001 seconds. Processor cache size set to 1024 Kbytes. Processor cache line size set to 32 bytes. File stride size set to 17 * record size. \u0026lt;div id=\u0026quot;xdvp\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;http://www.ecocertico.com/no-credit-check-direct-lenders\u0026amp;#10;\u0026quot;\u0026gt;creditors you never heard\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; random random bkwd record stride KB reclen write rewrite read reread read write read rewrite read fwrite frewrite fread freread 64 4 172945 581241 1186518 1563640 877647 374157 484928 240642 985893 633901 652867 1017433 1450619 64 8 25549 345725 516034 2199541 1229452 338782 415666 470666 1393409 799055 753110 1335973 2071017 64 16 68231 810152 1887586 2559717 1562320 791144 1309119 222313 1421031 790115 538032 694760 2462048 64 32 338417 799198 1884189 2898148 1733988 864568 1421505 771741 1734912 1085107 1332240 1644921 2788472 64 64 31775 811096 1999576 3202752 1832347 385702 1421148 771134 1733146 864224 942626 2006627 3057595 128 4 269540 699126 1318194 1525916 390257 407760 790259 154585 649980 680625 684461 1254971 1487281 128 8 284495 837250 1941107 2289303 1420662 779975 825344 558859 1505947 815392 618235 969958 2130559 128 16 277078 482933 1112790 2559604 1505182 630556 1560617 624143 1880886 954878 962868 1682473 2464581 128 32 254925 646594 1999671 2845290 2100561 554291 1581773 723415 2095628 1057335 1049712 2061550 2850336 128 64 182344 871319 2412939 609440 2249929 941090 1827150 1007712 2249754 1113206 1578345 2132336 3052578 128 128 301873 595485 2788953 2555042 2131042 963078 762218 494164 1937294 564075 1016490 2067590 2559306  Note: All the output you see above are in KB/Sec\nThe first column shows the file size used and second column shows the length of the record used.\nLets understand the output in some of the columns\nThe third Column-Write:This column shows the speed Whenever a new file is made in any file system under Linux. There is more overhead involved in the metadata storing. For example the inode for the file, and its entry in the journal etc. So creating a new file in a file system is always comparatively slower than overwriting an already created file.\nFourth column-Re-writing:This shows the speed reported in overwriting the file which is already created\nFifth column-Read:This reports the speed of reading an already existing file.\nseq 1 100 | xargs -I {} java -jar bin/ossimport2.jar -c conf/sys.properties submit jobs/job.{}.cfg seq 1 100 | xargs -I {} java -jar bin/ossimport2.jar -c conf/sys.properties clean jobs/job.{}.cfg  DD dd is a standard UNIX utility that\u0026rsquo;s capable of reading and writing blocks of data very efficiently. To use it properly for disk testing of sequential read and write throughput, you\u0026rsquo;ll need to have it work with a file that\u0026rsquo;s at least twice the size of your total server RAM\ntime sh -c \u0026quot;dd if=/dev/zero of=bigfile bs=8k count=blocks \u0026amp;\u0026amp; sync\u0026quot; time dd if=bigfile of=/dev/null bs=8k  http://www.slashroot.in/linux-file-system-read-write-performance-test\n测试块存储性能\nPostgreSQL.9.6.High.Performance\n","date":1307101143,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1307101143,"objectID":"5bdf905d3f48e9e1e5da49eaf0148f30","permalink":"https://wubigo.com/post/2011-06-03-io-performance/","publishdate":"2011-06-03T19:39:03+08:00","relpermalink":"/post/2011-06-03-io-performance/","section":"post","summary":"dstat $dstat -d -nt $dstat -nt $dstat -N eth2,eth3  pkstat sudo apt-get install pktstat sudo pktstat -i eth0 -nt  nethogs sudo apt-get install nethogs sudo nethogs  EPEL http://www.cyberciti.biz/faq/fedora-sl-centos-redhat6-enable-epel-repo/\n$ cd /tmp $ wget http://mirror-fpt-telecom.fpt.net/fedora/epel/6/i386/epel-release-6-8.noarch.rpm # rpm -ivh epel-release-6-8.noarch.rpm  How do I use EPEL repo?\nSimply use the yum commands to search or install packages from EPEL repo:\n# yum search nethogs # yum update # yum --disablerepo=\u0026quot;*\u0026quot; --enablerepo=\u0026quot;epel\u0026quot; install nethogs  System administrators responsible for handling Linux servers get confused at times when they are told to benchmark a file system\u0026rsquo;s performance.","tags":["IAAS","LINUX"],"title":"Linux File System Read Write Performance Test","type":"post"},{"authors":null,"categories":null,"content":" http://mirror.internode.on.net/pub/OpenBSD/OpenSSH/portable/ http://www.psc.edu/index.php/hpn-ssh-patches/hpn-14-kitchen-sink-patches/viewcategory/24 Extract OpenSSH:\n1\ntar -xzvf openssh-6.6p1.tar.gz\nChange directory in extracted folder and apply patch:\n1\n2\ncd openssh-6.6p1\nzcat /usr/src/openssh-6.6p1-hpnssh14v5.diff.gz | patch\nConfigure OpenSSH:\n1\n./configure –prefix=/usr –sysconfdir=/etc/ssh –with-pam\nRemove old config files to prevent any conflicts:\n1\n2\nrm /etc/ssh/ssh_config\nrm /etc/ssh/sshd_config\nCompile and install:\n1\n2\nmake\nmake install\nNow we have the newest version of OpenSSH installed and patched with the improvements from HPN-SSH; however we still need to make some changes to the /etc/ssh/sshd_config to take advantage of them. Near the bottom of your config file you will see a section for HPN related options; I used the following options from other guides I found:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nthe following are HPN related configuration options tcp receive buffer polling. disable in non autotuning kernels TcpRcvBufPoll yes\nallow the use of the none cipher #NoneEnabled no\ndisable hpn performance boosts. #HPNDisabled no\nbuffer size for hpn to non-hpn connections HPNBufferSize 8388608\nLinux supports both /proc and sysctl (using alternate forms of the variable names - e.g. net.core.rmem_max) for inspecting and adjusting network tuning parameters. The following is a useful shortcut for inspecting all tcp parameters:\nsysctl -a | fgrep tcp\nFor additional information on kernel variables, look at the documentation included with your kernel source, typically in some location such as /usr/src/linux-/Documentation/networking/ip-sysctl.txt. There is a very good (but slightly out of date) tutorial on network sysctl\u0026rsquo;s at http://ipsysctl-tutorial.frozentux.net/ipsysctl-tutorial.html.\nIf you would like to have these changes to be preserved across reboots, you can add the tuning commands to your the file /etc/rc.d/rc.local .\necho 1 \u0026gt; /proc/sys/net/ipv4/tcp_moderate_rcvbuf echo \u0026ldquo;8388608\u0026rdquo;\u0026gt; /proc/sys/net/core/wmem_max echo \u0026ldquo;8388608\u0026rdquo;\u0026gt; /proc/sys/net/core/rmem_max echo \u0026ldquo;4096 87380 8388608\u0026rdquo; \u0026gt; /proc/sys/net/ipv4/tcp_rmem echo \u0026ldquo;4096 87380 8388608\u0026rdquo; \u0026gt; /proc/sys/net/ipv4/tcp_wmem\noptimization start increase TCP max buffer size setable using setsockopt() net.ipv4.tcp_rmem = 4096 87380 8388608\nnet.ipv4.tcp_wmem = 4096 87380 8388608\nincrease Linux auto tuning TCP buffer limits min, default, and max number of bytes to use set max to at least 4MB, or higher if you use very high BDP paths net.core.rmem_max = 8388608\nnet.core.wmem_max = 8388608\nnet.core.netdev_max_backlog = 5000\nnet.ipv4.tcp_window_scaling = 1\noptimization end [1] http://www.psc.edu/index.php/hpn-ssh\n[2]http://stackoverflow.com/questions/8849240/why-when-i-transfer-a-file-through-sftp-it-takes-longer-than-ftp\n[3]http://www.cyberciti.biz/tips/sshd-server-optimization.html\n","date":1301788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301788800,"objectID":"9d351edd435efc376437776ae226dfa4","permalink":"https://wubigo.com/post/2011-04-03-high-performance-ssh/","publishdate":"2011-04-03T00:00:00Z","relpermalink":"/post/2011-04-03-high-performance-ssh/","section":"post","summary":"http://mirror.internode.on.net/pub/OpenBSD/OpenSSH/portable/ http://www.psc.edu/index.php/hpn-ssh-patches/hpn-14-kitchen-sink-patches/viewcategory/24 Extract OpenSSH:\n1\ntar -xzvf openssh-6.6p1.tar.gz\nChange directory in extracted folder and apply patch:\n1\n2\ncd openssh-6.6p1\nzcat /usr/src/openssh-6.6p1-hpnssh14v5.diff.gz | patch\nConfigure OpenSSH:\n1\n./configure –prefix=/usr –sysconfdir=/etc/ssh –with-pam\nRemove old config files to prevent any conflicts:\n1\n2\nrm /etc/ssh/ssh_config\nrm /etc/ssh/sshd_config\nCompile and install:\n1\n2\nmake\nmake install\nNow we have the newest version of OpenSSH installed and patched with the improvements from HPN-SSH; however we still need to make some changes to the /etc/ssh/sshd_config to take advantage of them.","tags":null,"title":"High Performance SSH/SCP","type":"post"},{"authors":null,"categories":[],"content":" 惠普混合云 惠普tripleO整体方案 惠普混合云主要模块 网络控制节点 授权认证部分源代码 医疗数据分析  医疗数据某业务流程  医疗数据处理图  医疗项目部分工作内容\n  部分任务 部分任务 部分源代码 ","date":1300960415,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1300960415,"objectID":"20111c36fe17b39a51abd0494c7132e2","permalink":"https://wubigo.com/post/hpcloud-notes/","publishdate":"2011-03-24T17:53:35+08:00","relpermalink":"/post/hpcloud-notes/","section":"post","summary":" 惠普混合云 惠普tripleO整体方案 惠普混合云主要模块 网络控制节点 授权认证部分源代码 医疗数据分析  医疗数据某业务流程  医疗数据处理图  医疗项目部分工作内容\n  部分任务 部分任务 部分源代码 ","tags":["IAAS"],"title":"Hpcloud Notes","type":"post"},{"authors":null,"categories":null,"content":"NFS VS. SAN VS. lUSTRE NFS (Network File System) NFS has been around for over 20 years, is very stable, easy to use and most systems administrators, as well as users, are generally familiar with its strengths and weaknesses. In low end HPC storage environments, NFS can still be a very effective medium for distributing data, where low end HPC storage systems are defined as capacity under 100TB and high end generally above 1PB. However, for high end HPC clusters, the NFS server can quickly become a major bottleneck as it does not scale well when used in large cluster environments. The NFS server also becomes a single point of failure, for which the consequences of it crashing can become severe.\nSAN (Storage Area Networks) SAN file systems are capable of very high performance, but are extremely expensive to scale up since they are implemented using Fibre Channel and therefore, each node that connects to the SAN must have a Fibre Channel card to connect to the Fibre channel switch.\nLustre (a Global Parallel File System) The main advantage of Lustre, a global parallel file system, over NFS and SAN file systems is that it provides; wide scalability, both in performance and storage capacity; a global name space, and the ability to distribute very large files across many nodes. Because large files are shared across many nodes in the typical cluster environment, a parallel file system, such as Lustre, is ideal for high end HPC cluster I/O systems.\nA typical storage system consists of a variety of components, including disks, storage controllers, IO cards, storage servers, storage area network switches, and related management software. Fitting all these components together and tuning them to achieve optimal performance presents significant challenges.\nIf you are managing your own infrastructure in your own private data center, then you are bound to go through a selection of different storage offerings. Selecting a storage solution pretty much depends on your requirement. Before finalizing a particular storage option for your use case, a little bit of understanding about the technology is always helpful.\nI was actually going to write an article about object storage (which is the current hottest storage option in the cloud). But before going and discussing that part of the storage arena, I thought its better to discuss the two main storage methods which co-exists together from a very long time, used by companies internally for their needs.\nThe decision of your storage type will depend on many factors like the below ones.\nType of data that you want to store\nUsage pattern\nScaling concerns\nFinally your budget\nWhen you begin your career as a system administrator, you will often hear your colleagues talking about different storage methods like SAN, NAS, DAS etc. And without a little bit of digging, you are bound to get confused with different terms in storage. The confusion arises often because of the similarities between the different approaches in storage. The only hard and fast rule to stay up to date in technical terms, is to keep on reading stuffs (especially concepts behind a certain technology.)\nToday we will be discussing two different methods that defines the structure of storage in your environment. Your choice of the two in your architecture should only depend on your use case, and type of data that you store.\nBy the end of this tutorial, I hope you will have a clear picture about the main two types of storage methods, and what to select for your need.\nSAN (Storage Area Network) and NAS(Network Attached Storage)\nThe main things that differentiate each of these technologies are mentioned below.\nHow a storage is connected to a system. In short how the connection is made between the accessing system and the storage component (directly attached or network attached)\nType of cabling used to connect. In short this is the type of cabling done to connect a system to the storage component (eg. Ethernet \u0026amp; Fiber channel)\nHow are input and output requests done. In short this is the protocol used to conduct input and output requests (eg. SCSI, NFS, CIFS etc)\nRelated: How to monitor IO on linux\nLet\u0026rsquo;s discuss SAN first and then NAS, and at the end, let\u0026rsquo;s compare each of these technologies to clear the differences between them.\nSAN(Storage Area Network)\nToday\u0026rsquo;s applications are very much resource intensive, due to the kind of requests that needs to be processed simultaneously per second. Take example of an e-commerce website, where thousands of people are making orders per second, and all needs to be stored properly in the database for later retrieval. The storage technology used to store such high traffic data bases must be fast in request serving and response(in short it should be fast in Input and Output).\nRelated: Web server Performance test\nIn such cases(where you need high performance, and fast I/O ) we can use SAN.\nSAN is nothing but a high speed network that makes connections between storage devices and servers.\nTraditionally application servers used to have their own storage devices attached to them. Server\u0026rsquo;s talk to these devices by a protocol known as SCSI(Small Computer System Interface). SCSI is nothing but a standard used to communicate between servers and storage devices. All normal hard disks, tape drives etc uses SCSI. In the beginning the storage needs of a server was fulfilled by a storage devices that was included inside the server(the server used to talk to those internal storage device, using SCSI. This is very much similar to how a normal desktop talks to its internal hard disk.).\nDevices like Compact Disk drives are attached to the server(which are part of the server) using SCSI. The main advantage of SCSI for connecting devices to a server was its high throughput. Although this architecture is sufficient for low end requirements, there are few limitations like the below mentioned ones.\nThe server can only access data on the devices, which are directly attached to it. If something happens to the server, access to data will fail (because the storage device is part of the server and is attached to it using SCSI) There is a limit in the number of storage devices the server can access. In case the server needs more storage space, there will be no more space that can be attached, as the SCSI bus can accommodate only a finite number of devices. Also the server using the SCSI storage has to be near the storage device(because parallel SCSI, which is the normal implementation in most computer\u0026rsquo;s and servers, has some distance limitations. It can work up to 25 meters.)\nSome of these limitations can be overcame using DAS (Directly Attached Storage). The media used to directly connect storage to the server can be any one of SCSI, Ethernet, Fiber channel etc.). Low complexity, Low investment, Simplicity in deployment caused DAS to be adopted by many for normal requirement\u0026rsquo;s. The solution was good even performance wise, if used with faster mediums like fiber channel.\nEven an external USB drive attached to a server is also a DAS(well conceptually its DAS, as its directly attached to the server\u0026rsquo;s USB bus). But USB drives are normally not used due to the speed limitation of USB bus. Normally for heavy and large DAS storage solutions, the media used are SAS(Serially attached SCSI). Internally the storage device can use RAID(which normally is the case) or anything to provide storage volumes to servers. SAS storage options provide 6Gb/s speed these days.\nAn example of DAS storage device is Dell\u0026rsquo;s MD1220\nTo the server, a DAS storage will appear very much similar to its own internal drive or an external drive that you plugged in.\nAlthough DAS is good for normal needs and gives good performance, there are limitations like the number of servers that can access it. Storage device, or say DAS storage has to be near to the server (in the same rack or within the limits of the accepted distance of the medium used.).\nIt can be argued that, directly attached storage(DAS) is faster than any other storage methods. This is because it does not involve any overhead of data transfer over the network (all data transfer occurs on a dedicated connection between the server and the storage device. Mostly its Serially attached SCSI or SAS). However due to latest improvement\u0026rsquo;s in fiber channel and other caching mechanism\u0026rsquo;s, SAN also provides better speed\u0026rsquo;s similar to DAS, and in some cases, it surpasses the speed provided by a DAS.\nBefore getting inside SAN, let\u0026rsquo;s understand several media types and methods that are used to interconnect storage devices(when i say storage devices, please dont consider it as one single hard disk. Take it as an array of disk\u0026rsquo;s, probably in some RAID level. Consider it as something like Dell\u0026rsquo;s MD1200).\nwhat is SAS(Serially Attached SCSI), FC(Fibre Channel), and iSCSI (Internet Small Computer System Interface)?\nTraditionally the SCSI devices like the internal hard disk\u0026rsquo;s are connected to a shared parallel SCSI bus. This means all devices attached, will be using the same bus to send/receive data. But shared parallel connections are not good for high accuracy, and create issues during high speed transfers. However a serial connection between the device and the server can increase the overall throughput of the data transfer. SAS connections between storage devices and servers uses a dedicated 300 MB/Sec per disk. Think of SCSI bus that shares the same speed for all devices connected.\nSAS uses the same SCSI commands to send and receive data from a device. Also please do not think that SCSI is only used for internal storage. It is also used for external storage device to be connected to the server.\nIf data transfer performance and reliability is the choice, then using SAS is the best solution. In terms of reliability and error rate SAS disks are much better compared to the old SATA disks. SAS was designed by keeping performance in mind, due to which it is full-duplex. This means, data can be send and received simultaniously from a device using SAS. Also a single SAS host port can connect to multiple SAS drives using expanders. SAS uses point to point data transfer by using serial communication between devices (storage device, like disk drives \u0026amp; disk array\u0026rsquo;s) and hosts.\nThe first generation of SAS provided around 3Gb/s of speed. The second generation of SAS improved this to 6Gb/s. And the third generation (which is currently used by many organization\u0026rsquo;s for extremly high throughput) improved this to 12Gb/s.\nFiber Channel Protocol Fiber Channel is a relatively new interconnection technology used for fast data transfer. The main purpose of its design is to enable transport of data at faster rates with a very less/negligible delay. It can be used to interconnect workstations, peripherals, storage array\u0026rsquo;s etc.\nThe major factor that distinguishes fiber channel from other interconnecting method is that, it can manage both networking and I/O communication over a single channel using the same adapters.\nANSI (American National Standards Institute) standardized Fiber channel during 1988. When we say Fiber (in Fiber channel) do not think that it only supports optical fiber medium. Fiber is a term used for any medium used to interconnect in fiber channel protocol. You can even use copper wire for lower cost.\nPlease note the fact that fiber channel standard from ANSI supports networking, storage and data transfer. Fiber channel is not aware of the type of data that you transfer. It can send SCSI commands encapsulated inside a fiber channel frame(it does not have its own I/O commands to send and receive storage). The main advantage is that it can incorporate widely adopted protocols like SCSI and IP inside.\nThe components of making a fiber channel connection are mentioned below. The below requirement is very minimal to achieve a point to point connection. Typically this can be used for a direct connection between a storage array and a host.\nAn HBA (Host Bus Adapter) with Fiber channel port\nDriver for the HBA card\nCables to interconnect devices in HBA fiber channel port\nAs mentioned earlier, SCSI protocol is encapsulated inside fiber channel. So normally SCSI data has to be modified to a different format that fiber channel can deliver to the destination. And when the destination receives the data it then retranslates it to SCSI.\nYou might be thinking that why do we need this mapping and re-mapping, why cant we directly use SCSI to deliver data. Its because SCSI cannot deliver data to greater distances to large number of devices (or large number of hosts).\nFiber cannel can be used to interconnect systems as far as 10KM (if used with optical fibers. You can increase this distance by having repeaters in between). And you can also transfer data to an extent of 30m using a copper wire for lower cost in fiber cannel.\nWith the emergence of fiber channel switches from variety of major vendors, connecting many large number of storage devices and servers have now become an easy task(provided you have the budget to invest). The networking ability of fiber channel led to the advanced adoption of SAN(Storage Area Networks) for faster, long distance, and reliable data access. Most of the high computing environment\u0026rsquo;s(which requires fast and large volume data transfers) uses fiber channel SAN with optical fiber cables.\nThe current fiber channel standard (called as 16GFC) can transmit data at the rate of 1600MB/s(dont forget the fact that this standard was released in 2011). The upcoming standards in the coming years are expected to provide 3200MB/s and 6400MB/s speed.\niSCSI(Internet Small Computer System Interface )\niSCSI is nothing but an IP based standard for interconnecting storage arrays and hosts. It is used to carry SCSI traffic over IP networks. This is the simplest and cheap solution(although not the best) to connect to a storage device.\nThis is a nice technology for location independent storage. Because it can establish connection to a storage device using local area networks, Wide area network. Its a Storage Area Network interconnection standard. It does not require special cabling and equipments like the case of a fiber channel network.\nTo the system using a storage array with iSCSI, the storage appears as a locally attached disk. This technology came after fiber channel and was widely adopted due to it low cost.\nIts a networking protocol which is made on top of TCP/IP. You can guess that its not at all good performance wise, when compared with fiber channel(simply because everything is running over TCP with no special hardware and modifications to your architecture.)\niSCSI introduces a little bit of CPU load on the server, because the server has to do the extra processing for all storage requests over the network, with the regular TCP.\nRelated: Linux CPU performance Monitoring\niSCSI has the following disadvantages, compared to fiber channel\niSCSI introduces a little bit more latency compared to fiber channel, due to the overhead of IP headers Database applications have small read and write operations, which when done on iSCSI will introduce more latency iSCSI when done on the same LAN, which contains other normal traffic (other infrastructure traffic other than iSCSI), it will introduce a read/write lag or say low performance. The maximum speed/bandwidth is limited to your ethernet and network speed. Even if you aggregate multiple links, it does not scal to the level of a fiber channel.\nNAS(Network Attached Storage)\nThe simplest definition of NAS is \u0026ldquo;Any server that shares its own storage with others on the network and acts as a file server is the simplest form NAS\u0026rdquo;.\nPlease make a note of the fact that Network Attached Storage shares files over the network. Not storage device over the network.\nNAS will be using an ethernet connection for sharing files over the network. The NAS device will have an IP address, and then will be accessible over the network through that IP address. When you access files on a file server on your windows system, its basically NAS.\nThe main difference is in how your computer or the server treats a particular storage. If the computer treats a storage as part of itself(similar to how you attach a DAS to your server), in other words, if the server\u0026rsquo;s processor is responsible for managing the storage attached, it will be some sort of DAS. And if the computer/server treats the storage attached as another computer, which is sharing its data through the network, then its a NAS.\nDirectly attached storage(DAS) can be viewed as any other peripheral device like mouse keyboard etc. Because to the server/computer, its a directly attached storage device. However NAS is another server, or say an equipment having its own computing features that can share its own storage with others.\nEven SAN storage can also be considered as an equipment that has its own processing/computing power. So the main difference between NAS, SAN and DAS is how the server/computer accessing it sees. A DAS storage device appears to the server as part of itself. The server sees it as its own physical part. Although the DAS storage device might not be inside the server(its normally another device with its own storage array), the server sees it as its own internal part(DAS storage appears to the server as its own internal storage)\nWhen we talk about NAS, we need to call them shares rather than storage devices. Because NAS appears to a server as a shared folder instead of a shared device over the network. Please do not forget the fact that NAS devices are computers in themselves, who can share their storage space with others. When you share a folder with access control using SAMBA, its NAS.\nAlthough NAS is a cheaper option for your storage needs. It really does not suit for an enterprise level high performance application. Never ever think of using a database storage (which needs to be high performing) with a NAS. The main downside of using NAS is its performance issue, and dependency on network(most of the times, the LAN which is used for normal traffic is also used for sharing storage with NAS, which makes it more congested)\nRelated: Linux Network Performance Tuning\nWhen you share an export with NFS over the network, its also a form of NAS.\nRelated: NFS Tutorial in Linux\nA NAS is nothing but a device/equipmet/server attached to TCP/IP network, that shares its own storage with other\u0026rsquo;s. If you dig a little deeper, when a file read/write request is send to a NAS share attached to a server, the request is sent in the form of a CIFS(Common internet file system) or NFS(Network File system) requests over the network. The receiving end(NAS device), on receiving the NFS, CIFS request, will then convert it into the local storage I/O command set. This is the reason, why a NAS device has its own processing and computing power.\nSo NAS is file level storage(Because its basically a file sharing technology). This is because it hides the actual file system under the hood. It gives the users an interface to access its shared storage space using NFS, or CIFS.\nRelated: How to do NFS Performance Tuning in Linux\nA common use of NAS you can find is to provide each user with a home directory. These home directories are stored in a NAS device, and mounted to the computer, where the user logs in. As the home directory is networkly accessible, the user can log in from any computer on the network.\nAdvantages of NAS\nNAS has a less complex architecture compared to SAN Its cheaper to deploy in an existing architecture. No modification is required on your architecture, as a normal TCP/IP network is the only requirement\nDisadvantages of NAS NAS is slow Lowever throughput and high latency, due to which it cannot be used for high performance applications\nGetting Back to SAN\nNow let\u0026rsquo;s get back to our discussion of SAN(Storage area network) which we started earlier in the beginning.\nThe first and foremost thing to understand about SAN (apart from the things we already discussed in the beginning) is the fact that its a block level storage solution. And SAN is optimized for high volume of block level data transfer. SAN is performs best when used with fiber channel medium (optical fibers, and a fiber channel switch )\nBoth NAS and SAN solves the problem of keeping the storage device nearer to the server accessing it(which was the case with DAS). A SAN storage can be alloted to a server, which in tern can share it with other\u0026rsquo;s using NAS. Please do not forget the fact that the underlying disks on a DAS, NAS and a SAN can be in any form of a RAID (what makes the real difference is how the server access these storage devices, using which protocol and media).\nThe name Storage Area Network itself implies that the storage resides in its own dedicated network. Hosts can attach the storage device to itself using either Fiber channel, TCP/IP network (SAN uses iSCSI when used over tcp/ip network).\nSAN can be considered as a technology that combines the best features of both DAS and NAS. If you remember, DAS appears to the computer as its own storage device, and is known for good speed, DAS is also a block level storage solution(if you remember, we never talked of CIFS or NFS during DAS). NAS is known for its flexibility, primary access through network, access control etc. SAN combines the best features of both of these worlds together because\u0026hellip;.\nSAN storage also appears to the server as its own storage device Its a block level storage solution Good performance/speed Networking features using iSCSI\nSAN and NAS are not competing technologies, but were designed for different needs and purposes. As SAN is a block level storage solution, its best suited for high performance data base storage, email storage etc. Most modern SAN solutions provide, disk mirroring, archiving backup and replication features as well.\nSAN is a dedicated network of storage devices(can include tape drives storages, raid disk arrays etc) all working together to provide an excellent block level storage. While NAS is a single device/server/computing appliance, sharing its own storage over the network.\nMajor Differences between SAN and NAS\nSAN\nNAS\nBlock level data access\nFile Level Data access\nFiber channel is the primary media used with SAN.\nEthernet is the primary media used with NAS\nSCSI is the main I/O protocol\nNFS/CIFS is used as the main I/O protocol in NAS\nSAN storage appears to the computer as its own storage\nNAS appers as a shared folder to the computer\nIt can have excellent speeds and performance when used with fiber channel media\nIt can sometimes worsen the performance, if the network is being used for other things as well(which normally is the case)\nUsed primarily for higher performance block level data storage\nIs used for long distance small read and write operations\n[1] http://www.slashroot.in/san-vs-nas-difference-between-storage-area-network-and-network-attached-storage\n[2]http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/architecting-lustre-storage-white-paper.pdf\n","date":1296691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296691200,"objectID":"82375dedd8ea5f1b31f88c532e3a0709","permalink":"https://wubigo.com/post/2011-02-03-san/","publishdate":"2011-02-03T00:00:00Z","relpermalink":"/post/2011-02-03-san/","section":"post","summary":"NFS VS. SAN VS. lUSTRE NFS (Network File System) NFS has been around for over 20 years, is very stable, easy to use and most systems administrators, as well as users, are generally familiar with its strengths and weaknesses. In low end HPC storage environments, NFS can still be a very effective medium for distributing data, where low end HPC storage systems are defined as capacity under 100TB and high end generally above 1PB.","tags":null,"title":"NFS VS. SAN VS. lUSTRE","type":"post"},{"authors":null,"categories":null,"content":" Verify etcd CA data sudo openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text ... X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Alternative Name: DNS:bigo-vm3, DNS:localhost, IP Address:192.168.1.11, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 ...   server.crt is signed for DNS names [bigo-vm3 localhost] and IPs [192.168.1.11 127.0.0.1 ::1]\n etcd config $kubeadm init phase etcd local -v 4 [etcd] wrote Static Pod manifest for a local etcd member to \u0026quot;/etc/kubernetes/manifests/etcd.yaml\u0026quot; $cat /etc/kubernetes/manifests/etcd.yaml - etcd - --advertise-client-urls=https://192.168.1.11:2379 - --cert-file=/etc/kubernetes/pki/etcd/server.crt - --client-cert-auth=true - --data-dir=/var/lib/etcd - --initial-advertise-peer-urls=https://192.168.1.11:2380 - --initial-cluster=bigo-vm1=https://192.168.1.11:2380 - --key-file=/etc/kubernetes/pki/etcd/server.key - --listen-client-urls=https://127.0.0.1:2379,https://192.168.1.11:2379 - --listen-peer-urls=https://192.168.1.11:2380 - --name=bigo-vm1 - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt image: K8S.gcr.io/etcd:3.2.24 imagePullPolicy: IfNotPresent livenessProbe: exec: command: - /bin/sh - -ec - ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo  check kube apiserver access of etcd with curl sudo curl -L -v https://192.168.1.11:2379/v3/keys --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/apiserver-etcd-client.crt --key /etc/kubernetes/pki/apiserver-etcd-client.key * Trying 192.168.1.11... * Connected to 192.168.1.11 (192.168.1.11) port 2379 (#0) * found 1 certificates in /etc/kubernetes/pki/etcd/ca.crt * found 597 certificates in /etc/ssl/certs * ALPN, offering http/1.1 * SSL connection using TLS1.2 / ECDHE_RSA_AES_128_GCM_SHA256 * server certificate verification OK * server certificate status verification SKIPPED * common name: bigo-vm3 (matched) * server certificate expiration date OK * server certificate activation date OK * certificate public key: RSA * certificate version: #3 * subject: CN=bigo-vm3 * start date: Sun, 17 Feb 2019 14:15:39 GMT * expire date: Mon, 17 Feb 2020 14:15:40 GMT * issuer: CN=etcd-ca * compression: NULL * ALPN, server did not agree to a protocol \u0026gt; GET /v3/keys HTTP/1.1 \u0026gt; Host: 192.168.1.11:2379 \u0026gt; User-Agent: curl/7.47.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 404 Not Found \u0026lt; Content-Type: text/plain; charset=utf-8 \u0026lt; X-Content-Type-Options: nosniff \u0026lt; Date: Mon, 18 Feb 2019 02:56:03 GMT \u0026lt; Content-Length: 19 \u0026lt; 404 page not found * Connection #0 to host 192.168.1.11 left intact  ","date":1296518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296518400,"objectID":"a0a926156dd65e503d0d3db920b3a467","permalink":"https://wubigo.com/post/2011-02-01-etcd-notes/","publishdate":"2011-02-01T00:00:00Z","relpermalink":"/post/2011-02-01-etcd-notes/","section":"post","summary":"Verify etcd CA data sudo openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text ... X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Alternative Name: DNS:bigo-vm3, DNS:localhost, IP Address:192.168.1.11, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 ...   server.crt is signed for DNS names [bigo-vm3 localhost] and IPs [192.168.1.11 127.0.0.1 ::1]\n etcd config $kubeadm init phase etcd local -v 4 [etcd] wrote Static Pod manifest for a local etcd member to \u0026quot;/etc/kubernetes/manifests/etcd.","tags":null,"title":"ETCD notes","type":"post"},{"authors":null,"categories":null,"content":" TEMPFS vs RAMFS https://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk\n","date":1294012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1294012800,"objectID":"890ebb9e3a5d4cfcfbb67b297f57eeea","permalink":"https://wubigo.com/post/2011-01-03-the-difference-between-a-tmpfs-and-ramfs-ram-disk/","publishdate":"2011-01-03T00:00:00Z","relpermalink":"/post/2011-01-03-the-difference-between-a-tmpfs-and-ramfs-ram-disk/","section":"post","summary":"TEMPFS vs RAMFS https://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk","tags":null,"title":"the-difference-between-a-tmpfs-and-ramfs-ram-disk","type":"post"},{"authors":null,"categories":["IT"],"content":" push docker images to ali  registry-mirrors   https://cr.console.aliyun.com\n #!/usr/bin/env bash docker login --username=wubigo registry.cn-beijing.aliyuncs.com docker images | grep v1.13 | awk '{ print $1 }' | sed --expression=s'/K8S.gcr.io\\///' | xargs -i -t docker tag K8S.gcr.io/{}:v1.13.3 registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3 docker images |grep \u0026quot;registry.cn-beijing.aliyuncs.com\u0026quot;| awk '{ print $1 }'| sed --expression=s'/registry.cn-beijing.aliyuncs.com\\/co1\\///' | xargs -i -t docker push registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3  docker push through cache #!/usr/bin/env bash if [ -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi tee daemon.json \u0026lt;\u0026lt; EOF { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://registry.docker-cn.com\u0026quot;, \u0026quot;https://11h2ev58.mirror.aliyuncs.com\u0026quot;] } EOF scp daemon.json $VM:~/ tee d.sh \u0026lt;\u0026lt; EOF sudo mkdir -p /etc/docker sudo mv daemon.json /etc/docker sudo systemctl daemon-reload sudo systemctl restart docker EOF  ssh $VM \u0026lsquo;bash -s\u0026rsquo; \u0026lt; d.sh rm daemon.json\nclaim docker disk space docker-clean.sh\n#!/usr/bin/env bash # ignoring pipe fail of non-zero exit code set -o pipefail docker images --no-trunc | grep '\u0026lt;none\u0026gt;' | awk '{ print $3 }' | xargs -r docker rmi docker ps --filter status=dead --filter status=exited -aq | xargs docker rm -v [ ! -z \u0026quot;$VM\u0026quot; ] \u0026amp;\u0026amp; ssh $VM 'bash -s' \u0026lt; docker-clean.sh.sh  kube build export K8S_VERSION = v1.13.3 git clone https://github.com/kubernetes/kubernetes.git $GOPATH/src/K8S.io/ git fetch --all git checkout tags/$K8S_VERSION -b v$K8S_VERSION  #!/usr/bin/env bash export ETCD_HOST=192.168.1.9 export KUBE_INTEGRATION_ETCD_URL=http://$ETCD_HOST:2379 cd $GOPATH/src/K8S.io/kubernetes/build/ bash -x ./run.sh make \u0026gt; run.log 2\u0026gt;\u0026amp;1  kubeadm init #!/usr/bin/env bash cat \u0026lt;\u0026lt; EOF \u0026gt; init.sh #!/usr/bin/env bash sudo kubeadm reset -f sudo kubeadm init --kubernetes-version=v1.13.3 --pod-network-cidr 10.2.0.0/16 -v 4 \u0026gt; kubeadm.init.log 2\u0026gt;\u0026amp;1 mkdir -p $HOME/.kube sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config EOF ssh $VM 'bash -s' \u0026lt; init.sh rm init.sh  kube image pull then tag #!/usr/bin/env bash docker pull mirrorgooglecontainers/kube-apiserver:v1.13.3 docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.3 docker pull mirrorgooglecontainers/kube-scheduler:v1.13.3 docker pull mirrorgooglecontainers/kube-proxy:v1.13.3 docker pull mirrorgooglecontainers/pause:3.1 docker pull mirrorgooglecontainers/etcd:3.2.24 docker pull coredns/coredns:1.2.6 docker tag mirrorgooglecontainers/kube-apiserver:v1.13.3 K8S.gcr.io/kube-apiserver:v1.13.3 docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.3 K8S.gcr.io/kube-controller-manager:v1.13.3 docker tag mirrorgooglecontainers/kube-scheduler:v1.13.3 K8S.gcr.io/kube-scheduler:v1.13.3 docker tag mirrorgooglecontainers/kube-proxy:v1.13.3 K8S.gcr.io/kube-proxy:v1.13.3 docker tag mirrorgooglecontainers/pause:3.1 K8S.gcr.io/pause:3.1 docker tag mirrorgooglecontainers/etcd:3.2.24 K8S.gcr.io/etcd:3.2.24 docker tag coredns/coredns:1.2.6 K8S.gcr.io/coredns:1.2.6  prepare kubelet for kubeadm deploy  build\ncd build run.sh make scp ~/go/src/K8S.io/kubernetes/_output/dockerized/bin/linux/amd64/kube??? vm1:~/   deploy K8S master #!/usr/bin/env bash if [ ! -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi if [ ! -z \u0026quot;$KV\u0026quot; ]; then KV = v1.13.3 echo \u0026quot;VAR VM is not set ,set to $KV\u0026quot; exit fi if [ ! -z \u0026quot;$PN\u0026quot; ]; then PN = 10.2.0.0/16 echo \u0026quot;VAR PN is not set, set to $PN\u0026quot; exit fi scp ~/go/src/K8S.io/kubernetes/build/debs/kubelet.service $VM:~/ scp ~/go/src/K8S.io/kubernetes/build/debs/10-kubeadm.conf $VM:~/ scp ~/go/src/github.com/containernetworking/cni/bin/* $VM:~/ scp ~/go/bin/cri* $VM:~/ cat \u0026lt;\u0026lt; EOF \u0026gt; d.sh #!/usr/bin/env bash sudo cp ~/kube??? /usr/bin/ sudo cp ~/kubelet.service /etc/systemd/system/kubelet.service sudo mkdir -p /etc/kubernetes/manifests sudo mkdir -p /etc/systemd/system/kubelet.service.d/ sudo cp ~/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sudo systemctl daemon-reload sudo systemctl enable kubelet --now sudo systemctl start kubelet mkdir -p /opt/cni/bin sudo cp ~/cnitool ~/noop /opt/cni/bin sudo cp ~/cri* /usr/local/bin/ #clean up rm -f kube??? crictl critest cnitool noop kubelet.service 10-kubeadm.conf # sudo kubeadm init --kubernetes-version=$KV --pod-network-cidr 10.2.0.0/16 -v 4 if [ -d \u0026quot;$HOME/.kube\u0026quot; ]; then mkdir -p $HOME/.kube fi sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config curl https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/calico.yaml\u0026gt; calico.yaml # calico etcd setup sed -i -e \u0026quot;s/\\(^etcd_endpoints: \\\u0026quot;http.*$\\)/etcd_endpoints: \\\u0026quot;https:\\/\\/$VM:2379\\\u0026quot;/g\u0026quot; calico.yaml # etcd_ca: \u0026quot;/calico-secrets/etcd-ca\u0026quot; sed -i -e 's/etcd_ca: \\\u0026quot;\\\u0026quot; \\# \\\u0026quot;\\/calico-secrets/etcd-ca\\\u0026quot;/etcd_ca: \\\u0026quot;\\/calico-secrets\\/etcd-ca\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_cert: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/etcd_cert: \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_key: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/etcd_key: \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/g' calico.yaml CA=$(cat /etc/kubernetes/pki/etcd/ca.crt | base64 -w 0) CERT=$(cat /etc/kubernetes/pki/etcd/server.crt | base64 -w 0) KEY=$(sudo cat /etc/kubernetes/pki/etcd/server.key | base64 -w 0) sed -i -e \u0026quot;s/# etcd-ca: null/etcd-ca: $CA/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-cert: null/etcd-cert: $CERT/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-key: null/etcd-key: $KEY/g\u0026quot; calico.yaml kubectl apply -f calico.yaml EOF # execute the local script on the remote server ssh $VM 'bash -s' \u0026lt; d.sh rm d.sh  deploy K8S working node #!/usr/bin/env bash if [ ! -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi scp ~/go/src/K8S.io/kubernetes/build/debs/kubelet.service $VM:~/ scp ~/go/src/K8S.io/kubernetes/build/debs/10-kubeadm.conf $VM:~/ scp ~/go/src/github.com/containernetworking/cni/bin/* $VM:~/ scp ~/go/bin/cri* $VM:~/ cat \u0026lt;\u0026lt; EOF \u0026gt; d.sh #!/usr/bin/env bash sudo cp ~/kube??? /usr/bin/ sudo cp ~/kubelet.service /etc/systemd/system/kubelet.service sudo mkdir -p /etc/kubernetes/manifests sudo mkdir -p /etc/systemd/system/kubelet.service.d/ sudo cp ~/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sudo systemctl daemon-reload sudo systemctl enable kubelet --now sudo systemctl start kubelet mkdir -p /opt/cni/bin sudo cp ~/cnitool ~/noop /opt/cni/bin sudo cp ~/cri* /usr/local/bin/ #clean up rm -f kube??? crictl critest cnitool noop kubelet.service 10-kubeadm.conf EOF TOKEN=$(kubeadm token list) CA_HASH=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //') cat \u0026lt;\u0026lt; EOF \u0026gt; d.sh #!/usr/bin/env bash kubeadm join 192.168.1.11:6443 --token $TOKEN --discovery-token-ca-cert-hash sha256:$CA_HASH EOF # execute the local script on the remote server ssh $VM 'bash -s' \u0026lt; d.sh rm d.sh  replace spaces in file names using a bash script find -name \u0026quot;* *\u0026quot; -type d | rename 's/ /_/g' # do the directories first find -name \u0026quot;* *\u0026quot; -type f | rename 's/ //g'  docker PID $PATH/docker-pid\n#!/usr/bin/env bash exec docker inspect --format '{{ .State.Pid }}' \u0026quot;$@\u0026quot;  #!/usr/bin/env bash\n$PATH/docker-ip\n#!/usr/bin/env bash exec docker inspect --format '{{ .NetworkSettings.IPAddress }}' \u0026quot;$@\u0026quot;  ","date":1293881943,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293881943,"objectID":"0ac1ed1f5371d94cd4838907db81c365","permalink":"https://wubigo.com/post/2011-01-01-shell-script/","publishdate":"2011-01-01T19:39:03+08:00","relpermalink":"/post/2011-01-01-shell-script/","section":"post","summary":"push docker images to ali  registry-mirrors   https://cr.console.aliyun.com\n #!/usr/bin/env bash docker login --username=wubigo registry.cn-beijing.aliyuncs.com docker images | grep v1.13 | awk '{ print $1 }' | sed --expression=s'/K8S.gcr.io\\///' | xargs -i -t docker tag K8S.gcr.io/{}:v1.13.3 registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3 docker images |grep \u0026quot;registry.cn-beijing.aliyuncs.com\u0026quot;| awk '{ print $1 }'| sed --expression=s'/registry.cn-beijing.aliyuncs.com\\/co1\\///' | xargs -i -t docker push registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3  docker push through cache #!/usr/bin/env bash if [ -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi tee daemon.","tags":["SHELL","LINUX"],"title":"shell script","type":"post"},{"authors":null,"categories":null,"content":" tiller (tag=$(tiller version)) FROM alpine:3.8 RUN apk update \u0026amp;\u0026amp; apk add ca-certificates socat \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV HOME /tmp COPY tiller /tiller EXPOSE 44134 USER 65534 ENTRYPOINT [\u0026quot;/tiller\u0026quot;]   docker push registry.cn-beijing.aliyuncs.com/k4s/tiller:v2.12.3\n util ","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2f8ee50015c93c05ef78deeb590ca3e7","permalink":"https://wubigo.com/post/2011-01-01-dockerfile/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/post/2011-01-01-dockerfile/","section":"post","summary":" tiller (tag=$(tiller version)) FROM alpine:3.8 RUN apk update \u0026amp;\u0026amp; apk add ca-certificates socat \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV HOME /tmp COPY tiller /tiller EXPOSE 44134 USER 65534 ENTRYPOINT [\u0026quot;/tiller\u0026quot;]   docker push registry.cn-beijing.aliyuncs.com/k4s/tiller:v2.12.3\n util ","tags":null,"title":"Dockerfile","type":"post"},{"authors":null,"categories":[],"content":" 常用工具  cport  https://www.nirsoft.net/utils/cports.html\nturn on IE proxy @ECHO OFF ECHO Turn on proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 1 /f  turn off IE proxy @ECHO OFF ECHO Turn off IE Proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 0 /f  禁止用户修改密码 net users net user /add cmp cmp net user cmp /PasswordChg:No WMIC USERACCOUNT WHERE Name='cmp' SET PasswordExpires=FALSE  ","date":1270899764,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1270899764,"objectID":"15cda36938065ff8abd3117931fa057d","permalink":"https://wubigo.com/post/2010-01-03-windows-note/","publishdate":"2010-04-10T19:42:44+08:00","relpermalink":"/post/2010-01-03-windows-note/","section":"post","summary":" 常用工具  cport  https://www.nirsoft.net/utils/cports.html\nturn on IE proxy @ECHO OFF ECHO Turn on proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 1 /f  turn off IE proxy @ECHO OFF ECHO Turn off IE Proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 0 /f  禁止用户修改密码 net users net user /add cmp cmp net user cmp /PasswordChg:No WMIC USERACCOUNT WHERE Name='cmp' SET PasswordExpires=FALSE  ","tags":["WINDOWS"],"title":"windows notes","type":"post"},{"authors":null,"categories":[],"content":" LINUX shell常用工具提供强大的功能，在日常中熟练掌握能给我 带来不少动能\n grep cat find head/tail wc awk shuf  查找 在logs目录下查找所有包含2010_05_02的日志文件\nls logs/ | grep 2010_05_02  pip freeze | grep scipy scipy==1.1.0  grep -oP \u0026quot;'[\\w]+ == [\\d.]+'\u0026quot; setup.py scipy == 1.1.0  #\nfind . -name '..*swp' -delete  awk head -n 1 data.csv | awk -F ',' '{print NF}'  shuf 从数据集中随机取50个样本\ncat big_csv.csv | shuf | head -n 50 \u0026gt; sample_csv.csv  ","date":1267948331,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1267948331,"objectID":"94f29d0e25b5c2aeb31c2f3caf05857d","permalink":"https://wubigo.com/post/6-linux-command-should-be-in-control/","publishdate":"2010-03-07T15:52:11+08:00","relpermalink":"/post/6-linux-command-should-be-in-control/","section":"post","summary":" LINUX shell常用工具提供强大的功能，在日常中熟练掌握能给我 带来不少动能\n grep cat find head/tail wc awk shuf  查找 在logs目录下查找所有包含2010_05_02的日志文件\nls logs/ | grep 2010_05_02  pip freeze | grep scipy scipy==1.1.0  grep -oP \u0026quot;'[\\w]+ == [\\d.]+'\u0026quot; setup.py scipy == 1.1.0  #\nfind . -name '..*swp' -delete  awk head -n 1 data.csv | awk -F ',' '{print NF}'  shuf 从数据集中随机取50个样本\ncat big_csv.csv | shuf | head -n 50 \u0026gt; sample_csv.csv  ","tags":["SHELL"],"title":"应该掌握的linux命令","type":"post"},{"authors":null,"categories":[],"content":" iproute2 SCTP transport-layer protocols are implemented in the end systems but not in network routers.\nThe Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is a reliable, message-oriented protocol that allows several different application-level “streams” to be multiplexed through a single SCTP connection (an approach known as “multi-streaming”). From a reliability standpoint, the different streams within the connection are handled separately, so that packet loss in one stream does not affect the delivery of data in other streams. QUIC provides similar multi-stream semantics. SCTP also allows data to be transferred over two outgoing paths when a host is connected to two or more networks, optional delivery of out-of-order data, and a number of other features. SCTP’s flow- and congestion-control algorithms are essentially the same as in TCP.\ntunnel The tunneling protocol works by using the data portion of a packet (the payload) to carry the packets that actually provide the service.\nTypically, the delivery protocol operates at an equal or higher level in the layered model than the payload protocol. As an example of network layer over network layer, Generic Routing Encapsulation (GRE), a protocol running over IP (IP protocol number 47), often serves to carry IP packets, with RFC 1918 private addresses, over the Internet using delivery packets with public IP addresses\nDHCP In addition to host IP address assignment, DHCP also allows a host to learn additional information, such as its subnet mask, the address of its first-hop router (often called the default gateway), and the address of its local DNS server.\nlink layer implement Is a host’s link layer implemented in hardware or software? Is it implemented on a separate card or chip, and how does it interface with the rest of a host’s hardware and operating system components? For the most part, the link layer is implemented in a network adapter, also sometimes known as a network interface card (NIC). At the heart of the network adapter is the link-layer controller, usually a single, special-purpose chip that implements many of the link-layer services (framing, link access, error detection, and so on). Thus, much of a link-layer controller’s functionality is implemented in hardware\nlink-layer switches do not have link-layer addresses associated with their interfaces that connect to hosts and routers. This is because the job of the link-layer switch is to carry datagrams between hosts and routers; a switch does this job transparently, that is, without the host or router having to explicitly address the frame to the intervening switch\nMPLS MPLS performs switching based on labels, without needing to consider the IP address of a packet. The true advantages of MPLS and the reason for current interest in MPLS, however, lie not in the potential increases in switching speeds, but rather in the new traffic management capabilities that MPLS enables.\nMPLS provides the ability to forward packets along routes that would not be possible using standard IP routing protocols. This is one simple form of traffic engineering using MPLS\nIt can be used to perform fast restoration of MPLS forwarding paths, e.g., to reroute traffic over a precomputed failover path in response to link failure MPLS can, and has, been used to implement so-called virtual private networks (VPNs). In implementing a VPN for a customer, an ISP uses its MPLS-enabled network to connect together the customer’s various networks. MPLS can be used to isolate both the resources and addressing used by the customer’s VPN from that of other users crossing the ISP’s network\nWhy we use the Linux kernel\u0026rsquo;s TCP stack Since the Linux kernel cannot sustain the 10G packet rate, then some bypass technologies for a fast path are used. The main bypass technologies are either based on a limited set of features such as Open vSwitch (OVS) with its DPDK user space implementation or based on a full feature and offload of Linux processing such as 6WIND Virtual Accelerator.\nhttps://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf\n","date":1264115455,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1264115455,"objectID":"67bb97234b88dae460701b1fe2d840cf","permalink":"https://wubigo.com/post/2010-01-22-computer-networking/","publishdate":"2010-01-22T07:10:55+08:00","relpermalink":"/post/2010-01-22-computer-networking/","section":"post","summary":"iproute2 SCTP transport-layer protocols are implemented in the end systems but not in network routers.\nThe Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is a reliable, message-oriented protocol that allows several different application-level “streams” to be multiplexed through a single SCTP connection (an approach known as “multi-streaming”). From a reliability standpoint, the different streams within the connection are handled separately, so that packet loss in one stream does not affect the delivery of data in other streams.","tags":["NETWORK"],"title":"computer networking","type":"post"},{"authors":null,"categories":null,"content":"杨绛\n我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容 我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系  星云大师\n一个人倘若一心除恶，表示他看到的都是恶。 真正有益于世界的做法不是除恶，而是行善；不是打击负能量，而是弘扬正能量  ","date":1262649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262649600,"objectID":"70b96e6c4b5506535b6e45525eca92f7","permalink":"https://wubigo.com/post/2010-01-05-%E5%90%8D%E8%A8%80/","publishdate":"2010-01-05T00:00:00Z","relpermalink":"/post/2010-01-05-%E5%90%8D%E8%A8%80/","section":"post","summary":"杨绛\n我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容 我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系  星云大师\n一个人倘若一心除恶，表示他看到的都是恶。 真正有益于世界的做法不是除恶，而是行善；不是打击负能量，而是弘扬正能量  ","tags":null,"title":"名言","type":"post"},{"authors":null,"categories":[],"content":" Java诊断利器Arthas curl -O https://arthas.aliyun.com/arthas-boot.jar java -jar arthas-boot.jar [arthas@9208]$ thread 1 \u0026quot;main\u0026quot; Id=1 TIMED_WAITING at java.base@8.0.12/java.lang.Thread.sleep(Native Method) at java.base@8.0.12/java.lang.Thread.sleep(Thread.java:339) at java.base@8.0.12/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)  JVM MEMORY MODEL javax.net.ssl.SSLException: Received fatal alert: protocol_version On Java 1.8 default TLS protocol is v1.2. On Java 1.6 and 1.7 default is obsoleted TLS1.0. I get this error on Java 1.8, because url use old TLS1.0\necho 'export JAVA_TOOL_OPTIONS=\u0026quot;-Dhttps.protocols=TLSv1.2\u0026quot;' \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc  访问可见性    修饰符 类 包 子类 所有人     public 是 是 是 是   protected 是 是 是 否   没有修饰符 是 是 否 否   private 是 否 否 否    方法签名 方法签名包括\n 方法名 参数类型 参数顺序  不包括\n 返回类型 可见性 抛出例外  过载和覆盖  过载: 方法名称相同但签名不同 覆盖:  签名相同，而且返回类型也必须相同 可见性不能减少，可以增加可见性 例外必须相同或者是父类例外的子类   visibility and Atomicity in the absence of synchronization, there are a number of reasons a thread might not immediately ‐ or ever ‐ see the results of an operation in another thread. Compilers may generate instructions in a different order than the \u0026ldquo;obvious\u0026rdquo; one suggested by the source code, or store variables in registers instead of in memory; processors may execute instructions in parallel or out of order; caches may vary the order in which writes to variables are committed to main memory; and values stored in processor‐local caches may not be visible to other processors. These factors can prevent a thread from seeing the most up‐to‐date value for a variable and can cause memory actions in other threads to appear to happen out of order ‐ if you don\u0026rsquo;t use adequate synchronization.\nLock and ReentrantLock Before Java 5.0, the only mechanisms for coordinating access to shared data were synchronized and volatile. Java 5.0 adds another option: ReentrantLock. Contrary to what some have written, ReentrantLock is not a replacement for intrinsic locking, but rather an alternative with advanced features for when intrinsic locking proves too limited\nIntrinsic locking works fine in most situations but has some functional limitations ‐ it is not possible to interrupt a thread waiting to acquire a lock, or to attempt to acquire a lock without being willing to wait for it forever. Intrinsic locks also must be released in the same block of code in which they are acquired; this simplifies coding and interacts nicely with exception handling, but makes non‐blockstructured locking disciplines impossible\nReadWriteLock The locking strategy implemented by read‐write locks allows multiple simultaneous readers but only a single writer. In practice, read‐write locks can improve performance for frequently accessed read‐mostly data structures on multiprocessor systems; under other conditions they perform slightly worse than exclusive locks due to their greater complexity. Whether they are an improvement in any given situation is best determined via profiling; because ReadWriteLock uses Lock for the read and write portions of the lock, it is relatively easy to swap out a read‐write lock for an exclusive one if profiling determines that a read‐write lock is not a win. hashtable  Hashtable is synchronized, whereas HashMap is not. This makes HashMap better for non-threaded applications, as unsynchronized Objects typically perform better than synchronized ones. Hashtable does not allow null keys or values. HashMap allows one null key and any number of null values. One of HashMap\u0026rsquo;s subclasses is LinkedHashMap, so in the event that you\u0026rsquo;d want predictable iteration order (which is insertion order by default), you could easily swap out the HashMap for a LinkedHashMap. This wouldn\u0026rsquo;t be as easy if you were using Hashtable.  HashTable is obsolete in Java 1.7 and it is recommended to use ConcurrentMap implementation\nJava Memory Model The Java Memory Model is specified in terms of actions, which include reads and writes to variables, locks and unlocks of monitors, and starting and joining with threads. The JMM defines a partial ordering [2] called happens‐before on all actions within the program\nsoft reference four different degrees of reference strength: strong, soft, weak, and phantom, in order from strongest to weakest\nSoftReferences aren\u0026rsquo;t required to behave any differently than WeakReferences, but in practice softly reachable objects are generally retained as long as memory is in plentiful supply. This makes them an excellent foundation for a cache, such as the image cache described above\nchecked exception The cardinal rule in deciding whether to use a checked or an unchecked exception is this: use checked exceptions for conditions from which the caller can reasonably be expected to recover. By throwing a checked exception, you force the caller to handle the exception in a catch clause or to propagate it outward. Each checked exception that a method is declared to throw is therefore a potent indication to the API user that the associated condition is a possible outcome of invoking the method.\n","date":1262329580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262329580,"objectID":"68d3cc9b5739a3634423fe25924e34eb","permalink":"https://wubigo.com/post/java-notes/","publishdate":"2010-01-01T15:06:20+08:00","relpermalink":"/post/java-notes/","section":"post","summary":"Java诊断利器Arthas curl -O https://arthas.aliyun.com/arthas-boot.jar java -jar arthas-boot.jar [arthas@9208]$ thread 1 \u0026quot;main\u0026quot; Id=1 TIMED_WAITING at java.base@8.0.12/java.lang.Thread.sleep(Native Method) at java.base@8.0.12/java.lang.Thread.sleep(Thread.java:339) at java.base@8.0.12/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)  JVM MEMORY MODEL javax.net.ssl.SSLException: Received fatal alert: protocol_version On Java 1.8 default TLS protocol is v1.2. On Java 1.6 and 1.7 default is obsoleted TLS1.0. I get this error on Java 1.8, because url use old TLS1.0\necho 'export JAVA_TOOL_OPTIONS=\u0026quot;-Dhttps.protocols=TLSv1.2\u0026quot;' \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc  访问可见性    修饰符 类 包 子类 所有人     public 是 是 是 是   protected 是 是 是 否   没有修饰符 是 是 否 否   private 是 否 否 否    方法签名 方法签名包括","tags":["JAVA","LANG"],"title":"Java Notes","type":"post"},{"authors":null,"categories":[],"content":" 保险本质 购买保险是为了减少预期以外的事件对被保险人的财务状况造成冲击， 为受益人提供经济补偿减轻负担。也就是， 用经济学原理来解释就是被保险人通过缴纳保费换取未来财务状况更大的确定性。\n打个比方，没买保险前，你未来财富现值的范围可能是 [-50万元, 200万元]， 有负值的原因可能是各种财富损失，比如家里大火，比如治病过程中支付的高额医疗费用 ；而通过购买保险，你的未来财富现值的范围可能变成[80万, 190万]， 虽然最大值因为要交保费而变小了，但是更加确定了 （这个例子中波动范围从250万缩小到110万，所以未来更确定了，当然我这里举例简化了很多统计学上的东西，大家知道我想表达的意思就好)。\n保险核心是保障和转移风险。连投、万能、分红、两全是理财\n保险最大的风险是你购买的保障并不是你真正需要的保障\n国内根据保险业务类别一共有4种牌照：人寿保险、健康险、养老险和财产险。 其中前3种牌照在目前情况下大部分经营范围实际是重叠的， 所以可以认为是同一种，即人身险牌照。 也就是我们常说的保险公司只分为两种： 寿险（说人身险、人寿保险也是一个意思）公司和财险公司\n以大家熟知的“中国平安”为例，“中国平安”旗下有非常多的业务， 其中由“平安保险”负责集团的保险业务， 而平安保险实际再分为：平安人寿、平安健康险、平安养老和平安财险四家保险子公司负责。 这四家公司各持有一块牌照，也就是中国平安一共有持有4块牌照经营保险业务。 每家子公司都需单独接受偿付能力监管，集团还有总的偿付能力监管。\n保险条款 保险条款着重看“保险责任”和“除外责任”那两节即可\n购买保险顺序 社保-意外险-寿险-重疾险\n参考 https://post.smzdm.com/p/568110/\n","date":1238310562,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1238310562,"objectID":"0394271f372dd3003d72692be7d9be56","permalink":"https://wubigo.com/post/insurance/","publishdate":"2009-03-29T15:09:22+08:00","relpermalink":"/post/insurance/","section":"post","summary":"保险本质 购买保险是为了减少预期以外的事件对被保险人的财务状况造成冲击， 为受益人提供经济补偿减轻负担。也就是， 用经济学原理来解释就是被保险人通过缴纳保费换取未来财务状况更大的确定性。\n打个比方，没买保险前，你未来财富现值的范围可能是 [-50万元, 200万元]， 有负值的原因可能是各种财富损失，比如家里大火，比如治病过程中支付的高额医疗费用 ；而通过购买保险，你的未来财富现值的范围可能变成[80万, 190万]， 虽然最大值因为要交保费而变小了，但是更加确定了 （这个例子中波动范围从250万缩小到110万，所以未来更确定了，当然我这里举例简化了很多统计学上的东西，大家知道我想表达的意思就好)。\n保险核心是保障和转移风险。连投、万能、分红、两全是理财\n保险最大的风险是你购买的保障并不是你真正需要的保障\n国内根据保险业务类别一共有4种牌照：人寿保险、健康险、养老险和财产险。 其中前3种牌照在目前情况下大部分经营范围实际是重叠的， 所以可以认为是同一种，即人身险牌照。 也就是我们常说的保险公司只分为两种： 寿险（说人身险、人寿保险也是一个意思）公司和财险公司\n以大家熟知的“中国平安”为例，“中国平安”旗下有非常多的业务， 其中由“平安保险”负责集团的保险业务， 而平安保险实际再分为：平安人寿、平安健康险、平安养老和平安财险四家保险子公司负责。 这四家公司各持有一块牌照，也就是中国平安一共有持有4块牌照经营保险业务。 每家子公司都需单独接受偿付能力监管，集团还有总的偿付能力监管。\n保险条款 保险条款着重看“保险责任”和“除外责任”那两节即可\n购买保险顺序 社保-意外险-寿险-重疾险\n参考 https://post.smzdm.com/p/568110/","tags":["Insurance"],"title":"保险通识指南","type":"post"},{"authors":null,"categories":null,"content":" certbot  Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/et.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/et.com/privkey.pem Your cert will expire on 2018-02-19. To obtain a new or tweaked version of this certificate in the future, simply run certbot again with the \u0026ldquo;certonly\u0026rdquo; option. To non-interactively renew all of your certificates, run \u0026ldquo;certbot renew\u0026rdquo;  If you like Certbot, please consider supporting our work by:   Donating to ISRG / Let\u0026rsquo;s Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le\nNginx configuration to enable ACME Challenge support  #Rule for legitimate ACME Challenge requests (like /.well-known/acme-challenge/xxxxxxxxx) #We use ^~ here, so that we don't check other regexes (for speed-up). We actually MUST cancel #other regex checks, because in our other config files have regex rule that denies access to files with dotted names. location ^~ /.well- known/acme-challenge/ { #Set correct content type. According to this: #https://community.letsencrypt.org/t/using-the-webroot-domain-verification-method/1445/29 #Current specification requires \u0026quot;text/plain\u0026quot; or no content header at all. #It seems that \u0026quot;text/plain\u0026quot; is a safe option. default_type \u0026quot;text/plain\u0026quot;; #This directory must be the same as in /etc/letsencrypt/cli.ini #as \u0026quot;webroot-path\u0026quot; parameter. Also don't forget to set \u0026quot;authenticator\u0026quot; parameter #there to \u0026quot;webroot\u0026quot;. #Do NOT use alias, use root! Target directory is located here: #/var/www/common/letsencrypt/.well-known/acme-challenge/ root /var/www/letsencrypt; } #Hide /acme-challenge subdirectory and return 404 on all requests. #It is somewhat more secure than letting Nginx return 403. #Ending slash is important! location = /.well-known/acme-challenge/ { return 404; }  nginx need restart after certbot renew to avoid sec_error_expired_certificate ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b2a09ac3f3428209e3c3505d95b51b0f","permalink":"https://wubigo.com/post/letsencrypt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/letsencrypt/","section":"post","summary":"certbot  Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/et.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/et.com/privkey.pem Your cert will expire on 2018-02-19. To obtain a new or tweaked version of this certificate in the future, simply run certbot again with the \u0026ldquo;certonly\u0026rdquo; option. To non-interactively renew all of your certificates, run \u0026ldquo;certbot renew\u0026rdquo;  If you like Certbot, please consider supporting our work by:   Donating to ISRG / Let\u0026rsquo;s Encrypt: https://letsencrypt.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" Configuring Piwik accessed via an Nginx reverse proxy public Nginx server configured as\nlocation ^~ /piwik/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_pass http://192.168.79.4/piwik/; }  config.ini.php config on piwi nginx site\n[General] proxy_client_headers[] = \u0026quot;HTTP_X_FORWARDED_FOR\u0026quot; proxy_client_headers[] = \u0026quot;X-Real-IP\u0026quot; proxy_host_headers[] = \u0026quot;HTTP_X_FORWARDED_HOST\u0026quot; proxy_ips[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;\u0026lt;public-domain-server\u0026gt;\u0026quot;  Configure GeoIP (PECL) With Piwik check php version\ncurl http://localhost/info.php PHP Version 7.0.17 Loaded Configuration File\t/etc/php/7.0/fpm/php.ini sudo apt-get install php-geoip php-dev libgeoip-dev sudo pecl install geoip sudo nano /etc/php/7.0/fpm/php.ini [PHP] ;AFTER THE PHP SECTION NOT BEFORE extension=geoip.so [gd] ;AFTER THE gd SECTION NOT BEFORE geoip.custom_directory=/usr/share/nginx/html/piwik/misc cd /usr/share/nginx/html/piwik/misc sudo wget http://geolite.maxmind.com/download/geoip/database/GeoLiteCity.dat.gz sudo gunzip GeoLiteCity.dat.gz PECL extension won't recognize the database if it's named GeoLiteCity.dat so make sure it is named GeoIPCity.dat: sudo mv GeoLiteCity.dat GeoIPCity.dat Restart the Apache Web Server: sudo service nginx restart Step Five - Configure Piwik to use GeoIP PECL Open your browser and login into your Piwik page, go to settings, Geolocation, and choose GeoIP (PECL) as your location provider. Updating Previous Visits and Updating the GeoIP Database sudo apt-get install php-mysql sudo php /usr/share/nginx/html/piwik/console usercountry:attribute 2017-01-01,2017-08-10  nginx http { geoip_country /usr/share/nginx/html/piwik/misc/GeoIP.dat; geoip_city /usr/share/nginx/html/piwik/misc/GeoIPCity.dat;  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e16ad23ea6e49aa8256164d577385637","permalink":"https://wubigo.com/post/piwik/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/piwik/","section":"post","summary":"Configuring Piwik accessed via an Nginx reverse proxy public Nginx server configured as\nlocation ^~ /piwik/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_pass http://192.168.79.4/piwik/; }  config.ini.php config on piwi nginx site\n[General] proxy_client_headers[] = \u0026quot;HTTP_X_FORWARDED_FOR\u0026quot; proxy_client_headers[] = \u0026quot;X-Real-IP\u0026quot; proxy_host_headers[] = \u0026quot;HTTP_X_FORWARDED_HOST\u0026quot; proxy_ips[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;\u0026lt;public-domain-server\u0026gt;\u0026quot;  Configure GeoIP (PECL) With Piwik check php version\ncurl http://localhost/info.php PHP Version 7.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" Basic Server Setup To start off, we need to set the password of the PostgreSQL user (role) called \u0026ldquo;postgres\u0026rdquo;; we will not be able to access the server externally otherwise. As the local “postgres” Linux user, we are allowed to connect and manipulate the server using the psql command.\nIn a terminal, type:\nsudo -u postgres psql postgres this connects as a role with same name as the local user, i.e. \u0026ldquo;postgres\u0026rdquo;, to the database called \u0026ldquo;postgres\u0026rdquo; (1st argument to psql).\nSet a password for the \u0026ldquo;postgres\u0026rdquo; database role using the command:\n\\password postgres and give your password when prompted. The password text will be hidden from the console for security purposes.\nType Control+D or \\q to exit the posgreSQL prompt.\nCreate database\nTo create the first database, which we will call \u0026ldquo;mydb\u0026rdquo;, simply type:\nsudo -u postgres createdb mydb\n#sudo nano /etc/postgresql/9.3/main/pg_hba.conf and change the line host all all 0.0.0.0/0 md5\nDatabase administrative login by Unix domain socket local all postgres peer to\nDatabase administrative login by Unix domain socket local all postgres md5 Now you should reload the server configuration changes and connect pgAdmin III to your PostgreSQL database server.\nsudo /etc/init.d/postgresql reload\ncayley_v0.6.1_linux_amd64$ cat cayley.cfg { \u0026quot;listen_host\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;sql\u0026quot;, \u0026quot;db_path\u0026quot;: \u0026quot;postgres://postgres:psql@db/cayley?sslmode=disable\u0026quot;, \u0026quot;read_only\u0026quot;: false } $cayley init --config=cayley.cfg $cayley http --config=cayley.cfg $cayley load --config=cayley.cfg --quads=data/testdata.nq  Cayley looks in the following locations for the configuration file Command line flag The environment variable $CAYLEY_CFG /etc/cayley.cfg  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c5f2097b456fdf36712de2e9110e7c5a","permalink":"https://wubigo.com/post/psql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/psql/","section":"post","summary":"Basic Server Setup To start off, we need to set the password of the PostgreSQL user (role) called \u0026ldquo;postgres\u0026rdquo;; we will not be able to access the server externally otherwise. As the local “postgres” Linux user, we are allowed to connect and manipulate the server using the psql command.\nIn a terminal, type:\nsudo -u postgres psql postgres this connects as a role with same name as the local user, i.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"redisson.getKeys().delete(\"STREAM:02041123\"))\r3. 动态检查消息队列列表，如果没有消费，立即处理(redisson.getKeys())\r4. 主节点配置为缓存，从节点配置为存储\r经过测试，现在redis消息队列每小时能处理2千万条的设备数据。\r还需要进一步优化写入速度。\r### REDIS消息处理确认流程图\r![REDIS消息队列](/img/redis-stream.svg)\r## 更换分布式监控\r## 更换实时数据库\r# REDIS数据可靠性\r## REDIS ON FLASH\r在许多情况下，SSD 的高性能也将延迟和吞吐量的性能瓶颈从设备 I/O 转移到了网络上。\r对于应用程序而言，将其架构设计为将数据存储在本地 SSD 上而不是使用远程数据存储\r服务变得更具吸引力。 这增加了对可嵌入应用程序的键值存储引擎的需求.\rRocksDB单节点的存储上限是100GB，超过100GB需要使用分区。\r![REDIS ON FLASH](/img/redis-on-flash.svg)\r## REDIS SHARD\r## 参考\r[1] [A COMPARISON OF DATA INGESTION PLATFORMS IN REAL-TIME STREAMING](https://www.doria.fi/bitstream/handle/10024/177865/tallberg_sebastian.pdf?sequence=2\u0026isAllowed=y)\r[2] [Interview with the Creator of Redisson](https://www.alibabacloud.com/blog/interview-with-the-creator-of-redisson-building-an-open-source-enterprise-redis-client_593854)\r[3] [What every software engineer should know about real-time data's unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)\r[4] [Optimization of RocksDB for Redis on Flash](http://www.kereno.com/rocksdb-rof.pdf)\r[5] [Memtier benchmark](https://github.com/RedisLabs/memtier_benchmark)\r[6] [Distributed key-value database](https://tikv.org/docs/5.1/reference/architecture/overview/)\r[7] [Evolution of Development in a Key-value Store Serving Large-scale Applications](https://dl.acm.org/doi/fullHtml/10.1145/3483840)\r[8] [Block storage performance](https://cloud.google.com/compute/docs/disks/performance)\r[9] [built CockroachDB on top of RocksDB](https://www.cockroachlabs.com/blog/cockroachdb-on-rocksd/)\r[10] [Separating Keys from Values in SSD-conscious Storage](https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf) ======= +++ title = \"Redis消息队列和实时数据处理\" date = 2022-02-05T11:40:58+08:00 draft = false # Tags and categories # For example, use `tags = []` for no tags, or the form `tags = [\"A Tag\", \"Another Tag\"]` for one or more tags. tags = [\"REDIS\", \"CACHE\", \"NOSQL\", \"KVS\"] categories = [] # Featured image # To use, add an image named `featured.jpg/png` to your page's folder. [image] # Caption (optional) caption = \"\" # Focal point (optional) # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight focal_point = \"\" +++ # REDIS Redis (Remote Dictionary Server)是一个流行的开源内存 提供高级键值抽象的键值存储。 Redis 是单线程的，它只处理一个命令 客户端在进程的主线程中一次。 不同于传统的KV 键是简单数据类型（通常是字符串）的系统，键 在 Redis 中可以用作复杂的数据类型，例如哈希、列表、 集和排序集。 此外，Redis 支持复杂的原子 对这些数据类型的操作（例如，从 一个列表，将具有给定分数的新值插入排序集等）。 Redis 抽象和高摄取速度已被证明特别重要 对于许多延迟敏感的任务很有用。 因此，Redis 已获得广泛采用，并被越来越多的人使用 生产环境中的公司. Redis 支持高可用性和持久性。 高可用性 是通过将数据从主节点复制到 从节点并同步它们。 当一个主进程失败时，它的 对应的从属进程已准备好接管后续进程 称为故障转移。 持久性可以通过以下任一方式配置 以下两个选项： 1. 使用时间点快照文件 称为 RDB（Redis 数据库） 2. 使用名为AOF（仅附加文件）。 注意这三种机制（AOF重写、RDB 快照和复制）依赖 fork 获取 进程内存的时间点快照并将其序列化 （而主进程继续为客户端命令提供服务） # 项目简介 最近在做工业自动化(IIoT)项目，涉及到很多场景需要对一系列设备进行监控和信号处理。 该类场景对实时处理能力，系统稳定性，高可用性，容灾能力等等要求非常高。 其中几个核心的需求： 1. 设备数据不能丢失 2. 实时告警(毫秒级延迟) 3. 设备数据必须优先在边缘节点处理，边缘节点的物理服务器只有两台 4. 每个边缘节点接入的设备上行数据量大概6万点/秒，数据包小于1K 5. 中心需要汇聚和分析所有边缘节点的设备数据 # 技术现状 ## 实时数据处理 实时数据处理是一个自 1990 年代以来一直在研究的问题 。 产生的数据量增加了，加上越来越复杂的软件解决方案 开发，需要满足这些需求出现了流式应用程序，例如 欺诈检测、网络监控和电子交易依赖于实时数据处理 确保所提供的服务被认为是正确和可靠的。 绝大多数现代应用程序使用某种数据库管理系统 处理数据。当应用程序收集或生成数据时，它会被存储和索引 它可以在以后由应用程序查询。但是，对于具有更严格 对实时数据处理的要求，这不是一个合适的方法。这是流的地方 处理开始发挥作用。流处理是在接收数据时直接处理数据。实时流 处理应用程序通常具有必须满足的某些关键要求。有低 输入和处理后的数据输出之间的延迟是实现实时的关键特征 应用。更传统的批处理方法需要以这样的方式收集数据 称为批次，其中处理只能在每个批次的最终数据块完成后开始 到达的。对于实时用例，这导致的延迟是不可接受的，因为这些实时的延迟 流应用程序最好在毫秒内。 ## 现有资源 公司现有的工控产品都是基于微服务架构实现的。用于满足超大型集团公司 工控自动化需求:DEV/OPS，PaaS，双活，AI/OPS等。需要上百个虚机来支 撑现有的工控平台。 # 技术调研 ## 更换消息中间件 使用REDIS STREAM替换消息中间件。 在测试中，发现REDIS作为队列，使用比较方便： 1. 消息队列动态创建，目前以时间戳为标识方便处理(例如\"STREAM:02041123\") 2. 消息队列消费完后删除，释放内存( isDelived \u0026\u0026 isSaved - redisson.getKeys().delete(\"STREAM:02041123\")) 3. 动态检查消息队列列表，如果没有消费，立即处理(redisson.getKeys()) 4. 主节点配置为缓存，从节点配置为存储 经过测试，现在redis消息队列每小时能处理2千万条的设备数据。 还需要进一步优化写入速度。 ### REDIS消息处理确认流程图 ![REDIS消息队列](/img/redis-stream.svg) ## 更换分布式监控 ## 更换实时数据库 # REDIS数据可靠性 ## REDIS ON FLASH 在许多情况下，SSD 的高性能也将延迟和吞吐量的性能瓶颈从设备 I/O 转移到了网络上。 对于应用程序而言，将其架构设计为将数据存储在本地 SSD 上而不是使用远程数据存储 服务变得更具吸引力。 这增加了对可嵌入应用程序的键值存储引擎的需求. RocksDB单节点的存储上限是100GB，超过100GB需要使用分区。 ![REDIS ON FLASH](/img/redis-on-flash.svg) ## REDIS SHARD ## 参考 [1] [A COMPARISON OF DATA INGESTION PLATFORMS IN REAL-TIME STREAMING](https://www.doria.fi/bitstream/handle/10024/177865/tallberg_sebastian.pdf?sequence=2\u0026isAllowed=y) [2] [Interview with the Creator of Redisson](https://www.alibabacloud.com/blog/interview-with-the-creator-of-redisson-building-an-open-source-enterprise-redis-client_593854) [3] [What every software engineer should know about real-time data's unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying) [4] [Optimization of RocksDB for Redis on Flash](http://www.kereno.com/rocksdb-rof.pdf) [5] [Memtier benchmark](https://github.com/RedisLabs/memtier_benchmark) [6] [Distributed key-value database](https://tikv.org/docs/5.1/reference/architecture/overview/) [7] [Evolution of Development in a Key-value Store Serving Large-scale Applications](https://dl.acm.org/doi/fullHtml/10.1145/3483840) [8] [Block storage performance](https://cloud.google.com/compute/docs/disks/performance) [9] [built CockroachDB on top of RocksDB](https://www.cockroachlabs.com/blog/cockroachdb-on-rocksd/) [10] [Separating Keys from Values in SSD-conscious Storage](https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf) [11] [Atlas: Baidu’s Key-value Storage System for Cloud Data](https://wubigo.com/talk/) [12] [Scaling HDFS to Manage Billions of Files with Key Value Stores(Hadoop Summit 2015)](https://www.slideshare.net/Hadoop_Summit/scaling-hdfs-to-manage-billions)  30f51d6a59a2471ebfe6cd088ba9fa23459acca0 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4015b07034399da36a4721a63f739864","permalink":"https://wubigo.com/post/redis-as-real-time-database/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/redis-as-real-time-database/","section":"post","summary":"redisson.getKeys().delete(\"STREAM:02041123\"))\r3. 动态检查消息队列列表，如果没有消费，立即处理(redisson.getKeys())\r4. 主节点配置为缓存，从节点配置为存储\r经过测试，现在redis消息队列每小时能处理2千万条的设备数据。\r还需要进一步优化写入速度。\r### REDIS消息处理确认流程图\r![REDIS消息队列](/img/redis-stream.svg)\r## 更换分布式监控\r## 更换实时数据库\r# REDIS数据可靠性\r## REDIS ON FLASH\r在许多情况下，SSD 的高性能也将延迟和吞吐量的性能瓶颈从设备 I/O 转移到了网络上。\r对于应用程序而言，将其架构设计为将数据存储在本地 SSD 上而不是使用远程数据存储\r服务变得更具吸引力。 这增加了对可嵌入应用程序的键值存储引擎的需求.\rRocksDB单节点的存储上限是100GB，超过100GB需要使用分区。\r![REDIS ON FLASH](/img/redis-on-flash.svg)\r## REDIS SHARD\r## 参考\r[1] [A COMPARISON OF DATA INGESTION PLATFORMS IN REAL-TIME STREAMING](https://www.doria.fi/bitstream/handle/10024/177865/tallberg_sebastian.pdf?sequence=2\u0026isAllowed=y)\r[2] [Interview with the Creator of Redisson](https://www.alibabacloud.com/blog/interview-with-the-creator-of-redisson-building-an-open-source-enterprise-redis-client_593854)\r[3] [What every software engineer should know about real-time data's unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)\r[4] [Optimization of RocksDB for Redis on Flash](http://www.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"https://stackoverflow.com/questions/21087564/gmail-smtp-is-not-working-in-ec2-instance\nhttps://linuxconfig.org/configuring-gmail-as-sendmail-email-relay\nGo to https://www.google.com/settings/security/lesssecureapps and set Access for less secure apps to On\ngoto : https://accounts.google.com/DisplayUnlockCaptcha\nand click continue this going to allow access from other servers.\nhttps://www.mkyong.com/java/javamail-api-sending-email-via-gmail-smtp-example/\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"53d0412037dd477a59505788df13a330","permalink":"https://wubigo.com/post/smtp_ec2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/smtp_ec2/","section":"post","summary":"https://stackoverflow.com/questions/21087564/gmail-smtp-is-not-working-in-ec2-instance\nhttps://linuxconfig.org/configuring-gmail-as-sendmail-email-relay\nGo to https://www.google.com/settings/security/lesssecureapps and set Access for less secure apps to On\ngoto : https://accounts.google.com/DisplayUnlockCaptcha\nand click continue this going to allow access from other servers.\nhttps://www.mkyong.com/java/javamail-api-sending-email-via-gmail-smtp-example/","tags":null,"title":"","type":"post"}]